{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyMl/i+HLS9eAAbkI2w5PrfP"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "iVgEgh4YaZ15",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684720572997,
     "user_tz": 420,
     "elapsed": 11045,
     "user": {
      "displayName": "Umit Basaran",
      "userId": "00089894248027113753"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-05-23T21:18:30.035840134Z",
     "start_time": "2023-05-23T21:18:29.068986965Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def _create_batch(unbatched_data, unbatched_label, unbatched_test_data, unbatched_test_label):\n",
    "    unbatched_data = torch.split(unbatched_data, 100)\n",
    "    unbatched_label = torch.split(unbatched_label, 100)\n",
    "    unbatched_test_data = torch.split(unbatched_test_data, 100)\n",
    "    unbatched_test_label = torch.split(unbatched_test_label, 100)\n",
    "    return unbatched_data, unbatched_label, unbatched_test_data, unbatched_test_label\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    # dataset\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "    ])\n",
    "\n",
    "    target_transform = Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), 1))\n",
    "\n",
    "    train_dataset = CIFAR10('./data', train=True, download=True, transform=transform, target_transform=target_transform)\n",
    "    test_dataset = CIFAR10('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=len(train_dataset))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "    train_data, train_label = next(iter(train_loader))\n",
    "    test_data, test_label = next(iter(test_loader))\n",
    "\n",
    "    print('train data: {}, train label: {}'.format(train_data.size(), train_label.size()))\n",
    "    print('test data: {}, test label: {}'.format(test_data.size(), test_label.size()))\n",
    "\n",
    "    sorted_train_label_arg = torch.argsort(torch.argmax(train_label, dim=1))\n",
    "    sorted_train_label = train_label[sorted_train_label_arg]\n",
    "    sorted_train_data = train_data[sorted_train_label_arg]\n",
    "\n",
    "    train_data_sampled = []\n",
    "    train_label_sampled = []\n",
    "    for class_idx in range(10):\n",
    "        class_idx = class_idx * 5000\n",
    "        train_data_sampled.append(sorted_train_data[class_idx:(class_idx + 1000)])\n",
    "        train_label_sampled.append(sorted_train_label[class_idx:(class_idx + 1000)])\n",
    "\n",
    "    rand_idx = torch.randperm(10000)\n",
    "    train_data_sampled = torch.concat(train_data_sampled, dim=0)[rand_idx]\n",
    "    train_label_sampled = torch.concat(train_label_sampled, dim=0)[rand_idx]\n",
    "    print(\n",
    "        'train_data_sampled: {}, train_label_sampled: {}'.format(train_data_sampled.size(), train_label_sampled.size()))\n",
    "\n",
    "    train_data_sampled, train_label_sampled, test_data, test_label = _create_batch(train_data_sampled,\n",
    "                                                                                   train_label_sampled, test_data,\n",
    "                                                                                   test_label)\n",
    "    print('train_data_sampled: {}, train_label_sampled: {}'.format(len(train_data_sampled), len(train_label_sampled)))\n",
    "    print('train_data_sampled: {}, train_label_sampled: {}'.format(train_data_sampled[0].size(),\n",
    "                                                                   train_label_sampled[0].size()))\n",
    "\n",
    "    return train_data_sampled, train_label_sampled, test_data, test_label"
   ],
   "metadata": {
    "id": "WtjJMvO6I7lb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684720572998,
     "user_tz": 420,
     "elapsed": 10,
     "user": {
      "displayName": "Umit Basaran",
      "userId": "00089894248027113753"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-05-23T21:18:30.410104715Z",
     "start_time": "2023-05-23T21:18:30.402088439Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train(train_data_sampled, train_label_sampled, test_data, test_label, augmentation=None, num_epoch=100, lr=0.0001):\n",
    "    model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    model.fc = nn.Linear(512, 10)\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    model = model.to(device)\n",
    "\n",
    "    running_loss = []\n",
    "    running_acc = []\n",
    "    loss = None\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        for idx, (data, label) in enumerate(zip(train_data_sampled, train_label_sampled)):\n",
    "            model.train()\n",
    "            if augmentation is not None:\n",
    "                data, label = augmentation(data, label)\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(data)\n",
    "            loss = criterion(preds, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "            # test\n",
    "            # if (idx + 1) % 10 == 0:\n",
    "        model.eval()\n",
    "        tot_acc = torch.zeros(1).to(device)\n",
    "        test_data_size = 0\n",
    "        with torch.no_grad():\n",
    "            for test_data_batch, test_label_batch in zip(test_data, test_label):\n",
    "                test_data_batch, test_label_batch = test_data_batch.to(device), test_label_batch.to(device)\n",
    "                test_preds = model(test_data_batch)\n",
    "                test_preds = torch.argmax(test_preds, dim=1)\n",
    "                tot_acc = tot_acc + torch.count_nonzero((test_preds == test_label_batch).long())\n",
    "                test_data_size += test_data_batch.size(0)\n",
    "            running_acc.append(tot_acc.item() / test_data_size)\n",
    "            # print(tot_acc)\n",
    "            # print(test_data_size)\n",
    "        print('epoch: {}, loss: {}, acc: {}'.format(epoch + 1, loss, tot_acc.item() / test_data_size))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(list(range(len(running_loss))), running_loss)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('loss vs. iteration')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(list(range(len(running_acc))), running_acc)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('acc')\n",
    "    plt.title('acc vs. iteration')\n",
    "    plt.show()\n",
    "\n",
    "    return running_acc, running_loss"
   ],
   "metadata": {
    "id": "cK6Rbe_AnuBt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1684720631045,
     "user_tz": 420,
     "elapsed": 467,
     "user": {
      "displayName": "Umit Basaran",
      "userId": "00089894248027113753"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-05-23T21:18:31.374310568Z",
     "start_time": "2023-05-23T21:18:31.372308861Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def mixup(data_batch, label_batch, alpha):\n",
    "    mixup_idx = np.random.choice(data_batch.size(0), data_batch.size(0))\n",
    "    mixup_samples = data_batch[mixup_idx]\n",
    "    mixup_labels = label_batch[mixup_idx]\n",
    "    lambda_arr = torch.tensor(np.random.beta(alpha, alpha, size=data_batch.size(0)))\n",
    "    for sample_idx, (data_sample, label_sample) in enumerate(zip(data_batch, label_batch)):\n",
    "        mixup_samples[sample_idx] = lambda_arr[sample_idx] * data_sample + (1 - lambda_arr[sample_idx]) * mixup_samples[\n",
    "            sample_idx]\n",
    "        mixup_labels[sample_idx] = lambda_arr[sample_idx] * label_sample + (1 - lambda_arr[sample_idx]) * mixup_labels[\n",
    "            sample_idx]\n",
    "    return mixup_samples, mixup_labels\n",
    "\n",
    "def cutout(data_batch, label_batch, k=16):\n",
    "    cutout_mask = np.random.choice(2, data_batch.size(0)) # if zero no cutoff if one cutoff\n",
    "    cutout_samples = torch.zeros(data_batch.size())\n",
    "    for sample_idx, (data_sample, label_sample) in enumerate(zip(data_batch, label_batch)):\n",
    "        if cutout_mask[sample_idx] == 1:\n",
    "            random_row = np.random.choice(data_sample.size(1), 1)[0]\n",
    "            random_col = np.random.choice(data_sample.size(2), 1)[0]\n",
    "            if k % 2 == 0:\n",
    "                cutout_row_min, cutout_row_max = int(max(0, random_row - ((k / 2) - 1))), int(min(data_sample.size(1) - 1, random_row + (k / 2)))\n",
    "                cutout_col_min, cutout_col_max = int(max(0, random_col - ((k / 2) - 1))), int(min(data_sample.size(1) - 1, random_col + (k / 2)))\n",
    "            else:\n",
    "                cutout_row_min, cutout_row_max = int(max(0, random_row - ((k - 1) / 2))), int(min(data_sample.size(1) - 1, random_row + ((k - 1) / 2)))\n",
    "                cutout_col_min, cutout_col_max = int(max(0, random_col - ((k - 1) / 2))), int(min(data_sample.size(1) - 1, random_col + ((k - 1) / 2)))\n",
    "            zero_filter = torch.zeros((data_sample.size(0), cutout_row_max - cutout_row_min + 1, cutout_col_max - cutout_col_min + 1))\n",
    "            data_sample[:, cutout_row_min:(cutout_row_max + 1), cutout_col_min:(cutout_col_max + 1)] = zero_filter\n",
    "            cutout_samples[sample_idx] = data_sample\n",
    "    return data_batch, label_batch\n",
    "\n",
    "def standard_augmentation(data_batch, label_batch, k=4):\n",
    "    standard_samples = torch.zeros(data_batch.size())\n",
    "    for sample_idx, (data_sample, label_sample) in enumerate(zip(data_batch, label_batch)):\n",
    "        upward_k, rightward_k = np.random.choice(list(range(-1*k, k+1)), 2).astype(int)\n",
    "        if upward_k > 0:\n",
    "            standard_samples[sample_idx, :, :(data_sample.size(1) - upward_k), :] = data_sample[:, upward_k:, :]\n",
    "        else:\n",
    "            upward_k = -1 * upward_k\n",
    "            standard_samples[sample_idx, :, upward_k:, :] = data_sample[:, :(data_sample.size(1) - upward_k), :]\n",
    "\n",
    "        if rightward_k > 0:\n",
    "            standard_samples[sample_idx, :, :, rightward_k:] = data_sample[:, :, :(data_sample.size(2) - rightward_k)]\n",
    "        else:\n",
    "            rightward_k = -1 * rightward_k\n",
    "            standard_samples[sample_idx, :, :, :(data_sample.size(2) - rightward_k)] = data_sample[:, :, rightward_k:]\n",
    "\n",
    "        flip_or_not = np.random.choice(2, 1) # if zero not flip if one flip\n",
    "        if flip_or_not == 1:\n",
    "            flip_r = torch.fliplr(standard_samples[sample_idx, 0, :, :])\n",
    "            flip_g = torch.fliplr(standard_samples[sample_idx, 1, :, :])\n",
    "            flip_b = torch.fliplr(standard_samples[sample_idx, 2, :, :])\n",
    "            standard_samples[sample_idx] = torch.stack([flip_r, flip_g, flip_b])\n",
    "    return standard_samples, label_batch\n",
    "\n",
    "def combined_augmentation(data_batch, label_batch, alpha, k_cutout=16, k_standard=4):\n",
    "    data_batch, label_batch = standard_augmentation(data_batch, label_batch, k=k_standard)\n",
    "    data_batch, label_batch = cutout(data_batch, label_batch, k=k_cutout)\n",
    "    return mixup(data_batch, label_batch, alpha)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-23T21:18:32.169075113Z",
     "start_time": "2023-05-23T21:18:32.165294986Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train data: torch.Size([50000, 3, 32, 32]), train label: torch.Size([50000, 10])\n",
      "test data: torch.Size([10000, 3, 32, 32]), test label: torch.Size([10000])\n",
      "train_data_sampled: torch.Size([10000, 3, 32, 32]), train_label_sampled: torch.Size([10000, 10])\n",
      "train_data_sampled: 100, train_label_sampled: 100\n",
      "train_data_sampled: torch.Size([100, 3, 32, 32]), train_label_sampled: torch.Size([100, 10])\n",
      "epoch: 1, loss: 0.08498416095972061, acc: 0.3082\n",
      "epoch: 2, loss: 0.0710790753364563, acc: 0.4341\n",
      "epoch: 3, loss: 0.05815282464027405, acc: 0.504\n",
      "epoch: 4, loss: 0.04718910902738571, acc: 0.5445\n",
      "epoch: 5, loss: 0.036710530519485474, acc: 0.5678\n",
      "epoch: 6, loss: 0.02772563323378563, acc: 0.5776\n",
      "epoch: 7, loss: 0.02106798253953457, acc: 0.5811\n",
      "epoch: 8, loss: 0.015844400972127914, acc: 0.583\n",
      "epoch: 9, loss: 0.012305676005780697, acc: 0.5854\n",
      "epoch: 10, loss: 0.01037162821739912, acc: 0.584\n",
      "epoch: 11, loss: 0.009008882567286491, acc: 0.5919\n",
      "epoch: 12, loss: 0.008049413561820984, acc: 0.591\n",
      "epoch: 13, loss: 0.008333681151270866, acc: 0.5918\n",
      "epoch: 14, loss: 0.006911959033459425, acc: 0.5962\n",
      "epoch: 15, loss: 0.00570400757715106, acc: 0.6011\n",
      "epoch: 16, loss: 0.005410592537373304, acc: 0.6106\n",
      "epoch: 17, loss: 0.005177916958928108, acc: 0.6079\n",
      "epoch: 18, loss: 0.006081689614802599, acc: 0.6061\n",
      "epoch: 19, loss: 0.005266717169433832, acc: 0.6096\n",
      "epoch: 20, loss: 0.0048721954226493835, acc: 0.6162\n",
      "epoch: 21, loss: 0.00488351471722126, acc: 0.6131\n",
      "epoch: 22, loss: 0.003203457687050104, acc: 0.6186\n",
      "epoch: 23, loss: 0.003843882819637656, acc: 0.6248\n",
      "epoch: 24, loss: 0.0036144512705504894, acc: 0.6197\n",
      "epoch: 25, loss: 0.0035551381297409534, acc: 0.6242\n",
      "epoch: 26, loss: 0.002968667773529887, acc: 0.6254\n",
      "epoch: 27, loss: 0.0030346026178449392, acc: 0.6283\n",
      "epoch: 28, loss: 0.002922381041571498, acc: 0.6247\n",
      "epoch: 29, loss: 0.00305731943808496, acc: 0.6316\n",
      "epoch: 30, loss: 0.0028699026443064213, acc: 0.6336\n",
      "epoch: 31, loss: 0.002477382542565465, acc: 0.6317\n",
      "epoch: 32, loss: 0.0024121450260281563, acc: 0.6313\n",
      "epoch: 33, loss: 0.0024938290007412434, acc: 0.6341\n",
      "epoch: 34, loss: 0.002757852431386709, acc: 0.6346\n",
      "epoch: 35, loss: 0.00449460931122303, acc: 0.6341\n",
      "epoch: 36, loss: 0.003335824701935053, acc: 0.6381\n",
      "epoch: 37, loss: 0.0023587122559547424, acc: 0.6431\n",
      "epoch: 38, loss: 0.0022790092043578625, acc: 0.6449\n",
      "epoch: 39, loss: 0.003125585615634918, acc: 0.6436\n",
      "epoch: 40, loss: 0.0028383047319948673, acc: 0.6454\n",
      "epoch: 41, loss: 0.0024809495080262423, acc: 0.6507\n",
      "epoch: 42, loss: 0.002103373408317566, acc: 0.6512\n",
      "epoch: 43, loss: 0.002594927093014121, acc: 0.6566\n",
      "epoch: 44, loss: 0.0024355349596589804, acc: 0.655\n",
      "epoch: 45, loss: 0.003547556698322296, acc: 0.6543\n",
      "epoch: 46, loss: 0.0032342032063752413, acc: 0.6612\n",
      "epoch: 47, loss: 0.0030301478691399097, acc: 0.6599\n",
      "epoch: 48, loss: 0.0020871644373983145, acc: 0.6624\n",
      "epoch: 49, loss: 0.0024816642981022596, acc: 0.6619\n",
      "epoch: 50, loss: 0.0032457357738167048, acc: 0.66\n",
      "epoch: 51, loss: 0.0027275620959699154, acc: 0.6601\n",
      "epoch: 52, loss: 0.002559841610491276, acc: 0.663\n",
      "epoch: 53, loss: 0.002208145335316658, acc: 0.669\n",
      "epoch: 54, loss: 0.0031114770099520683, acc: 0.6691\n",
      "epoch: 55, loss: 0.00238596647977829, acc: 0.6768\n",
      "epoch: 56, loss: 0.0022555701434612274, acc: 0.673\n",
      "epoch: 57, loss: 0.0019271991914138198, acc: 0.6772\n",
      "epoch: 58, loss: 0.0023019162472337484, acc: 0.6777\n",
      "epoch: 59, loss: 0.002591055817902088, acc: 0.6811\n",
      "epoch: 60, loss: 0.002091079717501998, acc: 0.6796\n",
      "epoch: 61, loss: 0.0018210136331617832, acc: 0.6801\n",
      "epoch: 62, loss: 0.002212767256423831, acc: 0.6798\n",
      "epoch: 63, loss: 0.002290342003107071, acc: 0.68\n",
      "epoch: 64, loss: 0.0018074506660923362, acc: 0.6813\n",
      "epoch: 65, loss: 0.0017777896719053388, acc: 0.689\n",
      "epoch: 66, loss: 0.001969721633940935, acc: 0.6854\n",
      "epoch: 67, loss: 0.0021089669317007065, acc: 0.6953\n",
      "epoch: 68, loss: 0.0018895335961133242, acc: 0.6893\n",
      "epoch: 69, loss: 0.0015319319209083915, acc: 0.6928\n",
      "epoch: 70, loss: 0.0013575141783803701, acc: 0.6935\n",
      "epoch: 71, loss: 0.0010419588070362806, acc: 0.6972\n",
      "epoch: 72, loss: 0.0014624262694269419, acc: 0.6978\n",
      "epoch: 73, loss: 0.0017671583918854594, acc: 0.698\n",
      "epoch: 74, loss: 0.0009170009288936853, acc: 0.6965\n",
      "epoch: 75, loss: 0.001306864432990551, acc: 0.6965\n",
      "epoch: 76, loss: 0.0023787228856235743, acc: 0.697\n",
      "epoch: 77, loss: 0.00457580853253603, acc: 0.6964\n",
      "epoch: 78, loss: 0.00228440691716969, acc: 0.7018\n",
      "epoch: 79, loss: 0.001392572419717908, acc: 0.7061\n",
      "epoch: 80, loss: 0.001155151636339724, acc: 0.7082\n",
      "epoch: 81, loss: 0.000977545976638794, acc: 0.7078\n",
      "epoch: 82, loss: 0.000841299828607589, acc: 0.7098\n",
      "epoch: 83, loss: 0.0006409477791748941, acc: 0.7088\n",
      "epoch: 84, loss: 0.0007018650067038834, acc: 0.7091\n",
      "epoch: 85, loss: 0.0010463513899594545, acc: 0.7078\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, test_X, test_y = get_dataset()\n",
    "running_loss_without_aug, running_acc_without_aug = train(train_X, train_y, test_X, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-05-23T21:18:32.943288386Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = get_dataset()\n",
    "running_loss_mixup_2, running_acc_mixup_2 = train(train_X, train_y, test_X, test_y, augmentation=lambda x, y: mixup(x, y, 0.2))\n",
    "train_X, train_y, test_X, test_y = get_dataset()\n",
    "running_loss_mixup_4, running_acc_mixup_4 = train(train_X, train_y, test_X, test_y, augmentation=lambda x, y: mixup(x, y, 0.4))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = get_dataset()\n",
    "running_loss_cutout, running_acc_cutout = train(train_X, train_y, test_X, test_y, augmentation=cutout)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = get_dataset()\n",
    "running_loss_standard, running_acc_standard = train(train_X, train_y, test_X, test_y, augmentation=standard_augmentation)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_X, train_y, test_X, test_y = get_dataset()\n",
    "running_loss_combined_2, running_acc_combined_2 = train(train_X, train_y, test_X, test_y, augmentation=lambda x, y: combined_augmentation(x, y, 0.2))\n",
    "train_X, train_y, test_X, test_y = get_dataset()\n",
    "running_loss_combined_4, running_acc_combined_4 = train(train_X, train_y, test_X, test_y, augmentation=lambda x, y: combined_augmentation(x, y, 0.4))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ]
}
