{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize\n",
    "from torch.optim import Adam"
   ],
   "metadata": {
    "id": "IFKYYDmlyrPo",
    "ExecuteTime": {
     "end_time": "2023-06-06T05:23:32.903548775Z",
     "start_time": "2023-06-06T05:23:32.057240926Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "SAVED_PARAMS_PATH = os.path.join('.', 'saved_params_without_noise')\n",
    "if not os.path.exists(SAVED_PARAMS_PATH):\n",
    "    os.mkdir(SAVED_PARAMS_PATH)\n",
    "    # logging config\n",
    "log_file_path = os.path.join('./', 'mnist_with_dropout_and_noise__without_noise.log')\n",
    "logging.basicConfig(filename=log_file_path, encoding='utf-8', level=logging.DEBUG, force=True)"
   ],
   "metadata": {
    "id": "SAbrG4UT5e6V",
    "ExecuteTime": {
     "end_time": "2023-06-06T05:23:32.906201113Z",
     "start_time": "2023-06-06T05:23:32.904891655Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 1 - Main Functions"
   ],
   "metadata": {
    "collapsed": false,
    "id": "sS1eoDW6yrPp"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset operations"
   ],
   "metadata": {
    "collapsed": false,
    "id": "s2qowRUVyrPq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def custom_dataloader(batch_size, num_class=10, noise=False):\n",
    "    # transformers\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # MNIST dataset train/test\n",
    "    train_dataset = MNIST('./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "    # sampling 1000 data from each class for simplicity\n",
    "    train_labels = train_dataset.targets\n",
    "    counter_dict = torch.zeros(10)\n",
    "    sampled_indices = []\n",
    "    for idx, label in enumerate(train_labels):\n",
    "        if counter_dict[label] < 1000:\n",
    "            sampled_indices.append(idx)\n",
    "            counter_dict[label] = counter_dict[label] + 1\n",
    "        if torch.sum(counter_dict).item() == 10000:\n",
    "            break\n",
    "\n",
    "    train_subset = Subset(train_dataset, torch.tensor(sampled_indices))\n",
    "\n",
    "    # loaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=len(train_subset), shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)\n",
    "\n",
    "    train_data, train_label = next(iter(train_loader))\n",
    "    test_data, test_label = next(iter(test_loader))\n",
    "\n",
    "    if noise:\n",
    "        # adding noise to train samples\n",
    "        num_of_noisy_samples = (train_data.size(0) * 4) // 10\n",
    "        selected_idx = torch.randperm(train_data.size(0))[:num_of_noisy_samples]\n",
    "\n",
    "        for selected_id in selected_idx:\n",
    "            # find noisy class\n",
    "            curr_class = train_label[selected_id]\n",
    "            noise = torch.randint(num_class, size=(1,))[0]\n",
    "            while curr_class != noise:\n",
    "                noise = torch.randint(num_class, size=(1,))[0]\n",
    "\n",
    "            train_label[selected_id] = noise\n",
    "        logging.info('noise is added to the dataset')\n",
    "\n",
    "    train_data, train_label = torch.split(train_data, batch_size), torch.split(train_label, batch_size)\n",
    "    test_data, test_label = torch.split(test_data, batch_size), torch.split(test_label, batch_size)\n",
    "\n",
    "    train_loader = list(zip(train_data, train_label))\n",
    "    test_loader = list(zip(test_data, test_label))\n",
    "\n",
    "    logging.info('dataset is handled')\n",
    "\n",
    "    return train_loader, test_loader"
   ],
   "metadata": {
    "id": "AmGSvmtbyrPr",
    "ExecuteTime": {
     "end_time": "2023-06-06T05:23:34.393543969Z",
     "start_time": "2023-06-06T05:23:34.391909087Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create model class"
   ],
   "metadata": {
    "collapsed": false,
    "id": "rENMPVexyrPt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, param_size_param, dropout_param, input_dimension=784, num_class=10):\n",
    "        super().__init__()\n",
    "        self._param_size_param = param_size_param\n",
    "        self._dropout_param = dropout_param\n",
    "        self._input_dimension = input_dimension\n",
    "        self._num_class = num_class\n",
    "\n",
    "        self.__hidden_layer = nn.Linear(self._input_dimension, self._param_size_param)\n",
    "        self.__out_layer = nn.Linear(self._param_size_param, self._num_class)\n",
    "        self.__relu = nn.ReLU()\n",
    "        if dropout_param != 1:\n",
    "            self.__dropout = nn.Dropout(self._dropout_param)\n",
    "        else:\n",
    "            self.__dropout = None\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        out = input_data.reshape(input_data.size(0), -1)\n",
    "        out = self.__hidden_layer(out)\n",
    "        out = self.__relu(out)\n",
    "        if self.__dropout:\n",
    "            out = self.__dropout(out)\n",
    "        out = self.__out_layer(out)\n",
    "        return out\n"
   ],
   "metadata": {
    "id": "5qdfMXeCyrPt",
    "ExecuteTime": {
     "end_time": "2023-06-06T05:23:35.895947376Z",
     "start_time": "2023-06-06T05:23:35.893310612Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train and test functions"
   ],
   "metadata": {
    "collapsed": false,
    "id": "i5BozTjuyrPu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def main_epoch(model, optimizer, criterion, device, train_loader, test_loader, epoch):\n",
    "    running_loss, running_training_acc = [], []\n",
    "    tot_loss = 0\n",
    "    for idx, (data, label) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data, label = data.to(device), label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(data)\n",
    "        loss = criterion(preds, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tot_loss += loss.item()\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "        if idx == 0 or (idx + 1) % 10 == 0:\n",
    "            model.eval()\n",
    "            tot_acc = 0\n",
    "            tot_data = 0\n",
    "            with torch.no_grad():\n",
    "                for train_data, train_label in train_loader:\n",
    "                    train_data, train_label = train_data.to(device), train_label.to(device)\n",
    "                    train_preds = model(train_data)\n",
    "                    train_pred_idx = torch.argmax(train_preds, dim=1)\n",
    "                    tot_acc += torch.count_nonzero((train_pred_idx == train_label).long())\n",
    "                    tot_data += train_data.shape[0]\n",
    "                running_training_acc.append(tot_acc.item() / tot_data)\n",
    "            if idx == 0:\n",
    "                print('loss: {}, train acc: {}'.format(loss, running_training_acc[-1]))\n",
    "                logging.info('loss: {}, train acc: {}'.format(loss, running_training_acc[-1]))\n",
    "            else:\n",
    "                print('loss: {}, train acc: {}'.format(tot_loss / 10, running_training_acc[-1]))\n",
    "                logging.info('loss: {}, train acc: {}'.format(tot_loss / 10, running_training_acc[-1]))\n",
    "                tot_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    tot_acc = 0\n",
    "    tot_data = 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (test_data, test_label) in enumerate(test_loader):\n",
    "            test_data, test_label = test_data.to(device), test_label.to(device)\n",
    "            test_preds = model(test_data)\n",
    "            test_pred_idx = torch.argmax(test_preds, dim=1)\n",
    "            tot_acc += torch.count_nonzero((test_pred_idx == test_label).long())\n",
    "            tot_data += test_data.shape[0]\n",
    "        accuracy = tot_acc.item() / tot_data\n",
    "    print('epoch: {}, loss: {}, train acc: {}, test acc: {}'.format(epoch, running_loss[-1], running_training_acc[-1], accuracy))\n",
    "    logging.info('epoch: {}, loss: {}, train acc: {}, test acc: {}'.format(epoch, running_loss[-1], running_training_acc[-1], accuracy))\n",
    "    return running_loss, running_training_acc, accuracy\n",
    "\n",
    "def main_param_dropout(batch_size, param_size_param, dropout_param, number_of_epochs=80, lr=0.001, noise=False):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = CustomModel(param_size_param, dropout_param).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if noise:\n",
    "        train_loader, test_loader = custom_dataloader(batch_size, noise=noise)\n",
    "    else:\n",
    "        train_loader, test_loader = custom_dataloader(batch_size, noise=noise)\n",
    "\n",
    "    running_loss, running_training_acc, running_accuracy = [], [], []\n",
    "\n",
    "    print('#####training and testing start with K:{}, P:{}######'.format(param_size_param, dropout_param))\n",
    "    logging.info('#####training and testing start with K:{}, P:{}######'.format(param_size_param, dropout_param))\n",
    "    for epoch in range(number_of_epochs):\n",
    "        curr_running_loss, curr_running_training_acc, curr_accuracy = main_epoch(model, optimizer, criterion, device, train_loader, test_loader, epoch)\n",
    "        running_loss += curr_running_loss\n",
    "        running_training_acc += curr_running_training_acc\n",
    "        running_accuracy.append(curr_accuracy)\n",
    "    print('#####training and testing end with K:{}, P:{}######'.format(param_size_param, dropout_param))\n",
    "    logging.info('#####training and testing end with K:{}, P:{}######'.format(param_size_param, dropout_param))\n",
    "    return running_loss, running_training_acc, running_accuracy\n",
    "\n",
    "def param_dropout_grid(batch_size, param_size_param_arr, dropout_param_arr, **kwargs):\n",
    "    for param_size_idx, param_size_param in enumerate(param_size_param_arr):\n",
    "        for dropout_idx, dropout_param in enumerate(dropout_param_arr):\n",
    "            running_loss, running_training_acc, running_accuracy = main_param_dropout(batch_size, param_size_param, dropout_param, **kwargs)\n",
    "            save_param_path = os.path.join(SAVED_PARAMS_PATH, 'exp_k_{}_p_{}.pth'.format(param_size_idx, dropout_idx))\n",
    "            torch.save({\n",
    "                'running_loss': running_loss,\n",
    "                'running_training_acc': running_training_acc, \n",
    "                'running_accuracy': running_accuracy\n",
    "            },save_param_path)"
   ],
   "metadata": {
    "id": "NFBCBqSYyrPv",
    "ExecuteTime": {
     "end_time": "2023-06-06T05:23:37.401707828Z",
     "start_time": "2023-06-06T05:23:37.400860415Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 2 - Parameter Grid"
   ],
   "metadata": {
    "collapsed": false,
    "id": "LqFRMi4AyrPw"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####training and testing start with K:1, P:0.1######\n",
      "loss: 2.368612289428711, train acc: 0.107\n",
      "loss: 2.4298451423645018, train acc: 0.1154\n",
      "loss: 2.356693887710571, train acc: 0.1543\n",
      "loss: 2.311783051490784, train acc: 0.1402\n",
      "loss: 2.27259247303009, train acc: 0.1878\n",
      "loss: 2.252685785293579, train acc: 0.2033\n",
      "loss: 2.2267510175704954, train acc: 0.2121\n",
      "loss: 2.214937949180603, train acc: 0.2112\n",
      "epoch: 0, loss: 2.0312681198120117, train acc: 0.2112, test acc: 0.2031\n",
      "loss: 2.122227191925049, train acc: 0.206\n",
      "loss: 2.217936372756958, train acc: 0.2106\n",
      "loss: 2.191518783569336, train acc: 0.2126\n",
      "loss: 2.189629411697388, train acc: 0.2083\n",
      "loss: 2.183908224105835, train acc: 0.2097\n",
      "loss: 2.146162009239197, train acc: 0.2049\n",
      "loss: 2.1473097085952757, train acc: 0.2143\n",
      "loss: 2.123350644111633, train acc: 0.2106\n",
      "epoch: 1, loss: 1.9440760612487793, train acc: 0.2106, test acc: 0.1952\n",
      "loss: 2.0900561809539795, train acc: 0.2012\n",
      "loss: 2.1590965509414675, train acc: 0.2144\n",
      "loss: 2.1320515632629395, train acc: 0.2072\n",
      "loss: 2.136222219467163, train acc: 0.2031\n",
      "loss: 2.119404602050781, train acc: 0.207\n",
      "loss: 2.106884551048279, train acc: 0.2012\n",
      "loss: 2.0958732843399046, train acc: 0.2099\n",
      "loss: 2.0959686160087587, train acc: 0.2144\n",
      "epoch: 2, loss: 1.8986473083496094, train acc: 0.2144, test acc: 0.1938\n",
      "loss: 2.0437228679656982, train acc: 0.1947\n",
      "loss: 2.129127931594849, train acc: 0.2087\n",
      "loss: 2.089471924304962, train acc: 0.205\n",
      "loss: 2.0925853490829467, train acc: 0.1992\n",
      "loss: 2.0684550166130067, train acc: 0.2019\n",
      "loss: 2.060364031791687, train acc: 0.2049\n",
      "loss: 2.0651600480079653, train acc: 0.2057\n",
      "loss: 2.0588656663894653, train acc: 0.2106\n",
      "epoch: 3, loss: 1.959327220916748, train acc: 0.2106, test acc: 0.1939\n",
      "loss: 2.0206618309020996, train acc: 0.197\n",
      "loss: 2.0833901882171633, train acc: 0.2059\n",
      "loss: 2.056192660331726, train acc: 0.2036\n",
      "loss: 2.0708699226379395, train acc: 0.196\n",
      "loss: 2.050250232219696, train acc: 0.2009\n",
      "loss: 2.0385602474212647, train acc: 0.2027\n",
      "loss: 2.041905069351196, train acc: 0.2036\n",
      "loss: 2.02817507982254, train acc: 0.2076\n",
      "epoch: 4, loss: 1.892492651939392, train acc: 0.2076, test acc: 0.1949\n",
      "loss: 1.9899131059646606, train acc: 0.1984\n",
      "loss: 2.054930102825165, train acc: 0.2022\n",
      "loss: 2.0368593454360964, train acc: 0.2009\n",
      "loss: 2.0325241088867188, train acc: 0.1961\n",
      "loss: 2.0439096331596374, train acc: 0.1995\n",
      "loss: 2.0215327501297, train acc: 0.2023\n",
      "loss: 2.022014892101288, train acc: 0.1998\n",
      "loss: 2.0042070508003236, train acc: 0.2071\n",
      "epoch: 5, loss: 1.9483271837234497, train acc: 0.2071, test acc: 0.195\n",
      "loss: 1.981427788734436, train acc: 0.1996\n",
      "loss: 2.030086600780487, train acc: 0.2002\n",
      "loss: 2.016732621192932, train acc: 0.1997\n",
      "loss: 2.0374547362327577, train acc: 0.1973\n",
      "loss: 2.0147595047950744, train acc: 0.1947\n",
      "loss: 2.0128081798553468, train acc: 0.2004\n",
      "loss: 1.9994177579879762, train acc: 0.1969\n",
      "loss: 1.995014774799347, train acc: 0.2028\n",
      "epoch: 6, loss: 1.9112393856048584, train acc: 0.2028, test acc: 0.1933\n",
      "loss: 1.997043490409851, train acc: 0.1984\n",
      "loss: 2.0244968056678774, train acc: 0.2026\n",
      "loss: 2.006129240989685, train acc: 0.1993\n",
      "loss: 2.018177163600922, train acc: 0.1966\n",
      "loss: 2.01581689119339, train acc: 0.1966\n",
      "loss: 1.9943230032920838, train acc: 0.2008\n",
      "loss: 1.9998111128807068, train acc: 0.1989\n",
      "loss: 1.9878785967826844, train acc: 0.2028\n",
      "epoch: 7, loss: 1.9861564636230469, train acc: 0.2028, test acc: 0.1877\n",
      "loss: 1.9453082084655762, train acc: 0.1942\n",
      "loss: 2.014391779899597, train acc: 0.2\n",
      "loss: 2.005029332637787, train acc: 0.1974\n",
      "loss: 2.0081263542175294, train acc: 0.195\n",
      "loss: 1.9874663710594178, train acc: 0.1952\n",
      "loss: 1.9882243156433106, train acc: 0.2003\n",
      "loss: 1.9834540486335754, train acc: 0.1984\n",
      "loss: 1.9643300294876098, train acc: 0.2034\n",
      "epoch: 8, loss: 1.9208862781524658, train acc: 0.2034, test acc: 0.1935\n",
      "loss: 1.9982051849365234, train acc: 0.1972\n",
      "loss: 2.0167480945587157, train acc: 0.2\n",
      "loss: 1.9942371129989624, train acc: 0.1994\n",
      "loss: 2.0146989822387695, train acc: 0.1969\n",
      "loss: 2.0075793981552126, train acc: 0.2019\n",
      "loss: 1.9860399603843688, train acc: 0.2042\n",
      "loss: 1.9665467858314514, train acc: 0.2022\n",
      "loss: 1.9717007279396057, train acc: 0.2061\n",
      "epoch: 9, loss: 1.9124770164489746, train acc: 0.2061, test acc: 0.1932\n",
      "loss: 1.9069031476974487, train acc: 0.1985\n",
      "loss: 1.9977155208587647, train acc: 0.2037\n",
      "loss: 1.9806642651557922, train acc: 0.202\n",
      "loss: 1.9902811288833617, train acc: 0.1994\n",
      "loss: 1.9779358625411987, train acc: 0.2042\n",
      "loss: 1.982156717777252, train acc: 0.2066\n",
      "loss: 1.9770495772361756, train acc: 0.2057\n",
      "loss: 1.9505560636520385, train acc: 0.209\n",
      "epoch: 10, loss: 2.0097618103027344, train acc: 0.209, test acc: 0.1972\n",
      "loss: 1.987536907196045, train acc: 0.2044\n",
      "loss: 1.9839202165603638, train acc: 0.2031\n",
      "loss: 1.9622061491012572, train acc: 0.2074\n",
      "loss: 1.9897351264953613, train acc: 0.2033\n",
      "loss: 1.956988799571991, train acc: 0.2058\n",
      "loss: 1.9591267704963684, train acc: 0.2089\n",
      "loss: 1.962915289402008, train acc: 0.2066\n",
      "loss: 1.951264452934265, train acc: 0.211\n",
      "epoch: 11, loss: 1.8682873249053955, train acc: 0.211, test acc: 0.1966\n",
      "loss: 1.9635570049285889, train acc: 0.2013\n",
      "loss: 1.9786139845848083, train acc: 0.2081\n",
      "loss: 1.963920247554779, train acc: 0.2109\n",
      "loss: 1.9815242052078248, train acc: 0.2057\n",
      "loss: 1.9777647256851196, train acc: 0.2115\n",
      "loss: 1.9643619775772094, train acc: 0.2119\n",
      "loss: 1.968220579624176, train acc: 0.2111\n",
      "loss: 1.9329934954643249, train acc: 0.2158\n",
      "epoch: 12, loss: 1.8642592430114746, train acc: 0.2158, test acc: 0.2009\n",
      "loss: 1.9274481534957886, train acc: 0.2072\n",
      "loss: 1.9683411955833434, train acc: 0.2122\n",
      "loss: 1.9475242853164674, train acc: 0.2123\n",
      "loss: 1.9677659511566161, train acc: 0.2077\n",
      "loss: 1.9631447196006775, train acc: 0.2138\n",
      "loss: 1.9378362059593202, train acc: 0.2156\n",
      "loss: 1.9551257967948914, train acc: 0.2129\n",
      "loss: 1.9417681694030762, train acc: 0.2196\n",
      "epoch: 13, loss: 1.8519799709320068, train acc: 0.2196, test acc: 0.1999\n",
      "loss: 1.943671703338623, train acc: 0.2044\n",
      "loss: 1.9637822151184081, train acc: 0.2134\n",
      "loss: 1.9475368142127991, train acc: 0.2179\n",
      "loss: 1.9597472429275513, train acc: 0.208\n",
      "loss: 1.96031311750412, train acc: 0.237\n",
      "loss: 1.9589509129524232, train acc: 0.2374\n",
      "loss: 1.9348087787628174, train acc: 0.2377\n",
      "loss: 1.9373731851577758, train acc: 0.2393\n",
      "epoch: 14, loss: 1.9496567249298096, train acc: 0.2393, test acc: 0.2282\n",
      "loss: 1.9029498100280762, train acc: 0.237\n",
      "loss: 1.9732908487319947, train acc: 0.2402\n",
      "loss: 1.9396274924278258, train acc: 0.238\n",
      "loss: 1.9716529607772828, train acc: 0.2377\n",
      "loss: 1.945644497871399, train acc: 0.2389\n",
      "loss: 1.9533567905426026, train acc: 0.2393\n",
      "loss: 1.9465112447738648, train acc: 0.2412\n",
      "loss: 1.9328099727630614, train acc: 0.24\n",
      "epoch: 15, loss: 1.9302213191986084, train acc: 0.24, test acc: 0.2306\n",
      "loss: 1.927091360092163, train acc: 0.2392\n",
      "loss: 1.9549827814102172, train acc: 0.2419\n",
      "loss: 1.9359286427497864, train acc: 0.2411\n",
      "loss: 1.9598213672637939, train acc: 0.2423\n",
      "loss: 1.9524426698684691, train acc: 0.2425\n",
      "loss: 1.920712983608246, train acc: 0.2434\n",
      "loss: 1.9133842229843139, train acc: 0.2431\n",
      "loss: 1.9164242506027223, train acc: 0.2423\n",
      "epoch: 16, loss: 1.9110815525054932, train acc: 0.2423, test acc: 0.2316\n",
      "loss: 1.9074454307556152, train acc: 0.2416\n",
      "loss: 1.9715499877929688, train acc: 0.2436\n",
      "loss: 1.9279167890548705, train acc: 0.2421\n",
      "loss: 1.9473853826522827, train acc: 0.2435\n",
      "loss: 1.9320852994918822, train acc: 0.2444\n",
      "loss: 1.924051582813263, train acc: 0.2463\n",
      "loss: 1.931867253780365, train acc: 0.2453\n",
      "loss: 1.9115386605262756, train acc: 0.2447\n",
      "epoch: 17, loss: 1.8552368879318237, train acc: 0.2447, test acc: 0.2317\n",
      "loss: 1.9200084209442139, train acc: 0.2448\n",
      "loss: 1.9681190729141236, train acc: 0.2459\n",
      "loss: 1.92977454662323, train acc: 0.243\n",
      "loss: 1.9582109689712524, train acc: 0.2466\n",
      "loss: 1.9386394381523133, train acc: 0.2477\n",
      "loss: 1.9490728855133057, train acc: 0.2489\n",
      "loss: 1.9246424198150636, train acc: 0.2474\n",
      "loss: 1.9045735120773315, train acc: 0.2498\n",
      "epoch: 18, loss: 1.8481353521347046, train acc: 0.2498, test acc: 0.2347\n",
      "loss: 1.867730736732483, train acc: 0.2497\n",
      "loss: 1.9244060754776, train acc: 0.249\n",
      "loss: 1.9253733158111572, train acc: 0.2448\n",
      "loss: 1.9522003769874572, train acc: 0.2481\n",
      "loss: 1.9363642692565919, train acc: 0.2504\n",
      "loss: 1.9354637026786805, train acc: 0.2505\n",
      "loss: 1.9288670659065246, train acc: 0.2503\n",
      "loss: 1.9242637515068055, train acc: 0.2476\n",
      "epoch: 19, loss: 1.8561961650848389, train acc: 0.2476, test acc: 0.2364\n",
      "loss: 1.9173678159713745, train acc: 0.2492\n",
      "loss: 1.95391343832016, train acc: 0.2493\n",
      "loss: 1.9375937342643739, train acc: 0.2463\n",
      "loss: 1.9267828822135926, train acc: 0.2512\n",
      "loss: 1.9185528874397277, train acc: 0.2541\n",
      "loss: 1.936370885372162, train acc: 0.2524\n",
      "loss: 1.9355547785758973, train acc: 0.2533\n",
      "loss: 1.9008428931236268, train acc: 0.2501\n",
      "epoch: 20, loss: 1.834328293800354, train acc: 0.2501, test acc: 0.237\n",
      "loss: 1.8838034868240356, train acc: 0.2506\n",
      "loss: 1.9449447631835937, train acc: 0.2521\n",
      "loss: 1.9250939607620239, train acc: 0.2483\n",
      "loss: 1.9338790774345398, train acc: 0.2539\n",
      "loss: 1.931162130832672, train acc: 0.2535\n",
      "loss: 1.9259552478790283, train acc: 0.2534\n",
      "loss: 1.9373802065849304, train acc: 0.2552\n",
      "loss: 1.887338638305664, train acc: 0.2535\n",
      "epoch: 21, loss: 2.008026123046875, train acc: 0.2535, test acc: 0.2387\n",
      "loss: 1.9058711528778076, train acc: 0.2507\n",
      "loss: 1.9389026641845704, train acc: 0.2524\n",
      "loss: 1.9289492130279542, train acc: 0.251\n",
      "loss: 1.9419783353805542, train acc: 0.2555\n",
      "loss: 1.8955822825431823, train acc: 0.2579\n",
      "loss: 1.9166366934776307, train acc: 0.2589\n",
      "loss: 1.9200459003448487, train acc: 0.2582\n",
      "loss: 1.9057674169540406, train acc: 0.2572\n",
      "epoch: 22, loss: 1.8185011148452759, train acc: 0.2572, test acc: 0.2434\n",
      "loss: 1.9027544260025024, train acc: 0.2561\n",
      "loss: 1.9323695182800293, train acc: 0.2583\n",
      "loss: 1.9084123492240905, train acc: 0.2588\n",
      "loss: 1.9585476756095885, train acc: 0.2614\n",
      "loss: 1.9217015743255614, train acc: 0.2625\n",
      "loss: 1.9154682874679565, train acc: 0.2618\n",
      "loss: 1.8997169733047485, train acc: 0.2642\n",
      "loss: 1.8966559052467347, train acc: 0.2604\n",
      "epoch: 23, loss: 1.8292642831802368, train acc: 0.2604, test acc: 0.2464\n",
      "loss: 1.849791169166565, train acc: 0.2606\n",
      "loss: 1.9409892439842225, train acc: 0.2633\n",
      "loss: 1.9222603678703307, train acc: 0.2617\n",
      "loss: 1.9717776298522949, train acc: 0.2656\n",
      "loss: 1.9240994572639465, train acc: 0.2666\n",
      "loss: 1.9003910183906556, train acc: 0.2681\n",
      "loss: 1.9078508377075196, train acc: 0.266\n",
      "loss: 1.8822705507278443, train acc: 0.2648\n",
      "epoch: 24, loss: 1.9563708305358887, train acc: 0.2648, test acc: 0.249\n",
      "loss: 1.874353051185608, train acc: 0.2645\n",
      "loss: 1.9319803833961486, train acc: 0.2645\n",
      "loss: 1.8936819434165955, train acc: 0.2647\n",
      "loss: 1.9365240216255188, train acc: 0.2681\n",
      "loss: 1.9182942986488343, train acc: 0.269\n",
      "loss: 1.8940276741981505, train acc: 0.2682\n",
      "loss: 1.899697184562683, train acc: 0.2668\n",
      "loss: 1.8919861793518067, train acc: 0.2642\n",
      "epoch: 25, loss: 1.8197424411773682, train acc: 0.2642, test acc: 0.2507\n",
      "loss: 1.8735214471817017, train acc: 0.2637\n",
      "loss: 1.918549692630768, train acc: 0.2658\n",
      "loss: 1.9100900173187256, train acc: 0.264\n",
      "loss: 1.9117153882980347, train acc: 0.2703\n",
      "loss: 1.905904197692871, train acc: 0.2673\n",
      "loss: 1.904126513004303, train acc: 0.2698\n",
      "loss: 1.9173986673355103, train acc: 0.2696\n",
      "loss: 1.88698011636734, train acc: 0.2692\n",
      "epoch: 26, loss: 1.84126877784729, train acc: 0.2692, test acc: 0.2548\n",
      "loss: 1.8164379596710205, train acc: 0.266\n",
      "loss: 1.9342783331871032, train acc: 0.2671\n",
      "loss: 1.9166617274284363, train acc: 0.2673\n",
      "loss: 1.9251672387123109, train acc: 0.2712\n",
      "loss: 1.9065635323524475, train acc: 0.2675\n",
      "loss: 1.9007508873939514, train acc: 0.2732\n",
      "loss: 1.8938843369483949, train acc: 0.2723\n",
      "loss: 1.8589223384857179, train acc: 0.2708\n",
      "epoch: 27, loss: 1.9201717376708984, train acc: 0.2708, test acc: 0.2559\n",
      "loss: 1.905794382095337, train acc: 0.2698\n",
      "loss: 1.9241142749786377, train acc: 0.2715\n",
      "loss: 1.9152830600738526, train acc: 0.2645\n",
      "loss: 1.9004800200462342, train acc: 0.2705\n",
      "loss: 1.9070984244346618, train acc: 0.2697\n",
      "loss: 1.9154837250709533, train acc: 0.2722\n",
      "loss: 1.8885986328125, train acc: 0.2708\n",
      "loss: 1.8695585608482361, train acc: 0.272\n",
      "epoch: 28, loss: 1.827915072441101, train acc: 0.272, test acc: 0.2542\n",
      "loss: 1.8616204261779785, train acc: 0.2703\n",
      "loss: 1.9198423862457275, train acc: 0.2722\n",
      "loss: 1.897005033493042, train acc: 0.2708\n",
      "loss: 1.9028794407844543, train acc: 0.2729\n",
      "loss: 1.9235203862190247, train acc: 0.2694\n",
      "loss: 1.9177539706230164, train acc: 0.273\n",
      "loss: 1.8809341549873353, train acc: 0.274\n",
      "loss: 1.8672702670097352, train acc: 0.2722\n",
      "epoch: 29, loss: 1.9324802160263062, train acc: 0.2722, test acc: 0.2587\n",
      "loss: 1.8300408124923706, train acc: 0.2706\n",
      "loss: 1.895809781551361, train acc: 0.2726\n",
      "loss: 1.90610271692276, train acc: 0.271\n",
      "loss: 1.9220078587532043, train acc: 0.2758\n",
      "loss: 1.8831890225410461, train acc: 0.2751\n",
      "loss: 1.8922950267791747, train acc: 0.2783\n",
      "loss: 1.8746387124061585, train acc: 0.2791\n",
      "loss: 1.8834240198135377, train acc: 0.277\n",
      "epoch: 30, loss: 1.778235673904419, train acc: 0.277, test acc: 0.2606\n",
      "loss: 1.8260793685913086, train acc: 0.2749\n",
      "loss: 1.8970939993858338, train acc: 0.2755\n",
      "loss: 1.8838950157165528, train acc: 0.2706\n",
      "loss: 1.9015663504600524, train acc: 0.2761\n",
      "loss: 1.9093771576881409, train acc: 0.2716\n",
      "loss: 1.8851070404052734, train acc: 0.2761\n",
      "loss: 1.8765794396400453, train acc: 0.2785\n",
      "loss: 1.8922449588775634, train acc: 0.2746\n",
      "epoch: 31, loss: 2.0484228134155273, train acc: 0.2746, test acc: 0.2634\n",
      "loss: 1.8607251644134521, train acc: 0.2741\n",
      "loss: 1.91161949634552, train acc: 0.2753\n",
      "loss: 1.9053799510002136, train acc: 0.2713\n",
      "loss: 1.9138599634170532, train acc: 0.2735\n",
      "loss: 1.897724199295044, train acc: 0.2768\n",
      "loss: 1.8695582270622253, train acc: 0.2783\n",
      "loss: 1.872453498840332, train acc: 0.2807\n",
      "loss: 1.8518580198287964, train acc: 0.2759\n",
      "epoch: 32, loss: 2.090019941329956, train acc: 0.2759, test acc: 0.2631\n",
      "loss: 1.8433181047439575, train acc: 0.2773\n",
      "loss: 1.9002699375152587, train acc: 0.2775\n",
      "loss: 1.8889809131622315, train acc: 0.2748\n",
      "loss: 1.901466977596283, train acc: 0.2762\n",
      "loss: 1.8987083911895752, train acc: 0.2777\n",
      "loss: 1.887434446811676, train acc: 0.281\n",
      "loss: 1.875883901119232, train acc: 0.2832\n",
      "loss: 1.854552721977234, train acc: 0.2782\n",
      "epoch: 33, loss: 2.040464162826538, train acc: 0.2782, test acc: 0.266\n",
      "loss: 1.8260066509246826, train acc: 0.2786\n",
      "loss: 1.905978500843048, train acc: 0.2783\n",
      "loss: 1.887823224067688, train acc: 0.2725\n",
      "loss: 1.9095363020896912, train acc: 0.2764\n",
      "loss: 1.8910616040229797, train acc: 0.2775\n",
      "loss: 1.8769463062286378, train acc: 0.2837\n",
      "loss: 1.8813371062278748, train acc: 0.28\n",
      "loss: 1.847910714149475, train acc: 0.2723\n",
      "epoch: 34, loss: 1.87460458278656, train acc: 0.2723, test acc: 0.2627\n",
      "loss: 1.7394605875015259, train acc: 0.2768\n",
      "loss: 1.8880160450935364, train acc: 0.2765\n",
      "loss: 1.8871409177780152, train acc: 0.2733\n",
      "loss: 1.8991296291351318, train acc: 0.278\n",
      "loss: 1.9030901074409485, train acc: 0.2769\n",
      "loss: 1.8869120836257935, train acc: 0.2856\n",
      "loss: 1.854050874710083, train acc: 0.2828\n",
      "loss: 1.863301944732666, train acc: 0.2779\n",
      "epoch: 35, loss: 1.7748838663101196, train acc: 0.2779, test acc: 0.2653\n",
      "loss: 1.8269156217575073, train acc: 0.2803\n",
      "loss: 1.881655740737915, train acc: 0.279\n",
      "loss: 1.8760934948921204, train acc: 0.2762\n",
      "loss: 1.891006863117218, train acc: 0.2779\n",
      "loss: 1.867349672317505, train acc: 0.2812\n",
      "loss: 1.8803723335266114, train acc: 0.2834\n",
      "loss: 1.8705952882766723, train acc: 0.2841\n",
      "loss: 1.8584912180900575, train acc: 0.2768\n",
      "epoch: 36, loss: 1.7782400846481323, train acc: 0.2768, test acc: 0.2682\n",
      "loss: 1.8314213752746582, train acc: 0.2828\n",
      "loss: 1.8773004293441773, train acc: 0.2823\n",
      "loss: 1.862213659286499, train acc: 0.279\n",
      "loss: 1.9028740763664245, train acc: 0.2791\n",
      "loss: 1.9002146124839783, train acc: 0.2816\n",
      "loss: 1.857684326171875, train acc: 0.2861\n",
      "loss: 1.856930387020111, train acc: 0.2854\n",
      "loss: 1.8310320258140564, train acc: 0.2787\n",
      "epoch: 37, loss: 1.8260915279388428, train acc: 0.2787, test acc: 0.2703\n",
      "loss: 1.7868849039077759, train acc: 0.2829\n",
      "loss: 1.8871013641357421, train acc: 0.2825\n",
      "loss: 1.8767826437950135, train acc: 0.2792\n",
      "loss: 1.8970394849777221, train acc: 0.2815\n",
      "loss: 1.8924128532409668, train acc: 0.2841\n",
      "loss: 1.8751691222190856, train acc: 0.2912\n",
      "loss: 1.8839215397834779, train acc: 0.2894\n",
      "loss: 1.839236605167389, train acc: 0.2796\n",
      "epoch: 38, loss: 1.7650243043899536, train acc: 0.2796, test acc: 0.2722\n",
      "loss: 1.8222465515136719, train acc: 0.2864\n",
      "loss: 1.8784895062446594, train acc: 0.2831\n",
      "loss: 1.8564213156700133, train acc: 0.2796\n",
      "loss: 1.906264352798462, train acc: 0.2812\n",
      "loss: 1.895498013496399, train acc: 0.2848\n",
      "loss: 1.8995410561561585, train acc: 0.2927\n",
      "loss: 1.8712327003479003, train acc: 0.2886\n",
      "loss: 1.853050208091736, train acc: 0.2812\n",
      "epoch: 39, loss: 1.8408395051956177, train acc: 0.2812, test acc: 0.2692\n",
      "loss: 1.8995598554611206, train acc: 0.288\n",
      "loss: 1.896734344959259, train acc: 0.2862\n",
      "loss: 1.8764973402023315, train acc: 0.2798\n",
      "loss: 1.9057640075683593, train acc: 0.2827\n",
      "loss: 1.898374116420746, train acc: 0.2804\n",
      "loss: 1.8852561950683593, train acc: 0.2937\n",
      "loss: 1.8807062745094298, train acc: 0.291\n",
      "loss: 1.8546135663986205, train acc: 0.2868\n",
      "epoch: 40, loss: 2.1111884117126465, train acc: 0.2868, test acc: 0.2714\n",
      "loss: 1.8013259172439575, train acc: 0.2913\n",
      "loss: 1.878163182735443, train acc: 0.2918\n",
      "loss: 1.8723057746887206, train acc: 0.2821\n",
      "loss: 1.8826938152313233, train acc: 0.2857\n",
      "loss: 1.930971622467041, train acc: 0.2816\n",
      "loss: 1.8753087639808654, train acc: 0.2949\n",
      "loss: 1.8713332056999206, train acc: 0.2929\n",
      "loss: 1.8463358879089355, train acc: 0.2864\n",
      "epoch: 41, loss: 1.9587714672088623, train acc: 0.2864, test acc: 0.2729\n",
      "loss: 1.9129008054733276, train acc: 0.2937\n",
      "loss: 1.9071273446083068, train acc: 0.2923\n",
      "loss: 1.8834569931030274, train acc: 0.2833\n",
      "loss: 1.9129010915756226, train acc: 0.2878\n",
      "loss: 1.9038103580474854, train acc: 0.2842\n",
      "loss: 1.8791838884353638, train acc: 0.2957\n",
      "loss: 1.853810429573059, train acc: 0.2948\n",
      "loss: 1.8179168105125427, train acc: 0.2879\n",
      "epoch: 42, loss: 2.145798683166504, train acc: 0.2879, test acc: 0.274\n",
      "loss: 1.7776408195495605, train acc: 0.294\n",
      "loss: 1.8715157508850098, train acc: 0.2943\n",
      "loss: 1.8541442513465882, train acc: 0.2836\n",
      "loss: 1.912069272994995, train acc: 0.2905\n",
      "loss: 1.855816662311554, train acc: 0.2905\n",
      "loss: 1.888112437725067, train acc: 0.2937\n",
      "loss: 1.8570922493934632, train acc: 0.2953\n",
      "loss: 1.855670189857483, train acc: 0.2868\n",
      "epoch: 43, loss: 1.9295644760131836, train acc: 0.2868, test acc: 0.2728\n",
      "loss: 1.7677702903747559, train acc: 0.2911\n",
      "loss: 1.8818707585334777, train acc: 0.2946\n",
      "loss: 1.8583201050758362, train acc: 0.2845\n",
      "loss: 1.8777738094329834, train acc: 0.2903\n",
      "loss: 1.868096947669983, train acc: 0.2919\n",
      "loss: 1.862982189655304, train acc: 0.2927\n",
      "loss: 1.86658935546875, train acc: 0.2974\n",
      "loss: 1.8672302484512329, train acc: 0.2896\n",
      "epoch: 44, loss: 1.7771934270858765, train acc: 0.2896, test acc: 0.2765\n",
      "loss: 1.8448125123977661, train acc: 0.2971\n",
      "loss: 1.883391761779785, train acc: 0.2943\n",
      "loss: 1.867784607410431, train acc: 0.2833\n",
      "loss: 1.887892735004425, train acc: 0.2877\n",
      "loss: 1.8744431972503661, train acc: 0.2871\n",
      "loss: 1.8944411396980285, train acc: 0.2931\n",
      "loss: 1.8271636724472047, train acc: 0.2933\n",
      "loss: 1.812304389476776, train acc: 0.2923\n",
      "epoch: 45, loss: 2.15419340133667, train acc: 0.2923, test acc: 0.2748\n",
      "loss: 1.8198482990264893, train acc: 0.2951\n",
      "loss: 1.8769707202911377, train acc: 0.2967\n",
      "loss: 1.8577674746513366, train acc: 0.286\n",
      "loss: 1.9001164197921754, train acc: 0.2932\n",
      "loss: 1.856076419353485, train acc: 0.2925\n",
      "loss: 1.8738088130950927, train acc: 0.2992\n",
      "loss: 1.8697978854179382, train acc: 0.3006\n",
      "loss: 1.862782633304596, train acc: 0.2904\n",
      "epoch: 46, loss: 1.8372972011566162, train acc: 0.2904, test acc: 0.2762\n",
      "loss: 1.7961047887802124, train acc: 0.2993\n",
      "loss: 1.8882065296173096, train acc: 0.2981\n",
      "loss: 1.8391891598701477, train acc: 0.2895\n",
      "loss: 1.873967981338501, train acc: 0.2947\n",
      "loss: 1.8920952677726746, train acc: 0.2923\n",
      "loss: 1.8572744846343994, train acc: 0.2986\n",
      "loss: 1.838429570198059, train acc: 0.2974\n",
      "loss: 1.822205698490143, train acc: 0.2893\n",
      "epoch: 47, loss: 1.9311366081237793, train acc: 0.2893, test acc: 0.2746\n",
      "loss: 1.7932143211364746, train acc: 0.296\n",
      "loss: 1.8772128582000733, train acc: 0.296\n",
      "loss: 1.862162959575653, train acc: 0.2813\n",
      "loss: 1.8856276750564576, train acc: 0.2928\n",
      "loss: 1.8885246515274048, train acc: 0.293\n",
      "loss: 1.8593459129333496, train acc: 0.298\n",
      "loss: 1.8389037728309632, train acc: 0.2987\n",
      "loss: 1.8097326040267945, train acc: 0.2926\n",
      "epoch: 48, loss: 2.1426525115966797, train acc: 0.2926, test acc: 0.2743\n",
      "loss: 1.7406567335128784, train acc: 0.2983\n",
      "loss: 1.8552770614624023, train acc: 0.2984\n",
      "loss: 1.8228565216064454, train acc: 0.2899\n",
      "loss: 1.8999116659164428, train acc: 0.2952\n",
      "loss: 1.861590564250946, train acc: 0.2916\n",
      "loss: 1.8864749431610108, train acc: 0.3004\n",
      "loss: 1.8479491353034974, train acc: 0.3001\n",
      "loss: 1.8318799018859864, train acc: 0.2908\n",
      "epoch: 49, loss: 1.8543639183044434, train acc: 0.2908, test acc: 0.2755\n",
      "loss: 1.7886778116226196, train acc: 0.3007\n",
      "loss: 1.8611650228500367, train acc: 0.3008\n",
      "loss: 1.8503437399864198, train acc: 0.2902\n",
      "loss: 1.8804992318153382, train acc: 0.2934\n",
      "loss: 1.8768842935562133, train acc: 0.2914\n",
      "loss: 1.8481595754623412, train acc: 0.3023\n",
      "loss: 1.8664319157600402, train acc: 0.3014\n",
      "loss: 1.8534927725791932, train acc: 0.2891\n",
      "epoch: 50, loss: 1.765356421470642, train acc: 0.2891, test acc: 0.2741\n",
      "loss: 1.8283261060714722, train acc: 0.3011\n",
      "loss: 1.9059645295143128, train acc: 0.2997\n",
      "loss: 1.8538670539855957, train acc: 0.2827\n",
      "loss: 1.889082908630371, train acc: 0.2904\n",
      "loss: 1.861472749710083, train acc: 0.2885\n",
      "loss: 1.858984112739563, train acc: 0.3024\n",
      "loss: 1.8460824608802795, train acc: 0.2989\n",
      "loss: 1.8100402116775514, train acc: 0.2885\n",
      "epoch: 51, loss: 1.8245391845703125, train acc: 0.2885, test acc: 0.2754\n",
      "loss: 1.7354904413223267, train acc: 0.3001\n",
      "loss: 1.8806838631629943, train acc: 0.3006\n",
      "loss: 1.858188796043396, train acc: 0.2863\n",
      "loss: 1.8722624063491822, train acc: 0.2975\n",
      "loss: 1.892840075492859, train acc: 0.2939\n",
      "loss: 1.8644347190856934, train acc: 0.3003\n",
      "loss: 1.839985740184784, train acc: 0.3\n",
      "loss: 1.8310598969459533, train acc: 0.2941\n",
      "epoch: 52, loss: 2.012830972671509, train acc: 0.2941, test acc: 0.275\n",
      "loss: 1.8045308589935303, train acc: 0.3001\n",
      "loss: 1.8767063736915588, train acc: 0.2977\n",
      "loss: 1.810476016998291, train acc: 0.2803\n",
      "loss: 1.8808276176452636, train acc: 0.2951\n",
      "loss: 1.884551429748535, train acc: 0.2898\n",
      "loss: 1.8536215901374817, train acc: 0.3004\n",
      "loss: 1.857652485370636, train acc: 0.3001\n",
      "loss: 1.8065942645072937, train acc: 0.2955\n",
      "epoch: 53, loss: 1.7533804178237915, train acc: 0.2955, test acc: 0.2748\n",
      "loss: 1.8458542823791504, train acc: 0.2996\n",
      "loss: 1.8754588603973388, train acc: 0.3016\n",
      "loss: 1.8141849637031555, train acc: 0.2835\n",
      "loss: 1.8611109495162963, train acc: 0.2986\n",
      "loss: 1.8668118000030518, train acc: 0.2947\n",
      "loss: 1.8450770854949952, train acc: 0.3026\n",
      "loss: 1.8578303813934327, train acc: 0.2985\n",
      "loss: 1.8315202474594117, train acc: 0.293\n",
      "epoch: 54, loss: 1.9020284414291382, train acc: 0.293, test acc: 0.274\n",
      "loss: 1.770391821861267, train acc: 0.3005\n",
      "loss: 1.8691256999969483, train acc: 0.302\n",
      "loss: 1.8437695741653441, train acc: 0.284\n",
      "loss: 1.8697445034980773, train acc: 0.2993\n",
      "loss: 1.8600926399230957, train acc: 0.2942\n",
      "loss: 1.8603710174560546, train acc: 0.2984\n",
      "loss: 1.809558391571045, train acc: 0.2988\n",
      "loss: 1.8391839742660523, train acc: 0.2958\n",
      "epoch: 55, loss: 1.8293665647506714, train acc: 0.2958, test acc: 0.2751\n",
      "loss: 1.7513225078582764, train acc: 0.2991\n",
      "loss: 1.8464841008186341, train acc: 0.3018\n",
      "loss: 1.843349552154541, train acc: 0.2855\n",
      "loss: 1.8496159315109253, train acc: 0.2972\n",
      "loss: 1.8563515305519105, train acc: 0.2984\n",
      "loss: 1.850699007511139, train acc: 0.3012\n",
      "loss: 1.7997088193893434, train acc: 0.2973\n",
      "loss: 1.8251978754997253, train acc: 0.2972\n",
      "epoch: 56, loss: 1.7602139711380005, train acc: 0.2972, test acc: 0.278\n",
      "loss: 1.7626575231552124, train acc: 0.301\n",
      "loss: 1.8770596742630006, train acc: 0.3026\n",
      "loss: 1.86450697183609, train acc: 0.2857\n",
      "loss: 1.8584855556488038, train acc: 0.2972\n",
      "loss: 1.8894773483276368, train acc: 0.2894\n",
      "loss: 1.8346836686134338, train acc: 0.3063\n",
      "loss: 1.8601069569587707, train acc: 0.303\n",
      "loss: 1.8334729552268982, train acc: 0.2959\n",
      "epoch: 57, loss: 1.7610176801681519, train acc: 0.2959, test acc: 0.2764\n",
      "loss: 1.8324244022369385, train acc: 0.3002\n",
      "loss: 1.8655062317848206, train acc: 0.3029\n",
      "loss: 1.8293221235275268, train acc: 0.2869\n",
      "loss: 1.8617403030395507, train acc: 0.2997\n",
      "loss: 1.8627582669258118, train acc: 0.2956\n",
      "loss: 1.8592750310897828, train acc: 0.3011\n",
      "loss: 1.8567740678787232, train acc: 0.3032\n",
      "loss: 1.8351282477378845, train acc: 0.2968\n",
      "epoch: 58, loss: 2.1429216861724854, train acc: 0.2968, test acc: 0.2802\n",
      "loss: 1.844664216041565, train acc: 0.3029\n",
      "loss: 1.867299747467041, train acc: 0.3021\n",
      "loss: 1.857476246356964, train acc: 0.2855\n",
      "loss: 1.8666529178619384, train acc: 0.2983\n",
      "loss: 1.8679115891456604, train acc: 0.2975\n",
      "loss: 1.8282878637313842, train acc: 0.3015\n",
      "loss: 1.8249333500862122, train acc: 0.3012\n",
      "loss: 1.8260075926780701, train acc: 0.2927\n",
      "epoch: 59, loss: 2.0746359825134277, train acc: 0.2927, test acc: 0.2782\n",
      "loss: 1.7600048780441284, train acc: 0.3021\n",
      "loss: 1.8816005349159242, train acc: 0.3035\n",
      "loss: 1.8351625800132751, train acc: 0.2845\n",
      "loss: 1.8643050909042358, train acc: 0.299\n",
      "loss: 1.882286262512207, train acc: 0.2964\n",
      "loss: 1.840841257572174, train acc: 0.3006\n",
      "loss: 1.8609667539596557, train acc: 0.3041\n",
      "loss: 1.7848416447639466, train acc: 0.2992\n",
      "epoch: 60, loss: 1.911287784576416, train acc: 0.2992, test acc: 0.2795\n",
      "loss: 1.785677433013916, train acc: 0.3021\n",
      "loss: 1.8744111061096191, train acc: 0.3048\n",
      "loss: 1.8104353070259094, train acc: 0.2844\n",
      "loss: 1.8936707019805907, train acc: 0.3004\n",
      "loss: 1.8804214954376222, train acc: 0.3008\n",
      "loss: 1.8706218123435974, train acc: 0.3021\n",
      "loss: 1.833555269241333, train acc: 0.3034\n",
      "loss: 1.831568670272827, train acc: 0.3003\n",
      "epoch: 61, loss: 1.7531126737594604, train acc: 0.3003, test acc: 0.2838\n",
      "loss: 1.8205580711364746, train acc: 0.3049\n",
      "loss: 1.8464039444923401, train acc: 0.3037\n",
      "loss: 1.829883086681366, train acc: 0.2899\n",
      "loss: 1.8595782160758971, train acc: 0.3012\n",
      "loss: 1.8700140833854675, train acc: 0.2998\n",
      "loss: 1.8302724838256836, train acc: 0.3018\n",
      "loss: 1.808252227306366, train acc: 0.3037\n",
      "loss: 1.8064456343650819, train acc: 0.3027\n",
      "epoch: 62, loss: 1.73908531665802, train acc: 0.3027, test acc: 0.2835\n",
      "loss: 1.7090340852737427, train acc: 0.3052\n",
      "loss: 1.848937726020813, train acc: 0.3045\n",
      "loss: 1.8271745562553405, train acc: 0.2924\n",
      "loss: 1.8852758646011352, train acc: 0.3039\n",
      "loss: 1.8634135603904725, train acc: 0.302\n",
      "loss: 1.8232820272445678, train acc: 0.3059\n",
      "loss: 1.822069275379181, train acc: 0.3061\n",
      "loss: 1.7927287936210632, train acc: 0.3046\n",
      "epoch: 63, loss: 1.9166184663772583, train acc: 0.3046, test acc: 0.2857\n",
      "loss: 1.7294337749481201, train acc: 0.3076\n",
      "loss: 1.8527264833450316, train acc: 0.3058\n",
      "loss: 1.856291651725769, train acc: 0.2888\n",
      "loss: 1.8777347445487975, train acc: 0.3021\n",
      "loss: 1.8552983760833741, train acc: 0.3017\n",
      "loss: 1.8280055046081543, train acc: 0.3067\n",
      "loss: 1.840714681148529, train acc: 0.3053\n",
      "loss: 1.787508475780487, train acc: 0.299\n",
      "epoch: 64, loss: 2.19600248336792, train acc: 0.299, test acc: 0.2841\n",
      "loss: 1.6916934251785278, train acc: 0.3051\n",
      "loss: 1.8561229825019836, train acc: 0.3042\n",
      "loss: 1.8416909217834472, train acc: 0.2925\n",
      "loss: 1.8674296617507935, train acc: 0.3032\n",
      "loss: 1.8691736102104186, train acc: 0.2997\n",
      "loss: 1.8384148240089417, train acc: 0.3063\n",
      "loss: 1.823885715007782, train acc: 0.3068\n",
      "loss: 1.8217398524284363, train acc: 0.3076\n",
      "epoch: 65, loss: 2.4875776767730713, train acc: 0.3076, test acc: 0.2855\n",
      "loss: 1.734928846359253, train acc: 0.3069\n",
      "loss: 1.8580003380775452, train acc: 0.3054\n",
      "loss: 1.8504450917243958, train acc: 0.2938\n",
      "loss: 1.8653805255889893, train acc: 0.3042\n",
      "loss: 1.8492439270019532, train acc: 0.3031\n",
      "loss: 1.8717654943466187, train acc: 0.3068\n",
      "loss: 1.8515328168869019, train acc: 0.3057\n",
      "loss: 1.7940536499023438, train acc: 0.3\n",
      "epoch: 66, loss: 1.7836304903030396, train acc: 0.3, test acc: 0.2859\n",
      "loss: 1.7434563636779785, train acc: 0.307\n",
      "loss: 1.8459107995033264, train acc: 0.308\n",
      "loss: 1.8167737245559692, train acc: 0.2955\n",
      "loss: 1.8632411718368531, train acc: 0.3044\n",
      "loss: 1.8542745113372803, train acc: 0.3032\n",
      "loss: 1.8292778968811034, train acc: 0.3094\n",
      "loss: 1.8160248279571534, train acc: 0.3075\n",
      "loss: 1.8269009590148926, train acc: 0.306\n",
      "epoch: 67, loss: 1.8379465341567993, train acc: 0.306, test acc: 0.286\n",
      "loss: 1.8568689823150635, train acc: 0.309\n",
      "loss: 1.8703611731529235, train acc: 0.3073\n",
      "loss: 1.8383877515792846, train acc: 0.2969\n",
      "loss: 1.878724956512451, train acc: 0.3046\n",
      "loss: 1.8750336527824403, train acc: 0.3041\n",
      "loss: 1.8534127950668335, train acc: 0.3081\n",
      "loss: 1.8294707179069518, train acc: 0.3078\n",
      "loss: 1.8157814025878907, train acc: 0.3093\n",
      "epoch: 68, loss: 1.7473973035812378, train acc: 0.3093, test acc: 0.2867\n",
      "loss: 1.7139129638671875, train acc: 0.3094\n",
      "loss: 1.8602375626564025, train acc: 0.3076\n",
      "loss: 1.8257948279380798, train acc: 0.295\n",
      "loss: 1.8947376489639283, train acc: 0.3047\n",
      "loss: 1.8566572070121765, train acc: 0.3037\n",
      "loss: 1.8498161077499389, train acc: 0.3086\n",
      "loss: 1.8360151529312134, train acc: 0.3093\n",
      "loss: 1.8141222834587096, train acc: 0.3038\n",
      "epoch: 69, loss: 1.8163456916809082, train acc: 0.3038, test acc: 0.2856\n",
      "loss: 1.7267587184906006, train acc: 0.3097\n",
      "loss: 1.8383219361305236, train acc: 0.3082\n",
      "loss: 1.8279213070869447, train acc: 0.2954\n",
      "loss: 1.874337363243103, train acc: 0.3048\n",
      "loss: 1.8692128658294678, train acc: 0.3043\n",
      "loss: 1.851073718070984, train acc: 0.3085\n",
      "loss: 1.8165022611618042, train acc: 0.3063\n",
      "loss: 1.7897653460502625, train acc: 0.3032\n",
      "epoch: 70, loss: 1.860777735710144, train acc: 0.3032, test acc: 0.2865\n",
      "loss: 1.7528419494628906, train acc: 0.3088\n",
      "loss: 1.8483926057815552, train acc: 0.309\n",
      "loss: 1.8334537744522095, train acc: 0.3013\n",
      "loss: 1.8595305323600768, train acc: 0.3059\n",
      "loss: 1.8409937858581542, train acc: 0.3054\n",
      "loss: 1.811805510520935, train acc: 0.3086\n",
      "loss: 1.8322665572166443, train acc: 0.3094\n",
      "loss: 1.8086809158325194, train acc: 0.3066\n",
      "epoch: 71, loss: 1.738445520401001, train acc: 0.3066, test acc: 0.2875\n",
      "loss: 1.7123247385025024, train acc: 0.3091\n",
      "loss: 1.858750343322754, train acc: 0.3077\n",
      "loss: 1.8054503202438354, train acc: 0.2985\n",
      "loss: 1.8579508304595946, train acc: 0.3053\n",
      "loss: 1.8812654614448547, train acc: 0.3008\n",
      "loss: 1.8683270692825318, train acc: 0.3096\n",
      "loss: 1.8240660786628724, train acc: 0.3087\n",
      "loss: 1.819569742679596, train acc: 0.3047\n",
      "epoch: 72, loss: 1.738378643989563, train acc: 0.3047, test acc: 0.2881\n",
      "loss: 1.7610788345336914, train acc: 0.3088\n",
      "loss: 1.8662066221237184, train acc: 0.3083\n",
      "loss: 1.792999231815338, train acc: 0.2966\n",
      "loss: 1.8385221362113953, train acc: 0.305\n",
      "loss: 1.8563478708267211, train acc: 0.3038\n",
      "loss: 1.850955331325531, train acc: 0.3101\n",
      "loss: 1.8325339913368226, train acc: 0.309\n",
      "loss: 1.777650249004364, train acc: 0.3046\n",
      "epoch: 73, loss: 2.1048285961151123, train acc: 0.3046, test acc: 0.2879\n",
      "loss: 1.688652515411377, train acc: 0.3077\n",
      "loss: 1.8370091795921326, train acc: 0.3081\n",
      "loss: 1.8254301428794861, train acc: 0.2974\n",
      "loss: 1.849958896636963, train acc: 0.3061\n",
      "loss: 1.8752908945083617, train acc: 0.3088\n",
      "loss: 1.8097018361091615, train acc: 0.3113\n",
      "loss: 1.8316542625427246, train acc: 0.3078\n",
      "loss: 1.8302551627159118, train acc: 0.3098\n",
      "epoch: 74, loss: 1.7827335596084595, train acc: 0.3098, test acc: 0.2886\n",
      "loss: 1.689940333366394, train acc: 0.3102\n",
      "loss: 1.8142518281936646, train acc: 0.3091\n",
      "loss: 1.8247225165367127, train acc: 0.2936\n",
      "loss: 1.8343310952186584, train acc: 0.3033\n",
      "loss: 1.8679328680038452, train acc: 0.3053\n",
      "loss: 1.8125241637229919, train acc: 0.313\n",
      "loss: 1.8088247776031494, train acc: 0.3085\n",
      "loss: 1.8304186582565307, train acc: 0.3076\n",
      "epoch: 75, loss: 1.7346725463867188, train acc: 0.3076, test acc: 0.2902\n",
      "loss: 1.7277123928070068, train acc: 0.3091\n",
      "loss: 1.8545718431472777, train acc: 0.3094\n",
      "loss: 1.8048650026321411, train acc: 0.3031\n",
      "loss: 1.850499439239502, train acc: 0.3062\n",
      "loss: 1.854999303817749, train acc: 0.3112\n",
      "loss: 1.8290728211402894, train acc: 0.3133\n",
      "loss: 1.821905219554901, train acc: 0.311\n",
      "loss: 1.790171706676483, train acc: 0.3069\n",
      "epoch: 76, loss: 1.735314965248108, train acc: 0.3069, test acc: 0.2902\n",
      "loss: 1.710545301437378, train acc: 0.3089\n",
      "loss: 1.836728274822235, train acc: 0.3095\n",
      "loss: 1.8314629554748536, train acc: 0.2931\n",
      "loss: 1.8743914008140563, train acc: 0.3067\n",
      "loss: 1.840894627571106, train acc: 0.3113\n",
      "loss: 1.8414318323135377, train acc: 0.3126\n",
      "loss: 1.8190321445465087, train acc: 0.311\n",
      "loss: 1.8213405013084412, train acc: 0.3081\n",
      "epoch: 77, loss: 1.7270621061325073, train acc: 0.3081, test acc: 0.2924\n",
      "loss: 1.7603634595870972, train acc: 0.3128\n",
      "loss: 1.8348588347434998, train acc: 0.3121\n",
      "loss: 1.803126323223114, train acc: 0.3034\n",
      "loss: 1.842489445209503, train acc: 0.3093\n",
      "loss: 1.860117745399475, train acc: 0.3021\n",
      "loss: 1.8244531273841857, train acc: 0.3141\n",
      "loss: 1.838119673728943, train acc: 0.3171\n",
      "loss: 1.8088429927825929, train acc: 0.3111\n",
      "epoch: 78, loss: 1.738133192062378, train acc: 0.3111, test acc: 0.295\n",
      "loss: 1.6550898551940918, train acc: 0.3131\n",
      "loss: 1.8844785690307617, train acc: 0.3137\n",
      "loss: 1.8250475764274596, train acc: 0.3031\n",
      "loss: 1.8611736536026, train acc: 0.308\n",
      "loss: 1.8339428901672363, train acc: 0.3083\n",
      "loss: 1.8331643700599671, train acc: 0.3149\n",
      "loss: 1.8172566890716553, train acc: 0.3193\n",
      "loss: 1.8067994952201842, train acc: 0.316\n",
      "epoch: 79, loss: 1.724050521850586, train acc: 0.316, test acc: 0.2982\n",
      "#####training and testing end with K:1, P:0.1######\n",
      "#####training and testing start with K:1, P:0.5######\n",
      "loss: 2.5650551319122314, train acc: 0.1\n",
      "loss: 2.5457722902297975, train acc: 0.1\n",
      "loss: 2.504329872131348, train acc: 0.1\n",
      "loss: 2.4303619861602783, train acc: 0.1004\n",
      "loss: 2.4123060941696166, train acc: 0.127\n",
      "loss: 2.3738818407058715, train acc: 0.1452\n",
      "loss: 2.4509021282196044, train acc: 0.1427\n",
      "loss: 2.3630707263946533, train acc: 0.153\n",
      "epoch: 0, loss: 2.2863028049468994, train acc: 0.153, test acc: 0.1534\n",
      "loss: 2.392122745513916, train acc: 0.1578\n",
      "loss: 2.3757214069366457, train acc: 0.157\n",
      "loss: 2.3802472591400146, train acc: 0.1631\n",
      "loss: 2.3295006275177004, train acc: 0.1722\n",
      "loss: 2.322717523574829, train acc: 0.1736\n",
      "loss: 2.3397802829742433, train acc: 0.1745\n",
      "loss: 2.373921275138855, train acc: 0.1676\n",
      "loss: 2.3627568244934083, train acc: 0.1719\n",
      "epoch: 1, loss: 2.4022834300994873, train acc: 0.1719, test acc: 0.17\n",
      "loss: 2.370032787322998, train acc: 0.1736\n",
      "loss: 2.3366573572158815, train acc: 0.1757\n",
      "loss: 2.3413068056106567, train acc: 0.1723\n",
      "loss: 2.3059500455856323, train acc: 0.1785\n",
      "loss: 2.284977126121521, train acc: 0.1808\n",
      "loss: 2.324439811706543, train acc: 0.1797\n",
      "loss: 2.3537469148635863, train acc: 0.1786\n",
      "loss: 2.3036317348480226, train acc: 0.1777\n",
      "epoch: 2, loss: 2.5470495223999023, train acc: 0.1777, test acc: 0.1693\n",
      "loss: 2.3218510150909424, train acc: 0.1738\n",
      "loss: 2.3213757991790773, train acc: 0.1744\n",
      "loss: 2.2933956146240235, train acc: 0.1755\n",
      "loss: 2.2845271110534666, train acc: 0.1802\n",
      "loss: 2.2728397607803346, train acc: 0.1808\n",
      "loss: 2.276955270767212, train acc: 0.1761\n",
      "loss: 2.3100880861282347, train acc: 0.1779\n",
      "loss: 2.298744821548462, train acc: 0.1786\n",
      "epoch: 3, loss: 2.515500545501709, train acc: 0.1786, test acc: 0.1667\n",
      "loss: 2.3332273960113525, train acc: 0.1719\n",
      "loss: 2.3045328140258787, train acc: 0.1786\n",
      "loss: 2.2645832538604735, train acc: 0.1787\n",
      "loss: 2.258834385871887, train acc: 0.1792\n",
      "loss: 2.2556509256362913, train acc: 0.1785\n",
      "loss: 2.277206802368164, train acc: 0.1761\n",
      "loss: 2.2874173164367675, train acc: 0.1787\n",
      "loss: 2.2550591945648195, train acc: 0.1781\n",
      "epoch: 4, loss: 2.1333415508270264, train acc: 0.1781, test acc: 0.1738\n",
      "loss: 2.332627296447754, train acc: 0.1784\n",
      "loss: 2.2767800092697144, train acc: 0.1748\n",
      "loss: 2.2426486253738402, train acc: 0.1784\n",
      "loss: 2.20872106552124, train acc: 0.1763\n",
      "loss: 2.2178100109100343, train acc: 0.1757\n",
      "loss: 2.2357768058776855, train acc: 0.1792\n",
      "loss: 2.247885251045227, train acc: 0.1803\n",
      "loss: 2.2239078521728515, train acc: 0.1791\n",
      "epoch: 5, loss: 2.11252760887146, train acc: 0.1791, test acc: 0.1734\n",
      "loss: 2.2198357582092285, train acc: 0.177\n",
      "loss: 2.242606019973755, train acc: 0.1806\n",
      "loss: 2.2246500492095946, train acc: 0.18\n",
      "loss: 2.2241557598114015, train acc: 0.1797\n",
      "loss: 2.178645873069763, train acc: 0.1811\n",
      "loss: 2.206903004646301, train acc: 0.178\n",
      "loss: 2.242512559890747, train acc: 0.1791\n",
      "loss: 2.188314604759216, train acc: 0.1775\n",
      "epoch: 6, loss: 2.071589469909668, train acc: 0.1775, test acc: 0.1662\n",
      "loss: 2.2328121662139893, train acc: 0.172\n",
      "loss: 2.2257330656051635, train acc: 0.1774\n",
      "loss: 2.20968234539032, train acc: 0.1731\n",
      "loss: 2.1952788829803467, train acc: 0.1777\n",
      "loss: 2.193000841140747, train acc: 0.1771\n",
      "loss: 2.2058192014694216, train acc: 0.1783\n",
      "loss: 2.227264666557312, train acc: 0.1808\n",
      "loss: 2.2214245557785035, train acc: 0.1778\n",
      "epoch: 7, loss: 2.2298991680145264, train acc: 0.1778, test acc: 0.1743\n",
      "loss: 2.269010305404663, train acc: 0.1754\n",
      "loss: 2.2259063005447386, train acc: 0.175\n",
      "loss: 2.1917656660079956, train acc: 0.1762\n",
      "loss: 2.194842886924744, train acc: 0.1792\n",
      "loss: 2.168270134925842, train acc: 0.1791\n",
      "loss: 2.1759262800216677, train acc: 0.1755\n",
      "loss: 2.215727424621582, train acc: 0.1784\n",
      "loss: 2.1888073682785034, train acc: 0.1726\n",
      "epoch: 8, loss: 2.3115005493164062, train acc: 0.1726, test acc: 0.1716\n",
      "loss: 2.267580509185791, train acc: 0.173\n",
      "loss: 2.197100305557251, train acc: 0.1777\n",
      "loss: 2.1850900650024414, train acc: 0.2552\n",
      "loss: 2.1824941873550414, train acc: 0.2581\n",
      "loss: 2.1551169395446776, train acc: 0.2557\n",
      "loss: 2.189753222465515, train acc: 0.2527\n",
      "loss: 2.201365256309509, train acc: 0.2529\n",
      "loss: 2.164395475387573, train acc: 0.2566\n",
      "epoch: 9, loss: 2.3441014289855957, train acc: 0.2566, test acc: 0.2411\n",
      "loss: 2.2002623081207275, train acc: 0.2428\n",
      "loss: 2.1747097969055176, train acc: 0.2558\n",
      "loss: 2.163496494293213, train acc: 0.2588\n",
      "loss: 2.1459555864334106, train acc: 0.2584\n",
      "loss: 2.1603116035461425, train acc: 0.2551\n",
      "loss: 2.180779480934143, train acc: 0.2566\n",
      "loss: 2.1924672365188598, train acc: 0.2545\n",
      "loss: 2.1649733543395997, train acc: 0.2538\n",
      "epoch: 10, loss: 2.2556843757629395, train acc: 0.2538, test acc: 0.2477\n",
      "loss: 2.2588889598846436, train acc: 0.2502\n",
      "loss: 2.1780346155166628, train acc: 0.2628\n",
      "loss: 2.1774545907974243, train acc: 0.2638\n",
      "loss: 2.16345694065094, train acc: 0.2694\n",
      "loss: 2.1620879411697387, train acc: 0.2561\n",
      "loss: 2.168577551841736, train acc: 0.2666\n",
      "loss: 2.196817493438721, train acc: 0.2517\n",
      "loss: 2.1332431554794313, train acc: 0.2678\n",
      "epoch: 11, loss: 2.0447933673858643, train acc: 0.2678, test acc: 0.2495\n",
      "loss: 2.176757335662842, train acc: 0.2513\n",
      "loss: 2.1537407636642456, train acc: 0.2643\n",
      "loss: 2.168826127052307, train acc: 0.2659\n",
      "loss: 2.1388911247253417, train acc: 0.2628\n",
      "loss: 2.1238345384597777, train acc: 0.265\n",
      "loss: 2.1417089700698853, train acc: 0.2594\n",
      "loss: 2.1851621866226196, train acc: 0.2566\n",
      "loss: 2.1368334531784057, train acc: 0.2641\n",
      "epoch: 12, loss: 2.130882501602173, train acc: 0.2641, test acc: 0.2506\n",
      "loss: 2.2096781730651855, train acc: 0.2546\n",
      "loss: 2.159933567047119, train acc: 0.2606\n",
      "loss: 2.1480710983276365, train acc: 0.2599\n",
      "loss: 2.145057964324951, train acc: 0.2625\n",
      "loss: 2.120494771003723, train acc: 0.2597\n",
      "loss: 2.1495785236358644, train acc: 0.2622\n",
      "loss: 2.1543168783187867, train acc: 0.2622\n",
      "loss: 2.1624301433563233, train acc: 0.2537\n",
      "epoch: 13, loss: 2.1386265754699707, train acc: 0.2537, test acc: 0.2543\n",
      "loss: 2.172128915786743, train acc: 0.2603\n",
      "loss: 2.163302516937256, train acc: 0.2572\n",
      "loss: 2.1417543411254885, train acc: 0.2658\n",
      "loss: 2.136538290977478, train acc: 0.2592\n",
      "loss: 2.1253008127212523, train acc: 0.2617\n",
      "loss: 2.127623438835144, train acc: 0.2547\n",
      "loss: 2.1641919374465943, train acc: 0.257\n",
      "loss: 2.1322455644607543, train acc: 0.2567\n",
      "epoch: 14, loss: 2.1976425647735596, train acc: 0.2567, test acc: 0.2528\n",
      "loss: 2.1274945735931396, train acc: 0.2619\n",
      "loss: 2.149579572677612, train acc: 0.2635\n",
      "loss: 2.1453930616378782, train acc: 0.2648\n",
      "loss: 2.1187068939208986, train acc: 0.2658\n",
      "loss: 2.128885841369629, train acc: 0.2677\n",
      "loss: 2.1336159229278566, train acc: 0.2662\n",
      "loss: 2.1413113832473756, train acc: 0.2613\n",
      "loss: 2.119257616996765, train acc: 0.2691\n",
      "epoch: 15, loss: 2.0388901233673096, train acc: 0.2691, test acc: 0.2581\n",
      "loss: 2.095874786376953, train acc: 0.2665\n",
      "loss: 2.147507357597351, train acc: 0.2653\n",
      "loss: 2.1420626878738402, train acc: 0.2673\n",
      "loss: 2.1073684453964234, train acc: 0.2642\n",
      "loss: 2.126643681526184, train acc: 0.2652\n",
      "loss: 2.118362808227539, train acc: 0.262\n",
      "loss: 2.1445123195648192, train acc: 0.2595\n",
      "loss: 2.1326621532440186, train acc: 0.2568\n",
      "epoch: 16, loss: 2.0414459705352783, train acc: 0.2568, test acc: 0.2566\n",
      "loss: 2.164752244949341, train acc: 0.2618\n",
      "loss: 2.158355379104614, train acc: 0.2652\n",
      "loss: 2.147608995437622, train acc: 0.2631\n",
      "loss: 2.1442078590393066, train acc: 0.2655\n",
      "loss: 2.120328760147095, train acc: 0.2675\n",
      "loss: 2.1197104930877684, train acc: 0.2671\n",
      "loss: 2.1380268573760985, train acc: 0.2664\n",
      "loss: 2.122282361984253, train acc: 0.2668\n",
      "epoch: 17, loss: 2.095844030380249, train acc: 0.2668, test acc: 0.2591\n",
      "loss: 2.1050829887390137, train acc: 0.2659\n",
      "loss: 2.140770673751831, train acc: 0.2627\n",
      "loss: 2.1150394916534423, train acc: 0.2643\n",
      "loss: 2.112381386756897, train acc: 0.2676\n",
      "loss: 2.1224079608917235, train acc: 0.269\n",
      "loss: 2.1506866455078124, train acc: 0.2639\n",
      "loss: 2.145321846008301, train acc: 0.2648\n",
      "loss: 2.1245071649551392, train acc: 0.2657\n",
      "epoch: 18, loss: 2.1007332801818848, train acc: 0.2657, test acc: 0.2519\n",
      "loss: 2.1276228427886963, train acc: 0.2606\n",
      "loss: 2.125644063949585, train acc: 0.2638\n",
      "loss: 2.147019147872925, train acc: 0.2621\n",
      "loss: 2.103900933265686, train acc: 0.2607\n",
      "loss: 2.1125566959381104, train acc: 0.2651\n",
      "loss: 2.1038593530654905, train acc: 0.2624\n",
      "loss: 2.1231306076049803, train acc: 0.2665\n",
      "loss: 2.132889175415039, train acc: 0.2587\n",
      "epoch: 19, loss: 1.871451497077942, train acc: 0.2587, test acc: 0.2558\n",
      "loss: 2.043654441833496, train acc: 0.2606\n",
      "loss: 2.121356725692749, train acc: 0.263\n",
      "loss: 2.1130220174789427, train acc: 0.2645\n",
      "loss: 2.1258125782012938, train acc: 0.2664\n",
      "loss: 2.1169017791748046, train acc: 0.2676\n",
      "loss: 2.109970045089722, train acc: 0.2643\n",
      "loss: 2.1387138605117797, train acc: 0.2636\n",
      "loss: 2.1339558124542237, train acc: 0.2612\n",
      "epoch: 20, loss: 2.000122308731079, train acc: 0.2612, test acc: 0.2528\n",
      "loss: 2.1527905464172363, train acc: 0.2589\n",
      "loss: 2.1495277166366575, train acc: 0.2595\n",
      "loss: 2.1278112888336183, train acc: 0.2611\n",
      "loss: 2.147472357749939, train acc: 0.2628\n",
      "loss: 2.11823627948761, train acc: 0.2645\n",
      "loss: 2.128468894958496, train acc: 0.2638\n",
      "loss: 2.1178907632827757, train acc: 0.2656\n",
      "loss: 2.140167760848999, train acc: 0.2658\n",
      "epoch: 21, loss: 2.3548104763031006, train acc: 0.2658, test acc: 0.2523\n",
      "loss: 2.1702680587768555, train acc: 0.26\n",
      "loss: 2.1391419172286987, train acc: 0.2602\n",
      "loss: 2.1184481620788573, train acc: 0.2641\n",
      "loss: 2.125771164894104, train acc: 0.2588\n",
      "loss: 2.127363419532776, train acc: 0.266\n",
      "loss: 2.116295886039734, train acc: 0.2634\n",
      "loss: 2.1434390544891357, train acc: 0.2648\n",
      "loss: 2.128289222717285, train acc: 0.2647\n",
      "epoch: 22, loss: 2.0882160663604736, train acc: 0.2647, test acc: 0.255\n",
      "loss: 2.0960512161254883, train acc: 0.2631\n",
      "loss: 2.129251408576965, train acc: 0.2658\n",
      "loss: 2.1199710607528686, train acc: 0.2624\n",
      "loss: 2.121582531929016, train acc: 0.2627\n",
      "loss: 2.142377758026123, train acc: 0.2615\n",
      "loss: 2.128280186653137, train acc: 0.2613\n",
      "loss: 2.1262259006500246, train acc: 0.2622\n",
      "loss: 2.109835076332092, train acc: 0.2619\n",
      "epoch: 23, loss: 1.9861061573028564, train acc: 0.2619, test acc: 0.2546\n",
      "loss: 2.106411933898926, train acc: 0.2567\n",
      "loss: 2.148044395446777, train acc: 0.2578\n",
      "loss: 2.1088368892669678, train acc: 0.2602\n",
      "loss: 2.1239654541015627, train acc: 0.2616\n",
      "loss: 2.143239688873291, train acc: 0.2651\n",
      "loss: 2.092384469509125, train acc: 0.2637\n",
      "loss: 2.140434217453003, train acc: 0.2579\n",
      "loss: 2.1306026935577393, train acc: 0.2634\n",
      "epoch: 24, loss: 2.2647864818573, train acc: 0.2634, test acc: 0.2537\n",
      "loss: 2.157907247543335, train acc: 0.2612\n",
      "loss: 2.133483910560608, train acc: 0.258\n",
      "loss: 2.141146731376648, train acc: 0.2566\n",
      "loss: 2.119306135177612, train acc: 0.2582\n",
      "loss: 2.1538134813308716, train acc: 0.2594\n",
      "loss: 2.125981903076172, train acc: 0.262\n",
      "loss: 2.174131965637207, train acc: 0.2524\n",
      "loss: 2.1184985637664795, train acc: 0.2625\n",
      "epoch: 25, loss: 2.288699150085449, train acc: 0.2625, test acc: 0.2556\n",
      "loss: 2.039024829864502, train acc: 0.2657\n",
      "loss: 2.1066392421722413, train acc: 0.266\n",
      "loss: 2.1271307468414307, train acc: 0.2637\n",
      "loss: 2.1143639087677, train acc: 0.2658\n",
      "loss: 2.115094041824341, train acc: 0.2683\n",
      "loss: 2.117380881309509, train acc: 0.268\n",
      "loss: 2.122292470932007, train acc: 0.2606\n",
      "loss: 2.1209352731704714, train acc: 0.2693\n",
      "epoch: 26, loss: 2.0080559253692627, train acc: 0.2693, test acc: 0.27\n",
      "loss: 2.1529531478881836, train acc: 0.2691\n",
      "loss: 2.108645224571228, train acc: 0.2621\n",
      "loss: 2.121363568305969, train acc: 0.2669\n",
      "loss: 2.1212272882461547, train acc: 0.2615\n",
      "loss: 2.1374627351760864, train acc: 0.2658\n",
      "loss: 2.122080183029175, train acc: 0.2683\n",
      "loss: 2.1435644388198853, train acc: 0.2692\n",
      "loss: 2.1200280666351317, train acc: 0.2693\n",
      "epoch: 27, loss: 2.0074591636657715, train acc: 0.2693, test acc: 0.2701\n",
      "loss: 2.1042327880859375, train acc: 0.2702\n",
      "loss: 2.1325763702392577, train acc: 0.2694\n",
      "loss: 2.117467999458313, train acc: 0.2669\n",
      "loss: 2.1410263538360597, train acc: 0.2659\n",
      "loss: 2.125898838043213, train acc: 0.2713\n",
      "loss: 2.0954076766967775, train acc: 0.2719\n",
      "loss: 2.1481884956359862, train acc: 0.2644\n",
      "loss: 2.120096445083618, train acc: 0.2701\n",
      "epoch: 28, loss: 2.004657030105591, train acc: 0.2701, test acc: 0.2718\n",
      "loss: 2.0878005027770996, train acc: 0.2729\n",
      "loss: 2.140626239776611, train acc: 0.2672\n",
      "loss: 2.108420896530151, train acc: 0.2644\n",
      "loss: 2.1174034595489504, train acc: 0.2689\n",
      "loss: 2.1093476533889772, train acc: 0.2694\n",
      "loss: 2.10978786945343, train acc: 0.2635\n",
      "loss: 2.106930708885193, train acc: 0.2724\n",
      "loss: 2.1498685836791993, train acc: 0.2628\n",
      "epoch: 29, loss: 1.9100995063781738, train acc: 0.2628, test acc: 0.2639\n",
      "loss: 2.1621999740600586, train acc: 0.2618\n",
      "loss: 2.1408777713775633, train acc: 0.2645\n",
      "loss: 2.119599795341492, train acc: 0.2626\n",
      "loss: 2.137331748008728, train acc: 0.2648\n",
      "loss: 2.1145299911499023, train acc: 0.2673\n",
      "loss: 2.107288861274719, train acc: 0.2757\n",
      "loss: 2.1418219804763794, train acc: 0.2677\n",
      "loss: 2.110589838027954, train acc: 0.2737\n",
      "epoch: 30, loss: 2.1660242080688477, train acc: 0.2737, test acc: 0.2672\n",
      "loss: 2.169935941696167, train acc: 0.2639\n",
      "loss: 2.1535136461257935, train acc: 0.2625\n",
      "loss: 2.1381167650222777, train acc: 0.2641\n",
      "loss: 2.110813784599304, train acc: 0.2681\n",
      "loss: 2.116311550140381, train acc: 0.2678\n",
      "loss: 2.1097721099853515, train acc: 0.276\n",
      "loss: 2.124253439903259, train acc: 0.2785\n",
      "loss: 2.12970449924469, train acc: 0.2713\n",
      "epoch: 31, loss: 1.983231544494629, train acc: 0.2713, test acc: 0.2557\n",
      "loss: 2.132904052734375, train acc: 0.2703\n",
      "loss: 2.138117241859436, train acc: 0.2641\n",
      "loss: 2.100308561325073, train acc: 0.2661\n",
      "loss: 2.113558840751648, train acc: 0.2682\n",
      "loss: 2.125676155090332, train acc: 0.2772\n",
      "loss: 2.1164000988006593, train acc: 0.2807\n",
      "loss: 2.1256361961364747, train acc: 0.2816\n",
      "loss: 2.1261593103408813, train acc: 0.2793\n",
      "epoch: 32, loss: 2.271536111831665, train acc: 0.2793, test acc: 0.2633\n",
      "loss: 2.1388330459594727, train acc: 0.2781\n",
      "loss: 2.1295823574066164, train acc: 0.2798\n",
      "loss: 2.1304283857345583, train acc: 0.2739\n",
      "loss: 2.12736554145813, train acc: 0.2778\n",
      "loss: 2.14426474571228, train acc: 0.2731\n",
      "loss: 2.0987558364868164, train acc: 0.2793\n",
      "loss: 2.106299567222595, train acc: 0.2782\n",
      "loss: 2.124040651321411, train acc: 0.281\n",
      "epoch: 33, loss: 2.0914394855499268, train acc: 0.281, test acc: 0.263\n",
      "loss: 2.1468346118927, train acc: 0.2785\n",
      "loss: 2.128170347213745, train acc: 0.2702\n",
      "loss: 2.0960280418396, train acc: 0.2749\n",
      "loss: 2.130200815200806, train acc: 0.2789\n",
      "loss: 2.131787872314453, train acc: 0.2758\n",
      "loss: 2.1204711198806763, train acc: 0.2853\n",
      "loss: 2.167638421058655, train acc: 0.2691\n",
      "loss: 2.132328653335571, train acc: 0.278\n",
      "epoch: 34, loss: 2.0934274196624756, train acc: 0.278, test acc: 0.2532\n",
      "loss: 2.0874500274658203, train acc: 0.2757\n",
      "loss: 2.100067687034607, train acc: 0.2751\n",
      "loss: 2.1089427709579467, train acc: 0.2685\n",
      "loss: 2.1218849658966064, train acc: 0.2768\n",
      "loss: 2.1213966608047485, train acc: 0.2813\n",
      "loss: 2.1440483570098876, train acc: 0.2809\n",
      "loss: 2.1259483814239504, train acc: 0.2813\n",
      "loss: 2.1307924032211303, train acc: 0.2781\n",
      "epoch: 35, loss: 2.092961311340332, train acc: 0.2781, test acc: 0.2581\n",
      "loss: 2.1436855792999268, train acc: 0.2793\n",
      "loss: 2.1250651597976686, train acc: 0.271\n",
      "loss: 2.1225104570388793, train acc: 0.2744\n",
      "loss: 2.1204232215881347, train acc: 0.2737\n",
      "loss: 2.1394796848297117, train acc: 0.2755\n",
      "loss: 2.092228388786316, train acc: 0.28\n",
      "loss: 2.1474899530410765, train acc: 0.27\n",
      "loss: 2.131906509399414, train acc: 0.2792\n",
      "epoch: 36, loss: 2.1537575721740723, train acc: 0.2792, test acc: 0.2665\n",
      "loss: 2.1567223072052, train acc: 0.28\n",
      "loss: 2.1313036680221558, train acc: 0.28\n",
      "loss: 2.1221765518188476, train acc: 0.2755\n",
      "loss: 2.1159419059753417, train acc: 0.2786\n",
      "loss: 2.0992912530899046, train acc: 0.2797\n",
      "loss: 2.1325774669647215, train acc: 0.2845\n",
      "loss: 2.136978507041931, train acc: 0.2862\n",
      "loss: 2.11182119846344, train acc: 0.2789\n",
      "epoch: 37, loss: 2.264984130859375, train acc: 0.2789, test acc: 0.2613\n",
      "loss: 2.1390585899353027, train acc: 0.2797\n",
      "loss: 2.1171450853347777, train acc: 0.2791\n",
      "loss: 2.0897338151931764, train acc: 0.2773\n",
      "loss: 2.1043126821517943, train acc: 0.2805\n",
      "loss: 2.1278486013412476, train acc: 0.2836\n",
      "loss: 2.1008597731590273, train acc: 0.2843\n",
      "loss: 2.1209668159484862, train acc: 0.2855\n",
      "loss: 2.1248720407485964, train acc: 0.2912\n",
      "epoch: 38, loss: 1.9943945407867432, train acc: 0.2912, test acc: 0.2663\n",
      "loss: 2.0981383323669434, train acc: 0.2861\n",
      "loss: 2.0976280212402343, train acc: 0.2746\n",
      "loss: 2.090959000587463, train acc: 0.2796\n",
      "loss: 2.125438618659973, train acc: 0.2651\n",
      "loss: 2.101415824890137, train acc: 0.2828\n",
      "loss: 2.1234364032745363, train acc: 0.2869\n",
      "loss: 2.1200873613357545, train acc: 0.2834\n",
      "loss: 2.1223431825637817, train acc: 0.2863\n",
      "epoch: 39, loss: 1.9982651472091675, train acc: 0.2863, test acc: 0.2692\n",
      "loss: 2.085583209991455, train acc: 0.2894\n",
      "loss: 2.1479433298110964, train acc: 0.2822\n",
      "loss: 2.112245965003967, train acc: 0.2874\n",
      "loss: 2.1264945030212403, train acc: 0.2877\n",
      "loss: 2.1163675427436828, train acc: 0.2752\n",
      "loss: 2.1173102855682373, train acc: 0.2858\n",
      "loss: 2.1323794603347777, train acc: 0.2832\n",
      "loss: 2.1251678466796875, train acc: 0.2835\n",
      "epoch: 40, loss: 2.1575751304626465, train acc: 0.2835, test acc: 0.2662\n",
      "loss: 2.0373878479003906, train acc: 0.2882\n",
      "loss: 2.1154229402542115, train acc: 0.2806\n",
      "loss: 2.116547393798828, train acc: 0.2783\n",
      "loss: 2.1313326835632322, train acc: 0.2849\n",
      "loss: 2.0992210268974305, train acc: 0.2877\n",
      "loss: 2.105264735221863, train acc: 0.2888\n",
      "loss: 2.140331268310547, train acc: 0.2903\n",
      "loss: 2.1420623302459716, train acc: 0.284\n",
      "epoch: 41, loss: 2.3384461402893066, train acc: 0.284, test acc: 0.2664\n",
      "loss: 2.1214966773986816, train acc: 0.2812\n",
      "loss: 2.1290239810943605, train acc: 0.2858\n",
      "loss: 2.1136510610580443, train acc: 0.2838\n",
      "loss: 2.115779757499695, train acc: 0.2823\n",
      "loss: 2.10096390247345, train acc: 0.2826\n",
      "loss: 2.1100627422332763, train acc: 0.2896\n",
      "loss: 2.1160152196884154, train acc: 0.2914\n",
      "loss: 2.1336089372634888, train acc: 0.292\n",
      "epoch: 42, loss: 2.1819679737091064, train acc: 0.292, test acc: 0.2786\n",
      "loss: 2.077089309692383, train acc: 0.2929\n",
      "loss: 2.1215974569320677, train acc: 0.2889\n",
      "loss: 2.1232723951339723, train acc: 0.2889\n",
      "loss: 2.108996295928955, train acc: 0.2866\n",
      "loss: 2.1364339351654054, train acc: 0.2874\n",
      "loss: 2.1034756183624266, train acc: 0.293\n",
      "loss: 2.132769012451172, train acc: 0.2905\n",
      "loss: 2.12951021194458, train acc: 0.2775\n",
      "epoch: 43, loss: 2.1520113945007324, train acc: 0.2775, test acc: 0.2677\n",
      "loss: 2.102389097213745, train acc: 0.2795\n",
      "loss: 2.1212563037872316, train acc: 0.2854\n",
      "loss: 2.112487030029297, train acc: 0.2833\n",
      "loss: 2.1327590703964234, train acc: 0.2732\n",
      "loss: 2.1270662546157837, train acc: 0.2639\n",
      "loss: 2.133181405067444, train acc: 0.2824\n",
      "loss: 2.1363542556762694, train acc: 0.2873\n",
      "loss: 2.1105905771255493, train acc: 0.2783\n",
      "epoch: 44, loss: 2.0790929794311523, train acc: 0.2783, test acc: 0.2717\n",
      "loss: 2.0790209770202637, train acc: 0.2884\n",
      "loss: 2.1039831399917603, train acc: 0.286\n",
      "loss: 2.138662576675415, train acc: 0.2864\n",
      "loss: 2.1010114908218385, train acc: 0.2873\n",
      "loss: 2.1055653572082518, train acc: 0.2906\n",
      "loss: 2.0930512666702272, train acc: 0.2876\n",
      "loss: 2.1120489597320558, train acc: 0.2878\n",
      "loss: 2.1125970125198363, train acc: 0.2759\n",
      "epoch: 45, loss: 1.8670384883880615, train acc: 0.2759, test acc: 0.268\n",
      "loss: 2.0612802505493164, train acc: 0.2857\n",
      "loss: 2.120432639122009, train acc: 0.2793\n",
      "loss: 2.1123244524002076, train acc: 0.2855\n",
      "loss: 2.145813775062561, train acc: 0.2841\n",
      "loss: 2.1019762992858886, train acc: 0.2815\n",
      "loss: 2.1064071893692016, train acc: 0.2911\n",
      "loss: 2.12713360786438, train acc: 0.2874\n",
      "loss: 2.138223099708557, train acc: 0.284\n",
      "epoch: 46, loss: 1.907233715057373, train acc: 0.284, test acc: 0.2684\n",
      "loss: 2.0773799419403076, train acc: 0.2881\n",
      "loss: 2.0793659687042236, train acc: 0.2887\n",
      "loss: 2.129453444480896, train acc: 0.2836\n",
      "loss: 2.1198201417922973, train acc: 0.2887\n",
      "loss: 2.106803822517395, train acc: 0.2887\n",
      "loss: 2.0914718389511107, train acc: 0.2911\n",
      "loss: 2.10964457988739, train acc: 0.2894\n",
      "loss: 2.1253906726837157, train acc: 0.2813\n",
      "epoch: 47, loss: 2.036024332046509, train acc: 0.2813, test acc: 0.2711\n",
      "loss: 2.0968148708343506, train acc: 0.2916\n",
      "loss: 2.119178056716919, train acc: 0.2803\n",
      "loss: 2.1175830364227295, train acc: 0.2885\n",
      "loss: 2.1343550205230715, train acc: 0.2818\n",
      "loss: 2.104004693031311, train acc: 0.2788\n",
      "loss: 2.1195964097976683, train acc: 0.2847\n",
      "loss: 2.136950159072876, train acc: 0.2851\n",
      "loss: 2.12196626663208, train acc: 0.2591\n",
      "epoch: 48, loss: 2.2155890464782715, train acc: 0.2591, test acc: 0.2727\n",
      "loss: 2.080167293548584, train acc: 0.2842\n",
      "loss: 2.1190905570983887, train acc: 0.287\n",
      "loss: 2.0986112117767335, train acc: 0.283\n",
      "loss: 2.1077409267425535, train acc: 0.2877\n",
      "loss: 2.1160897254943847, train acc: 0.2881\n",
      "loss: 2.115064191818237, train acc: 0.2895\n",
      "loss: 2.107388806343079, train acc: 0.2912\n",
      "loss: 2.123834419250488, train acc: 0.2862\n",
      "epoch: 49, loss: 2.0604467391967773, train acc: 0.2862, test acc: 0.2656\n",
      "loss: 2.0566611289978027, train acc: 0.288\n",
      "loss: 2.112771248817444, train acc: 0.2755\n",
      "loss: 2.103626847267151, train acc: 0.2911\n",
      "loss: 2.1166616916656493, train acc: 0.2925\n",
      "loss: 2.1088553190231325, train acc: 0.2743\n",
      "loss: 2.0917547702789308, train acc: 0.2937\n",
      "loss: 2.143542981147766, train acc: 0.2742\n",
      "loss: 2.1069036960601806, train acc: 0.2892\n",
      "epoch: 50, loss: 2.041701316833496, train acc: 0.2892, test acc: 0.2615\n",
      "loss: 2.112807273864746, train acc: 0.2868\n",
      "loss: 2.121387505531311, train acc: 0.2872\n",
      "loss: 2.074125361442566, train acc: 0.2899\n",
      "loss: 2.087294602394104, train acc: 0.2886\n",
      "loss: 2.130509090423584, train acc: 0.2813\n",
      "loss: 2.0954857587814333, train acc: 0.2856\n",
      "loss: 2.1127341985702515, train acc: 0.2846\n",
      "loss: 2.1076070785522463, train acc: 0.2955\n",
      "epoch: 51, loss: 2.2436599731445312, train acc: 0.2955, test acc: 0.2608\n",
      "loss: 2.0689613819122314, train acc: 0.2829\n",
      "loss: 2.0894974946975706, train acc: 0.2906\n",
      "loss: 2.0803433775901796, train acc: 0.2862\n",
      "loss: 2.119645428657532, train acc: 0.285\n",
      "loss: 2.1040090560913085, train acc: 0.2901\n",
      "loss: 2.1085007429122924, train acc: 0.2929\n",
      "loss: 2.09968044757843, train acc: 0.2907\n",
      "loss: 2.1391732692718506, train acc: 0.2852\n",
      "epoch: 52, loss: 2.1368353366851807, train acc: 0.2852, test acc: 0.2648\n",
      "loss: 2.089359760284424, train acc: 0.2948\n",
      "loss: 2.1232070207595823, train acc: 0.2906\n",
      "loss: 2.091237926483154, train acc: 0.291\n",
      "loss: 2.1302624225616453, train acc: 0.2904\n",
      "loss: 2.09363648891449, train acc: 0.2896\n",
      "loss: 2.1034029960632323, train acc: 0.293\n",
      "loss: 2.117014837265015, train acc: 0.2933\n",
      "loss: 2.1410855531692503, train acc: 0.2672\n",
      "epoch: 53, loss: 1.7756866216659546, train acc: 0.2672, test acc: 0.2673\n",
      "loss: 2.0688862800598145, train acc: 0.2936\n",
      "loss: 2.1025628566741945, train acc: 0.2935\n",
      "loss: 2.102520728111267, train acc: 0.2768\n",
      "loss: 2.0970211744308473, train acc: 0.2907\n",
      "loss: 2.1028959274291994, train acc: 0.2871\n",
      "loss: 2.0955755710601807, train acc: 0.2888\n",
      "loss: 2.1329044103622437, train acc: 0.2847\n",
      "loss: 2.089638185501099, train acc: 0.2876\n",
      "epoch: 54, loss: 2.0659432411193848, train acc: 0.2876, test acc: 0.2657\n",
      "loss: 2.1068456172943115, train acc: 0.2907\n",
      "loss: 2.113621139526367, train acc: 0.2871\n",
      "loss: 2.0897348165512084, train acc: 0.2888\n",
      "loss: 2.1085731983184814, train acc: 0.2887\n",
      "loss: 2.1097236394882204, train acc: 0.2847\n",
      "loss: 2.086109495162964, train acc: 0.2912\n",
      "loss: 2.113631319999695, train acc: 0.2865\n",
      "loss: 2.10829952955246, train acc: 0.2915\n",
      "epoch: 55, loss: 2.1196815967559814, train acc: 0.2915, test acc: 0.2633\n",
      "loss: 2.106832265853882, train acc: 0.2928\n",
      "loss: 2.1160815954208374, train acc: 0.2876\n",
      "loss: 2.126139521598816, train acc: 0.2841\n",
      "loss: 2.1082823991775514, train acc: 0.2883\n",
      "loss: 2.0718724489212037, train acc: 0.2907\n",
      "loss: 2.1161628484725954, train acc: 0.286\n",
      "loss: 2.1125590562820435, train acc: 0.2925\n",
      "loss: 2.1108388662338258, train acc: 0.2924\n",
      "epoch: 56, loss: 1.7947502136230469, train acc: 0.2924, test acc: 0.2652\n",
      "loss: 2.0887081623077393, train acc: 0.2904\n",
      "loss: 2.090785491466522, train acc: 0.2912\n",
      "loss: 2.102075958251953, train acc: 0.2912\n",
      "loss: 2.126790499687195, train acc: 0.2894\n",
      "loss: 2.108936309814453, train acc: 0.2897\n",
      "loss: 2.104153776168823, train acc: 0.2873\n",
      "loss: 2.1435070514678953, train acc: 0.2899\n",
      "loss: 2.118749761581421, train acc: 0.2886\n",
      "epoch: 57, loss: 2.0943078994750977, train acc: 0.2886, test acc: 0.2623\n",
      "loss: 2.1559813022613525, train acc: 0.2788\n",
      "loss: 2.1020947217941286, train acc: 0.289\n",
      "loss: 2.0859827995300293, train acc: 0.2878\n",
      "loss: 2.131132650375366, train acc: 0.2784\n",
      "loss: 2.0921454668045043, train acc: 0.2849\n",
      "loss: 2.1094441413879395, train acc: 0.2833\n",
      "loss: 2.1371506452560425, train acc: 0.2937\n",
      "loss: 2.1327630043029786, train acc: 0.2748\n",
      "epoch: 58, loss: 2.2701468467712402, train acc: 0.2748, test acc: 0.2642\n",
      "loss: 2.065156936645508, train acc: 0.2877\n",
      "loss: 2.0942972898483276, train acc: 0.2906\n",
      "loss: 2.1080698490142824, train acc: 0.289\n",
      "loss: 2.1266670703887938, train acc: 0.29\n",
      "loss: 2.097741985321045, train acc: 0.2846\n",
      "loss: 2.096877598762512, train acc: 0.2851\n",
      "loss: 2.149558091163635, train acc: 0.284\n",
      "loss: 2.097627592086792, train acc: 0.2862\n",
      "epoch: 59, loss: 2.316227436065674, train acc: 0.2862, test acc: 0.2677\n",
      "loss: 2.126640558242798, train acc: 0.2886\n",
      "loss: 2.100143551826477, train acc: 0.2915\n",
      "loss: 2.0876415491104128, train acc: 0.2911\n",
      "loss: 2.107643175125122, train acc: 0.2901\n",
      "loss: 2.1133691310882567, train acc: 0.2928\n",
      "loss: 2.1066561341285706, train acc: 0.2924\n",
      "loss: 2.1284260749816895, train acc: 0.2902\n",
      "loss: 2.1361059188842773, train acc: 0.2752\n",
      "epoch: 60, loss: 1.9882603883743286, train acc: 0.2752, test acc: 0.2673\n",
      "loss: 2.0483779907226562, train acc: 0.2842\n",
      "loss: 2.0933242559432985, train acc: 0.2867\n",
      "loss: 2.132505249977112, train acc: 0.2822\n",
      "loss: 2.121985673904419, train acc: 0.2893\n",
      "loss: 2.1351479291915894, train acc: 0.287\n",
      "loss: 2.1016631603240965, train acc: 0.2892\n",
      "loss: 2.1222777128219605, train acc: 0.2921\n",
      "loss: 2.113319492340088, train acc: 0.2823\n",
      "epoch: 61, loss: 2.014585494995117, train acc: 0.2823, test acc: 0.2721\n",
      "loss: 2.1224570274353027, train acc: 0.2943\n",
      "loss: 2.108615255355835, train acc: 0.2916\n",
      "loss: 2.096399736404419, train acc: 0.2943\n",
      "loss: 2.105915331840515, train acc: 0.2927\n",
      "loss: 2.118076753616333, train acc: 0.2808\n",
      "loss: 2.1027989625930785, train acc: 0.2887\n",
      "loss: 2.1331029415130613, train acc: 0.2898\n",
      "loss: 2.135947418212891, train acc: 0.2827\n",
      "epoch: 62, loss: 2.0526275634765625, train acc: 0.2827, test acc: 0.268\n",
      "loss: 2.0881197452545166, train acc: 0.2887\n",
      "loss: 2.0913971424102784, train acc: 0.288\n",
      "loss: 2.1217105388641357, train acc: 0.2892\n",
      "loss: 2.101981782913208, train acc: 0.2832\n",
      "loss: 2.0933533906936646, train acc: 0.285\n",
      "loss: 2.1043079137802123, train acc: 0.2862\n",
      "loss: 2.103399705886841, train acc: 0.2794\n",
      "loss: 2.110579085350037, train acc: 0.2897\n",
      "epoch: 63, loss: 1.981969952583313, train acc: 0.2897, test acc: 0.2623\n",
      "loss: 2.0931246280670166, train acc: 0.2815\n",
      "loss: 2.1034671545028685, train acc: 0.2875\n",
      "loss: 2.0943800687789915, train acc: 0.288\n",
      "loss: 2.1242984771728515, train acc: 0.2884\n",
      "loss: 2.1130267143249513, train acc: 0.2905\n",
      "loss: 2.0762473344802856, train acc: 0.2896\n",
      "loss: 2.1326183319091796, train acc: 0.2929\n",
      "loss: 2.090390753746033, train acc: 0.2878\n",
      "epoch: 64, loss: 2.1457693576812744, train acc: 0.2878, test acc: 0.2515\n",
      "loss: 2.1571788787841797, train acc: 0.277\n",
      "loss: 2.093223810195923, train acc: 0.2884\n",
      "loss: 2.078241300582886, train acc: 0.2912\n",
      "loss: 2.106869173049927, train acc: 0.2894\n",
      "loss: 2.105728006362915, train acc: 0.2867\n",
      "loss: 2.1154473543167116, train acc: 0.2837\n",
      "loss: 2.130291700363159, train acc: 0.2756\n",
      "loss: 2.1323655605316163, train acc: 0.2876\n",
      "epoch: 65, loss: 2.089092493057251, train acc: 0.2876, test acc: 0.2622\n",
      "loss: 2.1360602378845215, train acc: 0.2806\n",
      "loss: 2.109204339981079, train acc: 0.2918\n",
      "loss: 2.0962797164916993, train acc: 0.2893\n",
      "loss: 2.0927255630493162, train acc: 0.2874\n",
      "loss: 2.11503427028656, train acc: 0.2873\n",
      "loss: 2.108932685852051, train acc: 0.2902\n",
      "loss: 2.144994854927063, train acc: 0.2863\n",
      "loss: 2.0895720720291138, train acc: 0.289\n",
      "epoch: 66, loss: 2.0389418601989746, train acc: 0.289, test acc: 0.2611\n",
      "loss: 2.076169967651367, train acc: 0.2808\n",
      "loss: 2.102828359603882, train acc: 0.2905\n",
      "loss: 2.0978472948074343, train acc: 0.2914\n",
      "loss: 2.102083516120911, train acc: 0.2925\n",
      "loss: 2.1015169620513916, train acc: 0.2927\n",
      "loss: 2.0832756042480467, train acc: 0.2862\n",
      "loss: 2.1256971836090086, train acc: 0.2926\n",
      "loss: 2.091675615310669, train acc: 0.2926\n",
      "epoch: 67, loss: 2.0087547302246094, train acc: 0.2926, test acc: 0.2671\n",
      "loss: 2.0393664836883545, train acc: 0.2921\n",
      "loss: 2.0802728652954103, train acc: 0.2922\n",
      "loss: 2.0902387619018556, train acc: 0.2919\n",
      "loss: 2.1169360876083374, train acc: 0.2772\n",
      "loss: 2.107195019721985, train acc: 0.2894\n",
      "loss: 2.1029786586761476, train acc: 0.2876\n",
      "loss: 2.1098302364349366, train acc: 0.2857\n",
      "loss: 2.135834550857544, train acc: 0.2876\n",
      "epoch: 68, loss: 2.003002882003784, train acc: 0.2876, test acc: 0.2534\n",
      "loss: 2.064685821533203, train acc: 0.2767\n",
      "loss: 2.0799148321151733, train acc: 0.2876\n",
      "loss: 2.099757218360901, train acc: 0.2887\n",
      "loss: 2.099072289466858, train acc: 0.2854\n",
      "loss: 2.0773945331573485, train acc: 0.2903\n",
      "loss: 2.1089155197143556, train acc: 0.2917\n",
      "loss: 2.121824264526367, train acc: 0.2879\n",
      "loss: 2.1191869258880613, train acc: 0.2918\n",
      "epoch: 69, loss: 2.179456949234009, train acc: 0.2918, test acc: 0.2581\n",
      "loss: 2.079878807067871, train acc: 0.286\n",
      "loss: 2.1139450550079344, train acc: 0.2837\n",
      "loss: 2.1032185554504395, train acc: 0.2888\n",
      "loss: 2.0909384727478026, train acc: 0.2911\n",
      "loss: 2.0849703311920167, train acc: 0.2888\n",
      "loss: 2.093820595741272, train acc: 0.2954\n",
      "loss: 2.0984283447265626, train acc: 0.2799\n",
      "loss: 2.1240151166915893, train acc: 0.2881\n",
      "epoch: 70, loss: 2.1662189960479736, train acc: 0.2881, test acc: 0.2627\n",
      "loss: 2.0294840335845947, train acc: 0.2772\n",
      "loss: 2.106747269630432, train acc: 0.2801\n",
      "loss: 2.111818957328796, train acc: 0.2834\n",
      "loss: 2.12614369392395, train acc: 0.2837\n",
      "loss: 2.0767151355743407, train acc: 0.2851\n",
      "loss: 2.1110573530197145, train acc: 0.2853\n",
      "loss: 2.1054970979690553, train acc: 0.2862\n",
      "loss: 2.1092309236526487, train acc: 0.29\n",
      "epoch: 71, loss: 2.2911150455474854, train acc: 0.29, test acc: 0.2714\n",
      "loss: 1.9982914924621582, train acc: 0.294\n",
      "loss: 2.1166722059249876, train acc: 0.2827\n",
      "loss: 2.091236782073975, train acc: 0.2926\n",
      "loss: 2.125413250923157, train acc: 0.2904\n",
      "loss: 2.1227147817611693, train acc: 0.2671\n",
      "loss: 2.115331196784973, train acc: 0.2825\n",
      "loss: 2.1300052404403687, train acc: 0.2751\n",
      "loss: 2.117053198814392, train acc: 0.2894\n",
      "epoch: 72, loss: 2.041821241378784, train acc: 0.2894, test acc: 0.2595\n",
      "loss: 2.1081037521362305, train acc: 0.2815\n",
      "loss: 2.110202980041504, train acc: 0.289\n",
      "loss: 2.1162408113479616, train acc: 0.2878\n",
      "loss: 2.1034604787826536, train acc: 0.2864\n",
      "loss: 2.11106538772583, train acc: 0.2887\n",
      "loss: 2.0784438371658327, train acc: 0.2875\n",
      "loss: 2.0900871992111205, train acc: 0.2883\n",
      "loss: 2.125150442123413, train acc: 0.2753\n",
      "epoch: 73, loss: 2.2453370094299316, train acc: 0.2753, test acc: 0.2668\n",
      "loss: 2.110621929168701, train acc: 0.2873\n",
      "loss: 2.1110549688339235, train acc: 0.2882\n",
      "loss: 2.0827849388122557, train acc: 0.289\n",
      "loss: 2.105716896057129, train acc: 0.2907\n",
      "loss: 2.1108238220214846, train acc: 0.2827\n",
      "loss: 2.1129879474639894, train acc: 0.2901\n",
      "loss: 2.1278672218322754, train acc: 0.2874\n",
      "loss: 2.110157144069672, train acc: 0.2901\n",
      "epoch: 74, loss: 1.8551982641220093, train acc: 0.2901, test acc: 0.2703\n",
      "loss: 2.048849105834961, train acc: 0.2907\n",
      "loss: 2.0970266103744506, train acc: 0.2881\n",
      "loss: 2.1182923316955566, train acc: 0.2806\n",
      "loss: 2.108709192276001, train acc: 0.2893\n",
      "loss: 2.101836371421814, train acc: 0.2891\n",
      "loss: 2.093259501457214, train acc: 0.2897\n",
      "loss: 2.1455764055252073, train acc: 0.2777\n",
      "loss: 2.112178325653076, train acc: 0.2913\n",
      "epoch: 75, loss: 1.75596284866333, train acc: 0.2913, test acc: 0.2681\n",
      "loss: 2.0504636764526367, train acc: 0.2872\n",
      "loss: 2.081538677215576, train acc: 0.2845\n",
      "loss: 2.0972854137420653, train acc: 0.29\n",
      "loss: 2.1024209022521974, train acc: 0.2916\n",
      "loss: 2.0951417207717897, train acc: 0.2854\n",
      "loss: 2.0622488021850587, train acc: 0.291\n",
      "loss: 2.147146725654602, train acc: 0.2883\n",
      "loss: 2.114934277534485, train acc: 0.2868\n",
      "epoch: 76, loss: 2.0013554096221924, train acc: 0.2868, test acc: 0.273\n",
      "loss: 2.0546891689300537, train acc: 0.2912\n",
      "loss: 2.0842205286026, train acc: 0.2888\n",
      "loss: 2.112223410606384, train acc: 0.2886\n",
      "loss: 2.107674407958984, train acc: 0.2897\n",
      "loss: 2.108939027786255, train acc: 0.2933\n",
      "loss: 2.128944969177246, train acc: 0.2894\n",
      "loss: 2.1211178064346314, train acc: 0.2947\n",
      "loss: 2.1285348176956176, train acc: 0.2777\n",
      "epoch: 77, loss: 2.341458320617676, train acc: 0.2777, test acc: 0.2686\n",
      "loss: 2.101008653640747, train acc: 0.293\n",
      "loss: 2.0802132844924928, train acc: 0.2932\n",
      "loss: 2.099358654022217, train acc: 0.291\n",
      "loss: 2.090453267097473, train acc: 0.2914\n",
      "loss: 2.089057779312134, train acc: 0.2887\n",
      "loss: 2.102541995048523, train acc: 0.2888\n",
      "loss: 2.105116295814514, train acc: 0.2856\n",
      "loss: 2.1277854204177857, train acc: 0.2877\n",
      "epoch: 78, loss: 2.195495843887329, train acc: 0.2877, test acc: 0.256\n",
      "loss: 2.063941478729248, train acc: 0.2767\n",
      "loss: 2.1058002710342407, train acc: 0.2895\n",
      "loss: 2.096564841270447, train acc: 0.2876\n",
      "loss: 2.1066073179244995, train acc: 0.2885\n",
      "loss: 2.104808473587036, train acc: 0.2866\n",
      "loss: 2.109042453765869, train acc: 0.2866\n",
      "loss: 2.1180837631225584, train acc: 0.2855\n",
      "loss: 2.125384306907654, train acc: 0.2772\n",
      "epoch: 79, loss: 1.8034757375717163, train acc: 0.2772, test acc: 0.2694\n",
      "#####training and testing end with K:1, P:0.5######\n",
      "#####training and testing start with K:1, P:1######\n",
      "loss: 2.3868095874786377, train acc: 0.0914\n",
      "loss: 2.313207006454468, train acc: 0.1082\n",
      "loss: 2.2514162063598633, train acc: 0.1338\n",
      "loss: 2.2348825931549072, train acc: 0.1653\n",
      "loss: 2.1667858362197876, train acc: 0.1858\n",
      "loss: 2.1686363935470583, train acc: 0.1955\n",
      "loss: 2.1540786743164064, train acc: 0.2015\n",
      "loss: 2.1520496368408204, train acc: 0.195\n",
      "epoch: 0, loss: 2.238375186920166, train acc: 0.195, test acc: 0.1957\n",
      "loss: 2.117048501968384, train acc: 0.1961\n",
      "loss: 2.1068026065826415, train acc: 0.1874\n",
      "loss: 2.119226574897766, train acc: 0.1855\n",
      "loss: 2.142471957206726, train acc: 0.1884\n",
      "loss: 2.0852306127548217, train acc: 0.1911\n",
      "loss: 2.095634627342224, train acc: 0.1877\n",
      "loss: 2.0930233716964723, train acc: 0.1906\n",
      "loss: 2.0992377281188963, train acc: 0.1879\n",
      "epoch: 1, loss: 2.1919500827789307, train acc: 0.1879, test acc: 0.1902\n",
      "loss: 2.0594804286956787, train acc: 0.1908\n",
      "loss: 2.0555576205253603, train acc: 0.1866\n",
      "loss: 2.0742804169654847, train acc: 0.1842\n",
      "loss: 2.0990988492965696, train acc: 0.1884\n",
      "loss: 2.0449766755104064, train acc: 0.1877\n",
      "loss: 2.0520987153053283, train acc: 0.1869\n",
      "loss: 2.053917717933655, train acc: 0.1857\n",
      "loss: 2.0617804765701293, train acc: 0.1854\n",
      "epoch: 2, loss: 2.1358509063720703, train acc: 0.1854, test acc: 0.1888\n",
      "loss: 2.010303497314453, train acc: 0.1899\n",
      "loss: 2.0171168088912963, train acc: 0.1827\n",
      "loss: 2.0413985013961793, train acc: 0.1837\n",
      "loss: 2.0624047756195067, train acc: 0.1871\n",
      "loss: 2.011874759197235, train acc: 0.1857\n",
      "loss: 2.016314244270325, train acc: 0.1844\n",
      "loss: 2.0203501701354982, train acc: 0.1823\n",
      "loss: 2.0297135949134826, train acc: 0.1856\n",
      "epoch: 3, loss: 2.0789577960968018, train acc: 0.1856, test acc: 0.1861\n",
      "loss: 1.9619184732437134, train acc: 0.1877\n",
      "loss: 1.9827736020088196, train acc: 0.182\n",
      "loss: 2.0140026211738586, train acc: 0.1824\n",
      "loss: 2.031059467792511, train acc: 0.1846\n",
      "loss: 1.9852147340774535, train acc: 0.1836\n",
      "loss: 1.9864066004753114, train acc: 0.184\n",
      "loss: 1.992421305179596, train acc: 0.1777\n",
      "loss: 2.003127360343933, train acc: 0.1859\n",
      "epoch: 4, loss: 2.0282835960388184, train acc: 0.1859, test acc: 0.1869\n",
      "loss: 1.9246630668640137, train acc: 0.1876\n",
      "loss: 1.9554573178291321, train acc: 0.1811\n",
      "loss: 1.9920044660568237, train acc: 0.1808\n",
      "loss: 2.006140744686127, train acc: 0.1834\n",
      "loss: 1.9631546139717102, train acc: 0.1816\n",
      "loss: 1.96141619682312, train acc: 0.2154\n",
      "loss: 1.9691859483718872, train acc: 0.2096\n",
      "loss: 1.9803708434104919, train acc: 0.2158\n",
      "epoch: 5, loss: 1.998491644859314, train acc: 0.2158, test acc: 0.2176\n",
      "loss: 1.89480459690094, train acc: 0.2149\n",
      "loss: 1.9321670293807984, train acc: 0.2098\n",
      "loss: 1.9739072918891907, train acc: 0.2126\n",
      "loss: 1.985448944568634, train acc: 0.2132\n",
      "loss: 1.9445391416549682, train acc: 0.2154\n",
      "loss: 1.9409110069274902, train acc: 0.216\n",
      "loss: 1.95054931640625, train acc: 0.2121\n",
      "loss: 1.9611756801605225, train acc: 0.2169\n",
      "epoch: 6, loss: 1.9758714437484741, train acc: 0.2169, test acc: 0.219\n",
      "loss: 1.869897723197937, train acc: 0.2171\n",
      "loss: 1.9131260514259338, train acc: 0.2093\n",
      "loss: 1.9593560814857482, train acc: 0.215\n",
      "loss: 1.9683281183242798, train acc: 0.2158\n",
      "loss: 1.9268282651901245, train acc: 0.2172\n",
      "loss: 1.922693383693695, train acc: 0.2182\n",
      "loss: 1.932738184928894, train acc: 0.2148\n",
      "loss: 1.942462646961212, train acc: 0.218\n",
      "epoch: 7, loss: 1.954099178314209, train acc: 0.218, test acc: 0.2283\n",
      "loss: 1.8496161699295044, train acc: 0.2213\n",
      "loss: 1.8948970556259155, train acc: 0.2173\n",
      "loss: 1.9452433824539184, train acc: 0.2208\n",
      "loss: 1.9514627933502198, train acc: 0.2213\n",
      "loss: 1.9113434910774232, train acc: 0.2198\n",
      "loss: 1.9072561144828797, train acc: 0.2178\n",
      "loss: 1.91771000623703, train acc: 0.2185\n",
      "loss: 1.9250908970832825, train acc: 0.2229\n",
      "epoch: 8, loss: 1.9369312524795532, train acc: 0.2229, test acc: 0.2309\n",
      "loss: 1.8307609558105469, train acc: 0.2245\n",
      "loss: 1.8775084853172301, train acc: 0.2192\n",
      "loss: 1.9313002467155456, train acc: 0.2234\n",
      "loss: 1.9352909922599792, train acc: 0.226\n",
      "loss: 1.8951034545898438, train acc: 0.2229\n",
      "loss: 1.8909251689910889, train acc: 0.222\n",
      "loss: 1.9032097458839417, train acc: 0.2232\n",
      "loss: 1.908334457874298, train acc: 0.2269\n",
      "epoch: 9, loss: 1.927062749862671, train acc: 0.2269, test acc: 0.2336\n",
      "loss: 1.8120267391204834, train acc: 0.2306\n",
      "loss: 1.859208333492279, train acc: 0.2227\n",
      "loss: 1.9160995125770568, train acc: 0.2273\n",
      "loss: 1.9196953415870666, train acc: 0.2276\n",
      "loss: 1.8790226817131042, train acc: 0.2254\n",
      "loss: 1.8756428837776185, train acc: 0.2241\n",
      "loss: 1.8883111953735352, train acc: 0.2262\n",
      "loss: 1.8912383198738099, train acc: 0.2319\n",
      "epoch: 10, loss: 1.9179449081420898, train acc: 0.2319, test acc: 0.2319\n",
      "loss: 1.7963550090789795, train acc: 0.2321\n",
      "loss: 1.8420357942581176, train acc: 0.2272\n",
      "loss: 1.9004589796066285, train acc: 0.2294\n",
      "loss: 1.902740716934204, train acc: 0.231\n",
      "loss: 1.8640722990036012, train acc: 0.2299\n",
      "loss: 1.8614381790161132, train acc: 0.2287\n",
      "loss: 1.8743306875228882, train acc: 0.2317\n",
      "loss: 1.8739426732063293, train acc: 0.2324\n",
      "epoch: 11, loss: 1.8921799659729004, train acc: 0.2324, test acc: 0.2322\n",
      "loss: 1.7818371057510376, train acc: 0.2361\n",
      "loss: 1.825736975669861, train acc: 0.2282\n",
      "loss: 1.885507583618164, train acc: 0.2333\n",
      "loss: 1.8868244886398315, train acc: 0.2348\n",
      "loss: 1.8497950792312623, train acc: 0.2342\n",
      "loss: 1.8463712811470032, train acc: 0.2293\n",
      "loss: 1.8608393311500548, train acc: 0.2318\n",
      "loss: 1.8576867818832397, train acc: 0.2357\n",
      "epoch: 12, loss: 1.874617338180542, train acc: 0.2357, test acc: 0.2334\n",
      "loss: 1.765736699104309, train acc: 0.2367\n",
      "loss: 1.8099683403968811, train acc: 0.2275\n",
      "loss: 1.8701749205589295, train acc: 0.235\n",
      "loss: 1.8718532562255858, train acc: 0.2342\n",
      "loss: 1.8368647813796997, train acc: 0.2374\n",
      "loss: 1.8319114923477173, train acc: 0.2312\n",
      "loss: 1.8470356345176697, train acc: 0.2306\n",
      "loss: 1.8434563398361206, train acc: 0.2402\n",
      "epoch: 13, loss: 1.8582744598388672, train acc: 0.2402, test acc: 0.2385\n",
      "loss: 1.7484023571014404, train acc: 0.2405\n",
      "loss: 1.7924418330192566, train acc: 0.2309\n",
      "loss: 1.8532361388206482, train acc: 0.2407\n",
      "loss: 1.8563994288444519, train acc: 0.2391\n",
      "loss: 1.8249462604522706, train acc: 0.2408\n",
      "loss: 1.816750741004944, train acc: 0.2376\n",
      "loss: 1.8348460912704467, train acc: 0.2335\n",
      "loss: 1.8299668073654174, train acc: 0.2408\n",
      "epoch: 14, loss: 1.8478448390960693, train acc: 0.2408, test acc: 0.2423\n",
      "loss: 1.7330377101898193, train acc: 0.2449\n",
      "loss: 1.7775622725486755, train acc: 0.235\n",
      "loss: 1.837550973892212, train acc: 0.2434\n",
      "loss: 1.843609380722046, train acc: 0.2445\n",
      "loss: 1.8117174744606017, train acc: 0.2449\n",
      "loss: 1.80268235206604, train acc: 0.2428\n",
      "loss: 1.8219488382339477, train acc: 0.2383\n",
      "loss: 1.8157758116722107, train acc: 0.2446\n",
      "epoch: 15, loss: 1.8420319557189941, train acc: 0.2446, test acc: 0.2436\n",
      "loss: 1.7158554792404175, train acc: 0.2503\n",
      "loss: 1.7627431988716125, train acc: 0.2391\n",
      "loss: 1.822045350074768, train acc: 0.2487\n",
      "loss: 1.8295806527137757, train acc: 0.2493\n",
      "loss: 1.801384699344635, train acc: 0.2501\n",
      "loss: 1.788931632041931, train acc: 0.2451\n",
      "loss: 1.807987356185913, train acc: 0.2435\n",
      "loss: 1.8000480771064757, train acc: 0.2445\n",
      "epoch: 16, loss: 1.8175694942474365, train acc: 0.2445, test acc: 0.2496\n",
      "loss: 1.7003718614578247, train acc: 0.2547\n",
      "loss: 1.7498950839042664, train acc: 0.244\n",
      "loss: 1.8095656514167786, train acc: 0.2559\n",
      "loss: 1.817048692703247, train acc: 0.254\n",
      "loss: 1.7910044074058533, train acc: 0.2556\n",
      "loss: 1.7775512337684631, train acc: 0.2486\n",
      "loss: 1.7954123616218567, train acc: 0.2479\n",
      "loss: 1.7874191403388977, train acc: 0.2471\n",
      "epoch: 17, loss: 1.8046975135803223, train acc: 0.2471, test acc: 0.2524\n",
      "loss: 1.6888978481292725, train acc: 0.256\n",
      "loss: 1.7382428526878357, train acc: 0.2479\n",
      "loss: 1.7987563967704774, train acc: 0.2584\n",
      "loss: 1.8053497552871705, train acc: 0.2595\n",
      "loss: 1.7812734603881837, train acc: 0.2606\n",
      "loss: 1.7666294813156127, train acc: 0.252\n",
      "loss: 1.7850968956947326, train acc: 0.2514\n",
      "loss: 1.7762640476226808, train acc: 0.2535\n",
      "epoch: 18, loss: 1.796384572982788, train acc: 0.2535, test acc: 0.2531\n",
      "loss: 1.6765130758285522, train acc: 0.2585\n",
      "loss: 1.7280861377716064, train acc: 0.2534\n",
      "loss: 1.7881149411201478, train acc: 0.2624\n",
      "loss: 1.7952649474143982, train acc: 0.2631\n",
      "loss: 1.7722641348838806, train acc: 0.2642\n",
      "loss: 1.7561297059059142, train acc: 0.2557\n",
      "loss: 1.7748486638069152, train acc: 0.2522\n",
      "loss: 1.764039695262909, train acc: 0.2556\n",
      "epoch: 19, loss: 1.783904790878296, train acc: 0.2556, test acc: 0.2567\n",
      "loss: 1.6660419702529907, train acc: 0.2614\n",
      "loss: 1.7169565081596374, train acc: 0.2569\n",
      "loss: 1.7772193431854248, train acc: 0.2653\n",
      "loss: 1.7855515122413634, train acc: 0.2664\n",
      "loss: 1.7632995009422303, train acc: 0.2657\n",
      "loss: 1.7472912192344665, train acc: 0.2602\n",
      "loss: 1.766292357444763, train acc: 0.2542\n",
      "loss: 1.7525039672851563, train acc: 0.2605\n",
      "epoch: 20, loss: 1.7703626155853271, train acc: 0.2605, test acc: 0.2727\n",
      "loss: 1.6541838645935059, train acc: 0.2677\n",
      "loss: 1.7063100934028625, train acc: 0.2613\n",
      "loss: 1.767594563961029, train acc: 0.2686\n",
      "loss: 1.7769815802574158, train acc: 0.2707\n",
      "loss: 1.7542866945266724, train acc: 0.27\n",
      "loss: 1.7381948351860046, train acc: 0.2633\n",
      "loss: 1.7568033456802368, train acc: 0.2552\n",
      "loss: 1.7420092225074768, train acc: 0.2676\n",
      "epoch: 21, loss: 1.758375883102417, train acc: 0.2676, test acc: 0.2752\n",
      "loss: 1.6432563066482544, train acc: 0.2707\n",
      "loss: 1.6974010825157166, train acc: 0.2678\n",
      "loss: 1.7580500602722169, train acc: 0.2716\n",
      "loss: 1.767658817768097, train acc: 0.276\n",
      "loss: 1.7443100452423095, train acc: 0.2751\n",
      "loss: 1.7299678921699524, train acc: 0.2678\n",
      "loss: 1.7482934951782227, train acc: 0.26\n",
      "loss: 1.7318288445472718, train acc: 0.2711\n",
      "epoch: 22, loss: 1.744176983833313, train acc: 0.2711, test acc: 0.2788\n",
      "loss: 1.6318788528442383, train acc: 0.2733\n",
      "loss: 1.6896985650062561, train acc: 0.2697\n",
      "loss: 1.7495175838470458, train acc: 0.2749\n",
      "loss: 1.7584250450134278, train acc: 0.2805\n",
      "loss: 1.7358693122863769, train acc: 0.2792\n",
      "loss: 1.7214814186096192, train acc: 0.2703\n",
      "loss: 1.7405653715133667, train acc: 0.2618\n",
      "loss: 1.7228828072547913, train acc: 0.2732\n",
      "epoch: 23, loss: 1.7324037551879883, train acc: 0.2732, test acc: 0.2797\n",
      "loss: 1.6220238208770752, train acc: 0.2754\n",
      "loss: 1.6817726135253905, train acc: 0.2729\n",
      "loss: 1.7420311212539672, train acc: 0.2779\n",
      "loss: 1.7503602862358094, train acc: 0.2848\n",
      "loss: 1.7279266595840455, train acc: 0.28\n",
      "loss: 1.713333547115326, train acc: 0.2741\n",
      "loss: 1.7337785363197327, train acc: 0.2624\n",
      "loss: 1.7149467587471008, train acc: 0.2757\n",
      "epoch: 24, loss: 1.7225511074066162, train acc: 0.2757, test acc: 0.2812\n",
      "loss: 1.612555742263794, train acc: 0.2794\n",
      "loss: 1.6746053099632263, train acc: 0.2745\n",
      "loss: 1.7349113941192627, train acc: 0.2806\n",
      "loss: 1.7423348665237426, train acc: 0.2846\n",
      "loss: 1.7210109233856201, train acc: 0.284\n",
      "loss: 1.7054773688316345, train acc: 0.2768\n",
      "loss: 1.726207721233368, train acc: 0.2639\n",
      "loss: 1.7080918073654174, train acc: 0.278\n",
      "epoch: 25, loss: 1.7151304483413696, train acc: 0.278, test acc: 0.283\n",
      "loss: 1.6045615673065186, train acc: 0.2818\n",
      "loss: 1.6677387714385987, train acc: 0.2757\n",
      "loss: 1.7281269550323486, train acc: 0.2831\n",
      "loss: 1.7351367950439454, train acc: 0.2874\n",
      "loss: 1.7137837052345275, train acc: 0.2858\n",
      "loss: 1.6988300800323486, train acc: 0.2795\n",
      "loss: 1.7202579617500304, train acc: 0.2654\n",
      "loss: 1.7027780652046203, train acc: 0.2832\n",
      "epoch: 26, loss: 1.709747314453125, train acc: 0.2832, test acc: 0.2855\n",
      "loss: 1.5967806577682495, train acc: 0.2824\n",
      "loss: 1.6610916376113891, train acc: 0.2783\n",
      "loss: 1.7211872696876527, train acc: 0.2843\n",
      "loss: 1.7273937344551087, train acc: 0.2887\n",
      "loss: 1.707601547241211, train acc: 0.2874\n",
      "loss: 1.6916953802108765, train acc: 0.2813\n",
      "loss: 1.7135180234909058, train acc: 0.2697\n",
      "loss: 1.6962535500526428, train acc: 0.2843\n",
      "epoch: 27, loss: 1.7026606798171997, train acc: 0.2843, test acc: 0.2873\n",
      "loss: 1.5898449420928955, train acc: 0.2863\n",
      "loss: 1.6538332223892211, train acc: 0.2816\n",
      "loss: 1.7151040077209472, train acc: 0.2858\n",
      "loss: 1.7215780019760132, train acc: 0.2908\n",
      "loss: 1.7014110803604126, train acc: 0.2901\n",
      "loss: 1.6860313773155213, train acc: 0.2857\n",
      "loss: 1.70803302526474, train acc: 0.273\n",
      "loss: 1.6905942678451538, train acc: 0.2895\n",
      "epoch: 28, loss: 1.6973012685775757, train acc: 0.2895, test acc: 0.2908\n",
      "loss: 1.5832066535949707, train acc: 0.2897\n",
      "loss: 1.6480213284492493, train acc: 0.2854\n",
      "loss: 1.7098035216331482, train acc: 0.2883\n",
      "loss: 1.715251624584198, train acc: 0.2949\n",
      "loss: 1.695986771583557, train acc: 0.2943\n",
      "loss: 1.6801204800605773, train acc: 0.2877\n",
      "loss: 1.701841366291046, train acc: 0.2785\n",
      "loss: 1.6834911823272705, train acc: 0.2922\n",
      "epoch: 29, loss: 1.692036747932434, train acc: 0.2922, test acc: 0.2922\n",
      "loss: 1.5778409242630005, train acc: 0.2915\n",
      "loss: 1.6426585078239442, train acc: 0.2904\n",
      "loss: 1.7040502548217773, train acc: 0.2922\n",
      "loss: 1.7102020382881165, train acc: 0.2973\n",
      "loss: 1.6909592747688293, train acc: 0.2948\n",
      "loss: 1.6743599772453308, train acc: 0.2895\n",
      "loss: 1.6967405557632447, train acc: 0.2799\n",
      "loss: 1.678435707092285, train acc: 0.2951\n",
      "epoch: 30, loss: 1.688597559928894, train acc: 0.2951, test acc: 0.2936\n",
      "loss: 1.5713691711425781, train acc: 0.2937\n",
      "loss: 1.6379576802253724, train acc: 0.2926\n",
      "loss: 1.6993112683296203, train acc: 0.2935\n",
      "loss: 1.704556941986084, train acc: 0.3002\n",
      "loss: 1.6861976623535155, train acc: 0.2988\n",
      "loss: 1.6693013906478882, train acc: 0.2906\n",
      "loss: 1.6918375611305236, train acc: 0.2822\n",
      "loss: 1.6729252934455872, train acc: 0.2962\n",
      "epoch: 31, loss: 1.6828536987304688, train acc: 0.2962, test acc: 0.296\n",
      "loss: 1.5656459331512451, train acc: 0.2968\n",
      "loss: 1.6327001452445984, train acc: 0.2925\n",
      "loss: 1.6948211550712586, train acc: 0.2969\n",
      "loss: 1.7002105712890625, train acc: 0.3016\n",
      "loss: 1.6816251516342162, train acc: 0.3002\n",
      "loss: 1.6645482063293457, train acc: 0.2933\n",
      "loss: 1.6872544884681702, train acc: 0.2827\n",
      "loss: 1.668262243270874, train acc: 0.2991\n",
      "epoch: 32, loss: 1.6806867122650146, train acc: 0.2991, test acc: 0.2887\n",
      "loss: 1.5590039491653442, train acc: 0.298\n",
      "loss: 1.6279379606246949, train acc: 0.2941\n",
      "loss: 1.6901068806648254, train acc: 0.2977\n",
      "loss: 1.6953705430030823, train acc: 0.3035\n",
      "loss: 1.6774583816528321, train acc: 0.3024\n",
      "loss: 1.6599714636802674, train acc: 0.2934\n",
      "loss: 1.6826774597167968, train acc: 0.2857\n",
      "loss: 1.6635647535324096, train acc: 0.2983\n",
      "epoch: 33, loss: 1.6772814989089966, train acc: 0.2983, test acc: 0.2907\n",
      "loss: 1.5546495914459229, train acc: 0.3002\n",
      "loss: 1.6239789485931397, train acc: 0.3002\n",
      "loss: 1.6866044521331787, train acc: 0.3054\n",
      "loss: 1.691211426258087, train acc: 0.3037\n",
      "loss: 1.6736145257949828, train acc: 0.3057\n",
      "loss: 1.6568952441215514, train acc: 0.2959\n",
      "loss: 1.6781371355056762, train acc: 0.2884\n",
      "loss: 1.659275734424591, train acc: 0.3012\n",
      "epoch: 34, loss: 1.6720683574676514, train acc: 0.3012, test acc: 0.2927\n",
      "loss: 1.549534559249878, train acc: 0.3034\n",
      "loss: 1.6197688102722168, train acc: 0.3018\n",
      "loss: 1.6820380091667175, train acc: 0.3075\n",
      "loss: 1.6867687821388244, train acc: 0.3065\n",
      "loss: 1.6698419451713562, train acc: 0.3067\n",
      "loss: 1.6529152393341064, train acc: 0.2961\n",
      "loss: 1.6739298582077027, train acc: 0.2907\n",
      "loss: 1.654974138736725, train acc: 0.3025\n",
      "epoch: 35, loss: 1.6691685914993286, train acc: 0.3025, test acc: 0.2943\n",
      "loss: 1.5447239875793457, train acc: 0.3044\n",
      "loss: 1.6162020683288574, train acc: 0.3024\n",
      "loss: 1.6782289266586303, train acc: 0.3089\n",
      "loss: 1.6825021624565124, train acc: 0.3076\n",
      "loss: 1.666316521167755, train acc: 0.3096\n",
      "loss: 1.649398183822632, train acc: 0.298\n",
      "loss: 1.6698812603950501, train acc: 0.3006\n",
      "loss: 1.6507745027542113, train acc: 0.3043\n",
      "epoch: 36, loss: 1.661034345626831, train acc: 0.3043, test acc: 0.295\n",
      "loss: 1.5410603284835815, train acc: 0.3053\n",
      "loss: 1.611939263343811, train acc: 0.3049\n",
      "loss: 1.6748769998550415, train acc: 0.3093\n",
      "loss: 1.6773028135299684, train acc: 0.3184\n",
      "loss: 1.6636609196662904, train acc: 0.3099\n",
      "loss: 1.6453993201255799, train acc: 0.3014\n",
      "loss: 1.6674513339996337, train acc: 0.3039\n",
      "loss: 1.6470139741897583, train acc: 0.3052\n",
      "epoch: 37, loss: 1.6404318809509277, train acc: 0.3052, test acc: 0.2966\n",
      "loss: 1.5361326932907104, train acc: 0.3058\n",
      "loss: 1.6086993575096131, train acc: 0.3048\n",
      "loss: 1.6706796884536743, train acc: 0.3106\n",
      "loss: 1.6742135047912599, train acc: 0.3195\n",
      "loss: 1.659959352016449, train acc: 0.312\n",
      "loss: 1.641983449459076, train acc: 0.3018\n",
      "loss: 1.6645357728004455, train acc: 0.303\n",
      "loss: 1.6434629678726196, train acc: 0.3062\n",
      "epoch: 38, loss: 1.6299771070480347, train acc: 0.3062, test acc: 0.2984\n",
      "loss: 1.5325721502304077, train acc: 0.3074\n",
      "loss: 1.6054269790649414, train acc: 0.3073\n",
      "loss: 1.6670909762382506, train acc: 0.3123\n",
      "loss: 1.6710088729858399, train acc: 0.3214\n",
      "loss: 1.6561774015426636, train acc: 0.3129\n",
      "loss: 1.6385127902030945, train acc: 0.3135\n",
      "loss: 1.6615561485290526, train acc: 0.3038\n",
      "loss: 1.639814841747284, train acc: 0.3077\n",
      "epoch: 39, loss: 1.6234406232833862, train acc: 0.3077, test acc: 0.301\n",
      "loss: 1.5287302732467651, train acc: 0.3107\n",
      "loss: 1.6023727893829345, train acc: 0.3073\n",
      "loss: 1.66330988407135, train acc: 0.3163\n",
      "loss: 1.6681716680526733, train acc: 0.3228\n",
      "loss: 1.6530617356300354, train acc: 0.3259\n",
      "loss: 1.6356608748435975, train acc: 0.3161\n",
      "loss: 1.6589584469795227, train acc: 0.3044\n",
      "loss: 1.6366055727005004, train acc: 0.3088\n",
      "epoch: 40, loss: 1.6199995279312134, train acc: 0.3088, test acc: 0.3027\n",
      "loss: 1.5252882242202759, train acc: 0.3125\n",
      "loss: 1.5994009852409363, train acc: 0.3093\n",
      "loss: 1.660033631324768, train acc: 0.3167\n",
      "loss: 1.6659035086631775, train acc: 0.3222\n",
      "loss: 1.649706733226776, train acc: 0.326\n",
      "loss: 1.633090853691101, train acc: 0.3161\n",
      "loss: 1.65614994764328, train acc: 0.3057\n",
      "loss: 1.6337030529975891, train acc: 0.3104\n",
      "epoch: 41, loss: 1.615844488143921, train acc: 0.3104, test acc: 0.3058\n",
      "loss: 1.5227535963058472, train acc: 0.3146\n",
      "loss: 1.5966332793235778, train acc: 0.3129\n",
      "loss: 1.6568264603614806, train acc: 0.3182\n",
      "loss: 1.6633975028991699, train acc: 0.3234\n",
      "loss: 1.6463199138641358, train acc: 0.3283\n",
      "loss: 1.6306275010108948, train acc: 0.3187\n",
      "loss: 1.654976522922516, train acc: 0.3078\n",
      "loss: 1.6305367827415467, train acc: 0.3135\n",
      "epoch: 42, loss: 1.61015784740448, train acc: 0.3135, test acc: 0.3077\n",
      "loss: 1.5200459957122803, train acc: 0.3164\n",
      "loss: 1.5939204335212707, train acc: 0.3143\n",
      "loss: 1.6539011120796203, train acc: 0.3198\n",
      "loss: 1.6609760403633118, train acc: 0.3241\n",
      "loss: 1.643152153491974, train acc: 0.3301\n",
      "loss: 1.627703559398651, train acc: 0.3192\n",
      "loss: 1.651983368396759, train acc: 0.3097\n",
      "loss: 1.6270904660224914, train acc: 0.3148\n",
      "epoch: 43, loss: 1.606257438659668, train acc: 0.3148, test acc: 0.3097\n",
      "loss: 1.517600178718567, train acc: 0.3186\n",
      "loss: 1.591496968269348, train acc: 0.3173\n",
      "loss: 1.6509190917015075, train acc: 0.3215\n",
      "loss: 1.659144365787506, train acc: 0.3258\n",
      "loss: 1.6397567510604858, train acc: 0.3296\n",
      "loss: 1.6251564860343932, train acc: 0.3304\n",
      "loss: 1.6497185945510864, train acc: 0.3218\n",
      "loss: 1.624553143978119, train acc: 0.3251\n",
      "epoch: 44, loss: 1.6021373271942139, train acc: 0.3251, test acc: 0.3114\n",
      "loss: 1.5152182579040527, train acc: 0.3203\n",
      "loss: 1.5891578316688537, train acc: 0.317\n",
      "loss: 1.6486661672592162, train acc: 0.3218\n",
      "loss: 1.6564977645874024, train acc: 0.336\n",
      "loss: 1.6372180581092834, train acc: 0.3392\n",
      "loss: 1.6223603010177612, train acc: 0.3309\n",
      "loss: 1.6473014950752258, train acc: 0.3251\n",
      "loss: 1.6217486023902894, train acc: 0.3275\n",
      "epoch: 45, loss: 1.596909523010254, train acc: 0.3275, test acc: 0.3122\n",
      "loss: 1.5107073783874512, train acc: 0.3323\n",
      "loss: 1.58711678981781, train acc: 0.3318\n",
      "loss: 1.6461440801620484, train acc: 0.3328\n",
      "loss: 1.6545501708984376, train acc: 0.338\n",
      "loss: 1.6347208738327026, train acc: 0.3397\n",
      "loss: 1.6197537899017334, train acc: 0.3311\n",
      "loss: 1.6444168210029602, train acc: 0.3258\n",
      "loss: 1.6182140707969666, train acc: 0.3299\n",
      "epoch: 46, loss: 1.5931917428970337, train acc: 0.3299, test acc: 0.3141\n",
      "loss: 1.507886290550232, train acc: 0.3334\n",
      "loss: 1.584803318977356, train acc: 0.332\n",
      "loss: 1.6434702396392822, train acc: 0.335\n",
      "loss: 1.651885175704956, train acc: 0.3415\n",
      "loss: 1.6322505712509154, train acc: 0.3421\n",
      "loss: 1.6172232627868652, train acc: 0.3331\n",
      "loss: 1.6418935537338257, train acc: 0.3268\n",
      "loss: 1.615466797351837, train acc: 0.3312\n",
      "epoch: 47, loss: 1.5907902717590332, train acc: 0.3312, test acc: 0.3149\n",
      "loss: 1.505245327949524, train acc: 0.3351\n",
      "loss: 1.5832852244377136, train acc: 0.3349\n",
      "loss: 1.64029700756073, train acc: 0.337\n",
      "loss: 1.6500218391418457, train acc: 0.3445\n",
      "loss: 1.6297056436538697, train acc: 0.3439\n",
      "loss: 1.6145789861679076, train acc: 0.3346\n",
      "loss: 1.6392117738723755, train acc: 0.3286\n",
      "loss: 1.6126681566238403, train acc: 0.3327\n",
      "epoch: 48, loss: 1.5873420238494873, train acc: 0.3327, test acc: 0.3157\n",
      "loss: 1.5029653310775757, train acc: 0.3351\n",
      "loss: 1.5811540365219117, train acc: 0.3356\n",
      "loss: 1.6372771263122559, train acc: 0.3375\n",
      "loss: 1.648203158378601, train acc: 0.3446\n",
      "loss: 1.627359139919281, train acc: 0.3465\n",
      "loss: 1.6118964076042175, train acc: 0.3354\n",
      "loss: 1.637410831451416, train acc: 0.3279\n",
      "loss: 1.6104017615318298, train acc: 0.3353\n",
      "epoch: 49, loss: 1.5841609239578247, train acc: 0.3353, test acc: 0.317\n",
      "loss: 1.5006654262542725, train acc: 0.3351\n",
      "loss: 1.5790667414665223, train acc: 0.3364\n",
      "loss: 1.6347377061843873, train acc: 0.3385\n",
      "loss: 1.6465181827545166, train acc: 0.3456\n",
      "loss: 1.6254175782203675, train acc: 0.3485\n",
      "loss: 1.6099045038223267, train acc: 0.3367\n",
      "loss: 1.6355170726776123, train acc: 0.3287\n",
      "loss: 1.6081951379776, train acc: 0.3371\n",
      "epoch: 50, loss: 1.582087516784668, train acc: 0.3371, test acc: 0.318\n",
      "loss: 1.498986005783081, train acc: 0.338\n",
      "loss: 1.5771397948265076, train acc: 0.3404\n",
      "loss: 1.6322024941444397, train acc: 0.3401\n",
      "loss: 1.644753336906433, train acc: 0.3462\n",
      "loss: 1.6234201550483705, train acc: 0.35\n",
      "loss: 1.6078245162963867, train acc: 0.3386\n",
      "loss: 1.6332809448242187, train acc: 0.3311\n",
      "loss: 1.6057416558265687, train acc: 0.3376\n",
      "epoch: 51, loss: 1.5796875953674316, train acc: 0.3376, test acc: 0.3201\n",
      "loss: 1.4968538284301758, train acc: 0.3383\n",
      "loss: 1.5748056292533874, train acc: 0.3411\n",
      "loss: 1.6298885941505432, train acc: 0.3423\n",
      "loss: 1.6433222055435182, train acc: 0.3492\n",
      "loss: 1.6213862538337707, train acc: 0.3514\n",
      "loss: 1.6056849241256714, train acc: 0.3401\n",
      "loss: 1.6316465735435486, train acc: 0.3325\n",
      "loss: 1.6037009596824645, train acc: 0.3395\n",
      "epoch: 52, loss: 1.5776845216751099, train acc: 0.3395, test acc: 0.3216\n",
      "loss: 1.4949923753738403, train acc: 0.3399\n",
      "loss: 1.5728037118911744, train acc: 0.3426\n",
      "loss: 1.6276191830635072, train acc: 0.344\n",
      "loss: 1.6417562365531921, train acc: 0.3503\n",
      "loss: 1.6195654273033142, train acc: 0.3522\n",
      "loss: 1.6033334612846375, train acc: 0.3423\n",
      "loss: 1.629719579219818, train acc: 0.3353\n",
      "loss: 1.601331317424774, train acc: 0.3402\n",
      "epoch: 53, loss: 1.5768907070159912, train acc: 0.3402, test acc: 0.3235\n",
      "loss: 1.4931868314743042, train acc: 0.3404\n",
      "loss: 1.5708415389060975, train acc: 0.345\n",
      "loss: 1.6254490494728089, train acc: 0.3458\n",
      "loss: 1.639962899684906, train acc: 0.3519\n",
      "loss: 1.6175480723381042, train acc: 0.3536\n",
      "loss: 1.6011814594268798, train acc: 0.3443\n",
      "loss: 1.6276045203208924, train acc: 0.3377\n",
      "loss: 1.599363386631012, train acc: 0.3413\n",
      "epoch: 54, loss: 1.5755513906478882, train acc: 0.3413, test acc: 0.3244\n",
      "loss: 1.491686224937439, train acc: 0.343\n",
      "loss: 1.5690919756889343, train acc: 0.3468\n",
      "loss: 1.6231683492660522, train acc: 0.3472\n",
      "loss: 1.6386051654815674, train acc: 0.352\n",
      "loss: 1.6159399509429933, train acc: 0.3546\n",
      "loss: 1.6009955406188965, train acc: 0.346\n",
      "loss: 1.6253185510635375, train acc: 0.3418\n",
      "loss: 1.59682457447052, train acc: 0.3442\n",
      "epoch: 55, loss: 1.574652075767517, train acc: 0.3442, test acc: 0.329\n",
      "loss: 1.4899511337280273, train acc: 0.3449\n",
      "loss: 1.5676118016242981, train acc: 0.3459\n",
      "loss: 1.6213424801826477, train acc: 0.3494\n",
      "loss: 1.636808955669403, train acc: 0.3533\n",
      "loss: 1.6133284211158752, train acc: 0.355\n",
      "loss: 1.5979616403579713, train acc: 0.3466\n",
      "loss: 1.6242422342300415, train acc: 0.3441\n",
      "loss: 1.595007848739624, train acc: 0.3466\n",
      "epoch: 56, loss: 1.574670672416687, train acc: 0.3466, test acc: 0.3294\n",
      "loss: 1.4888094663619995, train acc: 0.3445\n",
      "loss: 1.5660595417022705, train acc: 0.349\n",
      "loss: 1.61969074010849, train acc: 0.3533\n",
      "loss: 1.635625410079956, train acc: 0.3563\n",
      "loss: 1.610999596118927, train acc: 0.3575\n",
      "loss: 1.5956627011299134, train acc: 0.35\n",
      "loss: 1.6231963872909545, train acc: 0.3449\n",
      "loss: 1.5928380370140076, train acc: 0.3473\n",
      "epoch: 57, loss: 1.573677897453308, train acc: 0.3473, test acc: 0.3293\n",
      "loss: 1.4862078428268433, train acc: 0.3464\n",
      "loss: 1.565126633644104, train acc: 0.3501\n",
      "loss: 1.617443311214447, train acc: 0.3543\n",
      "loss: 1.6335156440734864, train acc: 0.3594\n",
      "loss: 1.6080196857452393, train acc: 0.359\n",
      "loss: 1.5930440068244933, train acc: 0.3507\n",
      "loss: 1.6215169072151183, train acc: 0.3486\n",
      "loss: 1.5909659147262574, train acc: 0.3497\n",
      "epoch: 58, loss: 1.5721439123153687, train acc: 0.3497, test acc: 0.3329\n",
      "loss: 1.484184980392456, train acc: 0.3498\n",
      "loss: 1.5630979180335998, train acc: 0.3533\n",
      "loss: 1.615745484828949, train acc: 0.359\n",
      "loss: 1.632284653186798, train acc: 0.3617\n",
      "loss: 1.6053794741630554, train acc: 0.361\n",
      "loss: 1.5899003744125366, train acc: 0.3544\n",
      "loss: 1.6202984571456909, train acc: 0.3499\n",
      "loss: 1.5891137719154358, train acc: 0.3527\n",
      "epoch: 59, loss: 1.5723148584365845, train acc: 0.3527, test acc: 0.3365\n",
      "loss: 1.4824347496032715, train acc: 0.3536\n",
      "loss: 1.5608372330665587, train acc: 0.3554\n",
      "loss: 1.6140314579010009, train acc: 0.3622\n",
      "loss: 1.6307821989059448, train acc: 0.3645\n",
      "loss: 1.60298193693161, train acc: 0.3645\n",
      "loss: 1.5879907011985779, train acc: 0.3565\n",
      "loss: 1.6188381552696227, train acc: 0.3531\n",
      "loss: 1.5875485301017762, train acc: 0.3557\n",
      "epoch: 60, loss: 1.5708959102630615, train acc: 0.3557, test acc: 0.34\n",
      "loss: 1.4811540842056274, train acc: 0.3581\n",
      "loss: 1.557077169418335, train acc: 0.3586\n",
      "loss: 1.6123433709144592, train acc: 0.3633\n",
      "loss: 1.6281549334526062, train acc: 0.366\n",
      "loss: 1.6002407789230346, train acc: 0.3668\n",
      "loss: 1.5864229679107666, train acc: 0.3593\n",
      "loss: 1.6156790256500244, train acc: 0.3559\n",
      "loss: 1.5856578826904297, train acc: 0.3568\n",
      "epoch: 61, loss: 1.570568323135376, train acc: 0.3568, test acc: 0.3451\n",
      "loss: 1.4787768125534058, train acc: 0.3599\n",
      "loss: 1.5533827543258667, train acc: 0.3629\n",
      "loss: 1.6104039549827576, train acc: 0.367\n",
      "loss: 1.624940526485443, train acc: 0.3702\n",
      "loss: 1.597427988052368, train acc: 0.37\n",
      "loss: 1.584425413608551, train acc: 0.3628\n",
      "loss: 1.614211118221283, train acc: 0.3578\n",
      "loss: 1.5839625239372253, train acc: 0.3607\n",
      "epoch: 62, loss: 1.569930076599121, train acc: 0.3607, test acc: 0.3486\n",
      "loss: 1.4753848314285278, train acc: 0.3636\n",
      "loss: 1.5501765847206115, train acc: 0.3671\n",
      "loss: 1.608333909511566, train acc: 0.3695\n",
      "loss: 1.6226613402366639, train acc: 0.3744\n",
      "loss: 1.5950250267982482, train acc: 0.3744\n",
      "loss: 1.5816636204719543, train acc: 0.3659\n",
      "loss: 1.6123205423355103, train acc: 0.3615\n",
      "loss: 1.5817980766296387, train acc: 0.3624\n",
      "epoch: 63, loss: 1.569475769996643, train acc: 0.3624, test acc: 0.3537\n",
      "loss: 1.4725228548049927, train acc: 0.3668\n",
      "loss: 1.5465538501739502, train acc: 0.3701\n",
      "loss: 1.6066208243370057, train acc: 0.3737\n",
      "loss: 1.6203114628791808, train acc: 0.3773\n",
      "loss: 1.5925352454185486, train acc: 0.3783\n",
      "loss: 1.5795607328414918, train acc: 0.3689\n",
      "loss: 1.6091480731964112, train acc: 0.3624\n",
      "loss: 1.5801217198371886, train acc: 0.3669\n",
      "epoch: 64, loss: 1.5706400871276855, train acc: 0.3669, test acc: 0.3595\n",
      "loss: 1.4696272611618042, train acc: 0.3711\n",
      "loss: 1.5429040789604187, train acc: 0.3756\n",
      "loss: 1.6043865919113158, train acc: 0.3772\n",
      "loss: 1.6176230549812316, train acc: 0.3821\n",
      "loss: 1.5897902607917787, train acc: 0.3822\n",
      "loss: 1.57640141248703, train acc: 0.3747\n",
      "loss: 1.6067802786827088, train acc: 0.3657\n",
      "loss: 1.5771876573562622, train acc: 0.3715\n",
      "epoch: 65, loss: 1.5718719959259033, train acc: 0.3715, test acc: 0.3629\n",
      "loss: 1.4640510082244873, train acc: 0.377\n",
      "loss: 1.5379731893539428, train acc: 0.3798\n",
      "loss: 1.602182674407959, train acc: 0.3789\n",
      "loss: 1.6143901348114014, train acc: 0.3831\n",
      "loss: 1.5872299790382385, train acc: 0.3841\n",
      "loss: 1.5744065284729003, train acc: 0.3775\n",
      "loss: 1.6037544131278991, train acc: 0.3706\n",
      "loss: 1.5740392446517943, train acc: 0.3759\n",
      "epoch: 66, loss: 1.5724601745605469, train acc: 0.3759, test acc: 0.3684\n",
      "loss: 1.45740807056427, train acc: 0.3803\n",
      "loss: 1.533994233608246, train acc: 0.3845\n",
      "loss: 1.5992587447166442, train acc: 0.3835\n",
      "loss: 1.6109459280967713, train acc: 0.3864\n",
      "loss: 1.5839481830596924, train acc: 0.3882\n",
      "loss: 1.571084487438202, train acc: 0.3805\n",
      "loss: 1.6012860178947448, train acc: 0.3745\n",
      "loss: 1.5708366513252259, train acc: 0.3814\n",
      "epoch: 67, loss: 1.5747913122177124, train acc: 0.3814, test acc: 0.3714\n",
      "loss: 1.4488577842712402, train acc: 0.3822\n",
      "loss: 1.5297586441040039, train acc: 0.3879\n",
      "loss: 1.5966297745704652, train acc: 0.3844\n",
      "loss: 1.6070468306541443, train acc: 0.3905\n",
      "loss: 1.5798948407173157, train acc: 0.3892\n",
      "loss: 1.5674750566482545, train acc: 0.3825\n",
      "loss: 1.5984158396720887, train acc: 0.3771\n",
      "loss: 1.5672773480415345, train acc: 0.3835\n",
      "epoch: 68, loss: 1.5764774084091187, train acc: 0.3835, test acc: 0.3731\n",
      "loss: 1.4396629333496094, train acc: 0.3843\n",
      "loss: 1.5255486369132996, train acc: 0.3895\n",
      "loss: 1.5933958649635316, train acc: 0.3853\n",
      "loss: 1.6031803131103515, train acc: 0.3918\n",
      "loss: 1.5775708556175232, train acc: 0.3899\n",
      "loss: 1.565375804901123, train acc: 0.3821\n",
      "loss: 1.5937734842300415, train acc: 0.3809\n",
      "loss: 1.565234160423279, train acc: 0.3856\n",
      "epoch: 69, loss: 1.577669382095337, train acc: 0.3856, test acc: 0.376\n",
      "loss: 1.4326820373535156, train acc: 0.3867\n",
      "loss: 1.5213432788848877, train acc: 0.3898\n",
      "loss: 1.5904383540153504, train acc: 0.3878\n",
      "loss: 1.599413764476776, train acc: 0.3931\n",
      "loss: 1.573708951473236, train acc: 0.3911\n",
      "loss: 1.5622017145156861, train acc: 0.3858\n",
      "loss: 1.5911203622817993, train acc: 0.3818\n",
      "loss: 1.5624002695083619, train acc: 0.388\n",
      "epoch: 70, loss: 1.5796103477478027, train acc: 0.388, test acc: 0.3762\n",
      "loss: 1.4286977052688599, train acc: 0.3879\n",
      "loss: 1.5177195906639098, train acc: 0.3901\n",
      "loss: 1.5869853973388672, train acc: 0.3882\n",
      "loss: 1.5963095426559448, train acc: 0.3958\n",
      "loss: 1.5704137444496156, train acc: 0.3934\n",
      "loss: 1.5579361200332642, train acc: 0.3865\n",
      "loss: 1.5886417865753173, train acc: 0.3836\n",
      "loss: 1.5593299746513367, train acc: 0.3881\n",
      "epoch: 71, loss: 1.5804392099380493, train acc: 0.3881, test acc: 0.3784\n",
      "loss: 1.4255201816558838, train acc: 0.3894\n",
      "loss: 1.5143110275268554, train acc: 0.3913\n",
      "loss: 1.5839573383331298, train acc: 0.3889\n",
      "loss: 1.592898428440094, train acc: 0.397\n",
      "loss: 1.5670553684234618, train acc: 0.3939\n",
      "loss: 1.5549092292785645, train acc: 0.3885\n",
      "loss: 1.586133849620819, train acc: 0.3846\n",
      "loss: 1.5563876390457154, train acc: 0.3875\n",
      "epoch: 72, loss: 1.581288456916809, train acc: 0.3875, test acc: 0.3781\n",
      "loss: 1.4218147993087769, train acc: 0.3903\n",
      "loss: 1.5111124753952025, train acc: 0.3901\n",
      "loss: 1.5815497994422913, train acc: 0.3889\n",
      "loss: 1.5907473683357238, train acc: 0.3968\n",
      "loss: 1.564311146736145, train acc: 0.3935\n",
      "loss: 1.5517411589622498, train acc: 0.3876\n",
      "loss: 1.5844723105430603, train acc: 0.3835\n",
      "loss: 1.5539240002632142, train acc: 0.3882\n",
      "epoch: 73, loss: 1.5818835496902466, train acc: 0.3882, test acc: 0.3785\n",
      "loss: 1.4187215566635132, train acc: 0.392\n",
      "loss: 1.5087050437927245, train acc: 0.3931\n",
      "loss: 1.5786726832389832, train acc: 0.3902\n",
      "loss: 1.5890304684638976, train acc: 0.3965\n",
      "loss: 1.5622845888137817, train acc: 0.3926\n",
      "loss: 1.5493603587150573, train acc: 0.3873\n",
      "loss: 1.5821000814437867, train acc: 0.3833\n",
      "loss: 1.551337444782257, train acc: 0.3891\n",
      "epoch: 74, loss: 1.5807346105575562, train acc: 0.3891, test acc: 0.3783\n",
      "loss: 1.4152885675430298, train acc: 0.3933\n",
      "loss: 1.505643081665039, train acc: 0.391\n",
      "loss: 1.5764996767044068, train acc: 0.3896\n",
      "loss: 1.586077332496643, train acc: 0.3974\n",
      "loss: 1.5597304701805115, train acc: 0.3924\n",
      "loss: 1.5462507963180543, train acc: 0.3875\n",
      "loss: 1.5804526329040527, train acc: 0.3845\n",
      "loss: 1.5489485621452332, train acc: 0.3899\n",
      "epoch: 75, loss: 1.5796605348587036, train acc: 0.3899, test acc: 0.3788\n",
      "loss: 1.4124159812927246, train acc: 0.3942\n",
      "loss: 1.5032410979270936, train acc: 0.3923\n",
      "loss: 1.5745517134666442, train acc: 0.3901\n",
      "loss: 1.583381462097168, train acc: 0.3985\n",
      "loss: 1.5578769326210022, train acc: 0.3937\n",
      "loss: 1.543765413761139, train acc: 0.3884\n",
      "loss: 1.5789575099945068, train acc: 0.3862\n",
      "loss: 1.5466198682785035, train acc: 0.39\n",
      "epoch: 76, loss: 1.578667402267456, train acc: 0.39, test acc: 0.3795\n",
      "loss: 1.4102263450622559, train acc: 0.3943\n",
      "loss: 1.501069676876068, train acc: 0.3919\n",
      "loss: 1.5721980571746825, train acc: 0.3905\n",
      "loss: 1.5815818786621094, train acc: 0.397\n",
      "loss: 1.5556475639343261, train acc: 0.3927\n",
      "loss: 1.5418384194374084, train acc: 0.3874\n",
      "loss: 1.576943075656891, train acc: 0.3879\n",
      "loss: 1.544709360599518, train acc: 0.3896\n",
      "epoch: 77, loss: 1.5782221555709839, train acc: 0.3896, test acc: 0.3801\n",
      "loss: 1.4089851379394531, train acc: 0.3948\n",
      "loss: 1.4988442420959474, train acc: 0.3928\n",
      "loss: 1.571025562286377, train acc: 0.3898\n",
      "loss: 1.5792361855506898, train acc: 0.3968\n",
      "loss: 1.554458773136139, train acc: 0.3942\n",
      "loss: 1.5396010756492615, train acc: 0.3881\n",
      "loss: 1.575457227230072, train acc: 0.3875\n",
      "loss: 1.5427828073501586, train acc: 0.39\n",
      "epoch: 78, loss: 1.5775686502456665, train acc: 0.39, test acc: 0.3804\n",
      "loss: 1.4067790508270264, train acc: 0.3937\n",
      "loss: 1.4969413876533508, train acc: 0.3936\n",
      "loss: 1.5691965460777282, train acc: 0.3909\n",
      "loss: 1.5777696371078491, train acc: 0.3962\n",
      "loss: 1.5530143976211548, train acc: 0.3937\n",
      "loss: 1.5380817294120788, train acc: 0.3874\n",
      "loss: 1.5743363499641418, train acc: 0.3886\n",
      "loss: 1.5408425092697144, train acc: 0.391\n",
      "epoch: 79, loss: 1.5758923292160034, train acc: 0.391, test acc: 0.3803\n",
      "#####training and testing end with K:1, P:1######\n",
      "#####training and testing start with K:5, P:0.1######\n",
      "loss: 2.34517240524292, train acc: 0.1179\n",
      "loss: 2.2316099882125853, train acc: 0.2002\n",
      "loss: 2.0847570657730103, train acc: 0.208\n",
      "loss: 1.9820041060447693, train acc: 0.2852\n",
      "loss: 1.8515168666839599, train acc: 0.358\n",
      "loss: 1.8715549945831298, train acc: 0.4277\n",
      "loss: 1.7853596925735473, train acc: 0.4838\n",
      "loss: 1.7241458535194396, train acc: 0.5067\n",
      "epoch: 0, loss: 1.7287636995315552, train acc: 0.5067, test acc: 0.5125\n",
      "loss: 1.6394264698028564, train acc: 0.5142\n",
      "loss: 1.6417277693748473, train acc: 0.5219\n",
      "loss: 1.6270172834396361, train acc: 0.5346\n",
      "loss: 1.5926180005073547, train acc: 0.5473\n",
      "loss: 1.4923041224479676, train acc: 0.5551\n",
      "loss: 1.560834491252899, train acc: 0.5547\n",
      "loss: 1.4834267735481261, train acc: 0.5652\n",
      "loss: 1.4754333734512328, train acc: 0.5744\n",
      "epoch: 1, loss: 1.1366127729415894, train acc: 0.5744, test acc: 0.5821\n",
      "loss: 1.406098484992981, train acc: 0.5822\n",
      "loss: 1.4031115651130677, train acc: 0.605\n",
      "loss: 1.407664155960083, train acc: 0.6228\n",
      "loss: 1.4157142758369445, train acc: 0.6315\n",
      "loss: 1.3292297005653382, train acc: 0.6361\n",
      "loss: 1.3890567660331725, train acc: 0.6279\n",
      "loss: 1.3381116390228271, train acc: 0.6393\n",
      "loss: 1.3145157098770142, train acc: 0.6592\n",
      "epoch: 2, loss: 1.0491943359375, train acc: 0.6592, test acc: 0.6651\n",
      "loss: 1.2993863821029663, train acc: 0.6701\n",
      "loss: 1.266003954410553, train acc: 0.6745\n",
      "loss: 1.3070728302001953, train acc: 0.6884\n",
      "loss: 1.2701429963111877, train acc: 0.688\n",
      "loss: 1.2301998615264893, train acc: 0.6967\n",
      "loss: 1.2890340566635132, train acc: 0.6941\n",
      "loss: 1.2449741601943969, train acc: 0.7037\n",
      "loss: 1.2483380913734436, train acc: 0.7114\n",
      "epoch: 3, loss: 1.0045998096466064, train acc: 0.7114, test acc: 0.7025\n",
      "loss: 1.1856039762496948, train acc: 0.715\n",
      "loss: 1.1686084985733032, train acc: 0.7138\n",
      "loss: 1.2080443859100343, train acc: 0.7247\n",
      "loss: 1.2169778108596803, train acc: 0.7296\n",
      "loss: 1.1399381756782532, train acc: 0.7339\n",
      "loss: 1.220037055015564, train acc: 0.7301\n",
      "loss: 1.2007746934890746, train acc: 0.7325\n",
      "loss: 1.1657880187034606, train acc: 0.7405\n",
      "epoch: 4, loss: 0.8617708086967468, train acc: 0.7405, test acc: 0.7259\n",
      "loss: 1.2298704385757446, train acc: 0.7346\n",
      "loss: 1.1186888873577119, train acc: 0.7368\n",
      "loss: 1.1622334003448487, train acc: 0.7459\n",
      "loss: 1.1577593803405761, train acc: 0.752\n",
      "loss: 1.0947051167488098, train acc: 0.7497\n",
      "loss: 1.1594811797142028, train acc: 0.7474\n",
      "loss: 1.138456678390503, train acc: 0.7529\n",
      "loss: 1.1159623622894288, train acc: 0.7543\n",
      "epoch: 5, loss: 0.9146747589111328, train acc: 0.7543, test acc: 0.7409\n",
      "loss: 1.0921283960342407, train acc: 0.7519\n",
      "loss: 1.1012824892997741, train acc: 0.7558\n",
      "loss: 1.0979060053825378, train acc: 0.7601\n",
      "loss: 1.105911487340927, train acc: 0.7627\n",
      "loss: 1.06208593249321, train acc: 0.7637\n",
      "loss: 1.1561909675598145, train acc: 0.7603\n",
      "loss: 1.1244921743869782, train acc: 0.7631\n",
      "loss: 1.0742434024810792, train acc: 0.7663\n",
      "epoch: 6, loss: 0.8134337067604065, train acc: 0.7663, test acc: 0.748\n",
      "loss: 1.1448389291763306, train acc: 0.7637\n",
      "loss: 1.0757687926292419, train acc: 0.7698\n",
      "loss: 1.0492145419120789, train acc: 0.7688\n",
      "loss: 1.1334027409553529, train acc: 0.7753\n",
      "loss: 1.0660157382488251, train acc: 0.7748\n",
      "loss: 1.1225529909133911, train acc: 0.7673\n",
      "loss: 1.0692945003509522, train acc: 0.7705\n",
      "loss: 1.044416320323944, train acc: 0.7759\n",
      "epoch: 7, loss: 0.9765017628669739, train acc: 0.7759, test acc: 0.7592\n",
      "loss: 1.139593482017517, train acc: 0.7731\n",
      "loss: 1.0610885620117188, train acc: 0.7816\n",
      "loss: 1.0471324622631073, train acc: 0.7825\n",
      "loss: 1.0562455773353576, train acc: 0.785\n",
      "loss: 1.015138965845108, train acc: 0.7826\n",
      "loss: 1.0482104778289796, train acc: 0.776\n",
      "loss: 1.0822136282920838, train acc: 0.7798\n",
      "loss: 1.0317466795444488, train acc: 0.7859\n",
      "epoch: 8, loss: 0.538811981678009, train acc: 0.7859, test acc: 0.7595\n",
      "loss: 1.0357317924499512, train acc: 0.7788\n",
      "loss: 1.0067196309566497, train acc: 0.7849\n",
      "loss: 1.08071528673172, train acc: 0.7866\n",
      "loss: 1.0919289708137512, train acc: 0.7908\n",
      "loss: 0.976517528295517, train acc: 0.7911\n",
      "loss: 1.028514748811722, train acc: 0.785\n",
      "loss: 1.073736310005188, train acc: 0.7886\n",
      "loss: 1.0208267390727996, train acc: 0.7924\n",
      "epoch: 9, loss: 0.6777097582817078, train acc: 0.7924, test acc: 0.7671\n",
      "loss: 1.065416693687439, train acc: 0.7846\n",
      "loss: 1.035033929347992, train acc: 0.7949\n",
      "loss: 0.9858235895633698, train acc: 0.7922\n",
      "loss: 1.0420731902122498, train acc: 0.7968\n",
      "loss: 0.959607583284378, train acc: 0.7973\n",
      "loss: 1.0436817824840545, train acc: 0.7905\n",
      "loss: 1.0244500577449798, train acc: 0.7944\n",
      "loss: 1.0253764390945435, train acc: 0.7994\n",
      "epoch: 10, loss: 1.1138790845870972, train acc: 0.7994, test acc: 0.7721\n",
      "loss: 0.9893844723701477, train acc: 0.7947\n",
      "loss: 1.0091600358486175, train acc: 0.801\n",
      "loss: 0.9893902957439422, train acc: 0.7959\n",
      "loss: 1.0417303502559663, train acc: 0.8006\n",
      "loss: 0.9825723111629486, train acc: 0.8\n",
      "loss: 1.0533223390579223, train acc: 0.7998\n",
      "loss: 1.0139631867408752, train acc: 0.8007\n",
      "loss: 1.0011555075645446, train acc: 0.805\n",
      "epoch: 11, loss: 1.0916215181350708, train acc: 0.805, test acc: 0.7758\n",
      "loss: 1.117039442062378, train acc: 0.7952\n",
      "loss: 0.9971845924854279, train acc: 0.8016\n",
      "loss: 1.0038919627666474, train acc: 0.8049\n",
      "loss: 1.0334793508052826, train acc: 0.8058\n",
      "loss: 0.9371641993522644, train acc: 0.8055\n",
      "loss: 1.0041168630123138, train acc: 0.8034\n",
      "loss: 0.9811464369297027, train acc: 0.8056\n",
      "loss: 1.0117681264877318, train acc: 0.8058\n",
      "epoch: 12, loss: 0.6638286113739014, train acc: 0.8058, test acc: 0.7786\n",
      "loss: 0.9354174733161926, train acc: 0.8008\n",
      "loss: 0.9518908202648163, train acc: 0.8041\n",
      "loss: 1.018579614162445, train acc: 0.8131\n",
      "loss: 0.9878890573978424, train acc: 0.8115\n",
      "loss: 0.9570030093193054, train acc: 0.8122\n",
      "loss: 0.988037931919098, train acc: 0.8096\n",
      "loss: 0.9819814801216126, train acc: 0.8102\n",
      "loss: 0.9485925614833832, train acc: 0.8084\n",
      "epoch: 13, loss: 0.7610099911689758, train acc: 0.8084, test acc: 0.7825\n",
      "loss: 0.978809118270874, train acc: 0.8086\n",
      "loss: 0.923565536737442, train acc: 0.8139\n",
      "loss: 0.9413471579551697, train acc: 0.8154\n",
      "loss: 1.0357220709323882, train acc: 0.8147\n",
      "loss: 0.9413348853588104, train acc: 0.8153\n",
      "loss: 0.997229540348053, train acc: 0.8124\n",
      "loss: 0.9634153664112091, train acc: 0.8167\n",
      "loss: 0.9192226588726043, train acc: 0.8137\n",
      "epoch: 14, loss: 0.9841180443763733, train acc: 0.8137, test acc: 0.7829\n",
      "loss: 1.0235552787780762, train acc: 0.8103\n",
      "loss: 0.9314289033412934, train acc: 0.8174\n",
      "loss: 0.9632326781749725, train acc: 0.8176\n",
      "loss: 0.9734679937362671, train acc: 0.8206\n",
      "loss: 0.9341533780097961, train acc: 0.8196\n",
      "loss: 0.9493087112903595, train acc: 0.8206\n",
      "loss: 0.9363991498947144, train acc: 0.8187\n",
      "loss: 0.9327693462371827, train acc: 0.8217\n",
      "epoch: 15, loss: 0.8409273028373718, train acc: 0.8217, test acc: 0.7889\n",
      "loss: 1.0309386253356934, train acc: 0.8144\n",
      "loss: 0.9388093948364258, train acc: 0.8243\n",
      "loss: 0.9528375864028931, train acc: 0.824\n",
      "loss: 0.9407829940319061, train acc: 0.8237\n",
      "loss: 0.9278586208820343, train acc: 0.8192\n",
      "loss: 0.9920237898826599, train acc: 0.8261\n",
      "loss: 0.9878494679927826, train acc: 0.8243\n",
      "loss: 0.8924296736717224, train acc: 0.8198\n",
      "epoch: 16, loss: 0.3115965723991394, train acc: 0.8198, test acc: 0.7916\n",
      "loss: 1.0249261856079102, train acc: 0.8183\n",
      "loss: 0.9109886467456818, train acc: 0.8272\n",
      "loss: 0.9345155000686646, train acc: 0.8248\n",
      "loss: 0.9585190296173096, train acc: 0.8292\n",
      "loss: 0.8926014423370361, train acc: 0.8265\n",
      "loss: 0.947723788022995, train acc: 0.8297\n",
      "loss: 0.973126482963562, train acc: 0.8282\n",
      "loss: 0.9512931346893311, train acc: 0.8269\n",
      "epoch: 17, loss: 0.4545285105705261, train acc: 0.8269, test acc: 0.7985\n",
      "loss: 0.9571765661239624, train acc: 0.8225\n",
      "loss: 0.9203904509544373, train acc: 0.8321\n",
      "loss: 0.8713266551494598, train acc: 0.8282\n",
      "loss: 0.9579792737960815, train acc: 0.8323\n",
      "loss: 0.8908816695213317, train acc: 0.8308\n",
      "loss: 0.9511298716068268, train acc: 0.8319\n",
      "loss: 0.9303336203098297, train acc: 0.8313\n",
      "loss: 0.94100781083107, train acc: 0.8273\n",
      "epoch: 18, loss: 0.9574406147003174, train acc: 0.8273, test acc: 0.8022\n",
      "loss: 0.9944167733192444, train acc: 0.8263\n",
      "loss: 0.9248357295989991, train acc: 0.8328\n",
      "loss: 0.9241420328617096, train acc: 0.8311\n",
      "loss: 0.9547161340713501, train acc: 0.8355\n",
      "loss: 0.8944832682609558, train acc: 0.8314\n",
      "loss: 0.936588978767395, train acc: 0.8332\n",
      "loss: 0.9238801002502441, train acc: 0.8346\n",
      "loss: 0.8820651173591614, train acc: 0.8309\n",
      "epoch: 19, loss: 0.5683975219726562, train acc: 0.8309, test acc: 0.8023\n",
      "loss: 0.9407387375831604, train acc: 0.8276\n",
      "loss: 0.9218299925327301, train acc: 0.8364\n",
      "loss: 0.9550237596035004, train acc: 0.836\n",
      "loss: 0.9315033912658691, train acc: 0.8349\n",
      "loss: 0.8740138947963715, train acc: 0.8366\n",
      "loss: 0.9351093649864197, train acc: 0.8385\n",
      "loss: 0.9173043370246887, train acc: 0.8385\n",
      "loss: 0.9029555976390838, train acc: 0.8378\n",
      "epoch: 20, loss: 0.79218590259552, train acc: 0.8378, test acc: 0.8017\n",
      "loss: 0.892744779586792, train acc: 0.8311\n",
      "loss: 0.8925872385501862, train acc: 0.8376\n",
      "loss: 0.9227752685546875, train acc: 0.8372\n",
      "loss: 0.9295746624469757, train acc: 0.8379\n",
      "loss: 0.895685338973999, train acc: 0.841\n",
      "loss: 0.886385029554367, train acc: 0.8396\n",
      "loss: 0.9382479786872864, train acc: 0.8373\n",
      "loss: 0.8897925674915313, train acc: 0.8369\n",
      "epoch: 21, loss: 0.7884189486503601, train acc: 0.8369, test acc: 0.8081\n",
      "loss: 0.8699414730072021, train acc: 0.8361\n",
      "loss: 0.8921291351318359, train acc: 0.8399\n",
      "loss: 0.9080438792705536, train acc: 0.8422\n",
      "loss: 0.9086858808994294, train acc: 0.8408\n",
      "loss: 0.8460664391517639, train acc: 0.8413\n",
      "loss: 0.9002861380577087, train acc: 0.8418\n",
      "loss: 0.9405531942844391, train acc: 0.8419\n",
      "loss: 0.8568069875240326, train acc: 0.8377\n",
      "epoch: 22, loss: 1.0783720016479492, train acc: 0.8377, test acc: 0.807\n",
      "loss: 0.917095422744751, train acc: 0.8372\n",
      "loss: 0.8573266506195069, train acc: 0.8412\n",
      "loss: 0.8992771685123444, train acc: 0.8412\n",
      "loss: 0.8998592793941498, train acc: 0.8425\n",
      "loss: 0.8624429523944854, train acc: 0.8419\n",
      "loss: 0.901527738571167, train acc: 0.8445\n",
      "loss: 0.9330488085746765, train acc: 0.8426\n",
      "loss: 0.8989166975021362, train acc: 0.8421\n",
      "epoch: 23, loss: 0.5628490447998047, train acc: 0.8421, test acc: 0.8134\n",
      "loss: 0.8892511129379272, train acc: 0.8421\n",
      "loss: 0.8864313185214996, train acc: 0.8438\n",
      "loss: 0.9123446464538574, train acc: 0.845\n",
      "loss: 0.8840676486492157, train acc: 0.8431\n",
      "loss: 0.8684319674968719, train acc: 0.8434\n",
      "loss: 0.8669055581092835, train acc: 0.8469\n",
      "loss: 0.897162801027298, train acc: 0.8486\n",
      "loss: 0.8672729671001435, train acc: 0.8424\n",
      "epoch: 24, loss: 0.6253378987312317, train acc: 0.8424, test acc: 0.8113\n",
      "loss: 0.8904099464416504, train acc: 0.8417\n",
      "loss: 0.879010808467865, train acc: 0.8419\n",
      "loss: 0.9131335020065308, train acc: 0.8476\n",
      "loss: 0.8880519568920135, train acc: 0.8469\n",
      "loss: 0.835678631067276, train acc: 0.8435\n",
      "loss: 0.9050635576248169, train acc: 0.8466\n",
      "loss: 0.9111719608306885, train acc: 0.8467\n",
      "loss: 0.855966329574585, train acc: 0.8432\n",
      "epoch: 25, loss: 0.36788052320480347, train acc: 0.8432, test acc: 0.8099\n",
      "loss: 0.9419997930526733, train acc: 0.8388\n",
      "loss: 0.8624722063541412, train acc: 0.844\n",
      "loss: 0.8989573955535889, train acc: 0.8475\n",
      "loss: 0.8610310912132263, train acc: 0.8471\n",
      "loss: 0.867723286151886, train acc: 0.847\n",
      "loss: 0.9160500466823578, train acc: 0.847\n",
      "loss: 0.9249761641025543, train acc: 0.8479\n",
      "loss: 0.8337129414081573, train acc: 0.847\n",
      "epoch: 26, loss: 0.7680282592773438, train acc: 0.847, test acc: 0.8131\n",
      "loss: 0.8468637466430664, train acc: 0.8432\n",
      "loss: 0.8593448102474213, train acc: 0.8426\n",
      "loss: 0.8949535965919495, train acc: 0.8465\n",
      "loss: 0.8880592525005341, train acc: 0.8495\n",
      "loss: 0.8738775968551635, train acc: 0.8459\n",
      "loss: 0.8983421266078949, train acc: 0.8496\n",
      "loss: 0.9284051835536957, train acc: 0.8516\n",
      "loss: 0.8764608860015869, train acc: 0.8478\n",
      "epoch: 27, loss: 0.6127602458000183, train acc: 0.8478, test acc: 0.8151\n",
      "loss: 0.888931393623352, train acc: 0.8467\n",
      "loss: 0.8200545489788056, train acc: 0.8491\n",
      "loss: 0.8647945642471313, train acc: 0.85\n",
      "loss: 0.8794666171073914, train acc: 0.8504\n",
      "loss: 0.841941875219345, train acc: 0.8474\n",
      "loss: 0.8801270663738251, train acc: 0.8488\n",
      "loss: 0.8805628299713135, train acc: 0.851\n",
      "loss: 0.8523877382278442, train acc: 0.848\n",
      "epoch: 28, loss: 0.39306640625, train acc: 0.848, test acc: 0.8124\n",
      "loss: 0.7731647491455078, train acc: 0.8425\n",
      "loss: 0.8210776031017304, train acc: 0.848\n",
      "loss: 0.8882988631725312, train acc: 0.8511\n",
      "loss: 0.9053223252296447, train acc: 0.8511\n",
      "loss: 0.85420902967453, train acc: 0.8496\n",
      "loss: 0.9058131515979767, train acc: 0.852\n",
      "loss: 0.8855325043201446, train acc: 0.8527\n",
      "loss: 0.8505143582820892, train acc: 0.8493\n",
      "epoch: 29, loss: 0.5525738596916199, train acc: 0.8493, test acc: 0.8138\n",
      "loss: 0.9351224303245544, train acc: 0.8482\n",
      "loss: 0.8422268271446228, train acc: 0.849\n",
      "loss: 0.8779115736484527, train acc: 0.8504\n",
      "loss: 0.8617573499679565, train acc: 0.8514\n",
      "loss: 0.8423258662223816, train acc: 0.8505\n",
      "loss: 0.8820969641208649, train acc: 0.8517\n",
      "loss: 0.8910149753093719, train acc: 0.8537\n",
      "loss: 0.8335326433181762, train acc: 0.8481\n",
      "epoch: 30, loss: 0.67938631772995, train acc: 0.8481, test acc: 0.8165\n",
      "loss: 0.9073670506477356, train acc: 0.8498\n",
      "loss: 0.8439349949359893, train acc: 0.8504\n",
      "loss: 0.8415891528129578, train acc: 0.8534\n",
      "loss: 0.8910505771636963, train acc: 0.8537\n",
      "loss: 0.8330424070358277, train acc: 0.8503\n",
      "loss: 0.8924462556838989, train acc: 0.8534\n",
      "loss: 0.8979809582233429, train acc: 0.8552\n",
      "loss: 0.8666090726852417, train acc: 0.852\n",
      "epoch: 31, loss: 0.31250816583633423, train acc: 0.852, test acc: 0.8148\n",
      "loss: 0.8410929441452026, train acc: 0.849\n",
      "loss: 0.8210193812847137, train acc: 0.8497\n",
      "loss: 0.8529157400131225, train acc: 0.8524\n",
      "loss: 0.8470176875591278, train acc: 0.8546\n",
      "loss: 0.8568443357944489, train acc: 0.8509\n",
      "loss: 0.8212999820709228, train acc: 0.8532\n",
      "loss: 0.9132050991058349, train acc: 0.8544\n",
      "loss: 0.8115033149719239, train acc: 0.8525\n",
      "epoch: 32, loss: 0.23654170334339142, train acc: 0.8525, test acc: 0.8151\n",
      "loss: 0.8642623424530029, train acc: 0.8514\n",
      "loss: 0.8030969679355622, train acc: 0.8504\n",
      "loss: 0.8763623356819152, train acc: 0.8531\n",
      "loss: 0.8405723929405212, train acc: 0.8515\n",
      "loss: 0.8355044186115265, train acc: 0.8525\n",
      "loss: 0.8778495907783508, train acc: 0.8549\n",
      "loss: 0.8554299771785736, train acc: 0.8562\n",
      "loss: 0.8057832598686219, train acc: 0.8518\n",
      "epoch: 33, loss: 0.2800518274307251, train acc: 0.8518, test acc: 0.8164\n",
      "loss: 0.8003184795379639, train acc: 0.8484\n",
      "loss: 0.8331759214401245, train acc: 0.8509\n",
      "loss: 0.8520458936691284, train acc: 0.8567\n",
      "loss: 0.8619968712329864, train acc: 0.8532\n",
      "loss: 0.8242295801639556, train acc: 0.8528\n",
      "loss: 0.8568739414215087, train acc: 0.8561\n",
      "loss: 0.8484878182411194, train acc: 0.8558\n",
      "loss: 0.8693039000034333, train acc: 0.8513\n",
      "epoch: 34, loss: 0.4461156129837036, train acc: 0.8513, test acc: 0.815\n",
      "loss: 0.884784996509552, train acc: 0.8493\n",
      "loss: 0.7960957884788513, train acc: 0.8502\n",
      "loss: 0.9019775509834289, train acc: 0.8528\n",
      "loss: 0.916876345872879, train acc: 0.8525\n",
      "loss: 0.8288024544715882, train acc: 0.852\n",
      "loss: 0.8864500224590302, train acc: 0.8556\n",
      "loss: 0.8581776320934296, train acc: 0.8575\n",
      "loss: 0.814588439464569, train acc: 0.8532\n",
      "epoch: 35, loss: 0.4623267948627472, train acc: 0.8532, test acc: 0.8157\n",
      "loss: 0.9516994953155518, train acc: 0.8531\n",
      "loss: 0.8424779176712036, train acc: 0.8529\n",
      "loss: 0.8285487771034241, train acc: 0.8536\n",
      "loss: 0.8247738301753997, train acc: 0.8561\n",
      "loss: 0.7960939705371857, train acc: 0.8539\n",
      "loss: 0.8849657714366913, train acc: 0.8567\n",
      "loss: 0.8377268433570861, train acc: 0.8567\n",
      "loss: 0.8258111417293549, train acc: 0.8543\n",
      "epoch: 36, loss: 0.7325579524040222, train acc: 0.8543, test acc: 0.8134\n",
      "loss: 0.9062482714653015, train acc: 0.8497\n",
      "loss: 0.8267503678798676, train acc: 0.8511\n",
      "loss: 0.8906733810901641, train acc: 0.8539\n",
      "loss: 0.8309781849384308, train acc: 0.855\n",
      "loss: 0.8263491094112396, train acc: 0.8555\n",
      "loss: 0.850025349855423, train acc: 0.8584\n",
      "loss: 0.8825623631477356, train acc: 0.8569\n",
      "loss: 0.828154331445694, train acc: 0.8542\n",
      "epoch: 37, loss: 0.417664498090744, train acc: 0.8542, test acc: 0.8165\n",
      "loss: 0.9405972361564636, train acc: 0.8535\n",
      "loss: 0.8282953202724457, train acc: 0.8517\n",
      "loss: 0.8450501084327697, train acc: 0.8581\n",
      "loss: 0.8301568925380707, train acc: 0.8582\n",
      "loss: 0.8389076352119446, train acc: 0.8565\n",
      "loss: 0.8730423748493195, train acc: 0.8564\n",
      "loss: 0.855844932794571, train acc: 0.8577\n",
      "loss: 0.8241615116596221, train acc: 0.8555\n",
      "epoch: 38, loss: 0.3246680200099945, train acc: 0.8555, test acc: 0.8168\n",
      "loss: 0.8992178440093994, train acc: 0.853\n",
      "loss: 0.8013046741485595, train acc: 0.8535\n",
      "loss: 0.8339780688285827, train acc: 0.8579\n",
      "loss: 0.8167546629905701, train acc: 0.8571\n",
      "loss: 0.8083433091640473, train acc: 0.8562\n",
      "loss: 0.8567346215248108, train acc: 0.8584\n",
      "loss: 0.8762186527252197, train acc: 0.8582\n",
      "loss: 0.816343355178833, train acc: 0.8541\n",
      "epoch: 39, loss: 0.8819537162780762, train acc: 0.8541, test acc: 0.8164\n",
      "loss: 0.912970244884491, train acc: 0.8554\n",
      "loss: 0.8341757774353027, train acc: 0.8527\n",
      "loss: 0.8929147720336914, train acc: 0.8568\n",
      "loss: 0.8207562625408172, train acc: 0.8578\n",
      "loss: 0.8268886089324952, train acc: 0.8577\n",
      "loss: 0.8452394723892211, train acc: 0.8593\n",
      "loss: 0.8692232847213746, train acc: 0.8584\n",
      "loss: 0.8539407312870025, train acc: 0.8563\n",
      "epoch: 40, loss: 0.5029600858688354, train acc: 0.8563, test acc: 0.8179\n",
      "loss: 0.8686266541481018, train acc: 0.8562\n",
      "loss: 0.859385633468628, train acc: 0.8529\n",
      "loss: 0.8231882512569427, train acc: 0.8582\n",
      "loss: 0.8258321523666382, train acc: 0.8584\n",
      "loss: 0.7739435195922851, train acc: 0.86\n",
      "loss: 0.8301363527774811, train acc: 0.8584\n",
      "loss: 0.8440749108791351, train acc: 0.859\n",
      "loss: 0.8017974972724915, train acc: 0.8567\n",
      "epoch: 41, loss: 0.7248914241790771, train acc: 0.8567, test acc: 0.8156\n",
      "loss: 0.9295631647109985, train acc: 0.8557\n",
      "loss: 0.7939656853675843, train acc: 0.8534\n",
      "loss: 0.8561379075050354, train acc: 0.857\n",
      "loss: 0.8002634525299073, train acc: 0.8589\n",
      "loss: 0.8103083193302154, train acc: 0.86\n",
      "loss: 0.8534255385398865, train acc: 0.8592\n",
      "loss: 0.8564342975616455, train acc: 0.8604\n",
      "loss: 0.8067933797836304, train acc: 0.8592\n",
      "epoch: 42, loss: 0.615913987159729, train acc: 0.8592, test acc: 0.8177\n",
      "loss: 0.9321475028991699, train acc: 0.8566\n",
      "loss: 0.7720183432102203, train acc: 0.855\n",
      "loss: 0.8443463325500489, train acc: 0.8592\n",
      "loss: 0.8015491902828217, train acc: 0.8601\n",
      "loss: 0.8168105483055115, train acc: 0.8579\n",
      "loss: 0.8646066844463348, train acc: 0.8591\n",
      "loss: 0.8658737599849701, train acc: 0.8591\n",
      "loss: 0.8102004051208496, train acc: 0.8562\n",
      "epoch: 43, loss: 0.6964117884635925, train acc: 0.8562, test acc: 0.8134\n",
      "loss: 0.9121931195259094, train acc: 0.853\n",
      "loss: 0.8154850661754608, train acc: 0.8545\n",
      "loss: 0.8586740016937255, train acc: 0.8584\n",
      "loss: 0.8389689087867737, train acc: 0.8577\n",
      "loss: 0.7820773243904113, train acc: 0.86\n",
      "loss: 0.8711595594882965, train acc: 0.86\n",
      "loss: 0.8450217485427857, train acc: 0.8611\n",
      "loss: 0.8180817663669586, train acc: 0.8562\n",
      "epoch: 44, loss: 0.45190203189849854, train acc: 0.8562, test acc: 0.8154\n",
      "loss: 0.9359932541847229, train acc: 0.8558\n",
      "loss: 0.8338662981987, train acc: 0.8568\n",
      "loss: 0.8754148781299591, train acc: 0.8606\n",
      "loss: 0.8384644329547882, train acc: 0.8589\n",
      "loss: 0.8096498489379883, train acc: 0.8594\n",
      "loss: 0.8412028253078461, train acc: 0.86\n",
      "loss: 0.8641938686370849, train acc: 0.861\n",
      "loss: 0.8187640190124512, train acc: 0.8559\n",
      "epoch: 45, loss: 0.9779547452926636, train acc: 0.8559, test acc: 0.8127\n",
      "loss: 0.7590290904045105, train acc: 0.8521\n",
      "loss: 0.7973521888256073, train acc: 0.8538\n",
      "loss: 0.8129268407821655, train acc: 0.8578\n",
      "loss: 0.8092197716236115, train acc: 0.8605\n",
      "loss: 0.8313667297363281, train acc: 0.8593\n",
      "loss: 0.8283967316150666, train acc: 0.8585\n",
      "loss: 0.8453279316425324, train acc: 0.8591\n",
      "loss: 0.8297872185707093, train acc: 0.8575\n",
      "epoch: 46, loss: 0.21286727488040924, train acc: 0.8575, test acc: 0.8152\n",
      "loss: 0.9399153590202332, train acc: 0.8543\n",
      "loss: 0.8124746441841125, train acc: 0.8571\n",
      "loss: 0.8482739746570587, train acc: 0.8581\n",
      "loss: 0.868228816986084, train acc: 0.8603\n",
      "loss: 0.7800582706928253, train acc: 0.8615\n",
      "loss: 0.8019688427448273, train acc: 0.8608\n",
      "loss: 0.8170783221721649, train acc: 0.8607\n",
      "loss: 0.7688575625419617, train acc: 0.8573\n",
      "epoch: 47, loss: 0.32070016860961914, train acc: 0.8573, test acc: 0.8168\n",
      "loss: 0.7329926490783691, train acc: 0.8579\n",
      "loss: 0.7862771332263947, train acc: 0.8586\n",
      "loss: 0.8568183004856109, train acc: 0.8617\n",
      "loss: 0.7899198055267334, train acc: 0.8608\n",
      "loss: 0.799153083562851, train acc: 0.86\n",
      "loss: 0.873879486322403, train acc: 0.8623\n",
      "loss: 0.8880146741867065, train acc: 0.8613\n",
      "loss: 0.7844943821430206, train acc: 0.8603\n",
      "epoch: 48, loss: 0.5724490880966187, train acc: 0.8603, test acc: 0.8141\n",
      "loss: 0.8515469431877136, train acc: 0.8535\n",
      "loss: 0.8460142552852631, train acc: 0.8571\n",
      "loss: 0.8471457004547119, train acc: 0.8613\n",
      "loss: 0.8539013922214508, train acc: 0.8619\n",
      "loss: 0.7985480189323425, train acc: 0.8603\n",
      "loss: 0.8557004630565643, train acc: 0.8617\n",
      "loss: 0.8551645457744599, train acc: 0.8601\n",
      "loss: 0.7514718532562256, train acc: 0.8586\n",
      "epoch: 49, loss: 0.9286843538284302, train acc: 0.8586, test acc: 0.8125\n",
      "loss: 0.9382203817367554, train acc: 0.8536\n",
      "loss: 0.8185689866542816, train acc: 0.8576\n",
      "loss: 0.788627165555954, train acc: 0.8636\n",
      "loss: 0.8254354417324066, train acc: 0.8628\n",
      "loss: 0.7914054155349731, train acc: 0.8603\n",
      "loss: 0.8575845003128052, train acc: 0.8632\n",
      "loss: 0.8547292053699493, train acc: 0.8608\n",
      "loss: 0.777997362613678, train acc: 0.8576\n",
      "epoch: 50, loss: 0.276202529668808, train acc: 0.8576, test acc: 0.8137\n",
      "loss: 0.7818269729614258, train acc: 0.8575\n",
      "loss: 0.7829033195972442, train acc: 0.8576\n",
      "loss: 0.8225151479244233, train acc: 0.8629\n",
      "loss: 0.8145160377025604, train acc: 0.8631\n",
      "loss: 0.7858449280261993, train acc: 0.8623\n",
      "loss: 0.8218843400478363, train acc: 0.8622\n",
      "loss: 0.8572170794010162, train acc: 0.8637\n",
      "loss: 0.7681033909320831, train acc: 0.8606\n",
      "epoch: 51, loss: 0.45534077286720276, train acc: 0.8606, test acc: 0.8126\n",
      "loss: 0.7808555960655212, train acc: 0.8572\n",
      "loss: 0.7843674421310425, train acc: 0.8571\n",
      "loss: 0.8260941684246064, train acc: 0.8634\n",
      "loss: 0.8259388387203217, train acc: 0.8643\n",
      "loss: 0.801320493221283, train acc: 0.86\n",
      "loss: 0.8200532138347626, train acc: 0.862\n",
      "loss: 0.8402009308338165, train acc: 0.8625\n",
      "loss: 0.790221381187439, train acc: 0.8594\n",
      "epoch: 52, loss: 0.6773228645324707, train acc: 0.8594, test acc: 0.8132\n",
      "loss: 0.7431822419166565, train acc: 0.8569\n",
      "loss: 0.7618016302585602, train acc: 0.8585\n",
      "loss: 0.8549900472164154, train acc: 0.8614\n",
      "loss: 0.7935444712638855, train acc: 0.862\n",
      "loss: 0.7681560754776001, train acc: 0.8613\n",
      "loss: 0.831833016872406, train acc: 0.8649\n",
      "loss: 0.8642753303050995, train acc: 0.8622\n",
      "loss: 0.7863247096538544, train acc: 0.8598\n",
      "epoch: 53, loss: 0.21653075516223907, train acc: 0.8598, test acc: 0.8143\n",
      "loss: 0.8854202032089233, train acc: 0.8572\n",
      "loss: 0.8042512536048889, train acc: 0.859\n",
      "loss: 0.8373521387577056, train acc: 0.8627\n",
      "loss: 0.7653739094734192, train acc: 0.865\n",
      "loss: 0.8108036816120148, train acc: 0.8654\n",
      "loss: 0.864731764793396, train acc: 0.8641\n",
      "loss: 0.8481042623519898, train acc: 0.8652\n",
      "loss: 0.7595279037952423, train acc: 0.8634\n",
      "epoch: 54, loss: 0.5685001611709595, train acc: 0.8634, test acc: 0.8176\n",
      "loss: 0.970430850982666, train acc: 0.8578\n",
      "loss: 0.8173947691917419, train acc: 0.86\n",
      "loss: 0.8467534840106964, train acc: 0.8641\n",
      "loss: 0.8127277016639709, train acc: 0.8642\n",
      "loss: 0.8159729957580566, train acc: 0.863\n",
      "loss: 0.8252115368843078, train acc: 0.8654\n",
      "loss: 0.8343714237213135, train acc: 0.8622\n",
      "loss: 0.784368634223938, train acc: 0.8633\n",
      "epoch: 55, loss: 1.1499799489974976, train acc: 0.8633, test acc: 0.8151\n",
      "loss: 0.773340106010437, train acc: 0.8609\n",
      "loss: 0.7930184721946716, train acc: 0.8592\n",
      "loss: 0.8465644419193268, train acc: 0.8646\n",
      "loss: 0.7960283100605011, train acc: 0.8632\n",
      "loss: 0.7678150832653046, train acc: 0.8625\n",
      "loss: 0.8233104050159454, train acc: 0.8651\n",
      "loss: 0.8369752705097199, train acc: 0.8618\n",
      "loss: 0.8174373269081116, train acc: 0.8591\n",
      "epoch: 56, loss: 0.7100947499275208, train acc: 0.8591, test acc: 0.8126\n",
      "loss: 0.7381436228752136, train acc: 0.8596\n",
      "loss: 0.8089454948902131, train acc: 0.8614\n",
      "loss: 0.8343106091022492, train acc: 0.862\n",
      "loss: 0.7974801599979401, train acc: 0.8663\n",
      "loss: 0.8281710147857666, train acc: 0.8633\n",
      "loss: 0.8040729463100433, train acc: 0.8655\n",
      "loss: 0.8215314447879791, train acc: 0.8661\n",
      "loss: 0.7636859118938446, train acc: 0.8627\n",
      "epoch: 57, loss: 0.32489436864852905, train acc: 0.8627, test acc: 0.8109\n",
      "loss: 0.950109601020813, train acc: 0.8577\n",
      "loss: 0.822889894247055, train acc: 0.8617\n",
      "loss: 0.8238615453243255, train acc: 0.8649\n",
      "loss: 0.8084371447563171, train acc: 0.8655\n",
      "loss: 0.7678072690963745, train acc: 0.8648\n",
      "loss: 0.8231959462165832, train acc: 0.8653\n",
      "loss: 0.8390916287899017, train acc: 0.8644\n",
      "loss: 0.7506754696369171, train acc: 0.8642\n",
      "epoch: 58, loss: 0.3297536373138428, train acc: 0.8642, test acc: 0.8135\n",
      "loss: 0.8861953616142273, train acc: 0.8579\n",
      "loss: 0.7837613165378571, train acc: 0.862\n",
      "loss: 0.8440436899662018, train acc: 0.8645\n",
      "loss: 0.778525459766388, train acc: 0.8663\n",
      "loss: 0.7872960329055786, train acc: 0.8659\n",
      "loss: 0.8340562343597412, train acc: 0.8662\n",
      "loss: 0.8741414844989777, train acc: 0.8662\n",
      "loss: 0.7673649609088897, train acc: 0.863\n",
      "epoch: 59, loss: 0.5585305690765381, train acc: 0.863, test acc: 0.8125\n",
      "loss: 0.8161932826042175, train acc: 0.8578\n",
      "loss: 0.7739631175994873, train acc: 0.8621\n",
      "loss: 0.8186319172382355, train acc: 0.8625\n",
      "loss: 0.7820891261100769, train acc: 0.8661\n",
      "loss: 0.7839825034141541, train acc: 0.866\n",
      "loss: 0.8199189484119416, train acc: 0.8665\n",
      "loss: 0.8271388232707977, train acc: 0.8637\n",
      "loss: 0.7631954908370971, train acc: 0.8617\n",
      "epoch: 60, loss: 0.44134265184402466, train acc: 0.8617, test acc: 0.8113\n",
      "loss: 0.923141360282898, train acc: 0.8575\n",
      "loss: 0.7766851544380188, train acc: 0.8621\n",
      "loss: 0.8336331248283386, train acc: 0.8661\n",
      "loss: 0.8274905204772949, train acc: 0.8637\n",
      "loss: 0.7820614874362946, train acc: 0.8621\n",
      "loss: 0.8023381471633911, train acc: 0.8657\n",
      "loss: 0.810603666305542, train acc: 0.8655\n",
      "loss: 0.7972717106342315, train acc: 0.8638\n",
      "epoch: 61, loss: 0.43067553639411926, train acc: 0.8638, test acc: 0.8117\n",
      "loss: 0.8133692145347595, train acc: 0.8598\n",
      "loss: 0.783119797706604, train acc: 0.864\n",
      "loss: 0.8224899709224701, train acc: 0.8652\n",
      "loss: 0.7728188633918762, train acc: 0.8659\n",
      "loss: 0.8001270055770874, train acc: 0.8628\n",
      "loss: 0.8596328735351563, train acc: 0.8646\n",
      "loss: 0.8454223871231079, train acc: 0.8657\n",
      "loss: 0.7263649106025696, train acc: 0.8619\n",
      "epoch: 62, loss: 0.6516292691230774, train acc: 0.8619, test acc: 0.8104\n",
      "loss: 0.8233723640441895, train acc: 0.8584\n",
      "loss: 0.801576292514801, train acc: 0.863\n",
      "loss: 0.8360234260559082, train acc: 0.8676\n",
      "loss: 0.798494553565979, train acc: 0.8669\n",
      "loss: 0.76292182803154, train acc: 0.8642\n",
      "loss: 0.8275132358074189, train acc: 0.8651\n",
      "loss: 0.8237799346446991, train acc: 0.866\n",
      "loss: 0.787859320640564, train acc: 0.8647\n",
      "epoch: 63, loss: 0.4467509686946869, train acc: 0.8647, test acc: 0.8143\n",
      "loss: 0.7811299562454224, train acc: 0.8617\n",
      "loss: 0.7802658319473267, train acc: 0.8642\n",
      "loss: 0.811267739534378, train acc: 0.8656\n",
      "loss: 0.7799405634403229, train acc: 0.8668\n",
      "loss: 0.8217337548732757, train acc: 0.8647\n",
      "loss: 0.7863844692707062, train acc: 0.8674\n",
      "loss: 0.8586631834506988, train acc: 0.8673\n",
      "loss: 0.8046045780181885, train acc: 0.8647\n",
      "epoch: 64, loss: 0.2119566798210144, train acc: 0.8647, test acc: 0.8118\n",
      "loss: 0.8587031364440918, train acc: 0.8609\n",
      "loss: 0.7800957143306733, train acc: 0.8652\n",
      "loss: 0.8077756643295289, train acc: 0.8661\n",
      "loss: 0.7820788621902466, train acc: 0.867\n",
      "loss: 0.7757990062236786, train acc: 0.8621\n",
      "loss: 0.820162320137024, train acc: 0.867\n",
      "loss: 0.8215261101722717, train acc: 0.8657\n",
      "loss: 0.7806634187698365, train acc: 0.8618\n",
      "epoch: 65, loss: 0.6916421055793762, train acc: 0.8618, test acc: 0.805\n",
      "loss: 0.7483537197113037, train acc: 0.8566\n",
      "loss: 0.7683912873268127, train acc: 0.8624\n",
      "loss: 0.8387860894203186, train acc: 0.8667\n",
      "loss: 0.8075355470180512, train acc: 0.8666\n",
      "loss: 0.8131946861743927, train acc: 0.8645\n",
      "loss: 0.8376874148845672, train acc: 0.8668\n",
      "loss: 0.8169192731380462, train acc: 0.8679\n",
      "loss: 0.7823328673839569, train acc: 0.8641\n",
      "epoch: 66, loss: 0.35758835077285767, train acc: 0.8641, test acc: 0.8086\n",
      "loss: 0.7497043609619141, train acc: 0.8598\n",
      "loss: 0.7629090964794158, train acc: 0.8639\n",
      "loss: 0.818899405002594, train acc: 0.8665\n",
      "loss: 0.7768727123737336, train acc: 0.8666\n",
      "loss: 0.7736301243305206, train acc: 0.866\n",
      "loss: 0.8314819931983948, train acc: 0.8664\n",
      "loss: 0.8220549821853638, train acc: 0.8654\n",
      "loss: 0.7968478620052337, train acc: 0.8659\n",
      "epoch: 67, loss: 0.7545692324638367, train acc: 0.8659, test acc: 0.8102\n",
      "loss: 0.8175955414772034, train acc: 0.8611\n",
      "loss: 0.7602484405040741, train acc: 0.867\n",
      "loss: 0.8370477974414825, train acc: 0.868\n",
      "loss: 0.8104992806911469, train acc: 0.8679\n",
      "loss: 0.7651548445224762, train acc: 0.8666\n",
      "loss: 0.800080144405365, train acc: 0.8692\n",
      "loss: 0.8287894487380981, train acc: 0.8671\n",
      "loss: 0.7372770845890045, train acc: 0.8639\n",
      "epoch: 68, loss: 0.5555142760276794, train acc: 0.8639, test acc: 0.8123\n",
      "loss: 0.8919206261634827, train acc: 0.8617\n",
      "loss: 0.7811193287372589, train acc: 0.8646\n",
      "loss: 0.8278945744037628, train acc: 0.8675\n",
      "loss: 0.7998262405395508, train acc: 0.8684\n",
      "loss: 0.8013536751270294, train acc: 0.8658\n",
      "loss: 0.8307538688182831, train acc: 0.8681\n",
      "loss: 0.8139507234096527, train acc: 0.8654\n",
      "loss: 0.7515101611614228, train acc: 0.8643\n",
      "epoch: 69, loss: 0.5482094287872314, train acc: 0.8643, test acc: 0.8056\n",
      "loss: 0.7859143018722534, train acc: 0.8598\n",
      "loss: 0.7857701122760773, train acc: 0.8641\n",
      "loss: 0.8317625343799591, train acc: 0.8664\n",
      "loss: 0.7848084151744843, train acc: 0.8687\n",
      "loss: 0.7802636981010437, train acc: 0.8673\n",
      "loss: 0.8135544419288635, train acc: 0.8669\n",
      "loss: 0.8440338671207428, train acc: 0.8676\n",
      "loss: 0.787707531452179, train acc: 0.8666\n",
      "epoch: 70, loss: 0.21504512429237366, train acc: 0.8666, test acc: 0.8063\n",
      "loss: 0.7828691005706787, train acc: 0.8593\n",
      "loss: 0.7655803620815277, train acc: 0.8657\n",
      "loss: 0.795561558008194, train acc: 0.8655\n",
      "loss: 0.8002812445163727, train acc: 0.8693\n",
      "loss: 0.7974723517894745, train acc: 0.8647\n",
      "loss: 0.8120896816253662, train acc: 0.8688\n",
      "loss: 0.8210039913654328, train acc: 0.8663\n",
      "loss: 0.7546193361282348, train acc: 0.8663\n",
      "epoch: 71, loss: 0.5645791888237, train acc: 0.8663, test acc: 0.8103\n",
      "loss: 0.7775493860244751, train acc: 0.8595\n",
      "loss: 0.7718543350696564, train acc: 0.8654\n",
      "loss: 0.8078492999076843, train acc: 0.8685\n",
      "loss: 0.7655687272548676, train acc: 0.8678\n",
      "loss: 0.7978500485420227, train acc: 0.8681\n",
      "loss: 0.7956164181232452, train acc: 0.8678\n",
      "loss: 0.8051725625991821, train acc: 0.8674\n",
      "loss: 0.7827422380447387, train acc: 0.8673\n",
      "epoch: 72, loss: 0.3887399435043335, train acc: 0.8673, test acc: 0.8116\n",
      "loss: 0.729112982749939, train acc: 0.8596\n",
      "loss: 0.7645244240760803, train acc: 0.8634\n",
      "loss: 0.7603811740875244, train acc: 0.8674\n",
      "loss: 0.7883754670619965, train acc: 0.8673\n",
      "loss: 0.7885505735874176, train acc: 0.8684\n",
      "loss: 0.8052184402942657, train acc: 0.8702\n",
      "loss: 0.810794222354889, train acc: 0.8681\n",
      "loss: 0.7525300145149231, train acc: 0.8668\n",
      "epoch: 73, loss: 0.5027655959129333, train acc: 0.8668, test acc: 0.8117\n",
      "loss: 0.7552916407585144, train acc: 0.8599\n",
      "loss: 0.7657726645469666, train acc: 0.8674\n",
      "loss: 0.7921232104301452, train acc: 0.8694\n",
      "loss: 0.7741158306598663, train acc: 0.869\n",
      "loss: 0.7891293168067932, train acc: 0.8663\n",
      "loss: 0.8140256464481354, train acc: 0.8679\n",
      "loss: 0.7905749082565308, train acc: 0.8689\n",
      "loss: 0.7987126648426056, train acc: 0.8692\n",
      "epoch: 74, loss: 0.6099334359169006, train acc: 0.8692, test acc: 0.8084\n",
      "loss: 0.9105131030082703, train acc: 0.8588\n",
      "loss: 0.7829044044017792, train acc: 0.8632\n",
      "loss: 0.780773788690567, train acc: 0.8702\n",
      "loss: 0.8068317651748658, train acc: 0.8703\n",
      "loss: 0.770054566860199, train acc: 0.8694\n",
      "loss: 0.792779552936554, train acc: 0.8697\n",
      "loss: 0.8128193795681, train acc: 0.8682\n",
      "loss: 0.7899650871753693, train acc: 0.8664\n",
      "epoch: 75, loss: 0.6809349060058594, train acc: 0.8664, test acc: 0.8052\n",
      "loss: 0.7546451687812805, train acc: 0.8563\n",
      "loss: 0.7761281847953796, train acc: 0.8674\n",
      "loss: 0.8148475587368011, train acc: 0.8707\n",
      "loss: 0.7621055722236634, train acc: 0.8694\n",
      "loss: 0.7758373558521271, train acc: 0.8673\n",
      "loss: 0.7971100270748138, train acc: 0.8715\n",
      "loss: 0.8114913821220398, train acc: 0.8708\n",
      "loss: 0.7511260688304902, train acc: 0.8684\n",
      "epoch: 76, loss: 0.9180044531822205, train acc: 0.8684, test acc: 0.8129\n",
      "loss: 0.9389647245407104, train acc: 0.864\n",
      "loss: 0.7880516290664673, train acc: 0.8657\n",
      "loss: 0.7885852336883545, train acc: 0.8704\n",
      "loss: 0.79476038813591, train acc: 0.8705\n",
      "loss: 0.7726721048355103, train acc: 0.8712\n",
      "loss: 0.8336685061454773, train acc: 0.8684\n",
      "loss: 0.8311742663383483, train acc: 0.8677\n",
      "loss: 0.7749348402023315, train acc: 0.8675\n",
      "epoch: 77, loss: 0.8313273787498474, train acc: 0.8675, test acc: 0.8074\n",
      "loss: 0.8241090178489685, train acc: 0.8587\n",
      "loss: 0.8100049734115601, train acc: 0.865\n",
      "loss: 0.7800955474376678, train acc: 0.871\n",
      "loss: 0.781924706697464, train acc: 0.8637\n",
      "loss: 0.7708213806152344, train acc: 0.8653\n",
      "loss: 0.7877984941005707, train acc: 0.871\n",
      "loss: 0.8465646386146546, train acc: 0.867\n",
      "loss: 0.7725666582584381, train acc: 0.8708\n",
      "epoch: 78, loss: 0.3495887517929077, train acc: 0.8708, test acc: 0.8122\n",
      "loss: 0.9193997979164124, train acc: 0.8638\n",
      "loss: 0.790622329711914, train acc: 0.8673\n",
      "loss: 0.7960250794887542, train acc: 0.8716\n",
      "loss: 0.7387679159641266, train acc: 0.8714\n",
      "loss: 0.7859845697879791, train acc: 0.8681\n",
      "loss: 0.7688559710979461, train acc: 0.8702\n",
      "loss: 0.8021312654018402, train acc: 0.8711\n",
      "loss: 0.7711742162704468, train acc: 0.8695\n",
      "epoch: 79, loss: 0.4358305037021637, train acc: 0.8695, test acc: 0.8087\n",
      "#####training and testing end with K:5, P:0.1######\n",
      "#####training and testing start with K:5, P:0.5######\n",
      "loss: 2.361621379852295, train acc: 0.1567\n",
      "loss: 2.282442903518677, train acc: 0.2236\n",
      "loss: 2.1093055605888367, train acc: 0.4054\n",
      "loss: 2.0022876381874086, train acc: 0.4591\n",
      "loss: 1.9720410108566284, train acc: 0.5021\n",
      "loss: 1.907641553878784, train acc: 0.5275\n",
      "loss: 1.8560791969299317, train acc: 0.5417\n",
      "loss: 1.8338124394416808, train acc: 0.5426\n",
      "epoch: 0, loss: 1.7881571054458618, train acc: 0.5426, test acc: 0.584\n",
      "loss: 1.8513153791427612, train acc: 0.596\n",
      "loss: 1.8323855519294738, train acc: 0.5894\n",
      "loss: 1.8358722448348999, train acc: 0.6182\n",
      "loss: 1.8108370304107666, train acc: 0.6222\n",
      "loss: 1.736289393901825, train acc: 0.6321\n",
      "loss: 1.768142008781433, train acc: 0.6428\n",
      "loss: 1.7525922179222106, train acc: 0.6417\n",
      "loss: 1.7355986714363099, train acc: 0.6615\n",
      "epoch: 1, loss: 1.3383041620254517, train acc: 0.6615, test acc: 0.6502\n",
      "loss: 1.7151768207550049, train acc: 0.6544\n",
      "loss: 1.7106138110160827, train acc: 0.6826\n",
      "loss: 1.7164299845695496, train acc: 0.6919\n",
      "loss: 1.6666884541511535, train acc: 0.6902\n",
      "loss: 1.67567435503006, train acc: 0.7195\n",
      "loss: 1.686853051185608, train acc: 0.709\n",
      "loss: 1.7009021997451783, train acc: 0.7264\n",
      "loss: 1.680800724029541, train acc: 0.727\n",
      "epoch: 2, loss: 1.7387112379074097, train acc: 0.727, test acc: 0.6906\n",
      "loss: 1.713044285774231, train acc: 0.7172\n",
      "loss: 1.6512750744819642, train acc: 0.7388\n",
      "loss: 1.6582834839820861, train acc: 0.7277\n",
      "loss: 1.6954344153404235, train acc: 0.7438\n",
      "loss: 1.6756837010383605, train acc: 0.7399\n",
      "loss: 1.654811179637909, train acc: 0.741\n",
      "loss: 1.6159741044044496, train acc: 0.7453\n",
      "loss: 1.6504878282546998, train acc: 0.7536\n",
      "epoch: 3, loss: 1.6235929727554321, train acc: 0.7536, test acc: 0.7217\n",
      "loss: 1.5917006731033325, train acc: 0.7398\n",
      "loss: 1.6395421743392944, train acc: 0.7575\n",
      "loss: 1.614552652835846, train acc: 0.765\n",
      "loss: 1.64748512506485, train acc: 0.7676\n",
      "loss: 1.6280823230743409, train acc: 0.7612\n",
      "loss: 1.6009645581245422, train acc: 0.7628\n",
      "loss: 1.6271374583244325, train acc: 0.7721\n",
      "loss: 1.6284975051879882, train acc: 0.7703\n",
      "epoch: 4, loss: 1.716736912727356, train acc: 0.7703, test acc: 0.7402\n",
      "loss: 1.5906370878219604, train acc: 0.7665\n",
      "loss: 1.591474449634552, train acc: 0.7728\n",
      "loss: 1.6359846472740174, train acc: 0.7712\n",
      "loss: 1.6280524849891662, train acc: 0.7791\n",
      "loss: 1.6306881785392762, train acc: 0.7752\n",
      "loss: 1.6249524235725403, train acc: 0.7833\n",
      "loss: 1.6212528944015503, train acc: 0.7856\n",
      "loss: 1.606080448627472, train acc: 0.7873\n",
      "epoch: 5, loss: 1.4607142210006714, train acc: 0.7873, test acc: 0.7634\n",
      "loss: 1.5314979553222656, train acc: 0.7822\n",
      "loss: 1.576481556892395, train acc: 0.7869\n",
      "loss: 1.605054783821106, train acc: 0.7883\n",
      "loss: 1.659623372554779, train acc: 0.7924\n",
      "loss: 1.6078456878662108, train acc: 0.7928\n",
      "loss: 1.5759369015693665, train acc: 0.7926\n",
      "loss: 1.5828762173652648, train acc: 0.7998\n",
      "loss: 1.5942057013511657, train acc: 0.7993\n",
      "epoch: 6, loss: 1.4630728960037231, train acc: 0.7993, test acc: 0.7724\n",
      "loss: 1.6179248094558716, train acc: 0.7981\n",
      "loss: 1.5957605838775635, train acc: 0.7993\n",
      "loss: 1.5847680211067199, train acc: 0.7962\n",
      "loss: 1.6186937093734741, train acc: 0.7982\n",
      "loss: 1.5915971398353577, train acc: 0.7984\n",
      "loss: 1.6113906383514405, train acc: 0.8006\n",
      "loss: 1.6031822443008423, train acc: 0.8018\n",
      "loss: 1.6261111497879028, train acc: 0.7988\n",
      "epoch: 7, loss: 1.4725799560546875, train acc: 0.7988, test acc: 0.7722\n",
      "loss: 1.6230309009552002, train acc: 0.7992\n",
      "loss: 1.6026927709579468, train acc: 0.7953\n",
      "loss: 1.6003446698188781, train acc: 0.8005\n",
      "loss: 1.5716227293014526, train acc: 0.8063\n",
      "loss: 1.634345579147339, train acc: 0.8061\n",
      "loss: 1.5691481351852417, train acc: 0.8029\n",
      "loss: 1.5762015581130981, train acc: 0.8091\n",
      "loss: 1.5535010814666748, train acc: 0.8096\n",
      "epoch: 8, loss: 1.4971778392791748, train acc: 0.8096, test acc: 0.781\n",
      "loss: 1.5570858716964722, train acc: 0.8033\n",
      "loss: 1.5555533051490784, train acc: 0.811\n",
      "loss: 1.5406954765319825, train acc: 0.808\n",
      "loss: 1.5989342331886292, train acc: 0.81\n",
      "loss: 1.5986612677574157, train acc: 0.8073\n",
      "loss: 1.5641423225402833, train acc: 0.8081\n",
      "loss: 1.5554486155509948, train acc: 0.8144\n",
      "loss: 1.610782539844513, train acc: 0.81\n",
      "epoch: 9, loss: 1.5235438346862793, train acc: 0.81, test acc: 0.7822\n",
      "loss: 1.7131576538085938, train acc: 0.8079\n",
      "loss: 1.6256002187728882, train acc: 0.8136\n",
      "loss: 1.5997052788734436, train acc: 0.812\n",
      "loss: 1.576081359386444, train acc: 0.8117\n",
      "loss: 1.5253255248069764, train acc: 0.8041\n",
      "loss: 1.5322363257408143, train acc: 0.8098\n",
      "loss: 1.5837581157684326, train acc: 0.8133\n",
      "loss: 1.5571830034255982, train acc: 0.8139\n",
      "epoch: 10, loss: 1.3941839933395386, train acc: 0.8139, test acc: 0.7825\n",
      "loss: 1.6074485778808594, train acc: 0.8088\n",
      "loss: 1.5473262667655945, train acc: 0.8148\n",
      "loss: 1.5338472962379455, train acc: 0.8143\n",
      "loss: 1.5785783052444458, train acc: 0.817\n",
      "loss: 1.5557135343551636, train acc: 0.8152\n",
      "loss: 1.5735310792922974, train acc: 0.812\n",
      "loss: 1.5499873161315918, train acc: 0.8136\n",
      "loss: 1.5240415334701538, train acc: 0.812\n",
      "epoch: 11, loss: 1.5359916687011719, train acc: 0.812, test acc: 0.7832\n",
      "loss: 1.5964760780334473, train acc: 0.8091\n",
      "loss: 1.611265778541565, train acc: 0.8103\n",
      "loss: 1.5549382090568542, train acc: 0.8085\n",
      "loss: 1.5622298121452332, train acc: 0.8134\n",
      "loss: 1.5552454948425294, train acc: 0.8185\n",
      "loss: 1.5786830186843872, train acc: 0.8125\n",
      "loss: 1.591878068447113, train acc: 0.8185\n",
      "loss: 1.5364026069641112, train acc: 0.8181\n",
      "epoch: 12, loss: 1.3832143545150757, train acc: 0.8181, test acc: 0.7881\n",
      "loss: 1.6847883462905884, train acc: 0.8182\n",
      "loss: 1.5480218768119811, train acc: 0.8171\n",
      "loss: 1.5770012021064759, train acc: 0.8104\n",
      "loss: 1.5782868981361389, train acc: 0.8135\n",
      "loss: 1.5465558409690856, train acc: 0.8187\n",
      "loss: 1.5849923372268677, train acc: 0.8158\n",
      "loss: 1.5469670176506043, train acc: 0.821\n",
      "loss: 1.588967990875244, train acc: 0.8139\n",
      "epoch: 13, loss: 1.5010415315628052, train acc: 0.8139, test acc: 0.7928\n",
      "loss: 1.5596102476119995, train acc: 0.8182\n",
      "loss: 1.5591065168380738, train acc: 0.8156\n",
      "loss: 1.528053629398346, train acc: 0.8169\n",
      "loss: 1.5534839987754823, train acc: 0.8158\n",
      "loss: 1.5152911901474, train acc: 0.817\n",
      "loss: 1.529598307609558, train acc: 0.8162\n",
      "loss: 1.5449654579162597, train acc: 0.815\n",
      "loss: 1.5681962847709656, train acc: 0.8188\n",
      "epoch: 14, loss: 1.5196633338928223, train acc: 0.8188, test acc: 0.7861\n",
      "loss: 1.6124858856201172, train acc: 0.8169\n",
      "loss: 1.5755671262741089, train acc: 0.8159\n",
      "loss: 1.5371850967407226, train acc: 0.8137\n",
      "loss: 1.624710738658905, train acc: 0.8178\n",
      "loss: 1.56979957818985, train acc: 0.8165\n",
      "loss: 1.5663285374641418, train acc: 0.817\n",
      "loss: 1.5682132244110107, train acc: 0.822\n",
      "loss: 1.5678625583648682, train acc: 0.8178\n",
      "epoch: 15, loss: 1.5677298307418823, train acc: 0.8178, test acc: 0.7862\n",
      "loss: 1.5777302980422974, train acc: 0.8186\n",
      "loss: 1.5619668841362, train acc: 0.8202\n",
      "loss: 1.5232712626457214, train acc: 0.8193\n",
      "loss: 1.508184516429901, train acc: 0.8197\n",
      "loss: 1.5897761821746825, train acc: 0.8207\n",
      "loss: 1.565941333770752, train acc: 0.821\n",
      "loss: 1.5559338927268982, train acc: 0.8226\n",
      "loss: 1.538381826877594, train acc: 0.8194\n",
      "epoch: 16, loss: 1.5668039321899414, train acc: 0.8194, test acc: 0.7831\n",
      "loss: 1.52683687210083, train acc: 0.8157\n",
      "loss: 1.5259893774986266, train acc: 0.8182\n",
      "loss: 1.5579813718795776, train acc: 0.815\n",
      "loss: 1.587226688861847, train acc: 0.8198\n",
      "loss: 1.5733206391334533, train acc: 0.8232\n",
      "loss: 1.5687490105628967, train acc: 0.8225\n",
      "loss: 1.5683413743972778, train acc: 0.8232\n",
      "loss: 1.5717509746551515, train acc: 0.8183\n",
      "epoch: 17, loss: 1.639434576034546, train acc: 0.8183, test acc: 0.7815\n",
      "loss: 1.5145927667617798, train acc: 0.8083\n",
      "loss: 1.5411823630332946, train acc: 0.8085\n",
      "loss: 1.5396815299987794, train acc: 0.8186\n",
      "loss: 1.5690515041351318, train acc: 0.8127\n",
      "loss: 1.561244249343872, train acc: 0.8155\n",
      "loss: 1.538273561000824, train acc: 0.8141\n",
      "loss: 1.5404923677444458, train acc: 0.8211\n",
      "loss: 1.614993941783905, train acc: 0.8188\n",
      "epoch: 18, loss: 1.6106022596359253, train acc: 0.8188, test acc: 0.7876\n",
      "loss: 1.5228551626205444, train acc: 0.8213\n",
      "loss: 1.4995187759399413, train acc: 0.8148\n",
      "loss: 1.5430346369743346, train acc: 0.8123\n",
      "loss: 1.5683520674705504, train acc: 0.8201\n",
      "loss: 1.5614861369132995, train acc: 0.8175\n",
      "loss: 1.5783139705657958, train acc: 0.819\n",
      "loss: 1.5230957746505738, train acc: 0.822\n",
      "loss: 1.528574800491333, train acc: 0.8232\n",
      "epoch: 19, loss: 1.5338339805603027, train acc: 0.8232, test acc: 0.7886\n",
      "loss: 1.591444730758667, train acc: 0.8233\n",
      "loss: 1.5325432658195495, train acc: 0.8208\n",
      "loss: 1.5522321820259095, train acc: 0.8101\n",
      "loss: 1.5289969563484191, train acc: 0.8198\n",
      "loss: 1.5267103910446167, train acc: 0.8204\n",
      "loss: 1.5768334031105042, train acc: 0.8228\n",
      "loss: 1.5499764323234557, train acc: 0.8225\n",
      "loss: 1.5558237195014955, train acc: 0.8223\n",
      "epoch: 20, loss: 1.154234528541565, train acc: 0.8223, test acc: 0.7795\n",
      "loss: 1.4908629655838013, train acc: 0.8113\n",
      "loss: 1.5466879606246948, train acc: 0.8143\n",
      "loss: 1.5665672421455383, train acc: 0.808\n",
      "loss: 1.5839173078536988, train acc: 0.8197\n",
      "loss: 1.5634819626808167, train acc: 0.821\n",
      "loss: 1.5658958435058594, train acc: 0.8193\n",
      "loss: 1.5599351286888123, train acc: 0.8235\n",
      "loss: 1.5261247515678407, train acc: 0.8185\n",
      "epoch: 21, loss: 1.8364028930664062, train acc: 0.8185, test acc: 0.7793\n",
      "loss: 1.632229208946228, train acc: 0.818\n",
      "loss: 1.569836711883545, train acc: 0.8177\n",
      "loss: 1.5141636848449707, train acc: 0.8186\n",
      "loss: 1.5525978922843933, train acc: 0.821\n",
      "loss: 1.5734106659889222, train acc: 0.8171\n",
      "loss: 1.5580904722213744, train acc: 0.8195\n",
      "loss: 1.5866210818290711, train acc: 0.8216\n",
      "loss: 1.5396713614463806, train acc: 0.8248\n",
      "epoch: 22, loss: 1.6043922901153564, train acc: 0.8248, test acc: 0.7836\n",
      "loss: 1.4762532711029053, train acc: 0.8204\n",
      "loss: 1.528136956691742, train acc: 0.8226\n",
      "loss: 1.5328083872795104, train acc: 0.8136\n",
      "loss: 1.529344367980957, train acc: 0.8162\n",
      "loss: 1.557467234134674, train acc: 0.8235\n",
      "loss: 1.5632559180259704, train acc: 0.8216\n",
      "loss: 1.5250691533088685, train acc: 0.823\n",
      "loss: 1.5389280438423156, train acc: 0.8213\n",
      "epoch: 23, loss: 1.7540501356124878, train acc: 0.8213, test acc: 0.786\n",
      "loss: 1.5683355331420898, train acc: 0.8199\n",
      "loss: 1.5620340824127197, train acc: 0.8157\n",
      "loss: 1.5426343560218811, train acc: 0.8122\n",
      "loss: 1.5574041604995728, train acc: 0.8205\n",
      "loss: 1.5819136261940003, train acc: 0.818\n",
      "loss: 1.5373862504959106, train acc: 0.8242\n",
      "loss: 1.5490403294563293, train acc: 0.8227\n",
      "loss: 1.5594173312187194, train acc: 0.8252\n",
      "epoch: 24, loss: 1.5053627490997314, train acc: 0.8252, test acc: 0.788\n",
      "loss: 1.7079042196273804, train acc: 0.8226\n",
      "loss: 1.5266010642051697, train acc: 0.8175\n",
      "loss: 1.510528290271759, train acc: 0.8159\n",
      "loss: 1.5117088317871095, train acc: 0.8231\n",
      "loss: 1.5469486355781554, train acc: 0.8192\n",
      "loss: 1.5503875970840455, train acc: 0.8245\n",
      "loss: 1.5421025037765503, train acc: 0.8254\n",
      "loss: 1.5151324987411499, train acc: 0.8262\n",
      "epoch: 25, loss: 1.348525047302246, train acc: 0.8262, test acc: 0.7865\n",
      "loss: 1.6594797372817993, train acc: 0.8243\n",
      "loss: 1.5445428609848022, train acc: 0.8105\n",
      "loss: 1.5210226416587829, train acc: 0.8207\n",
      "loss: 1.5513289093971252, train acc: 0.823\n",
      "loss: 1.5672998547554016, train acc: 0.8203\n",
      "loss: 1.5382197976112366, train acc: 0.82\n",
      "loss: 1.5581423759460449, train acc: 0.8232\n",
      "loss: 1.512503695487976, train acc: 0.8207\n",
      "epoch: 26, loss: 1.1979217529296875, train acc: 0.8207, test acc: 0.7855\n",
      "loss: 1.5128147602081299, train acc: 0.8211\n",
      "loss: 1.5061505317687989, train acc: 0.8196\n",
      "loss: 1.526270604133606, train acc: 0.8212\n",
      "loss: 1.4997840523719788, train acc: 0.8226\n",
      "loss: 1.5135985732078552, train acc: 0.8224\n",
      "loss: 1.551973330974579, train acc: 0.8239\n",
      "loss: 1.5635647177696228, train acc: 0.8244\n",
      "loss: 1.5330957412719726, train acc: 0.8197\n",
      "epoch: 27, loss: 1.4526840448379517, train acc: 0.8197, test acc: 0.7879\n",
      "loss: 1.6026626825332642, train acc: 0.8217\n",
      "loss: 1.5074157118797302, train acc: 0.8233\n",
      "loss: 1.526865816116333, train acc: 0.818\n",
      "loss: 1.5334837079048156, train acc: 0.8238\n",
      "loss: 1.5087332844734191, train acc: 0.8246\n",
      "loss: 1.540312910079956, train acc: 0.8224\n",
      "loss: 1.5063976049423218, train acc: 0.8214\n",
      "loss: 1.5735566377639771, train acc: 0.8216\n",
      "epoch: 28, loss: 1.697184681892395, train acc: 0.8216, test acc: 0.7828\n",
      "loss: 1.6633859872817993, train acc: 0.8154\n",
      "loss: 1.513857650756836, train acc: 0.8169\n",
      "loss: 1.519504976272583, train acc: 0.8205\n",
      "loss: 1.5496829867362976, train acc: 0.8187\n",
      "loss: 1.530476975440979, train acc: 0.8226\n",
      "loss: 1.5588702917099, train acc: 0.8199\n",
      "loss: 1.4739904761314393, train acc: 0.8255\n",
      "loss: 1.5310931563377381, train acc: 0.8221\n",
      "epoch: 29, loss: 1.5821161270141602, train acc: 0.8221, test acc: 0.7851\n",
      "loss: 1.6056058406829834, train acc: 0.8221\n",
      "loss: 1.5154383063316346, train acc: 0.8254\n",
      "loss: 1.52715665102005, train acc: 0.8208\n",
      "loss: 1.5293602108955384, train acc: 0.8268\n",
      "loss: 1.5299835801124573, train acc: 0.8219\n",
      "loss: 1.5520451426506043, train acc: 0.8226\n",
      "loss: 1.5168067336082458, train acc: 0.8223\n",
      "loss: 1.4996690988540649, train acc: 0.8172\n",
      "epoch: 30, loss: 1.3056925535202026, train acc: 0.8172, test acc: 0.783\n",
      "loss: 1.4503756761550903, train acc: 0.8171\n",
      "loss: 1.525101113319397, train acc: 0.8203\n",
      "loss: 1.5477120280265808, train acc: 0.8202\n",
      "loss: 1.5345281720161439, train acc: 0.8169\n",
      "loss: 1.5752470731735229, train acc: 0.8205\n",
      "loss: 1.5108009457588196, train acc: 0.8226\n",
      "loss: 1.5307995080947876, train acc: 0.8236\n",
      "loss: 1.5408823013305664, train acc: 0.8226\n",
      "epoch: 31, loss: 1.2946465015411377, train acc: 0.8226, test acc: 0.7887\n",
      "loss: 1.486993670463562, train acc: 0.8193\n",
      "loss: 1.5094630718231201, train acc: 0.8204\n",
      "loss: 1.5144675850868226, train acc: 0.8208\n",
      "loss: 1.5431840419769287, train acc: 0.8235\n",
      "loss: 1.5238479018211364, train acc: 0.8236\n",
      "loss: 1.54091796875, train acc: 0.8229\n",
      "loss: 1.5172141075134278, train acc: 0.8241\n",
      "loss: 1.5255692601203918, train acc: 0.8226\n",
      "epoch: 32, loss: 1.2513408660888672, train acc: 0.8226, test acc: 0.7903\n",
      "loss: 1.5694339275360107, train acc: 0.823\n",
      "loss: 1.5089521288871766, train acc: 0.8252\n",
      "loss: 1.5263046145439148, train acc: 0.8203\n",
      "loss: 1.5605846524238587, train acc: 0.8212\n",
      "loss: 1.5370481967926026, train acc: 0.8263\n",
      "loss: 1.5303362607955933, train acc: 0.8242\n",
      "loss: 1.5024405598640442, train acc: 0.8255\n",
      "loss: 1.51624538898468, train acc: 0.8232\n",
      "epoch: 33, loss: 1.3436115980148315, train acc: 0.8232, test acc: 0.7891\n",
      "loss: 1.5290310382843018, train acc: 0.8206\n",
      "loss: 1.525260829925537, train acc: 0.8254\n",
      "loss: 1.5084751725196839, train acc: 0.8217\n",
      "loss: 1.5256906747817993, train acc: 0.8231\n",
      "loss: 1.4998373866081238, train acc: 0.8203\n",
      "loss: 1.530441904067993, train acc: 0.8216\n",
      "loss: 1.491880977153778, train acc: 0.8241\n",
      "loss: 1.5345639348030091, train acc: 0.8233\n",
      "epoch: 34, loss: 1.6770095825195312, train acc: 0.8233, test acc: 0.7817\n",
      "loss: 1.5649138689041138, train acc: 0.8169\n",
      "loss: 1.5203471422195434, train acc: 0.8224\n",
      "loss: 1.4897723078727723, train acc: 0.8237\n",
      "loss: 1.5115403413772583, train acc: 0.8261\n",
      "loss: 1.4977493286132812, train acc: 0.8223\n",
      "loss: 1.5542001724243164, train acc: 0.8244\n",
      "loss: 1.5301603078842163, train acc: 0.8252\n",
      "loss: 1.5079349160194397, train acc: 0.8243\n",
      "epoch: 35, loss: 1.785387635231018, train acc: 0.8243, test acc: 0.7866\n",
      "loss: 1.5414601564407349, train acc: 0.8205\n",
      "loss: 1.5077468037605286, train acc: 0.8231\n",
      "loss: 1.5233880639076234, train acc: 0.8232\n",
      "loss: 1.5675062537193298, train acc: 0.8245\n",
      "loss: 1.520479416847229, train acc: 0.8242\n",
      "loss: 1.5260887742042542, train acc: 0.8236\n",
      "loss: 1.5606701493263244, train acc: 0.8232\n",
      "loss: 1.5240272641181947, train acc: 0.8218\n",
      "epoch: 36, loss: 1.6584110260009766, train acc: 0.8218, test acc: 0.7892\n",
      "loss: 1.5981193780899048, train acc: 0.8246\n",
      "loss: 1.5449124097824096, train acc: 0.8146\n",
      "loss: 1.5102228045463562, train acc: 0.8187\n",
      "loss: 1.5679153203964233, train acc: 0.8188\n",
      "loss: 1.5222291946411133, train acc: 0.82\n",
      "loss: 1.5202717661857605, train acc: 0.8234\n",
      "loss: 1.5177793979644776, train acc: 0.8243\n",
      "loss: 1.4763207674026488, train acc: 0.8202\n",
      "epoch: 37, loss: 1.368151307106018, train acc: 0.8202, test acc: 0.7855\n",
      "loss: 1.5238864421844482, train acc: 0.8195\n",
      "loss: 1.5526373982429504, train acc: 0.822\n",
      "loss: 1.5107906103134154, train acc: 0.8184\n",
      "loss: 1.5455859422683715, train acc: 0.8225\n",
      "loss: 1.516815459728241, train acc: 0.8243\n",
      "loss: 1.5245208621025086, train acc: 0.8265\n",
      "loss: 1.505332887172699, train acc: 0.8272\n",
      "loss: 1.4944032549858093, train acc: 0.8283\n",
      "epoch: 38, loss: 1.3807061910629272, train acc: 0.8283, test acc: 0.784\n",
      "loss: 1.429522156715393, train acc: 0.8179\n",
      "loss: 1.498861312866211, train acc: 0.8188\n",
      "loss: 1.5309945344924927, train acc: 0.8193\n",
      "loss: 1.5093666315078735, train acc: 0.822\n",
      "loss: 1.5230356931686402, train acc: 0.8243\n",
      "loss: 1.5201107263565063, train acc: 0.824\n",
      "loss: 1.5243326902389527, train acc: 0.8228\n",
      "loss: 1.528428065776825, train acc: 0.8234\n",
      "epoch: 39, loss: 1.057047724723816, train acc: 0.8234, test acc: 0.788\n",
      "loss: 1.4419549703598022, train acc: 0.8249\n",
      "loss: 1.5174793601036072, train acc: 0.8224\n",
      "loss: 1.5036932468414306, train acc: 0.8184\n",
      "loss: 1.5144940853118896, train acc: 0.8228\n",
      "loss: 1.5329047560691833, train acc: 0.8221\n",
      "loss: 1.5370263576507568, train acc: 0.8257\n",
      "loss: 1.5371589541435242, train acc: 0.8274\n",
      "loss: 1.5200885891914369, train acc: 0.8255\n",
      "epoch: 40, loss: 1.289286494255066, train acc: 0.8255, test acc: 0.7912\n",
      "loss: 1.4954237937927246, train acc: 0.823\n",
      "loss: 1.4984935522079468, train acc: 0.8249\n",
      "loss: 1.4711858749389648, train acc: 0.8251\n",
      "loss: 1.532520055770874, train acc: 0.8249\n",
      "loss: 1.544983685016632, train acc: 0.8233\n",
      "loss: 1.5234917759895326, train acc: 0.8271\n",
      "loss: 1.5048975110054017, train acc: 0.8252\n",
      "loss: 1.5426884770393372, train acc: 0.8246\n",
      "epoch: 41, loss: 1.6753467321395874, train acc: 0.8246, test acc: 0.7845\n",
      "loss: 1.504148006439209, train acc: 0.8191\n",
      "loss: 1.4952180624008178, train acc: 0.825\n",
      "loss: 1.5131102204322815, train acc: 0.8176\n",
      "loss: 1.5289257168769836, train acc: 0.8253\n",
      "loss: 1.517556357383728, train acc: 0.8286\n",
      "loss: 1.518685758113861, train acc: 0.8261\n",
      "loss: 1.5177556157112122, train acc: 0.8291\n",
      "loss: 1.533917450904846, train acc: 0.8237\n",
      "epoch: 42, loss: 1.3727370500564575, train acc: 0.8237, test acc: 0.7917\n",
      "loss: 1.4489175081253052, train acc: 0.8245\n",
      "loss: 1.4916634798049926, train acc: 0.8241\n",
      "loss: 1.4970123291015625, train acc: 0.8241\n",
      "loss: 1.5122213959693909, train acc: 0.8239\n",
      "loss: 1.4878417253494263, train acc: 0.8281\n",
      "loss: 1.523658239841461, train acc: 0.828\n",
      "loss: 1.525676441192627, train acc: 0.8254\n",
      "loss: 1.520752215385437, train acc: 0.8258\n",
      "epoch: 43, loss: 0.9315099716186523, train acc: 0.8258, test acc: 0.7886\n",
      "loss: 1.5235188007354736, train acc: 0.8244\n",
      "loss: 1.5010215997695924, train acc: 0.8234\n",
      "loss: 1.5162972450256347, train acc: 0.8194\n",
      "loss: 1.5151304244995116, train acc: 0.8264\n",
      "loss: 1.5315449833869934, train acc: 0.8165\n",
      "loss: 1.5289928317070007, train acc: 0.8243\n",
      "loss: 1.4848897576332092, train acc: 0.8242\n",
      "loss: 1.5066470980644227, train acc: 0.8258\n",
      "epoch: 44, loss: 1.7777650356292725, train acc: 0.8258, test acc: 0.7901\n",
      "loss: 1.5464590787887573, train acc: 0.8262\n",
      "loss: 1.4714266419410706, train acc: 0.8254\n",
      "loss: 1.514855396747589, train acc: 0.8246\n",
      "loss: 1.5330800056457519, train acc: 0.8252\n",
      "loss: 1.5088136553764344, train acc: 0.8212\n",
      "loss: 1.4836376667022706, train acc: 0.8246\n",
      "loss: 1.5316633462905884, train acc: 0.8265\n",
      "loss: 1.5632307529449463, train acc: 0.8259\n",
      "epoch: 45, loss: 1.2757045030593872, train acc: 0.8259, test acc: 0.7804\n",
      "loss: 1.4745805263519287, train acc: 0.8185\n",
      "loss: 1.5323753595352172, train acc: 0.8238\n",
      "loss: 1.5451401352882386, train acc: 0.8193\n",
      "loss: 1.514924454689026, train acc: 0.825\n",
      "loss: 1.4991159915924073, train acc: 0.8217\n",
      "loss: 1.4924904942512511, train acc: 0.8248\n",
      "loss: 1.5263176083564758, train acc: 0.8234\n",
      "loss: 1.4996417760849, train acc: 0.823\n",
      "epoch: 46, loss: 1.246569037437439, train acc: 0.823, test acc: 0.7831\n",
      "loss: 1.4219471216201782, train acc: 0.8248\n",
      "loss: 1.499234712123871, train acc: 0.8253\n",
      "loss: 1.5384504914283752, train acc: 0.8087\n",
      "loss: 1.539663052558899, train acc: 0.8248\n",
      "loss: 1.4794002056121827, train acc: 0.8255\n",
      "loss: 1.5140767812728881, train acc: 0.8247\n",
      "loss: 1.5478594541549682, train acc: 0.8239\n",
      "loss: 1.5112768411636353, train acc: 0.8257\n",
      "epoch: 47, loss: 1.5279639959335327, train acc: 0.8257, test acc: 0.7872\n",
      "loss: 1.5207372903823853, train acc: 0.8207\n",
      "loss: 1.5253222465515137, train acc: 0.8272\n",
      "loss: 1.5139650225639343, train acc: 0.8232\n",
      "loss: 1.5283961415290832, train acc: 0.8285\n",
      "loss: 1.5325691103935242, train acc: 0.8281\n",
      "loss: 1.5233480334281921, train acc: 0.8304\n",
      "loss: 1.5237275958061218, train acc: 0.8295\n",
      "loss: 1.483396339416504, train acc: 0.8247\n",
      "epoch: 48, loss: 1.4249966144561768, train acc: 0.8247, test acc: 0.7827\n",
      "loss: 1.6414713859558105, train acc: 0.8198\n",
      "loss: 1.5274345517158507, train acc: 0.8193\n",
      "loss: 1.5172760844230653, train acc: 0.8227\n",
      "loss: 1.5720061898231505, train acc: 0.8226\n",
      "loss: 1.5346836686134337, train acc: 0.822\n",
      "loss: 1.5281869292259216, train acc: 0.8288\n",
      "loss: 1.50202876329422, train acc: 0.8261\n",
      "loss: 1.5267550706863404, train acc: 0.8247\n",
      "epoch: 49, loss: 1.5674382448196411, train acc: 0.8247, test acc: 0.786\n",
      "loss: 1.3764419555664062, train acc: 0.8235\n",
      "loss: 1.488452970981598, train acc: 0.8294\n",
      "loss: 1.5366825103759765, train acc: 0.8273\n",
      "loss: 1.541811192035675, train acc: 0.826\n",
      "loss: 1.5170350551605225, train acc: 0.8256\n",
      "loss: 1.5381325721740722, train acc: 0.8288\n",
      "loss: 1.4863320112228393, train acc: 0.8274\n",
      "loss: 1.5098082304000855, train acc: 0.8284\n",
      "epoch: 50, loss: 1.587754249572754, train acc: 0.8284, test acc: 0.7811\n",
      "loss: 1.5088616609573364, train acc: 0.8236\n",
      "loss: 1.5459614276885987, train acc: 0.8294\n",
      "loss: 1.5443088293075562, train acc: 0.8278\n",
      "loss: 1.5379352569580078, train acc: 0.8296\n",
      "loss: 1.5259788036346436, train acc: 0.8288\n",
      "loss: 1.4584770679473877, train acc: 0.8306\n",
      "loss: 1.5060261964797974, train acc: 0.8265\n",
      "loss: 1.531218910217285, train acc: 0.8253\n",
      "epoch: 51, loss: 1.6764678955078125, train acc: 0.8253, test acc: 0.7799\n",
      "loss: 1.5206148624420166, train acc: 0.8226\n",
      "loss: 1.5304536461830138, train acc: 0.8264\n",
      "loss: 1.5335286736488343, train acc: 0.8117\n",
      "loss: 1.4915762662887573, train acc: 0.8283\n",
      "loss: 1.5088669180870056, train acc: 0.8301\n",
      "loss: 1.476527214050293, train acc: 0.829\n",
      "loss: 1.5088627338409424, train acc: 0.8305\n",
      "loss: 1.5260825157165527, train acc: 0.8224\n",
      "epoch: 52, loss: 1.9326281547546387, train acc: 0.8224, test acc: 0.7834\n",
      "loss: 1.4276961088180542, train acc: 0.8285\n",
      "loss: 1.4725033283233642, train acc: 0.8277\n",
      "loss: 1.5300177812576294, train acc: 0.8282\n",
      "loss: 1.5267022013664246, train acc: 0.8316\n",
      "loss: 1.5084721922874451, train acc: 0.8274\n",
      "loss: 1.5430575370788575, train acc: 0.829\n",
      "loss: 1.5150217533111572, train acc: 0.8263\n",
      "loss: 1.5070760130882264, train acc: 0.83\n",
      "epoch: 53, loss: 1.527145504951477, train acc: 0.83, test acc: 0.7772\n",
      "loss: 1.5231274366378784, train acc: 0.8201\n",
      "loss: 1.4901482343673706, train acc: 0.8325\n",
      "loss: 1.4600274324417115, train acc: 0.8273\n",
      "loss: 1.496320378780365, train acc: 0.831\n",
      "loss: 1.5114952802658081, train acc: 0.8321\n",
      "loss: 1.4950207114219665, train acc: 0.828\n",
      "loss: 1.5001692652702332, train acc: 0.8298\n",
      "loss: 1.5560450673103332, train acc: 0.8263\n",
      "epoch: 54, loss: 1.4125401973724365, train acc: 0.8263, test acc: 0.7843\n",
      "loss: 1.506643533706665, train acc: 0.8228\n",
      "loss: 1.491362166404724, train acc: 0.8258\n",
      "loss: 1.450832974910736, train acc: 0.8212\n",
      "loss: 1.4737484812736512, train acc: 0.8281\n",
      "loss: 1.5262996673583984, train acc: 0.825\n",
      "loss: 1.533666181564331, train acc: 0.8285\n",
      "loss: 1.5065608024597168, train acc: 0.8264\n",
      "loss: 1.4786054491996765, train acc: 0.8286\n",
      "epoch: 55, loss: 1.3968753814697266, train acc: 0.8286, test acc: 0.7869\n",
      "loss: 1.4835174083709717, train acc: 0.8293\n",
      "loss: 1.4765697121620178, train acc: 0.8213\n",
      "loss: 1.53091242313385, train acc: 0.8312\n",
      "loss: 1.5229147315025329, train acc: 0.8281\n",
      "loss: 1.5142500758171082, train acc: 0.8272\n",
      "loss: 1.5216818928718567, train acc: 0.8295\n",
      "loss: 1.5010111927986145, train acc: 0.8306\n",
      "loss: 1.5025742173194885, train acc: 0.833\n",
      "epoch: 56, loss: 1.2880076169967651, train acc: 0.833, test acc: 0.7877\n",
      "loss: 1.5843688249588013, train acc: 0.8299\n",
      "loss: 1.5346220850944519, train acc: 0.8288\n",
      "loss: 1.464847159385681, train acc: 0.8285\n",
      "loss: 1.5170825362205504, train acc: 0.8273\n",
      "loss: 1.4846052408218384, train acc: 0.8245\n",
      "loss: 1.5428286075592041, train acc: 0.833\n",
      "loss: 1.4797032833099366, train acc: 0.8344\n",
      "loss: 1.5038671970367432, train acc: 0.8328\n",
      "epoch: 57, loss: 1.5611193180084229, train acc: 0.8328, test acc: 0.7873\n",
      "loss: 1.5108412504196167, train acc: 0.8316\n",
      "loss: 1.4984747171401978, train acc: 0.8293\n",
      "loss: 1.5217668294906617, train acc: 0.8296\n",
      "loss: 1.5025124430656434, train acc: 0.8305\n",
      "loss: 1.5338795065879822, train acc: 0.8234\n",
      "loss: 1.5461647629737854, train acc: 0.8279\n",
      "loss: 1.5316255927085876, train acc: 0.832\n",
      "loss: 1.5293615579605102, train acc: 0.83\n",
      "epoch: 58, loss: 1.2987220287322998, train acc: 0.83, test acc: 0.787\n",
      "loss: 1.5891705751419067, train acc: 0.8281\n",
      "loss: 1.5242269396781922, train acc: 0.8331\n",
      "loss: 1.5010583400726318, train acc: 0.8255\n",
      "loss: 1.5355262398719787, train acc: 0.8289\n",
      "loss: 1.5253190040588378, train acc: 0.828\n",
      "loss: 1.537956166267395, train acc: 0.8311\n",
      "loss: 1.5098295211791992, train acc: 0.8314\n",
      "loss: 1.5299785375595092, train acc: 0.8328\n",
      "epoch: 59, loss: 1.774800419807434, train acc: 0.8328, test acc: 0.7865\n",
      "loss: 1.4919631481170654, train acc: 0.8286\n",
      "loss: 1.537718653678894, train acc: 0.8289\n",
      "loss: 1.4772939801216125, train acc: 0.8309\n",
      "loss: 1.541081464290619, train acc: 0.8342\n",
      "loss: 1.497288990020752, train acc: 0.8317\n",
      "loss: 1.517761218547821, train acc: 0.8347\n",
      "loss: 1.487639343738556, train acc: 0.832\n",
      "loss: 1.4670822262763976, train acc: 0.8305\n",
      "epoch: 60, loss: 1.4744514226913452, train acc: 0.8305, test acc: 0.7768\n",
      "loss: 1.4363173246383667, train acc: 0.819\n",
      "loss: 1.4839760065078735, train acc: 0.8271\n",
      "loss: 1.523518717288971, train acc: 0.8139\n",
      "loss: 1.494523048400879, train acc: 0.8301\n",
      "loss: 1.5166834354400636, train acc: 0.8302\n",
      "loss: 1.4868723034858704, train acc: 0.8331\n",
      "loss: 1.4756177544593811, train acc: 0.8317\n",
      "loss: 1.5037870407104492, train acc: 0.8335\n",
      "epoch: 61, loss: 1.4835318326950073, train acc: 0.8335, test acc: 0.7882\n",
      "loss: 1.5314909219741821, train acc: 0.8303\n",
      "loss: 1.529082715511322, train acc: 0.8331\n",
      "loss: 1.480126428604126, train acc: 0.8317\n",
      "loss: 1.554150676727295, train acc: 0.8322\n",
      "loss: 1.5393572330474854, train acc: 0.828\n",
      "loss: 1.5560251832008363, train acc: 0.825\n",
      "loss: 1.4858256220817565, train acc: 0.8309\n",
      "loss: 1.5409096479415894, train acc: 0.8319\n",
      "epoch: 62, loss: 1.3448479175567627, train acc: 0.8319, test acc: 0.7855\n",
      "loss: 1.3525776863098145, train acc: 0.8317\n",
      "loss: 1.5032955765724183, train acc: 0.8311\n",
      "loss: 1.4895083904266357, train acc: 0.834\n",
      "loss: 1.5517345309257506, train acc: 0.8254\n",
      "loss: 1.5413543701171875, train acc: 0.8272\n",
      "loss: 1.5174511551856995, train acc: 0.831\n",
      "loss: 1.500598418712616, train acc: 0.8276\n",
      "loss: 1.5000742197036743, train acc: 0.8308\n",
      "epoch: 63, loss: 1.3878175020217896, train acc: 0.8308, test acc: 0.7872\n",
      "loss: 1.6303609609603882, train acc: 0.828\n",
      "loss: 1.5274189710617065, train acc: 0.8325\n",
      "loss: 1.472694182395935, train acc: 0.8323\n",
      "loss: 1.5528483390808105, train acc: 0.8273\n",
      "loss: 1.4711034059524537, train acc: 0.8316\n",
      "loss: 1.5300412893295288, train acc: 0.8333\n",
      "loss: 1.5290756464004516, train acc: 0.8341\n",
      "loss: 1.5299830675125121, train acc: 0.8359\n",
      "epoch: 64, loss: 1.6245537996292114, train acc: 0.8359, test acc: 0.7888\n",
      "loss: 1.5316367149353027, train acc: 0.8327\n",
      "loss: 1.5544839262962342, train acc: 0.8349\n",
      "loss: 1.4688381671905517, train acc: 0.834\n",
      "loss: 1.4908202528953551, train acc: 0.8334\n",
      "loss: 1.4989451050758362, train acc: 0.8363\n",
      "loss: 1.52369065284729, train acc: 0.833\n",
      "loss: 1.4652365446090698, train acc: 0.8319\n",
      "loss: 1.4983654499053956, train acc: 0.8379\n",
      "epoch: 65, loss: 1.7141071557998657, train acc: 0.8379, test acc: 0.7834\n",
      "loss: 1.6071983575820923, train acc: 0.8313\n",
      "loss: 1.5369026422500611, train acc: 0.8368\n",
      "loss: 1.5121477723121644, train acc: 0.8377\n",
      "loss: 1.548758614063263, train acc: 0.8315\n",
      "loss: 1.4579029321670531, train acc: 0.8267\n",
      "loss: 1.539661693572998, train acc: 0.8294\n",
      "loss: 1.5288171291351318, train acc: 0.8342\n",
      "loss: 1.529171073436737, train acc: 0.8331\n",
      "epoch: 66, loss: 1.4932619333267212, train acc: 0.8331, test acc: 0.7884\n",
      "loss: 1.3315346240997314, train acc: 0.8316\n",
      "loss: 1.4119938611984253, train acc: 0.8329\n",
      "loss: 1.494112002849579, train acc: 0.828\n",
      "loss: 1.514656388759613, train acc: 0.8344\n",
      "loss: 1.495090126991272, train acc: 0.8322\n",
      "loss: 1.486644947528839, train acc: 0.8314\n",
      "loss: 1.5148327827453614, train acc: 0.8304\n",
      "loss: 1.5090766906738282, train acc: 0.8306\n",
      "epoch: 67, loss: 1.4047292470932007, train acc: 0.8306, test acc: 0.7858\n",
      "loss: 1.5275532007217407, train acc: 0.8301\n",
      "loss: 1.5306844472885133, train acc: 0.8344\n",
      "loss: 1.5085631608963013, train acc: 0.8341\n",
      "loss: 1.5170108556747437, train acc: 0.8343\n",
      "loss: 1.5195288181304931, train acc: 0.8326\n",
      "loss: 1.494336223602295, train acc: 0.8335\n",
      "loss: 1.4621797323226928, train acc: 0.8283\n",
      "loss: 1.543561625480652, train acc: 0.8297\n",
      "epoch: 68, loss: 1.6700435876846313, train acc: 0.8297, test acc: 0.7873\n",
      "loss: 1.4889534711837769, train acc: 0.8312\n",
      "loss: 1.5019471526145936, train acc: 0.8331\n",
      "loss: 1.4911330223083497, train acc: 0.8181\n",
      "loss: 1.5060486674308777, train acc: 0.8317\n",
      "loss: 1.4648615956306457, train acc: 0.831\n",
      "loss: 1.5165023684501648, train acc: 0.8292\n",
      "loss: 1.5130299091339112, train acc: 0.8331\n",
      "loss: 1.5141713976860047, train acc: 0.8343\n",
      "epoch: 69, loss: 1.5073215961456299, train acc: 0.8343, test acc: 0.7825\n",
      "loss: 1.4233386516571045, train acc: 0.8287\n",
      "loss: 1.4938547730445861, train acc: 0.834\n",
      "loss: 1.4843618988990783, train acc: 0.8289\n",
      "loss: 1.4907158970832826, train acc: 0.8309\n",
      "loss: 1.5264086723327637, train acc: 0.8296\n",
      "loss: 1.5373679757118226, train acc: 0.8293\n",
      "loss: 1.5073872447013854, train acc: 0.8339\n",
      "loss: 1.4828561425209046, train acc: 0.8338\n",
      "epoch: 70, loss: 1.5072922706604004, train acc: 0.8338, test acc: 0.784\n",
      "loss: 1.4449598789215088, train acc: 0.8315\n",
      "loss: 1.4644746422767638, train acc: 0.8342\n",
      "loss: 1.486684513092041, train acc: 0.8319\n",
      "loss: 1.5104115724563598, train acc: 0.8321\n",
      "loss: 1.4935425519943237, train acc: 0.8335\n",
      "loss: 1.5226179242134095, train acc: 0.832\n",
      "loss: 1.482127058506012, train acc: 0.8333\n",
      "loss: 1.5220107197761537, train acc: 0.8362\n",
      "epoch: 71, loss: 1.6200881004333496, train acc: 0.8362, test acc: 0.7889\n",
      "loss: 1.3609167337417603, train acc: 0.8316\n",
      "loss: 1.5137152671813965, train acc: 0.8335\n",
      "loss: 1.4857473850250245, train acc: 0.8326\n",
      "loss: 1.5638588070869446, train acc: 0.8329\n",
      "loss: 1.488745093345642, train acc: 0.8317\n",
      "loss: 1.4950483798980714, train acc: 0.8338\n",
      "loss: 1.509275984764099, train acc: 0.8333\n",
      "loss: 1.476635468006134, train acc: 0.8377\n",
      "epoch: 72, loss: 1.3013089895248413, train acc: 0.8377, test acc: 0.7853\n",
      "loss: 1.3742649555206299, train acc: 0.8279\n",
      "loss: 1.5278820395469666, train acc: 0.8356\n",
      "loss: 1.4810715556144713, train acc: 0.8326\n",
      "loss: 1.4897005558013916, train acc: 0.8352\n",
      "loss: 1.4577872037887574, train acc: 0.8334\n",
      "loss: 1.5096644043922425, train acc: 0.8286\n",
      "loss: 1.4616289496421815, train acc: 0.8328\n",
      "loss: 1.4641759514808654, train acc: 0.8374\n",
      "epoch: 73, loss: 1.2321739196777344, train acc: 0.8374, test acc: 0.7898\n",
      "loss: 1.5511372089385986, train acc: 0.834\n",
      "loss: 1.4711822152137757, train acc: 0.839\n",
      "loss: 1.4800766229629516, train acc: 0.8335\n",
      "loss: 1.5287684559822083, train acc: 0.8339\n",
      "loss: 1.4929713129997253, train acc: 0.8358\n",
      "loss: 1.526594054698944, train acc: 0.8337\n",
      "loss: 1.5053310155868531, train acc: 0.8348\n",
      "loss: 1.4990341067314148, train acc: 0.8332\n",
      "epoch: 74, loss: 1.088204264640808, train acc: 0.8332, test acc: 0.7827\n",
      "loss: 1.4881023168563843, train acc: 0.828\n",
      "loss: 1.47378009557724, train acc: 0.836\n",
      "loss: 1.5396349549293518, train acc: 0.8329\n",
      "loss: 1.5029130339622498, train acc: 0.8377\n",
      "loss: 1.4608601093292237, train acc: 0.8387\n",
      "loss: 1.5005636096000672, train acc: 0.838\n",
      "loss: 1.4849894523620606, train acc: 0.8324\n",
      "loss: 1.5242944002151488, train acc: 0.8317\n",
      "epoch: 75, loss: 1.1834137439727783, train acc: 0.8317, test acc: 0.786\n",
      "loss: 1.6026734113693237, train acc: 0.8326\n",
      "loss: 1.513893151283264, train acc: 0.8348\n",
      "loss: 1.506926465034485, train acc: 0.8351\n",
      "loss: 1.5408282160758973, train acc: 0.8315\n",
      "loss: 1.504040288925171, train acc: 0.8322\n",
      "loss: 1.5176396489143371, train acc: 0.8263\n",
      "loss: 1.449247419834137, train acc: 0.8352\n",
      "loss: 1.4662631630897522, train acc: 0.8341\n",
      "epoch: 76, loss: 1.5437129735946655, train acc: 0.8341, test acc: 0.7834\n",
      "loss: 1.5098636150360107, train acc: 0.8352\n",
      "loss: 1.506619107723236, train acc: 0.8406\n",
      "loss: 1.5427620172500611, train acc: 0.8335\n",
      "loss: 1.512264323234558, train acc: 0.8285\n",
      "loss: 1.4965053081512452, train acc: 0.8343\n",
      "loss: 1.5017640829086303, train acc: 0.8314\n",
      "loss: 1.5067793369293212, train acc: 0.8368\n",
      "loss: 1.521496343612671, train acc: 0.8342\n",
      "epoch: 77, loss: 1.4422955513000488, train acc: 0.8342, test acc: 0.7843\n",
      "loss: 1.4514591693878174, train acc: 0.8328\n",
      "loss: 1.4770663142204286, train acc: 0.8421\n",
      "loss: 1.455892527103424, train acc: 0.8325\n",
      "loss: 1.4833089113235474, train acc: 0.8375\n",
      "loss: 1.487804388999939, train acc: 0.8373\n",
      "loss: 1.5180561780929565, train acc: 0.8353\n",
      "loss: 1.4641313433647156, train acc: 0.8347\n",
      "loss: 1.4973565697669984, train acc: 0.8386\n",
      "epoch: 78, loss: 1.4158402681350708, train acc: 0.8386, test acc: 0.7852\n",
      "loss: 1.5870492458343506, train acc: 0.8361\n",
      "loss: 1.508684766292572, train acc: 0.838\n",
      "loss: 1.463628351688385, train acc: 0.8373\n",
      "loss: 1.4589062571525573, train acc: 0.8395\n",
      "loss: 1.417852807044983, train acc: 0.8376\n",
      "loss: 1.4931281447410583, train acc: 0.8382\n",
      "loss: 1.4893642902374267, train acc: 0.8384\n",
      "loss: 1.510118055343628, train acc: 0.8382\n",
      "epoch: 79, loss: 1.2375730276107788, train acc: 0.8382, test acc: 0.7857\n",
      "#####training and testing end with K:5, P:0.5######\n",
      "#####training and testing start with K:5, P:1######\n",
      "loss: 2.314932107925415, train acc: 0.1023\n",
      "loss: 2.249406266212463, train acc: 0.1519\n",
      "loss: 2.0990240812301635, train acc: 0.2123\n",
      "loss: 2.039447271823883, train acc: 0.2558\n",
      "loss: 1.8913922667503358, train acc: 0.337\n",
      "loss: 1.820646047592163, train acc: 0.3517\n",
      "loss: 1.761246430873871, train acc: 0.4007\n",
      "loss: 1.6989250302314758, train acc: 0.4095\n",
      "epoch: 0, loss: 1.5289692878723145, train acc: 0.4095, test acc: 0.4386\n",
      "loss: 1.5318236351013184, train acc: 0.4246\n",
      "loss: 1.5355328798294068, train acc: 0.4523\n",
      "loss: 1.4865063905715943, train acc: 0.5174\n",
      "loss: 1.4713366746902465, train acc: 0.5444\n",
      "loss: 1.3847899079322814, train acc: 0.5672\n",
      "loss: 1.3244736433029174, train acc: 0.5765\n",
      "loss: 1.307484996318817, train acc: 0.6192\n",
      "loss: 1.2359328031539918, train acc: 0.6201\n",
      "epoch: 1, loss: 1.0316095352172852, train acc: 0.6201, test acc: 0.6177\n",
      "loss: 1.0877795219421387, train acc: 0.6379\n",
      "loss: 1.1286904513835907, train acc: 0.6289\n",
      "loss: 1.107105302810669, train acc: 0.6347\n",
      "loss: 1.0989502966403961, train acc: 0.646\n",
      "loss: 1.0662457644939423, train acc: 0.6671\n",
      "loss: 1.0090281248092652, train acc: 0.6742\n",
      "loss: 1.0095869243144988, train acc: 0.6991\n",
      "loss: 0.9301827669143676, train acc: 0.7119\n",
      "epoch: 2, loss: 0.7932015657424927, train acc: 0.7119, test acc: 0.7057\n",
      "loss: 0.7439139485359192, train acc: 0.7234\n",
      "loss: 0.8403328001499176, train acc: 0.7396\n",
      "loss: 0.813919460773468, train acc: 0.7532\n",
      "loss: 0.8251349449157714, train acc: 0.7749\n",
      "loss: 0.792234355211258, train acc: 0.7813\n",
      "loss: 0.741847038269043, train acc: 0.7831\n",
      "loss: 0.7964253485202789, train acc: 0.7968\n",
      "loss: 0.7130230009555817, train acc: 0.8064\n",
      "epoch: 3, loss: 0.6226323843002319, train acc: 0.8064, test acc: 0.787\n",
      "loss: 0.519294023513794, train acc: 0.808\n",
      "loss: 0.6560329318046569, train acc: 0.8118\n",
      "loss: 0.6434264779090881, train acc: 0.8127\n",
      "loss: 0.6829742312431335, train acc: 0.8198\n",
      "loss: 0.6537189304828643, train acc: 0.8182\n",
      "loss: 0.6219289362430572, train acc: 0.8182\n",
      "loss: 0.6923621237277985, train acc: 0.8264\n",
      "loss: 0.6181501388549805, train acc: 0.8322\n",
      "epoch: 4, loss: 0.48637232184410095, train acc: 0.8322, test acc: 0.8092\n",
      "loss: 0.43566831946372986, train acc: 0.8323\n",
      "loss: 0.5741224616765976, train acc: 0.8347\n",
      "loss: 0.5652165412902832, train acc: 0.8341\n",
      "loss: 0.6207085490226746, train acc: 0.8364\n",
      "loss: 0.5895516127347946, train acc: 0.8379\n",
      "loss: 0.5623369663953781, train acc: 0.8326\n",
      "loss: 0.6371963441371917, train acc: 0.8396\n",
      "loss: 0.5665830850601197, train acc: 0.8441\n",
      "epoch: 5, loss: 0.3995222747325897, train acc: 0.8441, test acc: 0.8211\n",
      "loss: 0.3891156315803528, train acc: 0.8451\n",
      "loss: 0.5290576756000519, train acc: 0.8467\n",
      "loss: 0.5186015456914902, train acc: 0.844\n",
      "loss: 0.583831113576889, train acc: 0.8486\n",
      "loss: 0.5482236117124557, train acc: 0.8481\n",
      "loss: 0.5241518676280975, train acc: 0.8434\n",
      "loss: 0.600121209025383, train acc: 0.8499\n",
      "loss: 0.5309955954551697, train acc: 0.8526\n",
      "epoch: 6, loss: 0.3380756974220276, train acc: 0.8526, test acc: 0.8295\n",
      "loss: 0.35737526416778564, train acc: 0.8541\n",
      "loss: 0.49843392372131345, train acc: 0.8547\n",
      "loss: 0.48511534333229067, train acc: 0.8535\n",
      "loss: 0.5559710741043091, train acc: 0.8566\n",
      "loss: 0.5171871095895767, train acc: 0.8554\n",
      "loss: 0.4968198210000992, train acc: 0.8503\n",
      "loss: 0.5722807735204697, train acc: 0.8573\n",
      "loss: 0.5047088533639907, train acc: 0.859\n",
      "epoch: 7, loss: 0.28732579946517944, train acc: 0.859, test acc: 0.8338\n",
      "loss: 0.3311946392059326, train acc: 0.8614\n",
      "loss: 0.47482108175754545, train acc: 0.862\n",
      "loss: 0.4595726072788239, train acc: 0.8611\n",
      "loss: 0.5335317134857178, train acc: 0.8619\n",
      "loss: 0.493365666270256, train acc: 0.8618\n",
      "loss: 0.4757050216197968, train acc: 0.8575\n",
      "loss: 0.5503802180290223, train acc: 0.8623\n",
      "loss: 0.4836949735879898, train acc: 0.8647\n",
      "epoch: 8, loss: 0.2497008740901947, train acc: 0.8647, test acc: 0.839\n",
      "loss: 0.31109827756881714, train acc: 0.8658\n",
      "loss: 0.4561783641576767, train acc: 0.8683\n",
      "loss: 0.4391592174768448, train acc: 0.8648\n",
      "loss: 0.515959870815277, train acc: 0.8671\n",
      "loss: 0.47442074716091154, train acc: 0.8668\n",
      "loss: 0.45868963897228243, train acc: 0.8639\n",
      "loss: 0.5315896779298782, train acc: 0.8664\n",
      "loss: 0.4662801504135132, train acc: 0.8703\n",
      "epoch: 9, loss: 0.2181696891784668, train acc: 0.8703, test acc: 0.8418\n",
      "loss: 0.2951892018318176, train acc: 0.8706\n",
      "loss: 0.44007907509803773, train acc: 0.8708\n",
      "loss: 0.42258079051971437, train acc: 0.8689\n",
      "loss: 0.500791534781456, train acc: 0.8716\n",
      "loss: 0.45755656659603117, train acc: 0.872\n",
      "loss: 0.4443567156791687, train acc: 0.8683\n",
      "loss: 0.515580627322197, train acc: 0.8706\n",
      "loss: 0.45149960219860075, train acc: 0.873\n",
      "epoch: 10, loss: 0.19416873157024384, train acc: 0.873, test acc: 0.8452\n",
      "loss: 0.28223344683647156, train acc: 0.8739\n",
      "loss: 0.4263420134782791, train acc: 0.8755\n",
      "loss: 0.4084417879581451, train acc: 0.8727\n",
      "loss: 0.4872218519449234, train acc: 0.8741\n",
      "loss: 0.4431639164686203, train acc: 0.8756\n",
      "loss: 0.4316165953874588, train acc: 0.873\n",
      "loss: 0.5015400648117065, train acc: 0.8749\n",
      "loss: 0.43842201828956606, train acc: 0.8765\n",
      "epoch: 11, loss: 0.17417845129966736, train acc: 0.8765, test acc: 0.8479\n",
      "loss: 0.27195751667022705, train acc: 0.8777\n",
      "loss: 0.414320507645607, train acc: 0.8783\n",
      "loss: 0.3962720662355423, train acc: 0.8768\n",
      "loss: 0.4753962516784668, train acc: 0.8779\n",
      "loss: 0.4306473135948181, train acc: 0.8782\n",
      "loss: 0.42041838765144346, train acc: 0.878\n",
      "loss: 0.4886781334877014, train acc: 0.8787\n",
      "loss: 0.4261305838823318, train acc: 0.8798\n",
      "epoch: 12, loss: 0.15855300426483154, train acc: 0.8798, test acc: 0.8498\n",
      "loss: 0.2632751166820526, train acc: 0.8814\n",
      "loss: 0.40351179540157317, train acc: 0.8807\n",
      "loss: 0.38562349379062655, train acc: 0.8793\n",
      "loss: 0.46457759439945223, train acc: 0.8804\n",
      "loss: 0.41966850459575655, train acc: 0.8808\n",
      "loss: 0.4103087574243546, train acc: 0.8827\n",
      "loss: 0.47809567153453825, train acc: 0.8811\n",
      "loss: 0.41600203812122344, train acc: 0.883\n",
      "epoch: 13, loss: 0.1452927440404892, train acc: 0.883, test acc: 0.8531\n",
      "loss: 0.25580430030822754, train acc: 0.8834\n",
      "loss: 0.3942328542470932, train acc: 0.8832\n",
      "loss: 0.3758404850959778, train acc: 0.8825\n",
      "loss: 0.4548113703727722, train acc: 0.8827\n",
      "loss: 0.41029285490512846, train acc: 0.8839\n",
      "loss: 0.4012589305639267, train acc: 0.8851\n",
      "loss: 0.46799102127552034, train acc: 0.8841\n",
      "loss: 0.4065812826156616, train acc: 0.8849\n",
      "epoch: 14, loss: 0.1342000961303711, train acc: 0.8849, test acc: 0.8565\n",
      "loss: 0.24934060871601105, train acc: 0.8852\n",
      "loss: 0.3861805722117424, train acc: 0.8857\n",
      "loss: 0.36739107966423035, train acc: 0.8846\n",
      "loss: 0.44629656970500947, train acc: 0.8846\n",
      "loss: 0.40141922831535337, train acc: 0.8859\n",
      "loss: 0.39232406914234164, train acc: 0.8862\n",
      "loss: 0.459396031498909, train acc: 0.887\n",
      "loss: 0.3976407080888748, train acc: 0.887\n",
      "epoch: 15, loss: 0.1239900067448616, train acc: 0.887, test acc: 0.8582\n",
      "loss: 0.2435813993215561, train acc: 0.8873\n",
      "loss: 0.37912516444921496, train acc: 0.8868\n",
      "loss: 0.35970960557460785, train acc: 0.887\n",
      "loss: 0.43877913653850553, train acc: 0.8873\n",
      "loss: 0.39244069159030914, train acc: 0.8883\n",
      "loss: 0.3842888206243515, train acc: 0.8893\n",
      "loss: 0.45102265775203704, train acc: 0.8894\n",
      "loss: 0.3894683539867401, train acc: 0.8883\n",
      "epoch: 16, loss: 0.11558037251234055, train acc: 0.8883, test acc: 0.8605\n",
      "loss: 0.23940201103687286, train acc: 0.8895\n",
      "loss: 0.37258169502019883, train acc: 0.8886\n",
      "loss: 0.3528498888015747, train acc: 0.8893\n",
      "loss: 0.4318916857242584, train acc: 0.8894\n",
      "loss: 0.3847728371620178, train acc: 0.8902\n",
      "loss: 0.3766384333372116, train acc: 0.8909\n",
      "loss: 0.44354539811611177, train acc: 0.8916\n",
      "loss: 0.382245934009552, train acc: 0.8906\n",
      "epoch: 17, loss: 0.10847604274749756, train acc: 0.8906, test acc: 0.8622\n",
      "loss: 0.23571020364761353, train acc: 0.8906\n",
      "loss: 0.366814985871315, train acc: 0.8903\n",
      "loss: 0.3467076748609543, train acc: 0.8923\n",
      "loss: 0.4258250921964645, train acc: 0.8909\n",
      "loss: 0.3776378810405731, train acc: 0.8914\n",
      "loss: 0.36939436197280884, train acc: 0.8919\n",
      "loss: 0.4365867614746094, train acc: 0.8932\n",
      "loss: 0.3756241679191589, train acc: 0.8929\n",
      "epoch: 18, loss: 0.1024121642112732, train acc: 0.8929, test acc: 0.8642\n",
      "loss: 0.2323441207408905, train acc: 0.8912\n",
      "loss: 0.3615627378225327, train acc: 0.8922\n",
      "loss: 0.3412273496389389, train acc: 0.8935\n",
      "loss: 0.42034381330013276, train acc: 0.8928\n",
      "loss: 0.37096821069717406, train acc: 0.8932\n",
      "loss: 0.36275710463523864, train acc: 0.8932\n",
      "loss: 0.4302494913339615, train acc: 0.8953\n",
      "loss: 0.3692149639129639, train acc: 0.8941\n",
      "epoch: 19, loss: 0.09690222144126892, train acc: 0.8941, test acc: 0.8655\n",
      "loss: 0.22948022186756134, train acc: 0.8936\n",
      "loss: 0.3568881079554558, train acc: 0.8939\n",
      "loss: 0.33576543629169464, train acc: 0.8955\n",
      "loss: 0.41536372900009155, train acc: 0.8941\n",
      "loss: 0.3649281769990921, train acc: 0.8948\n",
      "loss: 0.35650008022785185, train acc: 0.8951\n",
      "loss: 0.4244641989469528, train acc: 0.8961\n",
      "loss: 0.36354853212833405, train acc: 0.8968\n",
      "epoch: 20, loss: 0.09158678352832794, train acc: 0.8968, test acc: 0.8661\n",
      "loss: 0.22681333124637604, train acc: 0.8954\n",
      "loss: 0.35250653326511383, train acc: 0.8951\n",
      "loss: 0.3308128327131271, train acc: 0.8973\n",
      "loss: 0.41055206060409544, train acc: 0.8962\n",
      "loss: 0.3590936839580536, train acc: 0.8961\n",
      "loss: 0.3507456123828888, train acc: 0.8961\n",
      "loss: 0.41956806778907774, train acc: 0.8978\n",
      "loss: 0.358636936545372, train acc: 0.8993\n",
      "epoch: 21, loss: 0.08717157691717148, train acc: 0.8993, test acc: 0.8671\n",
      "loss: 0.22456593811511993, train acc: 0.8971\n",
      "loss: 0.34832064211368563, train acc: 0.8971\n",
      "loss: 0.32612876296043397, train acc: 0.8982\n",
      "loss: 0.4058292001485825, train acc: 0.8975\n",
      "loss: 0.3537480875849724, train acc: 0.8977\n",
      "loss: 0.3452822804450989, train acc: 0.8978\n",
      "loss: 0.4142123430967331, train acc: 0.8985\n",
      "loss: 0.3538804978132248, train acc: 0.9009\n",
      "epoch: 22, loss: 0.08358000218868256, train acc: 0.9009, test acc: 0.8673\n",
      "loss: 0.2223426103591919, train acc: 0.8989\n",
      "loss: 0.34441924542188646, train acc: 0.8983\n",
      "loss: 0.32180028557777407, train acc: 0.8998\n",
      "loss: 0.401719206571579, train acc: 0.8985\n",
      "loss: 0.3487825825810432, train acc: 0.8994\n",
      "loss: 0.3401805520057678, train acc: 0.8985\n",
      "loss: 0.40936665534973143, train acc: 0.8998\n",
      "loss: 0.34949187338352206, train acc: 0.9016\n",
      "epoch: 23, loss: 0.08022402226924896, train acc: 0.9016, test acc: 0.8679\n",
      "loss: 0.2207670658826828, train acc: 0.8994\n",
      "loss: 0.34062274396419523, train acc: 0.8994\n",
      "loss: 0.3178688645362854, train acc: 0.9017\n",
      "loss: 0.3976146727800369, train acc: 0.8997\n",
      "loss: 0.34341394156217575, train acc: 0.9007\n",
      "loss: 0.3356624037027359, train acc: 0.8998\n",
      "loss: 0.40481801629066466, train acc: 0.9004\n",
      "loss: 0.34540108144283294, train acc: 0.9023\n",
      "epoch: 24, loss: 0.07720541208982468, train acc: 0.9023, test acc: 0.8691\n",
      "loss: 0.2192419469356537, train acc: 0.9\n",
      "loss: 0.337130980193615, train acc: 0.9002\n",
      "loss: 0.3139610767364502, train acc: 0.9022\n",
      "loss: 0.39383083283901216, train acc: 0.901\n",
      "loss: 0.3387009993195534, train acc: 0.9024\n",
      "loss: 0.33131286799907683, train acc: 0.9011\n",
      "loss: 0.40003584027290345, train acc: 0.9016\n",
      "loss: 0.34149385094642637, train acc: 0.9027\n",
      "epoch: 25, loss: 0.07482010126113892, train acc: 0.9027, test acc: 0.8695\n",
      "loss: 0.21750406920909882, train acc: 0.9009\n",
      "loss: 0.333955904841423, train acc: 0.9012\n",
      "loss: 0.3104225516319275, train acc: 0.9032\n",
      "loss: 0.3904540628194809, train acc: 0.9022\n",
      "loss: 0.3344071254134178, train acc: 0.9036\n",
      "loss: 0.3269706130027771, train acc: 0.9017\n",
      "loss: 0.3959007799625397, train acc: 0.9034\n",
      "loss: 0.337934672832489, train acc: 0.9036\n",
      "epoch: 26, loss: 0.07244136929512024, train acc: 0.9036, test acc: 0.8705\n",
      "loss: 0.216340109705925, train acc: 0.9017\n",
      "loss: 0.33084637820720675, train acc: 0.9021\n",
      "loss: 0.307284939289093, train acc: 0.9038\n",
      "loss: 0.38682082295417786, train acc: 0.903\n",
      "loss: 0.33011100590229037, train acc: 0.9042\n",
      "loss: 0.32285168766975403, train acc: 0.9033\n",
      "loss: 0.3918798416852951, train acc: 0.9045\n",
      "loss: 0.3345525413751602, train acc: 0.9033\n",
      "epoch: 27, loss: 0.07036939263343811, train acc: 0.9033, test acc: 0.872\n",
      "loss: 0.21505023539066315, train acc: 0.9024\n",
      "loss: 0.3279470562934875, train acc: 0.9037\n",
      "loss: 0.3038379207253456, train acc: 0.9049\n",
      "loss: 0.3836036264896393, train acc: 0.9039\n",
      "loss: 0.32624364346265794, train acc: 0.9057\n",
      "loss: 0.31902087330818174, train acc: 0.9036\n",
      "loss: 0.38810681998729707, train acc: 0.9051\n",
      "loss: 0.3315331742167473, train acc: 0.9041\n",
      "epoch: 28, loss: 0.06832477450370789, train acc: 0.9041, test acc: 0.8728\n",
      "loss: 0.21365362405776978, train acc: 0.904\n",
      "loss: 0.3250064417719841, train acc: 0.9045\n",
      "loss: 0.3006048917770386, train acc: 0.9062\n",
      "loss: 0.38045385777950286, train acc: 0.9048\n",
      "loss: 0.32274539619684217, train acc: 0.9061\n",
      "loss: 0.31538995206356046, train acc: 0.9046\n",
      "loss: 0.38447631895542145, train acc: 0.9059\n",
      "loss: 0.32897616028785703, train acc: 0.9045\n",
      "epoch: 29, loss: 0.06661872565746307, train acc: 0.9045, test acc: 0.8728\n",
      "loss: 0.21145622432231903, train acc: 0.9045\n",
      "loss: 0.3224038854241371, train acc: 0.9058\n",
      "loss: 0.297696079313755, train acc: 0.9074\n",
      "loss: 0.37735499143600465, train acc: 0.9047\n",
      "loss: 0.31952799260616305, train acc: 0.9081\n",
      "loss: 0.31188716292381286, train acc: 0.9056\n",
      "loss: 0.38051740229129793, train acc: 0.9066\n",
      "loss: 0.32653103321790694, train acc: 0.9054\n",
      "epoch: 30, loss: 0.06496712565422058, train acc: 0.9054, test acc: 0.8721\n",
      "loss: 0.21005135774612427, train acc: 0.9059\n",
      "loss: 0.3199662774801254, train acc: 0.906\n",
      "loss: 0.29468886703252795, train acc: 0.908\n",
      "loss: 0.3743147298693657, train acc: 0.9073\n",
      "loss: 0.3160504102706909, train acc: 0.909\n",
      "loss: 0.30860019475221634, train acc: 0.9055\n",
      "loss: 0.3769960761070251, train acc: 0.9073\n",
      "loss: 0.3240690752863884, train acc: 0.907\n",
      "epoch: 31, loss: 0.06343764066696167, train acc: 0.907, test acc: 0.8721\n",
      "loss: 0.20927880704402924, train acc: 0.9073\n",
      "loss: 0.3176224887371063, train acc: 0.9072\n",
      "loss: 0.2919713094830513, train acc: 0.9092\n",
      "loss: 0.37134109288454054, train acc: 0.9078\n",
      "loss: 0.31345768123865125, train acc: 0.9095\n",
      "loss: 0.30527749508619306, train acc: 0.9064\n",
      "loss: 0.3733923017978668, train acc: 0.9078\n",
      "loss: 0.32178990095853804, train acc: 0.9078\n",
      "epoch: 32, loss: 0.06218823045492172, train acc: 0.9078, test acc: 0.8726\n",
      "loss: 0.20801351964473724, train acc: 0.9083\n",
      "loss: 0.3154938891530037, train acc: 0.9076\n",
      "loss: 0.2893180623650551, train acc: 0.9099\n",
      "loss: 0.36892992407083514, train acc: 0.908\n",
      "loss: 0.3104596123099327, train acc: 0.9105\n",
      "loss: 0.30222525596618655, train acc: 0.907\n",
      "loss: 0.36996974647045133, train acc: 0.9082\n",
      "loss: 0.3196814075112343, train acc: 0.9081\n",
      "epoch: 33, loss: 0.061051759868860245, train acc: 0.9081, test acc: 0.8727\n",
      "loss: 0.20723143219947815, train acc: 0.9078\n",
      "loss: 0.3136083945631981, train acc: 0.9086\n",
      "loss: 0.28697248101234435, train acc: 0.9102\n",
      "loss: 0.36663061529397967, train acc: 0.9085\n",
      "loss: 0.30784420371055604, train acc: 0.911\n",
      "loss: 0.29942028820514677, train acc: 0.9077\n",
      "loss: 0.3669425189495087, train acc: 0.9094\n",
      "loss: 0.3177207410335541, train acc: 0.9091\n",
      "epoch: 34, loss: 0.05984096601605415, train acc: 0.9091, test acc: 0.8733\n",
      "loss: 0.2060052454471588, train acc: 0.9091\n",
      "loss: 0.31146581172943116, train acc: 0.9099\n",
      "loss: 0.2843779757618904, train acc: 0.911\n",
      "loss: 0.3639845699071884, train acc: 0.9087\n",
      "loss: 0.3053347900509834, train acc: 0.9117\n",
      "loss: 0.2969624221324921, train acc: 0.909\n",
      "loss: 0.36384022533893584, train acc: 0.9105\n",
      "loss: 0.31583261489868164, train acc: 0.9105\n",
      "epoch: 35, loss: 0.05871479585766792, train acc: 0.9105, test acc: 0.8734\n",
      "loss: 0.2055240422487259, train acc: 0.9102\n",
      "loss: 0.30995863676071167, train acc: 0.9101\n",
      "loss: 0.282152883708477, train acc: 0.9122\n",
      "loss: 0.3618411585688591, train acc: 0.9095\n",
      "loss: 0.3023903086781502, train acc: 0.9125\n",
      "loss: 0.2943703427910805, train acc: 0.91\n",
      "loss: 0.36098975539207456, train acc: 0.9115\n",
      "loss: 0.3139807105064392, train acc: 0.9115\n",
      "epoch: 36, loss: 0.05760090798139572, train acc: 0.9115, test acc: 0.8736\n",
      "loss: 0.20495344698429108, train acc: 0.9108\n",
      "loss: 0.3081574186682701, train acc: 0.9105\n",
      "loss: 0.2796843722462654, train acc: 0.9132\n",
      "loss: 0.35937766879796984, train acc: 0.91\n",
      "loss: 0.2996675446629524, train acc: 0.9128\n",
      "loss: 0.29189805686473846, train acc: 0.911\n",
      "loss: 0.35831727385520934, train acc: 0.9119\n",
      "loss: 0.31229924261569975, train acc: 0.9116\n",
      "epoch: 37, loss: 0.05713307112455368, train acc: 0.9116, test acc: 0.8736\n",
      "loss: 0.20362815260887146, train acc: 0.9113\n",
      "loss: 0.30650815516710284, train acc: 0.9109\n",
      "loss: 0.27761800587177277, train acc: 0.9136\n",
      "loss: 0.35703831166028976, train acc: 0.9106\n",
      "loss: 0.2978535830974579, train acc: 0.9137\n",
      "loss: 0.289620041847229, train acc: 0.9111\n",
      "loss: 0.35534208714962007, train acc: 0.9122\n",
      "loss: 0.3104745253920555, train acc: 0.9129\n",
      "epoch: 38, loss: 0.05604895204305649, train acc: 0.9129, test acc: 0.8729\n",
      "loss: 0.20286396145820618, train acc: 0.912\n",
      "loss: 0.30472526848316195, train acc: 0.912\n",
      "loss: 0.2752735197544098, train acc: 0.9143\n",
      "loss: 0.3552136287093163, train acc: 0.9116\n",
      "loss: 0.29530408084392545, train acc: 0.9143\n",
      "loss: 0.28716249465942384, train acc: 0.9116\n",
      "loss: 0.353088042140007, train acc: 0.913\n",
      "loss: 0.309107831120491, train acc: 0.9135\n",
      "epoch: 39, loss: 0.05545603111386299, train acc: 0.9135, test acc: 0.8731\n",
      "loss: 0.2013053447008133, train acc: 0.9128\n",
      "loss: 0.3031928434967995, train acc: 0.913\n",
      "loss: 0.27315640449523926, train acc: 0.9149\n",
      "loss: 0.3532042160630226, train acc: 0.9122\n",
      "loss: 0.2929142415523529, train acc: 0.9148\n",
      "loss: 0.28485864847898484, train acc: 0.912\n",
      "loss: 0.35050915032625196, train acc: 0.9134\n",
      "loss: 0.30751843303442, train acc: 0.9141\n",
      "epoch: 40, loss: 0.05479647219181061, train acc: 0.9141, test acc: 0.8733\n",
      "loss: 0.20033548772335052, train acc: 0.9129\n",
      "loss: 0.3018632262945175, train acc: 0.9129\n",
      "loss: 0.27127473056316376, train acc: 0.9155\n",
      "loss: 0.35103563964366913, train acc: 0.9126\n",
      "loss: 0.2910770833492279, train acc: 0.9156\n",
      "loss: 0.28307726979255676, train acc: 0.9122\n",
      "loss: 0.3470501884818077, train acc: 0.914\n",
      "loss: 0.3055960893630981, train acc: 0.9149\n",
      "epoch: 41, loss: 0.0537119098007679, train acc: 0.9149, test acc: 0.8743\n",
      "loss: 0.20012186467647552, train acc: 0.9132\n",
      "loss: 0.3003780394792557, train acc: 0.9135\n",
      "loss: 0.2692011296749115, train acc: 0.9163\n",
      "loss: 0.34926979839801786, train acc: 0.9134\n",
      "loss: 0.28870296478271484, train acc: 0.9162\n",
      "loss: 0.28103815019130707, train acc: 0.9131\n",
      "loss: 0.3451808050274849, train acc: 0.9148\n",
      "loss: 0.3045341208577156, train acc: 0.9151\n",
      "epoch: 42, loss: 0.05327847972512245, train acc: 0.9151, test acc: 0.874\n",
      "loss: 0.19847343862056732, train acc: 0.9137\n",
      "loss: 0.298986479640007, train acc: 0.9139\n",
      "loss: 0.2672028034925461, train acc: 0.9172\n",
      "loss: 0.3473825231194496, train acc: 0.9141\n",
      "loss: 0.2866767719388008, train acc: 0.9168\n",
      "loss: 0.2791698217391968, train acc: 0.9134\n",
      "loss: 0.34283901154994967, train acc: 0.9154\n",
      "loss: 0.30309692621231077, train acc: 0.9162\n",
      "epoch: 43, loss: 0.05246016010642052, train acc: 0.9162, test acc: 0.874\n",
      "loss: 0.19755400717258453, train acc: 0.9146\n",
      "loss: 0.2975985214114189, train acc: 0.9145\n",
      "loss: 0.26525360643863677, train acc: 0.9176\n",
      "loss: 0.34543091505765916, train acc: 0.9148\n",
      "loss: 0.2849215120077133, train acc: 0.9175\n",
      "loss: 0.2772897705435753, train acc: 0.914\n",
      "loss: 0.3404998168349266, train acc: 0.9162\n",
      "loss: 0.3017341583967209, train acc: 0.9163\n",
      "epoch: 44, loss: 0.05169438570737839, train acc: 0.9163, test acc: 0.8739\n",
      "loss: 0.1966906040906906, train acc: 0.9149\n",
      "loss: 0.2962277576327324, train acc: 0.9152\n",
      "loss: 0.26328749060630796, train acc: 0.9179\n",
      "loss: 0.343877749145031, train acc: 0.9159\n",
      "loss: 0.2828627660870552, train acc: 0.918\n",
      "loss: 0.27540687918663026, train acc: 0.9142\n",
      "loss: 0.33848225623369216, train acc: 0.9169\n",
      "loss: 0.3005647897720337, train acc: 0.9169\n",
      "epoch: 45, loss: 0.051170483231544495, train acc: 0.9169, test acc: 0.8738\n",
      "loss: 0.1950852870941162, train acc: 0.9149\n",
      "loss: 0.294986255466938, train acc: 0.9159\n",
      "loss: 0.26140564382076265, train acc: 0.918\n",
      "loss: 0.3422882094979286, train acc: 0.916\n",
      "loss: 0.28089452236890794, train acc: 0.9188\n",
      "loss: 0.2737638860940933, train acc: 0.9145\n",
      "loss: 0.33660426139831545, train acc: 0.9168\n",
      "loss: 0.2992790326476097, train acc: 0.917\n",
      "epoch: 46, loss: 0.050366852432489395, train acc: 0.917, test acc: 0.8739\n",
      "loss: 0.1943211853504181, train acc: 0.9159\n",
      "loss: 0.2936262711882591, train acc: 0.9166\n",
      "loss: 0.25963126569986344, train acc: 0.9182\n",
      "loss: 0.3406136006116867, train acc: 0.9163\n",
      "loss: 0.2794754609465599, train acc: 0.9186\n",
      "loss: 0.2721582755446434, train acc: 0.9153\n",
      "loss: 0.3346696048974991, train acc: 0.9174\n",
      "loss: 0.2979552075266838, train acc: 0.9171\n",
      "epoch: 47, loss: 0.04993844032287598, train acc: 0.9171, test acc: 0.8744\n",
      "loss: 0.19263754785060883, train acc: 0.9162\n",
      "loss: 0.29231851249933244, train acc: 0.917\n",
      "loss: 0.25769561529159546, train acc: 0.9189\n",
      "loss: 0.33936132341623304, train acc: 0.9162\n",
      "loss: 0.277718648314476, train acc: 0.9195\n",
      "loss: 0.27045038193464277, train acc: 0.9149\n",
      "loss: 0.3330066308379173, train acc: 0.9181\n",
      "loss: 0.2970260590314865, train acc: 0.9177\n",
      "epoch: 48, loss: 0.04947707802057266, train acc: 0.9177, test acc: 0.8746\n",
      "loss: 0.1918310672044754, train acc: 0.9168\n",
      "loss: 0.29091397672891617, train acc: 0.9171\n",
      "loss: 0.25588284581899645, train acc: 0.9191\n",
      "loss: 0.337639319896698, train acc: 0.9171\n",
      "loss: 0.27623090893030167, train acc: 0.9201\n",
      "loss: 0.26907089054584504, train acc: 0.9154\n",
      "loss: 0.33113754987716676, train acc: 0.9185\n",
      "loss: 0.29591488242149355, train acc: 0.9185\n",
      "epoch: 49, loss: 0.04913284629583359, train acc: 0.9185, test acc: 0.8744\n",
      "loss: 0.19012339413166046, train acc: 0.9176\n",
      "loss: 0.2897347241640091, train acc: 0.9172\n",
      "loss: 0.25407190471887586, train acc: 0.9203\n",
      "loss: 0.33644348233938215, train acc: 0.9176\n",
      "loss: 0.27427965253591535, train acc: 0.9207\n",
      "loss: 0.2673558756709099, train acc: 0.9152\n",
      "loss: 0.32923029363155365, train acc: 0.919\n",
      "loss: 0.2949258789420128, train acc: 0.919\n",
      "epoch: 50, loss: 0.04856546223163605, train acc: 0.919, test acc: 0.8746\n",
      "loss: 0.18978910148143768, train acc: 0.9185\n",
      "loss: 0.28873854875564575, train acc: 0.9174\n",
      "loss: 0.25243801027536394, train acc: 0.9203\n",
      "loss: 0.3347660019993782, train acc: 0.918\n",
      "loss: 0.27285992801189424, train acc: 0.9208\n",
      "loss: 0.26616772413253786, train acc: 0.9156\n",
      "loss: 0.32743098586797714, train acc: 0.9197\n",
      "loss: 0.29357813149690626, train acc: 0.9197\n",
      "epoch: 51, loss: 0.048194803297519684, train acc: 0.9197, test acc: 0.8747\n",
      "loss: 0.18834327161312103, train acc: 0.9189\n",
      "loss: 0.2875934153795242, train acc: 0.9175\n",
      "loss: 0.25093794465065, train acc: 0.9207\n",
      "loss: 0.33374225795269014, train acc: 0.9185\n",
      "loss: 0.27105906903743743, train acc: 0.9214\n",
      "loss: 0.2644758477807045, train acc: 0.9158\n",
      "loss: 0.3258154779672623, train acc: 0.9195\n",
      "loss: 0.2928630232810974, train acc: 0.92\n",
      "epoch: 52, loss: 0.04793433099985123, train acc: 0.92, test acc: 0.8745\n",
      "loss: 0.18778033554553986, train acc: 0.9199\n",
      "loss: 0.2865913912653923, train acc: 0.9175\n",
      "loss: 0.24940330982208253, train acc: 0.9212\n",
      "loss: 0.3323711693286896, train acc: 0.9183\n",
      "loss: 0.26984623074531555, train acc: 0.9218\n",
      "loss: 0.2633824273943901, train acc: 0.9161\n",
      "loss: 0.3241882249712944, train acc: 0.9198\n",
      "loss: 0.29157153218984605, train acc: 0.9202\n",
      "epoch: 53, loss: 0.047500912100076675, train acc: 0.9202, test acc: 0.8748\n",
      "loss: 0.18654005229473114, train acc: 0.9202\n",
      "loss: 0.2854106917977333, train acc: 0.9181\n",
      "loss: 0.24790670424699784, train acc: 0.9213\n",
      "loss: 0.33136324137449263, train acc: 0.9196\n",
      "loss: 0.2682454541325569, train acc: 0.9223\n",
      "loss: 0.26173432618379594, train acc: 0.9162\n",
      "loss: 0.32227534651756284, train acc: 0.9205\n",
      "loss: 0.2907770797610283, train acc: 0.9204\n",
      "epoch: 54, loss: 0.047351568937301636, train acc: 0.9204, test acc: 0.8749\n",
      "loss: 0.1858603060245514, train acc: 0.9206\n",
      "loss: 0.28459232300519943, train acc: 0.9186\n",
      "loss: 0.24641456007957457, train acc: 0.9219\n",
      "loss: 0.33024316728115083, train acc: 0.9199\n",
      "loss: 0.2667193070054054, train acc: 0.9227\n",
      "loss: 0.2602667912840843, train acc: 0.9167\n",
      "loss: 0.3206199213862419, train acc: 0.9212\n",
      "loss: 0.2898854032158852, train acc: 0.921\n",
      "epoch: 55, loss: 0.04702123999595642, train acc: 0.921, test acc: 0.875\n",
      "loss: 0.18502256274223328, train acc: 0.9211\n",
      "loss: 0.2832438454031944, train acc: 0.9189\n",
      "loss: 0.2450430139899254, train acc: 0.9218\n",
      "loss: 0.3287135288119316, train acc: 0.9199\n",
      "loss: 0.26575897485017774, train acc: 0.9229\n",
      "loss: 0.25920215249061584, train acc: 0.917\n",
      "loss: 0.31934163719415665, train acc: 0.9217\n",
      "loss: 0.28874080926179885, train acc: 0.9213\n",
      "epoch: 56, loss: 0.04670530557632446, train acc: 0.9213, test acc: 0.8752\n",
      "loss: 0.1839766651391983, train acc: 0.9217\n",
      "loss: 0.28227517902851107, train acc: 0.9196\n",
      "loss: 0.2435778945684433, train acc: 0.9225\n",
      "loss: 0.32768415957689284, train acc: 0.9207\n",
      "loss: 0.2639495447278023, train acc: 0.923\n",
      "loss: 0.2577790230512619, train acc: 0.9178\n",
      "loss: 0.3176028400659561, train acc: 0.922\n",
      "loss: 0.28792339712381365, train acc: 0.9222\n",
      "epoch: 57, loss: 0.04635060578584671, train acc: 0.9222, test acc: 0.8749\n",
      "loss: 0.1833864152431488, train acc: 0.922\n",
      "loss: 0.2815252497792244, train acc: 0.9202\n",
      "loss: 0.24180707484483718, train acc: 0.9227\n",
      "loss: 0.32643737345933915, train acc: 0.9207\n",
      "loss: 0.26250709444284437, train acc: 0.9233\n",
      "loss: 0.2564121410250664, train acc: 0.9179\n",
      "loss: 0.31612755805253984, train acc: 0.9228\n",
      "loss: 0.28706667721271517, train acc: 0.9227\n",
      "epoch: 58, loss: 0.04620709270238876, train acc: 0.9227, test acc: 0.8753\n",
      "loss: 0.18331608176231384, train acc: 0.9225\n",
      "loss: 0.2805844068527222, train acc: 0.9206\n",
      "loss: 0.24029089659452438, train acc: 0.9233\n",
      "loss: 0.32556278705596925, train acc: 0.9212\n",
      "loss: 0.26111501455307007, train acc: 0.9235\n",
      "loss: 0.25527341812849047, train acc: 0.9184\n",
      "loss: 0.31472097635269164, train acc: 0.9232\n",
      "loss: 0.2861091434955597, train acc: 0.9236\n",
      "epoch: 59, loss: 0.045744381844997406, train acc: 0.9236, test acc: 0.8757\n",
      "loss: 0.1823744773864746, train acc: 0.9225\n",
      "loss: 0.2796074882149696, train acc: 0.9211\n",
      "loss: 0.2390494868159294, train acc: 0.9234\n",
      "loss: 0.3243866339325905, train acc: 0.9213\n",
      "loss: 0.2602435603737831, train acc: 0.923\n",
      "loss: 0.2538701981306076, train acc: 0.9186\n",
      "loss: 0.31306260675191877, train acc: 0.9237\n",
      "loss: 0.2853576853871346, train acc: 0.9237\n",
      "epoch: 60, loss: 0.04571589082479477, train acc: 0.9237, test acc: 0.8758\n",
      "loss: 0.18133851885795593, train acc: 0.9233\n",
      "loss: 0.2787615552544594, train acc: 0.9211\n",
      "loss: 0.23748495429754257, train acc: 0.9236\n",
      "loss: 0.3234132707118988, train acc: 0.9214\n",
      "loss: 0.2588441476225853, train acc: 0.9235\n",
      "loss: 0.25307305455207824, train acc: 0.9189\n",
      "loss: 0.3117883428931236, train acc: 0.9247\n",
      "loss: 0.2840425238013268, train acc: 0.9238\n",
      "epoch: 61, loss: 0.04559042304754257, train acc: 0.9238, test acc: 0.8749\n",
      "loss: 0.18162667751312256, train acc: 0.9235\n",
      "loss: 0.27806191593408586, train acc: 0.9214\n",
      "loss: 0.2364390715956688, train acc: 0.9239\n",
      "loss: 0.32275602519512175, train acc: 0.9219\n",
      "loss: 0.2572539493441582, train acc: 0.924\n",
      "loss: 0.2514312744140625, train acc: 0.9194\n",
      "loss: 0.3100927367806435, train acc: 0.9247\n",
      "loss: 0.28355983793735506, train acc: 0.9234\n",
      "epoch: 62, loss: 0.045459240674972534, train acc: 0.9234, test acc: 0.875\n",
      "loss: 0.18054364621639252, train acc: 0.9239\n",
      "loss: 0.2770623713731766, train acc: 0.9221\n",
      "loss: 0.23470594584941865, train acc: 0.9242\n",
      "loss: 0.3217542111873627, train acc: 0.9223\n",
      "loss: 0.2559832066297531, train acc: 0.9239\n",
      "loss: 0.25049560964107515, train acc: 0.9196\n",
      "loss: 0.30898206532001493, train acc: 0.9254\n",
      "loss: 0.28269132524728774, train acc: 0.9243\n",
      "epoch: 63, loss: 0.04493797570466995, train acc: 0.9243, test acc: 0.8755\n",
      "loss: 0.1795583814382553, train acc: 0.9244\n",
      "loss: 0.275773911178112, train acc: 0.9223\n",
      "loss: 0.23333732634782792, train acc: 0.9243\n",
      "loss: 0.3206911265850067, train acc: 0.9225\n",
      "loss: 0.25501543581485747, train acc: 0.9244\n",
      "loss: 0.2492672398686409, train acc: 0.9198\n",
      "loss: 0.30725187361240386, train acc: 0.9251\n",
      "loss: 0.2818855807185173, train acc: 0.9244\n",
      "epoch: 64, loss: 0.04484579339623451, train acc: 0.9244, test acc: 0.8752\n",
      "loss: 0.17935606837272644, train acc: 0.9244\n",
      "loss: 0.2753434062004089, train acc: 0.9223\n",
      "loss: 0.2322662889957428, train acc: 0.9248\n",
      "loss: 0.3200594559311867, train acc: 0.9227\n",
      "loss: 0.25382077544927595, train acc: 0.9244\n",
      "loss: 0.24834865927696229, train acc: 0.9201\n",
      "loss: 0.30610906779766084, train acc: 0.9258\n",
      "loss: 0.28089339286088943, train acc: 0.925\n",
      "epoch: 65, loss: 0.044381797313690186, train acc: 0.925, test acc: 0.8747\n",
      "loss: 0.178751140832901, train acc: 0.9248\n",
      "loss: 0.2744222581386566, train acc: 0.923\n",
      "loss: 0.2307861015200615, train acc: 0.9247\n",
      "loss: 0.3193432316184044, train acc: 0.9227\n",
      "loss: 0.2522627472877502, train acc: 0.9244\n",
      "loss: 0.24731103777885438, train acc: 0.9198\n",
      "loss: 0.3047080338001251, train acc: 0.9256\n",
      "loss: 0.280342797935009, train acc: 0.9257\n",
      "epoch: 66, loss: 0.04407702013850212, train acc: 0.9257, test acc: 0.8747\n",
      "loss: 0.17867687344551086, train acc: 0.9252\n",
      "loss: 0.2737154453992844, train acc: 0.9227\n",
      "loss: 0.22967790812253952, train acc: 0.9248\n",
      "loss: 0.3183507740497589, train acc: 0.9223\n",
      "loss: 0.25138934552669523, train acc: 0.9256\n",
      "loss: 0.24601213186979293, train acc: 0.92\n",
      "loss: 0.3032065093517303, train acc: 0.9256\n",
      "loss: 0.2794739991426468, train acc: 0.9258\n",
      "epoch: 67, loss: 0.043926041573286057, train acc: 0.9258, test acc: 0.8747\n",
      "loss: 0.17731738090515137, train acc: 0.9249\n",
      "loss: 0.2729992508888245, train acc: 0.9231\n",
      "loss: 0.22869755327701569, train acc: 0.9253\n",
      "loss: 0.3176355570554733, train acc: 0.9228\n",
      "loss: 0.2503247082233429, train acc: 0.9255\n",
      "loss: 0.24529569447040558, train acc: 0.921\n",
      "loss: 0.3017620727419853, train acc: 0.9261\n",
      "loss: 0.27864384800195696, train acc: 0.9259\n",
      "epoch: 68, loss: 0.04311748594045639, train acc: 0.9259, test acc: 0.8744\n",
      "loss: 0.17749375104904175, train acc: 0.9248\n",
      "loss: 0.27227704375982287, train acc: 0.9231\n",
      "loss: 0.22755005061626435, train acc: 0.9255\n",
      "loss: 0.31672983169555663, train acc: 0.9229\n",
      "loss: 0.2490047037601471, train acc: 0.9257\n",
      "loss: 0.24422084540128708, train acc: 0.9211\n",
      "loss: 0.3004846692085266, train acc: 0.9262\n",
      "loss: 0.27801498472690583, train acc: 0.9259\n",
      "epoch: 69, loss: 0.04319630563259125, train acc: 0.9259, test acc: 0.8746\n",
      "loss: 0.1763523370027542, train acc: 0.9247\n",
      "loss: 0.27155071049928664, train acc: 0.9237\n",
      "loss: 0.2266392707824707, train acc: 0.9255\n",
      "loss: 0.31605125963687897, train acc: 0.9232\n",
      "loss: 0.2482314735651016, train acc: 0.926\n",
      "loss: 0.24330868422985077, train acc: 0.921\n",
      "loss: 0.29911229461431504, train acc: 0.9264\n",
      "loss: 0.2771338909864426, train acc: 0.9261\n",
      "epoch: 70, loss: 0.04296611249446869, train acc: 0.9261, test acc: 0.8739\n",
      "loss: 0.17563864588737488, train acc: 0.9251\n",
      "loss: 0.27074252963066103, train acc: 0.9237\n",
      "loss: 0.22523729205131532, train acc: 0.926\n",
      "loss: 0.31513226926326754, train acc: 0.9233\n",
      "loss: 0.24724694192409516, train acc: 0.9259\n",
      "loss: 0.24247094839811326, train acc: 0.9215\n",
      "loss: 0.2976661309599876, train acc: 0.9268\n",
      "loss: 0.27643427550792693, train acc: 0.9261\n",
      "epoch: 71, loss: 0.042632944881916046, train acc: 0.9261, test acc: 0.8742\n",
      "loss: 0.17534375190734863, train acc: 0.9252\n",
      "loss: 0.27021137028932574, train acc: 0.9239\n",
      "loss: 0.22430919259786605, train acc: 0.9263\n",
      "loss: 0.3146334528923035, train acc: 0.924\n",
      "loss: 0.24594827145338058, train acc: 0.9258\n",
      "loss: 0.24152905344963074, train acc: 0.9211\n",
      "loss: 0.2966622933745384, train acc: 0.9269\n",
      "loss: 0.27562898099422456, train acc: 0.9268\n",
      "epoch: 72, loss: 0.04240366816520691, train acc: 0.9268, test acc: 0.8742\n",
      "loss: 0.1740788370370865, train acc: 0.9258\n",
      "loss: 0.2692320987582207, train acc: 0.9239\n",
      "loss: 0.22296976447105407, train acc: 0.9264\n",
      "loss: 0.313616569340229, train acc: 0.9238\n",
      "loss: 0.2452452749013901, train acc: 0.9264\n",
      "loss: 0.2409529283642769, train acc: 0.9218\n",
      "loss: 0.29527172446250916, train acc: 0.9267\n",
      "loss: 0.2747653305530548, train acc: 0.9271\n",
      "epoch: 73, loss: 0.04192349314689636, train acc: 0.9271, test acc: 0.8742\n",
      "loss: 0.17422479391098022, train acc: 0.9261\n",
      "loss: 0.2686254605650902, train acc: 0.9242\n",
      "loss: 0.2221727505326271, train acc: 0.9261\n",
      "loss: 0.3131275877356529, train acc: 0.9237\n",
      "loss: 0.24434085041284562, train acc: 0.9263\n",
      "loss: 0.23991756588220597, train acc: 0.922\n",
      "loss: 0.29414886981248856, train acc: 0.9272\n",
      "loss: 0.27403112053871154, train acc: 0.9272\n",
      "epoch: 74, loss: 0.04185933619737625, train acc: 0.9272, test acc: 0.8739\n",
      "loss: 0.17252232134342194, train acc: 0.9258\n",
      "loss: 0.2678461492061615, train acc: 0.9245\n",
      "loss: 0.22119463235139847, train acc: 0.9264\n",
      "loss: 0.3124437600374222, train acc: 0.9244\n",
      "loss: 0.24330509454011917, train acc: 0.9267\n",
      "loss: 0.23913516849279404, train acc: 0.922\n",
      "loss: 0.292722050845623, train acc: 0.9273\n",
      "loss: 0.27350309044122695, train acc: 0.9275\n",
      "epoch: 75, loss: 0.04195822402834892, train acc: 0.9275, test acc: 0.8738\n",
      "loss: 0.17254763841629028, train acc: 0.9263\n",
      "loss: 0.2672173097729683, train acc: 0.9255\n",
      "loss: 0.22010874152183532, train acc: 0.9265\n",
      "loss: 0.31188992261886594, train acc: 0.9243\n",
      "loss: 0.24261151254177094, train acc: 0.9267\n",
      "loss: 0.23833632171154023, train acc: 0.9224\n",
      "loss: 0.29145086109638213, train acc: 0.9273\n",
      "loss: 0.2729191854596138, train acc: 0.9275\n",
      "epoch: 76, loss: 0.04157581925392151, train acc: 0.9275, test acc: 0.8738\n",
      "loss: 0.17154785990715027, train acc: 0.9261\n",
      "loss: 0.26644132286310196, train acc: 0.9254\n",
      "loss: 0.21909641176462175, train acc: 0.9266\n",
      "loss: 0.3115728095173836, train acc: 0.9244\n",
      "loss: 0.24135676473379136, train acc: 0.9272\n",
      "loss: 0.23738379031419754, train acc: 0.9228\n",
      "loss: 0.2904116243124008, train acc: 0.9276\n",
      "loss: 0.2722371071577072, train acc: 0.9278\n",
      "epoch: 77, loss: 0.04140305146574974, train acc: 0.9278, test acc: 0.8736\n",
      "loss: 0.17095856368541718, train acc: 0.9262\n",
      "loss: 0.26575449407100676, train acc: 0.9252\n",
      "loss: 0.21818338632583617, train acc: 0.9269\n",
      "loss: 0.3106814160943031, train acc: 0.9247\n",
      "loss: 0.2407246321439743, train acc: 0.9268\n",
      "loss: 0.2366932973265648, train acc: 0.9228\n",
      "loss: 0.2890888169407845, train acc: 0.9276\n",
      "loss: 0.2715148895978928, train acc: 0.9279\n",
      "epoch: 78, loss: 0.04141482710838318, train acc: 0.9279, test acc: 0.8735\n",
      "loss: 0.1706988364458084, train acc: 0.9263\n",
      "loss: 0.26535334140062333, train acc: 0.9253\n",
      "loss: 0.2173711746931076, train acc: 0.9271\n",
      "loss: 0.31018164157867434, train acc: 0.9244\n",
      "loss: 0.2398372009396553, train acc: 0.9272\n",
      "loss: 0.2360896036028862, train acc: 0.9232\n",
      "loss: 0.28811475038528445, train acc: 0.9279\n",
      "loss: 0.2708605319261551, train acc: 0.9284\n",
      "epoch: 79, loss: 0.04074445739388466, train acc: 0.9284, test acc: 0.8734\n",
      "#####training and testing end with K:5, P:1######\n",
      "#####training and testing start with K:10, P:0.1######\n",
      "loss: 2.349277973175049, train acc: 0.1132\n",
      "loss: 2.217956018447876, train acc: 0.1842\n",
      "loss: 2.050059461593628, train acc: 0.2583\n",
      "loss: 1.8940942645072938, train acc: 0.2564\n",
      "loss: 1.694874393939972, train acc: 0.3563\n",
      "loss: 1.5934508442878723, train acc: 0.5114\n",
      "loss: 1.4820818781852723, train acc: 0.6139\n",
      "loss: 1.3097098112106322, train acc: 0.723\n",
      "epoch: 0, loss: 1.0883307456970215, train acc: 0.723, test acc: 0.7436\n",
      "loss: 0.9569660425186157, train acc: 0.7473\n",
      "loss: 1.0930887758731842, train acc: 0.7891\n",
      "loss: 1.0050196826457978, train acc: 0.8136\n",
      "loss: 0.9440988600254059, train acc: 0.8298\n",
      "loss: 0.796273010969162, train acc: 0.8413\n",
      "loss: 0.8058523714542389, train acc: 0.8469\n",
      "loss: 0.7688383281230926, train acc: 0.8551\n",
      "loss: 0.7606953740119934, train acc: 0.8604\n",
      "epoch: 1, loss: 0.5011114478111267, train acc: 0.8604, test acc: 0.8566\n",
      "loss: 0.5792447328567505, train acc: 0.8641\n",
      "loss: 0.7367660760879516, train acc: 0.8664\n",
      "loss: 0.7172824025154114, train acc: 0.8687\n",
      "loss: 0.6742828071117402, train acc: 0.8758\n",
      "loss: 0.6369616448879242, train acc: 0.8726\n",
      "loss: 0.6635357141494751, train acc: 0.8776\n",
      "loss: 0.6345817387104035, train acc: 0.8827\n",
      "loss: 0.6502606064081192, train acc: 0.8848\n",
      "epoch: 2, loss: 0.5004494190216064, train acc: 0.8848, test acc: 0.8716\n",
      "loss: 0.47178587317466736, train acc: 0.8836\n",
      "loss: 0.6427725285291672, train acc: 0.8858\n",
      "loss: 0.6164157062768936, train acc: 0.8862\n",
      "loss: 0.5967550784349441, train acc: 0.8897\n",
      "loss: 0.5453268319368363, train acc: 0.8887\n",
      "loss: 0.5940310597419739, train acc: 0.8906\n",
      "loss: 0.5772817641496658, train acc: 0.8917\n",
      "loss: 0.5586769133806229, train acc: 0.8962\n",
      "epoch: 3, loss: 0.4938811957836151, train acc: 0.8962, test acc: 0.8807\n",
      "loss: 0.4358147978782654, train acc: 0.8955\n",
      "loss: 0.5820203214883805, train acc: 0.898\n",
      "loss: 0.5641773611307144, train acc: 0.9005\n",
      "loss: 0.5347058117389679, train acc: 0.9011\n",
      "loss: 0.4962232530117035, train acc: 0.9005\n",
      "loss: 0.49405494034290315, train acc: 0.9024\n",
      "loss: 0.5077613770961762, train acc: 0.9048\n",
      "loss: 0.5314482897520065, train acc: 0.9025\n",
      "epoch: 4, loss: 0.46478715538978577, train acc: 0.9025, test acc: 0.8864\n",
      "loss: 0.47338634729385376, train acc: 0.9027\n",
      "loss: 0.5761589080095291, train acc: 0.904\n",
      "loss: 0.5381470829248428, train acc: 0.9068\n",
      "loss: 0.46869554817676545, train acc: 0.909\n",
      "loss: 0.43796919882297514, train acc: 0.9078\n",
      "loss: 0.4612981051206589, train acc: 0.9089\n",
      "loss: 0.4907408684492111, train acc: 0.9112\n",
      "loss: 0.46859645545482637, train acc: 0.9111\n",
      "epoch: 5, loss: 0.18896663188934326, train acc: 0.9111, test acc: 0.8894\n",
      "loss: 0.4684213697910309, train acc: 0.909\n",
      "loss: 0.5409523993730545, train acc: 0.9073\n",
      "loss: 0.4877367615699768, train acc: 0.9124\n",
      "loss: 0.4863851428031921, train acc: 0.913\n",
      "loss: 0.4411274194717407, train acc: 0.9119\n",
      "loss: 0.4555025964975357, train acc: 0.9127\n",
      "loss: 0.4747282832860947, train acc: 0.9132\n",
      "loss: 0.4838123470544815, train acc: 0.9121\n",
      "epoch: 6, loss: 0.3424045741558075, train acc: 0.9121, test acc: 0.889\n",
      "loss: 0.3889484703540802, train acc: 0.9106\n",
      "loss: 0.47347154319286344, train acc: 0.9147\n",
      "loss: 0.5027962774038315, train acc: 0.9135\n",
      "loss: 0.47419261634349824, train acc: 0.915\n",
      "loss: 0.40299038887023925, train acc: 0.9167\n",
      "loss: 0.40277975499629975, train acc: 0.9165\n",
      "loss: 0.46402171552181243, train acc: 0.9174\n",
      "loss: 0.4325718700885773, train acc: 0.9175\n",
      "epoch: 7, loss: 0.15419644117355347, train acc: 0.9175, test acc: 0.8919\n",
      "loss: 0.3409290313720703, train acc: 0.9171\n",
      "loss: 0.48623930513858793, train acc: 0.9158\n",
      "loss: 0.47827797532081606, train acc: 0.9185\n",
      "loss: 0.4429385900497437, train acc: 0.9187\n",
      "loss: 0.41980122625827787, train acc: 0.9187\n",
      "loss: 0.40226125717163086, train acc: 0.919\n",
      "loss: 0.4703048408031464, train acc: 0.9199\n",
      "loss: 0.41064442992210387, train acc: 0.9191\n",
      "epoch: 8, loss: 0.17540404200553894, train acc: 0.9191, test acc: 0.8946\n",
      "loss: 0.453572154045105, train acc: 0.9175\n",
      "loss: 0.489008703827858, train acc: 0.9205\n",
      "loss: 0.4444831907749176, train acc: 0.9206\n",
      "loss: 0.4552477538585663, train acc: 0.9224\n",
      "loss: 0.3907678246498108, train acc: 0.9222\n",
      "loss: 0.4018859460949898, train acc: 0.9189\n",
      "loss: 0.44060121178627015, train acc: 0.9225\n",
      "loss: 0.41056942343711855, train acc: 0.9205\n",
      "epoch: 9, loss: 0.3515337109565735, train acc: 0.9205, test acc: 0.8947\n",
      "loss: 0.33568474650382996, train acc: 0.9198\n",
      "loss: 0.47766064703464506, train acc: 0.9226\n",
      "loss: 0.4371520608663559, train acc: 0.9222\n",
      "loss: 0.4200178563594818, train acc: 0.9243\n",
      "loss: 0.3820402920246124, train acc: 0.9229\n",
      "loss: 0.39926064312458037, train acc: 0.9243\n",
      "loss: 0.43708153665065763, train acc: 0.9233\n",
      "loss: 0.41283870339393614, train acc: 0.9251\n",
      "epoch: 10, loss: 0.3262499272823334, train acc: 0.9251, test acc: 0.8979\n",
      "loss: 0.3449781537055969, train acc: 0.9251\n",
      "loss: 0.445809605717659, train acc: 0.9261\n",
      "loss: 0.4269765496253967, train acc: 0.9272\n",
      "loss: 0.4147511273622513, train acc: 0.9242\n",
      "loss: 0.387215331196785, train acc: 0.927\n",
      "loss: 0.3700107097625732, train acc: 0.9255\n",
      "loss: 0.4261956065893173, train acc: 0.9273\n",
      "loss: 0.40115607380867, train acc: 0.9266\n",
      "epoch: 11, loss: 0.2703191041946411, train acc: 0.9266, test acc: 0.8967\n",
      "loss: 0.34243127703666687, train acc: 0.9254\n",
      "loss: 0.4462624996900558, train acc: 0.9259\n",
      "loss: 0.43839083313941957, train acc: 0.9295\n",
      "loss: 0.3992208391427994, train acc: 0.9289\n",
      "loss: 0.3787974536418915, train acc: 0.9279\n",
      "loss: 0.3683426082134247, train acc: 0.9273\n",
      "loss: 0.4187010884284973, train acc: 0.9275\n",
      "loss: 0.38259676694869993, train acc: 0.9285\n",
      "epoch: 12, loss: 0.13490960001945496, train acc: 0.9285, test acc: 0.8988\n",
      "loss: 0.3149518370628357, train acc: 0.9276\n",
      "loss: 0.4215955913066864, train acc: 0.9279\n",
      "loss: 0.4210434198379517, train acc: 0.9304\n",
      "loss: 0.39686411023139956, train acc: 0.9266\n",
      "loss: 0.3677175611257553, train acc: 0.9298\n",
      "loss: 0.36413194835186, train acc: 0.9274\n",
      "loss: 0.4117616474628448, train acc: 0.9293\n",
      "loss: 0.3975916564464569, train acc: 0.9309\n",
      "epoch: 13, loss: 0.24503953754901886, train acc: 0.9309, test acc: 0.897\n",
      "loss: 0.3508889079093933, train acc: 0.929\n",
      "loss: 0.44129822254180906, train acc: 0.9291\n",
      "loss: 0.4016416609287262, train acc: 0.9317\n",
      "loss: 0.3693618029356003, train acc: 0.9292\n",
      "loss: 0.36000752449035645, train acc: 0.9327\n",
      "loss: 0.35636832416057584, train acc: 0.9311\n",
      "loss: 0.39880805015563964, train acc: 0.9308\n",
      "loss: 0.3833583116531372, train acc: 0.9325\n",
      "epoch: 14, loss: 0.33269375562667847, train acc: 0.9325, test acc: 0.8986\n",
      "loss: 0.3125714957714081, train acc: 0.9295\n",
      "loss: 0.41266370713710787, train acc: 0.9297\n",
      "loss: 0.3929789066314697, train acc: 0.9346\n",
      "loss: 0.3791277647018433, train acc: 0.9294\n",
      "loss: 0.3875494509935379, train acc: 0.9336\n",
      "loss: 0.37357010543346403, train acc: 0.9302\n",
      "loss: 0.3882327198982239, train acc: 0.9327\n",
      "loss: 0.3786323994398117, train acc: 0.9337\n",
      "epoch: 15, loss: 0.42133069038391113, train acc: 0.9337, test acc: 0.9006\n",
      "loss: 0.31432467699050903, train acc: 0.93\n",
      "loss: 0.43645086586475373, train acc: 0.9317\n",
      "loss: 0.40691038966178894, train acc: 0.9356\n",
      "loss: 0.37117541432380674, train acc: 0.9319\n",
      "loss: 0.353947314620018, train acc: 0.9347\n",
      "loss: 0.33794757425785066, train acc: 0.9332\n",
      "loss: 0.3949917584657669, train acc: 0.9308\n",
      "loss: 0.370795863866806, train acc: 0.9344\n",
      "epoch: 16, loss: 0.15319538116455078, train acc: 0.9344, test acc: 0.9\n",
      "loss: 0.3120010495185852, train acc: 0.9325\n",
      "loss: 0.40084528028964994, train acc: 0.9319\n",
      "loss: 0.39932924807071685, train acc: 0.9337\n",
      "loss: 0.37658070772886276, train acc: 0.9307\n",
      "loss: 0.3491354644298553, train acc: 0.9353\n",
      "loss: 0.3412908986210823, train acc: 0.9343\n",
      "loss: 0.3884624749422073, train acc: 0.9337\n",
      "loss: 0.36847702264785764, train acc: 0.9356\n",
      "epoch: 17, loss: 0.18125642836093903, train acc: 0.9356, test acc: 0.8994\n",
      "loss: 0.3221086263656616, train acc: 0.9339\n",
      "loss: 0.4268381357192993, train acc: 0.9341\n",
      "loss: 0.37826665937900544, train acc: 0.9359\n",
      "loss: 0.3887331306934357, train acc: 0.933\n",
      "loss: 0.3555978059768677, train acc: 0.9365\n",
      "loss: 0.34119216799736024, train acc: 0.9354\n",
      "loss: 0.37677587270736695, train acc: 0.9367\n",
      "loss: 0.34878166317939757, train acc: 0.937\n",
      "epoch: 18, loss: 0.1303355097770691, train acc: 0.937, test acc: 0.8998\n",
      "loss: 0.26532575488090515, train acc: 0.9329\n",
      "loss: 0.38843221962451935, train acc: 0.9365\n",
      "loss: 0.397567555308342, train acc: 0.9374\n",
      "loss: 0.35747957825660703, train acc: 0.9353\n",
      "loss: 0.3286863550543785, train acc: 0.9366\n",
      "loss: 0.33045128881931307, train acc: 0.9352\n",
      "loss: 0.37249203622341154, train acc: 0.9363\n",
      "loss: 0.3656580299139023, train acc: 0.9359\n",
      "epoch: 19, loss: 0.19988194108009338, train acc: 0.9359, test acc: 0.9024\n",
      "loss: 0.30888476967811584, train acc: 0.9353\n",
      "loss: 0.3970892608165741, train acc: 0.9364\n",
      "loss: 0.3876896291971207, train acc: 0.9386\n",
      "loss: 0.34848579615354536, train acc: 0.9377\n",
      "loss: 0.3223156347870827, train acc: 0.9361\n",
      "loss: 0.334157706797123, train acc: 0.9355\n",
      "loss: 0.3907608062028885, train acc: 0.9365\n",
      "loss: 0.3463477835059166, train acc: 0.9366\n",
      "epoch: 20, loss: 0.060798849910497665, train acc: 0.9366, test acc: 0.9022\n",
      "loss: 0.33502206206321716, train acc: 0.9351\n",
      "loss: 0.3814482927322388, train acc: 0.9376\n",
      "loss: 0.3888534069061279, train acc: 0.9386\n",
      "loss: 0.3759164497256279, train acc: 0.9355\n",
      "loss: 0.3515946686267853, train acc: 0.9378\n",
      "loss: 0.3217068433761597, train acc: 0.9355\n",
      "loss: 0.3581182166934013, train acc: 0.9377\n",
      "loss: 0.32489032447338106, train acc: 0.9386\n",
      "epoch: 21, loss: 0.22826147079467773, train acc: 0.9386, test acc: 0.9006\n",
      "loss: 0.26770326495170593, train acc: 0.9361\n",
      "loss: 0.3725977450609207, train acc: 0.9397\n",
      "loss: 0.37302995324134824, train acc: 0.9395\n",
      "loss: 0.3690712213516235, train acc: 0.939\n",
      "loss: 0.3233893126249313, train acc: 0.9386\n",
      "loss: 0.3361907422542572, train acc: 0.9368\n",
      "loss: 0.3639438822865486, train acc: 0.9376\n",
      "loss: 0.3416153430938721, train acc: 0.9404\n",
      "epoch: 22, loss: 0.12858162820339203, train acc: 0.9404, test acc: 0.8994\n",
      "loss: 0.29377099871635437, train acc: 0.9353\n",
      "loss: 0.3761948823928833, train acc: 0.938\n",
      "loss: 0.3639127016067505, train acc: 0.9402\n",
      "loss: 0.3573078513145447, train acc: 0.938\n",
      "loss: 0.329835844039917, train acc: 0.9394\n",
      "loss: 0.32897879034280775, train acc: 0.9375\n",
      "loss: 0.35312018543481827, train acc: 0.94\n",
      "loss: 0.321575453877449, train acc: 0.9398\n",
      "epoch: 23, loss: 0.09824661910533905, train acc: 0.9398, test acc: 0.9013\n",
      "loss: 0.317219078540802, train acc: 0.9379\n",
      "loss: 0.38095170855522154, train acc: 0.9411\n",
      "loss: 0.36423082947731017, train acc: 0.9397\n",
      "loss: 0.3558132141828537, train acc: 0.9385\n",
      "loss: 0.31964291036129, train acc: 0.9413\n",
      "loss: 0.30669029653072355, train acc: 0.9384\n",
      "loss: 0.36541690677404404, train acc: 0.9401\n",
      "loss: 0.34244699478149415, train acc: 0.9415\n",
      "epoch: 24, loss: 0.20208072662353516, train acc: 0.9415, test acc: 0.9014\n",
      "loss: 0.33873435854911804, train acc: 0.9361\n",
      "loss: 0.3808064579963684, train acc: 0.9406\n",
      "loss: 0.37293443977832796, train acc: 0.9411\n",
      "loss: 0.36741152703762053, train acc: 0.9412\n",
      "loss: 0.31421337425708773, train acc: 0.9411\n",
      "loss: 0.3154324784874916, train acc: 0.9398\n",
      "loss: 0.3633616387844086, train acc: 0.9407\n",
      "loss: 0.31109441369771956, train acc: 0.9421\n",
      "epoch: 25, loss: 0.18457382917404175, train acc: 0.9421, test acc: 0.9011\n",
      "loss: 0.352151483297348, train acc: 0.9367\n",
      "loss: 0.38662676215171815, train acc: 0.9406\n",
      "loss: 0.36889610141515733, train acc: 0.9427\n",
      "loss: 0.3359918147325516, train acc: 0.9388\n",
      "loss: 0.30722252279520035, train acc: 0.941\n",
      "loss: 0.30118185430765154, train acc: 0.9379\n",
      "loss: 0.3750209927558899, train acc: 0.9419\n",
      "loss: 0.33413776755332947, train acc: 0.9425\n",
      "epoch: 26, loss: 0.22698485851287842, train acc: 0.9425, test acc: 0.9017\n",
      "loss: 0.30338960886001587, train acc: 0.9388\n",
      "loss: 0.36635143756866456, train acc: 0.9424\n",
      "loss: 0.3679243326187134, train acc: 0.9423\n",
      "loss: 0.33838026970624924, train acc: 0.9387\n",
      "loss: 0.31643600165843966, train acc: 0.9419\n",
      "loss: 0.2885910838842392, train acc: 0.9388\n",
      "loss: 0.3271971732378006, train acc: 0.9422\n",
      "loss: 0.32995039522647857, train acc: 0.9424\n",
      "epoch: 27, loss: 0.10821372270584106, train acc: 0.9424, test acc: 0.9006\n",
      "loss: 0.29986080527305603, train acc: 0.9407\n",
      "loss: 0.36516056060791013, train acc: 0.9422\n",
      "loss: 0.3829521775245667, train acc: 0.9432\n",
      "loss: 0.3385829791426659, train acc: 0.9415\n",
      "loss: 0.32293753772974015, train acc: 0.9437\n",
      "loss: 0.29521570950746534, train acc: 0.9411\n",
      "loss: 0.33832859694957734, train acc: 0.9405\n",
      "loss: 0.32709750682115557, train acc: 0.9428\n",
      "epoch: 28, loss: 0.11200729012489319, train acc: 0.9428, test acc: 0.8992\n",
      "loss: 0.35381391644477844, train acc: 0.9378\n",
      "loss: 0.34471731036901476, train acc: 0.9426\n",
      "loss: 0.36594252586364745, train acc: 0.9438\n",
      "loss: 0.36348140239715576, train acc: 0.942\n",
      "loss: 0.3077170953154564, train acc: 0.9432\n",
      "loss: 0.3129023492336273, train acc: 0.9434\n",
      "loss: 0.34155266880989077, train acc: 0.9425\n",
      "loss: 0.31577280461788176, train acc: 0.9438\n",
      "epoch: 29, loss: 0.31122148036956787, train acc: 0.9438, test acc: 0.8994\n",
      "loss: 0.41747814416885376, train acc: 0.9405\n",
      "loss: 0.36608012169599535, train acc: 0.943\n",
      "loss: 0.3666715115308762, train acc: 0.9422\n",
      "loss: 0.36224846392869947, train acc: 0.9416\n",
      "loss: 0.3184905588626862, train acc: 0.9453\n",
      "loss: 0.2866977483034134, train acc: 0.9415\n",
      "loss: 0.3641231253743172, train acc: 0.9426\n",
      "loss: 0.29928785711526873, train acc: 0.9446\n",
      "epoch: 30, loss: 0.13136756420135498, train acc: 0.9446, test acc: 0.9\n",
      "loss: 0.2605394124984741, train acc: 0.9426\n",
      "loss: 0.358048340678215, train acc: 0.9428\n",
      "loss: 0.3380495920777321, train acc: 0.9447\n",
      "loss: 0.3232555851340294, train acc: 0.944\n",
      "loss: 0.30501000583171844, train acc: 0.9448\n",
      "loss: 0.3009611487388611, train acc: 0.9429\n",
      "loss: 0.35370445251464844, train acc: 0.9446\n",
      "loss: 0.3087803050875664, train acc: 0.9458\n",
      "epoch: 31, loss: 0.19253729283809662, train acc: 0.9458, test acc: 0.9007\n",
      "loss: 0.29201069474220276, train acc: 0.9418\n",
      "loss: 0.3453131467103958, train acc: 0.943\n",
      "loss: 0.33285906463861464, train acc: 0.9433\n",
      "loss: 0.3423470139503479, train acc: 0.9427\n",
      "loss: 0.2926057890057564, train acc: 0.9468\n",
      "loss: 0.3025523334741592, train acc: 0.9432\n",
      "loss: 0.3498381346464157, train acc: 0.9406\n",
      "loss: 0.3307035878300667, train acc: 0.9438\n",
      "epoch: 32, loss: 0.24042242765426636, train acc: 0.9438, test acc: 0.9018\n",
      "loss: 0.2709284722805023, train acc: 0.943\n",
      "loss: 0.3301406651735306, train acc: 0.9437\n",
      "loss: 0.3445389926433563, train acc: 0.9459\n",
      "loss: 0.32995909452438354, train acc: 0.9434\n",
      "loss: 0.29868238866329194, train acc: 0.9466\n",
      "loss: 0.2988027364015579, train acc: 0.9443\n",
      "loss: 0.365804123878479, train acc: 0.9446\n",
      "loss: 0.3281915783882141, train acc: 0.9458\n",
      "epoch: 33, loss: 0.09506679326295853, train acc: 0.9458, test acc: 0.8984\n",
      "loss: 0.3454158306121826, train acc: 0.9399\n",
      "loss: 0.33861847072839735, train acc: 0.9443\n",
      "loss: 0.3322469890117645, train acc: 0.9446\n",
      "loss: 0.32950053960084913, train acc: 0.9423\n",
      "loss: 0.32090398818254473, train acc: 0.9471\n",
      "loss: 0.3298718869686127, train acc: 0.9448\n",
      "loss: 0.3440281391143799, train acc: 0.9439\n",
      "loss: 0.31210734993219375, train acc: 0.9467\n",
      "epoch: 34, loss: 0.23679381608963013, train acc: 0.9467, test acc: 0.8971\n",
      "loss: 0.3013538420200348, train acc: 0.943\n",
      "loss: 0.35981924533843995, train acc: 0.9429\n",
      "loss: 0.3399632900953293, train acc: 0.946\n",
      "loss: 0.3418306827545166, train acc: 0.9414\n",
      "loss: 0.2987122908234596, train acc: 0.9473\n",
      "loss: 0.28611810207366944, train acc: 0.9435\n",
      "loss: 0.3351737275719643, train acc: 0.9452\n",
      "loss: 0.3024534210562706, train acc: 0.9475\n",
      "epoch: 35, loss: 0.06126819923520088, train acc: 0.9475, test acc: 0.8993\n",
      "loss: 0.31481069326400757, train acc: 0.9431\n",
      "loss: 0.3547449216246605, train acc: 0.9461\n",
      "loss: 0.33460934460163116, train acc: 0.9463\n",
      "loss: 0.3237271666526794, train acc: 0.9445\n",
      "loss: 0.31712098568677904, train acc: 0.9439\n",
      "loss: 0.3282198831439018, train acc: 0.9444\n",
      "loss: 0.3301555335521698, train acc: 0.9447\n",
      "loss: 0.30891761779785154, train acc: 0.9456\n",
      "epoch: 36, loss: 0.05068426579236984, train acc: 0.9456, test acc: 0.9003\n",
      "loss: 0.2732142508029938, train acc: 0.9446\n",
      "loss: 0.37513333559036255, train acc: 0.9461\n",
      "loss: 0.3424938827753067, train acc: 0.9456\n",
      "loss: 0.3244915634393692, train acc: 0.9455\n",
      "loss: 0.28854172974824904, train acc: 0.9468\n",
      "loss: 0.3056475192308426, train acc: 0.9455\n",
      "loss: 0.3554244935512543, train acc: 0.9457\n",
      "loss: 0.31307157725095747, train acc: 0.9455\n",
      "epoch: 37, loss: 0.08379393815994263, train acc: 0.9455, test acc: 0.8987\n",
      "loss: 0.3576572835445404, train acc: 0.9437\n",
      "loss: 0.35529719591140746, train acc: 0.9456\n",
      "loss: 0.3347133040428162, train acc: 0.9487\n",
      "loss: 0.3043879956007004, train acc: 0.9486\n",
      "loss: 0.29849111586809157, train acc: 0.95\n",
      "loss: 0.29994975328445433, train acc: 0.9465\n",
      "loss: 0.34037221372127535, train acc: 0.9459\n",
      "loss: 0.2863589897751808, train acc: 0.9483\n",
      "epoch: 38, loss: 0.05552809685468674, train acc: 0.9483, test acc: 0.899\n",
      "loss: 0.26325318217277527, train acc: 0.9438\n",
      "loss: 0.3628076076507568, train acc: 0.9464\n",
      "loss: 0.32165872901678083, train acc: 0.948\n",
      "loss: 0.3478417843580246, train acc: 0.948\n",
      "loss: 0.2813246130943298, train acc: 0.9489\n",
      "loss: 0.2823947325348854, train acc: 0.9423\n",
      "loss: 0.3183999091386795, train acc: 0.9479\n",
      "loss: 0.30473379194736483, train acc: 0.9481\n",
      "epoch: 39, loss: 0.21395601332187653, train acc: 0.9481, test acc: 0.8965\n",
      "loss: 0.32317665219306946, train acc: 0.9453\n",
      "loss: 0.34733980894088745, train acc: 0.9453\n",
      "loss: 0.31810242533683775, train acc: 0.948\n",
      "loss: 0.3213546350598335, train acc: 0.9428\n",
      "loss: 0.28597295433282854, train acc: 0.9478\n",
      "loss: 0.29844161570072175, train acc: 0.9459\n",
      "loss: 0.32081577479839324, train acc: 0.9467\n",
      "loss: 0.29241396188735963, train acc: 0.9499\n",
      "epoch: 40, loss: 0.2733214497566223, train acc: 0.9499, test acc: 0.8987\n",
      "loss: 0.35014808177948, train acc: 0.9471\n",
      "loss: 0.357069730758667, train acc: 0.9485\n",
      "loss: 0.33241822719573977, train acc: 0.9494\n",
      "loss: 0.30201429724693296, train acc: 0.9421\n",
      "loss: 0.2617999166250229, train acc: 0.9481\n",
      "loss: 0.28426743298768997, train acc: 0.9457\n",
      "loss: 0.32308389991521835, train acc: 0.9484\n",
      "loss: 0.3007434710860252, train acc: 0.9495\n",
      "epoch: 41, loss: 0.11836019903421402, train acc: 0.9495, test acc: 0.8982\n",
      "loss: 0.3117697238922119, train acc: 0.9451\n",
      "loss: 0.34233257323503496, train acc: 0.9481\n",
      "loss: 0.3257382869720459, train acc: 0.9497\n",
      "loss: 0.3073151662945747, train acc: 0.9491\n",
      "loss: 0.28996298760175704, train acc: 0.9495\n",
      "loss: 0.297219842672348, train acc: 0.9465\n",
      "loss: 0.31403947472572324, train acc: 0.9502\n",
      "loss: 0.3069499760866165, train acc: 0.9503\n",
      "epoch: 42, loss: 0.10249277949333191, train acc: 0.9503, test acc: 0.897\n",
      "loss: 0.295490562915802, train acc: 0.9478\n",
      "loss: 0.3405848041176796, train acc: 0.9488\n",
      "loss: 0.3140187174081802, train acc: 0.9488\n",
      "loss: 0.31442803144454956, train acc: 0.9467\n",
      "loss: 0.2861285820603371, train acc: 0.9506\n",
      "loss: 0.2691419214010239, train acc: 0.9485\n",
      "loss: 0.3147993966937065, train acc: 0.9495\n",
      "loss: 0.298110331594944, train acc: 0.9503\n",
      "epoch: 43, loss: 0.08952265977859497, train acc: 0.9503, test acc: 0.8957\n",
      "loss: 0.33571240305900574, train acc: 0.9457\n",
      "loss: 0.3384549155831337, train acc: 0.9483\n",
      "loss: 0.33842199444770815, train acc: 0.9507\n",
      "loss: 0.31449353992938994, train acc: 0.9469\n",
      "loss: 0.2924728438258171, train acc: 0.9502\n",
      "loss: 0.27557465583086016, train acc: 0.9499\n",
      "loss: 0.32988193929195403, train acc: 0.9494\n",
      "loss: 0.3117378890514374, train acc: 0.9513\n",
      "epoch: 44, loss: 0.2484903186559677, train acc: 0.9513, test acc: 0.8964\n",
      "loss: 0.3079691529273987, train acc: 0.9471\n",
      "loss: 0.3484013557434082, train acc: 0.9525\n",
      "loss: 0.30960720032453537, train acc: 0.952\n",
      "loss: 0.3259333372116089, train acc: 0.947\n",
      "loss: 0.30118287801742555, train acc: 0.9502\n",
      "loss: 0.28691664040088655, train acc: 0.9476\n",
      "loss: 0.3144263833761215, train acc: 0.9505\n",
      "loss: 0.3107658803462982, train acc: 0.9511\n",
      "epoch: 45, loss: 0.057393938302993774, train acc: 0.9511, test acc: 0.8948\n",
      "loss: 0.30748817324638367, train acc: 0.9459\n",
      "loss: 0.34132302850484847, train acc: 0.949\n",
      "loss: 0.3307722270488739, train acc: 0.9488\n",
      "loss: 0.32602414935827256, train acc: 0.9477\n",
      "loss: 0.2599793508648872, train acc: 0.949\n",
      "loss: 0.26680972427129745, train acc: 0.949\n",
      "loss: 0.3201608553528786, train acc: 0.9487\n",
      "loss: 0.3002391532063484, train acc: 0.9497\n",
      "epoch: 46, loss: 0.0657350942492485, train acc: 0.9497, test acc: 0.8987\n",
      "loss: 0.2707768976688385, train acc: 0.9496\n",
      "loss: 0.32890382409095764, train acc: 0.9493\n",
      "loss: 0.3096738427877426, train acc: 0.9527\n",
      "loss: 0.30924350768327713, train acc: 0.9517\n",
      "loss: 0.27728493213653566, train acc: 0.9505\n",
      "loss: 0.24321682900190353, train acc: 0.951\n",
      "loss: 0.32142759263515475, train acc: 0.9497\n",
      "loss: 0.2826434627175331, train acc: 0.953\n",
      "epoch: 47, loss: 0.2674504816532135, train acc: 0.953, test acc: 0.8994\n",
      "loss: 0.22410209476947784, train acc: 0.9507\n",
      "loss: 0.316942423582077, train acc: 0.9511\n",
      "loss: 0.32668488174676896, train acc: 0.9524\n",
      "loss: 0.3361899986863136, train acc: 0.951\n",
      "loss: 0.27415980249643324, train acc: 0.9505\n",
      "loss: 0.2928285926580429, train acc: 0.9512\n",
      "loss: 0.3139468774199486, train acc: 0.9541\n",
      "loss: 0.27686755955219267, train acc: 0.9525\n",
      "epoch: 48, loss: 0.2105943262577057, train acc: 0.9525, test acc: 0.8982\n",
      "loss: 0.3112020492553711, train acc: 0.9506\n",
      "loss: 0.28391931653022767, train acc: 0.9512\n",
      "loss: 0.3023292735219002, train acc: 0.9512\n",
      "loss: 0.3013650864362717, train acc: 0.9512\n",
      "loss: 0.26134656369686127, train acc: 0.9508\n",
      "loss: 0.2819188073277473, train acc: 0.9509\n",
      "loss: 0.32883875966072085, train acc: 0.9523\n",
      "loss: 0.26394798904657363, train acc: 0.9506\n",
      "epoch: 49, loss: 0.20385272800922394, train acc: 0.9506, test acc: 0.896\n",
      "loss: 0.2663334012031555, train acc: 0.947\n",
      "loss: 0.30933532416820525, train acc: 0.95\n",
      "loss: 0.2986688122153282, train acc: 0.9528\n",
      "loss: 0.3152899593114853, train acc: 0.9499\n",
      "loss: 0.27461435794830324, train acc: 0.952\n",
      "loss: 0.28611437827348707, train acc: 0.9493\n",
      "loss: 0.28887938559055326, train acc: 0.9508\n",
      "loss: 0.279278963804245, train acc: 0.9539\n",
      "epoch: 50, loss: 0.24933941662311554, train acc: 0.9539, test acc: 0.8992\n",
      "loss: 0.23519399762153625, train acc: 0.951\n",
      "loss: 0.3258669376373291, train acc: 0.9523\n",
      "loss: 0.3232392340898514, train acc: 0.9529\n",
      "loss: 0.31638526916503906, train acc: 0.9503\n",
      "loss: 0.28411871045827863, train acc: 0.9533\n",
      "loss: 0.26904808133840563, train acc: 0.9533\n",
      "loss: 0.29061259329319, train acc: 0.9511\n",
      "loss: 0.2793280392885208, train acc: 0.9543\n",
      "epoch: 51, loss: 0.15980520844459534, train acc: 0.9543, test acc: 0.8974\n",
      "loss: 0.3140661418437958, train acc: 0.9505\n",
      "loss: 0.3153461828827858, train acc: 0.9509\n",
      "loss: 0.33124726712703706, train acc: 0.9541\n",
      "loss: 0.2943951696157455, train acc: 0.9515\n",
      "loss: 0.2934462517499924, train acc: 0.9534\n",
      "loss: 0.2658988207578659, train acc: 0.9504\n",
      "loss: 0.31602573990821836, train acc: 0.9503\n",
      "loss: 0.2758647531270981, train acc: 0.9547\n",
      "epoch: 52, loss: 0.08191332221031189, train acc: 0.9547, test acc: 0.8984\n",
      "loss: 0.23955312371253967, train acc: 0.9494\n",
      "loss: 0.33439981639385224, train acc: 0.9518\n",
      "loss: 0.3067303538322449, train acc: 0.9542\n",
      "loss: 0.29369578659534457, train acc: 0.9528\n",
      "loss: 0.28565932959318163, train acc: 0.9537\n",
      "loss: 0.273216013610363, train acc: 0.9517\n",
      "loss: 0.3012111231684685, train acc: 0.9521\n",
      "loss: 0.27531827092170713, train acc: 0.955\n",
      "epoch: 53, loss: 0.22140033543109894, train acc: 0.955, test acc: 0.8973\n",
      "loss: 0.33150964975357056, train acc: 0.9511\n",
      "loss: 0.33376948684453966, train acc: 0.954\n",
      "loss: 0.29042515605688096, train acc: 0.9532\n",
      "loss: 0.31580218076705935, train acc: 0.9498\n",
      "loss: 0.28728605061769485, train acc: 0.9537\n",
      "loss: 0.2563271686434746, train acc: 0.9522\n",
      "loss: 0.29874429702758787, train acc: 0.954\n",
      "loss: 0.2789754748344421, train acc: 0.9541\n",
      "epoch: 54, loss: 0.05751975625753403, train acc: 0.9541, test acc: 0.8968\n",
      "loss: 0.3080800473690033, train acc: 0.953\n",
      "loss: 0.31320770382881163, train acc: 0.9526\n",
      "loss: 0.3201641798019409, train acc: 0.9541\n",
      "loss: 0.31621868163347244, train acc: 0.9509\n",
      "loss: 0.28044440895318984, train acc: 0.9544\n",
      "loss: 0.25745861530303954, train acc: 0.9525\n",
      "loss: 0.32007458060979843, train acc: 0.9527\n",
      "loss: 0.2861191615462303, train acc: 0.9545\n",
      "epoch: 55, loss: 0.08415594696998596, train acc: 0.9545, test acc: 0.8973\n",
      "loss: 0.22482042014598846, train acc: 0.9534\n",
      "loss: 0.29274460673332214, train acc: 0.9538\n",
      "loss: 0.2939805597066879, train acc: 0.955\n",
      "loss: 0.3178190037608147, train acc: 0.9528\n",
      "loss: 0.28070458620786665, train acc: 0.954\n",
      "loss: 0.2830364555120468, train acc: 0.9532\n",
      "loss: 0.28004817515611646, train acc: 0.9528\n",
      "loss: 0.28236566931009294, train acc: 0.9545\n",
      "epoch: 56, loss: 0.2953522503376007, train acc: 0.9545, test acc: 0.8958\n",
      "loss: 0.3018069565296173, train acc: 0.9507\n",
      "loss: 0.322233809530735, train acc: 0.9539\n",
      "loss: 0.288885785639286, train acc: 0.9552\n",
      "loss: 0.3086246892809868, train acc: 0.9517\n",
      "loss: 0.25082109719514845, train acc: 0.952\n",
      "loss: 0.25922679901123047, train acc: 0.9544\n",
      "loss: 0.30241497457027433, train acc: 0.9534\n",
      "loss: 0.2828348964452744, train acc: 0.9553\n",
      "epoch: 57, loss: 0.11733986437320709, train acc: 0.9553, test acc: 0.8949\n",
      "loss: 0.36406242847442627, train acc: 0.9515\n",
      "loss: 0.31163012981414795, train acc: 0.9547\n",
      "loss: 0.3130846619606018, train acc: 0.9551\n",
      "loss: 0.2903759092092514, train acc: 0.9547\n",
      "loss: 0.23818607330322267, train acc: 0.9552\n",
      "loss: 0.263973642885685, train acc: 0.9523\n",
      "loss: 0.3174517348408699, train acc: 0.9526\n",
      "loss: 0.2620653912425041, train acc: 0.9548\n",
      "epoch: 58, loss: 0.19392646849155426, train acc: 0.9548, test acc: 0.896\n",
      "loss: 0.325114369392395, train acc: 0.9537\n",
      "loss: 0.33756206184625626, train acc: 0.9556\n",
      "loss: 0.3096866190433502, train acc: 0.9536\n",
      "loss: 0.2926396280527115, train acc: 0.95\n",
      "loss: 0.2869640842080116, train acc: 0.955\n",
      "loss: 0.27946453988552095, train acc: 0.9529\n",
      "loss: 0.28616631478071214, train acc: 0.9527\n",
      "loss: 0.25400886237621306, train acc: 0.9557\n",
      "epoch: 59, loss: 0.1250413954257965, train acc: 0.9557, test acc: 0.8942\n",
      "loss: 0.38629376888275146, train acc: 0.9526\n",
      "loss: 0.3350192129611969, train acc: 0.954\n",
      "loss: 0.2956232473254204, train acc: 0.9571\n",
      "loss: 0.2819181397557259, train acc: 0.9533\n",
      "loss: 0.27136213332414627, train acc: 0.9558\n",
      "loss: 0.25723466873168943, train acc: 0.9555\n",
      "loss: 0.2822564721107483, train acc: 0.9556\n",
      "loss: 0.25218272805213926, train acc: 0.9593\n",
      "epoch: 60, loss: 0.06476260721683502, train acc: 0.9593, test acc: 0.8998\n",
      "loss: 0.2809181809425354, train acc: 0.9564\n",
      "loss: 0.3132674068212509, train acc: 0.9556\n",
      "loss: 0.30526432394981384, train acc: 0.9578\n",
      "loss: 0.29583756923675536, train acc: 0.9554\n",
      "loss: 0.28386570811271666, train acc: 0.9564\n",
      "loss: 0.26588077545166017, train acc: 0.9551\n",
      "loss: 0.309507192671299, train acc: 0.9562\n",
      "loss: 0.27067032903432847, train acc: 0.9585\n",
      "epoch: 61, loss: 0.22316575050354004, train acc: 0.9585, test acc: 0.8985\n",
      "loss: 0.2958793640136719, train acc: 0.9574\n",
      "loss: 0.2979337438941002, train acc: 0.957\n",
      "loss: 0.3044235587120056, train acc: 0.9593\n",
      "loss: 0.30671861320734023, train acc: 0.9538\n",
      "loss: 0.2695690467953682, train acc: 0.9532\n",
      "loss: 0.27437162697315215, train acc: 0.9565\n",
      "loss: 0.2927066802978516, train acc: 0.9566\n",
      "loss: 0.24821201115846633, train acc: 0.9574\n",
      "epoch: 62, loss: 0.09625145047903061, train acc: 0.9574, test acc: 0.8965\n",
      "loss: 0.2403368353843689, train acc: 0.9558\n",
      "loss: 0.295627573132515, train acc: 0.9553\n",
      "loss: 0.297259595990181, train acc: 0.9586\n",
      "loss: 0.28682749569416044, train acc: 0.956\n",
      "loss: 0.26734641641378404, train acc: 0.9591\n",
      "loss: 0.2759905993938446, train acc: 0.9566\n",
      "loss: 0.31673078387975695, train acc: 0.9558\n",
      "loss: 0.25877567380666733, train acc: 0.9575\n",
      "epoch: 63, loss: 0.2064702957868576, train acc: 0.9575, test acc: 0.896\n",
      "loss: 0.23200084269046783, train acc: 0.9556\n",
      "loss: 0.28307614624500277, train acc: 0.9587\n",
      "loss: 0.29873750656843184, train acc: 0.9594\n",
      "loss: 0.2917570799589157, train acc: 0.9525\n",
      "loss: 0.2770116850733757, train acc: 0.9563\n",
      "loss: 0.249549101293087, train acc: 0.9541\n",
      "loss: 0.31463485956192017, train acc: 0.9549\n",
      "loss: 0.24857882559299468, train acc: 0.9596\n",
      "epoch: 64, loss: 0.2277022898197174, train acc: 0.9596, test acc: 0.8971\n",
      "loss: 0.3243153691291809, train acc: 0.9573\n",
      "loss: 0.3125773474574089, train acc: 0.9545\n",
      "loss: 0.2921170130372047, train acc: 0.9594\n",
      "loss: 0.2760015740990639, train acc: 0.954\n",
      "loss: 0.25580334961414336, train acc: 0.9569\n",
      "loss: 0.26154727190732957, train acc: 0.9547\n",
      "loss: 0.279571695625782, train acc: 0.9578\n",
      "loss: 0.23431550711393356, train acc: 0.9589\n",
      "epoch: 65, loss: 0.13948746025562286, train acc: 0.9589, test acc: 0.8939\n",
      "loss: 0.25504153966903687, train acc: 0.9527\n",
      "loss: 0.3030109629034996, train acc: 0.9582\n",
      "loss: 0.28890765756368636, train acc: 0.9555\n",
      "loss: 0.30245343744754793, train acc: 0.9573\n",
      "loss: 0.24813265204429627, train acc: 0.9572\n",
      "loss: 0.25605191141366956, train acc: 0.9544\n",
      "loss: 0.28850863724946973, train acc: 0.9574\n",
      "loss: 0.2664515256881714, train acc: 0.9607\n",
      "epoch: 66, loss: 0.03933097422122955, train acc: 0.9607, test acc: 0.8958\n",
      "loss: 0.3500329256057739, train acc: 0.9556\n",
      "loss: 0.3079847991466522, train acc: 0.9587\n",
      "loss: 0.29838512390851973, train acc: 0.9607\n",
      "loss: 0.2938591495156288, train acc: 0.9558\n",
      "loss: 0.24896601140499114, train acc: 0.9578\n",
      "loss: 0.2609890684485435, train acc: 0.9569\n",
      "loss: 0.29536910355091095, train acc: 0.959\n",
      "loss: 0.23071532398462297, train acc: 0.9589\n",
      "epoch: 67, loss: 0.10372374951839447, train acc: 0.9589, test acc: 0.8986\n",
      "loss: 0.2636913061141968, train acc: 0.9585\n",
      "loss: 0.27309724390506745, train acc: 0.9559\n",
      "loss: 0.30852115750312803, train acc: 0.9587\n",
      "loss: 0.27659083604812623, train acc: 0.9558\n",
      "loss: 0.2574360311031342, train acc: 0.9594\n",
      "loss: 0.24877974689006804, train acc: 0.9562\n",
      "loss: 0.3137443780899048, train acc: 0.9562\n",
      "loss: 0.24067357331514358, train acc: 0.9605\n",
      "epoch: 68, loss: 0.06869171559810638, train acc: 0.9605, test acc: 0.8985\n",
      "loss: 0.22835806012153625, train acc: 0.9574\n",
      "loss: 0.28720743060112, train acc: 0.9603\n",
      "loss: 0.2667373090982437, train acc: 0.9597\n",
      "loss: 0.27411737143993375, train acc: 0.9563\n",
      "loss: 0.25558908432722094, train acc: 0.9589\n",
      "loss: 0.2471674785017967, train acc: 0.9578\n",
      "loss: 0.27967884838581086, train acc: 0.9591\n",
      "loss: 0.2522826254367828, train acc: 0.9604\n",
      "epoch: 69, loss: 0.09820115566253662, train acc: 0.9604, test acc: 0.8954\n",
      "loss: 0.1855417639017105, train acc: 0.9565\n",
      "loss: 0.294332417845726, train acc: 0.959\n",
      "loss: 0.2896338149905205, train acc: 0.9596\n",
      "loss: 0.3058451429009438, train acc: 0.9549\n",
      "loss: 0.23191377222537995, train acc: 0.9563\n",
      "loss: 0.23623640686273575, train acc: 0.9579\n",
      "loss: 0.27145522385835646, train acc: 0.9578\n",
      "loss: 0.26163032799959185, train acc: 0.9614\n",
      "epoch: 70, loss: 0.024102939292788506, train acc: 0.9614, test acc: 0.896\n",
      "loss: 0.31724369525909424, train acc: 0.9584\n",
      "loss: 0.281407842040062, train acc: 0.9606\n",
      "loss: 0.2951532393693924, train acc: 0.961\n",
      "loss: 0.2803765252232552, train acc: 0.9552\n",
      "loss: 0.2590099111199379, train acc: 0.9582\n",
      "loss: 0.2484993040561676, train acc: 0.9548\n",
      "loss: 0.2763083949685097, train acc: 0.9593\n",
      "loss: 0.2671708971261978, train acc: 0.9596\n",
      "epoch: 71, loss: 0.40685296058654785, train acc: 0.9596, test acc: 0.8939\n",
      "loss: 0.2841697037220001, train acc: 0.9582\n",
      "loss: 0.2794907450675964, train acc: 0.956\n",
      "loss: 0.26563723385334015, train acc: 0.9592\n",
      "loss: 0.2944472312927246, train acc: 0.9538\n",
      "loss: 0.2806318745017052, train acc: 0.9579\n",
      "loss: 0.24798183739185334, train acc: 0.9566\n",
      "loss: 0.27614183723926544, train acc: 0.9573\n",
      "loss: 0.26059472411870954, train acc: 0.9602\n",
      "epoch: 72, loss: 0.16283152997493744, train acc: 0.9602, test acc: 0.8978\n",
      "loss: 0.29513221979141235, train acc: 0.959\n",
      "loss: 0.2976279780268669, train acc: 0.9587\n",
      "loss: 0.2943829894065857, train acc: 0.9626\n",
      "loss: 0.2802295461297035, train acc: 0.96\n",
      "loss: 0.25509325265884397, train acc: 0.9589\n",
      "loss: 0.2637651592493057, train acc: 0.958\n",
      "loss: 0.2976683035492897, train acc: 0.9584\n",
      "loss: 0.24870446920394898, train acc: 0.9606\n",
      "epoch: 73, loss: 0.11071315407752991, train acc: 0.9606, test acc: 0.8941\n",
      "loss: 0.25005149841308594, train acc: 0.9574\n",
      "loss: 0.29433904886245726, train acc: 0.9612\n",
      "loss: 0.28137940764427183, train acc: 0.9602\n",
      "loss: 0.28966321498155595, train acc: 0.9552\n",
      "loss: 0.26743618696928023, train acc: 0.9599\n",
      "loss: 0.27262941002845764, train acc: 0.9536\n",
      "loss: 0.28812109082937243, train acc: 0.9565\n",
      "loss: 0.24902924448251723, train acc: 0.9602\n",
      "epoch: 74, loss: 0.10855137556791306, train acc: 0.9602, test acc: 0.8975\n",
      "loss: 0.36973750591278076, train acc: 0.9599\n",
      "loss: 0.3142797023057938, train acc: 0.9595\n",
      "loss: 0.2747179210186005, train acc: 0.9617\n",
      "loss: 0.2610844671726227, train acc: 0.955\n",
      "loss: 0.2690227061510086, train acc: 0.9611\n",
      "loss: 0.25451142340898514, train acc: 0.9584\n",
      "loss: 0.25103510469198226, train acc: 0.9596\n",
      "loss: 0.228739944845438, train acc: 0.9617\n",
      "epoch: 75, loss: 0.10903476178646088, train acc: 0.9617, test acc: 0.8979\n",
      "loss: 0.2879267632961273, train acc: 0.9583\n",
      "loss: 0.27233823835849763, train acc: 0.9616\n",
      "loss: 0.2881870448589325, train acc: 0.9598\n",
      "loss: 0.26820252388715743, train acc: 0.9566\n",
      "loss: 0.24521021395921708, train acc: 0.9621\n",
      "loss: 0.22388226836919783, train acc: 0.9568\n",
      "loss: 0.2769413322210312, train acc: 0.9589\n",
      "loss: 0.2616808474063873, train acc: 0.961\n",
      "epoch: 76, loss: 0.09353401511907578, train acc: 0.961, test acc: 0.8954\n",
      "loss: 0.23150651156902313, train acc: 0.9601\n",
      "loss: 0.27768675684928895, train acc: 0.9618\n",
      "loss: 0.288509176671505, train acc: 0.9616\n",
      "loss: 0.29129317253828046, train acc: 0.9591\n",
      "loss: 0.2790697544813156, train acc: 0.959\n",
      "loss: 0.2645519196987152, train acc: 0.9588\n",
      "loss: 0.27673106640577316, train acc: 0.9602\n",
      "loss: 0.23812359273433686, train acc: 0.9623\n",
      "epoch: 77, loss: 0.05112377181649208, train acc: 0.9623, test acc: 0.8954\n",
      "loss: 0.2223910540342331, train acc: 0.9602\n",
      "loss: 0.2551905259490013, train acc: 0.9623\n",
      "loss: 0.2760787606239319, train acc: 0.9592\n",
      "loss: 0.30140441060066225, train acc: 0.954\n",
      "loss: 0.26453808695077896, train acc: 0.9619\n",
      "loss: 0.25783964097499845, train acc: 0.9607\n",
      "loss: 0.2757675975561142, train acc: 0.9603\n",
      "loss: 0.2610344454646111, train acc: 0.9618\n",
      "epoch: 78, loss: 0.07919379323720932, train acc: 0.9618, test acc: 0.8955\n",
      "loss: 0.3357052803039551, train acc: 0.962\n",
      "loss: 0.32110683619976044, train acc: 0.9587\n",
      "loss: 0.2762068763375282, train acc: 0.9609\n",
      "loss: 0.2870152711868286, train acc: 0.9596\n",
      "loss: 0.2311024084687233, train acc: 0.9626\n",
      "loss: 0.25341645926237105, train acc: 0.9593\n",
      "loss: 0.2879821866750717, train acc: 0.9612\n",
      "loss: 0.25444107949733735, train acc: 0.9608\n",
      "epoch: 79, loss: 0.07959633320569992, train acc: 0.9608, test acc: 0.8945\n",
      "#####training and testing end with K:10, P:0.1######\n",
      "#####training and testing start with K:10, P:0.5######\n",
      "loss: 2.408479928970337, train acc: 0.0891\n",
      "loss: 2.2900449991226197, train acc: 0.1603\n",
      "loss: 2.1750489234924317, train acc: 0.2359\n",
      "loss: 2.0567576050758363, train acc: 0.3858\n",
      "loss: 1.953438138961792, train acc: 0.5026\n",
      "loss: 1.8610713958740235, train acc: 0.5745\n",
      "loss: 1.7818976044654846, train acc: 0.632\n",
      "loss: 1.6811423897743225, train acc: 0.6793\n",
      "epoch: 0, loss: 1.9134892225265503, train acc: 0.6793, test acc: 0.6902\n",
      "loss: 1.6741985082626343, train acc: 0.7198\n",
      "loss: 1.601801872253418, train acc: 0.7586\n",
      "loss: 1.526848828792572, train acc: 0.7719\n",
      "loss: 1.524153959751129, train acc: 0.8061\n",
      "loss: 1.4553453087806703, train acc: 0.809\n",
      "loss: 1.4448662281036377, train acc: 0.8172\n",
      "loss: 1.4066657662391662, train acc: 0.8293\n",
      "loss: 1.420643961429596, train acc: 0.828\n",
      "epoch: 1, loss: 1.5845814943313599, train acc: 0.828, test acc: 0.8169\n",
      "loss: 1.4316012859344482, train acc: 0.8335\n",
      "loss: 1.4162318348884582, train acc: 0.8395\n",
      "loss: 1.3600671648979188, train acc: 0.8361\n",
      "loss: 1.3851333618164063, train acc: 0.8447\n",
      "loss: 1.3586106777191163, train acc: 0.8489\n",
      "loss: 1.3316990613937378, train acc: 0.8454\n",
      "loss: 1.3252548694610595, train acc: 0.8484\n",
      "loss: 1.3378120422363282, train acc: 0.8527\n",
      "epoch: 2, loss: 1.2827867269515991, train acc: 0.8527, test acc: 0.8406\n",
      "loss: 1.2319419384002686, train acc: 0.8529\n",
      "loss: 1.4058658003807067, train acc: 0.8563\n",
      "loss: 1.3067460775375366, train acc: 0.8494\n",
      "loss: 1.31885826587677, train acc: 0.8545\n",
      "loss: 1.2569477915763856, train acc: 0.8532\n",
      "loss: 1.26498544216156, train acc: 0.8603\n",
      "loss: 1.2889487147331238, train acc: 0.8528\n",
      "loss: 1.2878309726715087, train acc: 0.8582\n",
      "epoch: 3, loss: 1.1236878633499146, train acc: 0.8582, test acc: 0.845\n",
      "loss: 1.1820515394210815, train acc: 0.8605\n",
      "loss: 1.3292339205741883, train acc: 0.8624\n",
      "loss: 1.2493899583816528, train acc: 0.8557\n",
      "loss: 1.2494894504547118, train acc: 0.8662\n",
      "loss: 1.2617569804191588, train acc: 0.8684\n",
      "loss: 1.248349702358246, train acc: 0.8664\n",
      "loss: 1.2328197240829468, train acc: 0.8669\n",
      "loss: 1.2306357979774476, train acc: 0.8659\n",
      "epoch: 4, loss: 1.2323229312896729, train acc: 0.8659, test acc: 0.8502\n",
      "loss: 1.2948459386825562, train acc: 0.8669\n",
      "loss: 1.2857332348823547, train acc: 0.8679\n",
      "loss: 1.2045702934265137, train acc: 0.8623\n",
      "loss: 1.2440564632415771, train acc: 0.8696\n",
      "loss: 1.2092135190963744, train acc: 0.8662\n",
      "loss: 1.2217860102653504, train acc: 0.8654\n",
      "loss: 1.2630515694618225, train acc: 0.8628\n",
      "loss: 1.2253133058547974, train acc: 0.8674\n",
      "epoch: 5, loss: 1.0871124267578125, train acc: 0.8674, test acc: 0.8427\n",
      "loss: 1.275282621383667, train acc: 0.8642\n",
      "loss: 1.2355462431907653, train acc: 0.8712\n",
      "loss: 1.244765317440033, train acc: 0.8717\n",
      "loss: 1.232319176197052, train acc: 0.8717\n",
      "loss: 1.2512561559677124, train acc: 0.8689\n",
      "loss: 1.2141840577125549, train acc: 0.8698\n",
      "loss: 1.2048694491386414, train acc: 0.867\n",
      "loss: 1.1893041133880615, train acc: 0.8713\n",
      "epoch: 6, loss: 1.3419296741485596, train acc: 0.8713, test acc: 0.8495\n",
      "loss: 1.2856651544570923, train acc: 0.8708\n",
      "loss: 1.304316556453705, train acc: 0.8678\n",
      "loss: 1.1901530504226685, train acc: 0.8704\n",
      "loss: 1.2577236652374268, train acc: 0.8731\n",
      "loss: 1.2107656955718995, train acc: 0.8713\n",
      "loss: 1.2189867496490479, train acc: 0.8712\n",
      "loss: 1.1889871954917908, train acc: 0.8718\n",
      "loss: 1.1989077925682068, train acc: 0.8738\n",
      "epoch: 7, loss: 1.232425332069397, train acc: 0.8738, test acc: 0.8553\n",
      "loss: 1.099172830581665, train acc: 0.8744\n",
      "loss: 1.2734655857086181, train acc: 0.8749\n",
      "loss: 1.1989941358566285, train acc: 0.8756\n",
      "loss: 1.2272449374198913, train acc: 0.8781\n",
      "loss: 1.2030956387519836, train acc: 0.877\n",
      "loss: 1.1954094290733337, train acc: 0.8752\n",
      "loss: 1.1945690393447876, train acc: 0.8722\n",
      "loss: 1.1432130575180053, train acc: 0.8719\n",
      "epoch: 8, loss: 0.8248763680458069, train acc: 0.8719, test acc: 0.8513\n",
      "loss: 1.205572485923767, train acc: 0.8759\n",
      "loss: 1.2199529528617858, train acc: 0.8796\n",
      "loss: 1.227229940891266, train acc: 0.8848\n",
      "loss: 1.2291343569755555, train acc: 0.8814\n",
      "loss: 1.1801087260246277, train acc: 0.8822\n",
      "loss: 1.1490275979042053, train acc: 0.8788\n",
      "loss: 1.1973161697387695, train acc: 0.8781\n",
      "loss: 1.1588033080101012, train acc: 0.8794\n",
      "epoch: 9, loss: 1.0167970657348633, train acc: 0.8794, test acc: 0.8516\n",
      "loss: 1.2583800554275513, train acc: 0.8794\n",
      "loss: 1.252447748184204, train acc: 0.8827\n",
      "loss: 1.1685524106025695, train acc: 0.8758\n",
      "loss: 1.1266751050949098, train acc: 0.8826\n",
      "loss: 1.1970932245254517, train acc: 0.8812\n",
      "loss: 1.1471823453903198, train acc: 0.8809\n",
      "loss: 1.1475845932960511, train acc: 0.8756\n",
      "loss: 1.1467223465442657, train acc: 0.8835\n",
      "epoch: 10, loss: 1.0146979093551636, train acc: 0.8835, test acc: 0.8589\n",
      "loss: 1.0916489362716675, train acc: 0.8851\n",
      "loss: 1.2698973774909974, train acc: 0.8863\n",
      "loss: 1.1587373852729796, train acc: 0.878\n",
      "loss: 1.1587270140647887, train acc: 0.8803\n",
      "loss: 1.175579833984375, train acc: 0.883\n",
      "loss: 1.1258901238441468, train acc: 0.8816\n",
      "loss: 1.178022450208664, train acc: 0.8808\n",
      "loss: 1.1294950008392335, train acc: 0.8802\n",
      "epoch: 11, loss: 0.7134748101234436, train acc: 0.8802, test acc: 0.8571\n",
      "loss: 1.2215641736984253, train acc: 0.8855\n",
      "loss: 1.2553179025650025, train acc: 0.8846\n",
      "loss: 1.1255511164665222, train acc: 0.8851\n",
      "loss: 1.1889352560043336, train acc: 0.8858\n",
      "loss: 1.1427860975265502, train acc: 0.883\n",
      "loss: 1.1590492367744445, train acc: 0.8856\n",
      "loss: 1.117227041721344, train acc: 0.8842\n",
      "loss: 1.122944527864456, train acc: 0.8869\n",
      "epoch: 12, loss: 1.5794925689697266, train acc: 0.8869, test acc: 0.8565\n",
      "loss: 1.2124392986297607, train acc: 0.8855\n",
      "loss: 1.1971220135688783, train acc: 0.8881\n",
      "loss: 1.1336989641189574, train acc: 0.8855\n",
      "loss: 1.151141619682312, train acc: 0.8878\n",
      "loss: 1.1698994755744934, train acc: 0.8855\n",
      "loss: 1.1533436059951783, train acc: 0.8821\n",
      "loss: 1.1044883489608766, train acc: 0.8877\n",
      "loss: 1.139357143640518, train acc: 0.8881\n",
      "epoch: 13, loss: 1.1089985370635986, train acc: 0.8881, test acc: 0.8618\n",
      "loss: 1.0832297801971436, train acc: 0.8908\n",
      "loss: 1.200204563140869, train acc: 0.8917\n",
      "loss: 1.1370084047317506, train acc: 0.8897\n",
      "loss: 1.1673289716243744, train acc: 0.888\n",
      "loss: 1.144980102777481, train acc: 0.8872\n",
      "loss: 1.1521403908729553, train acc: 0.8869\n",
      "loss: 1.1382523000240325, train acc: 0.8873\n",
      "loss: 1.145671546459198, train acc: 0.8889\n",
      "epoch: 14, loss: 0.9549387097358704, train acc: 0.8889, test acc: 0.8646\n",
      "loss: 1.1362569332122803, train acc: 0.8926\n",
      "loss: 1.1930564641952515, train acc: 0.8926\n",
      "loss: 1.128501534461975, train acc: 0.8911\n",
      "loss: 1.1389530777931214, train acc: 0.8914\n",
      "loss: 1.1619647026062012, train acc: 0.8894\n",
      "loss: 1.1009003281593324, train acc: 0.8869\n",
      "loss: 1.150943386554718, train acc: 0.8881\n",
      "loss: 1.085036152601242, train acc: 0.8918\n",
      "epoch: 15, loss: 1.0063982009887695, train acc: 0.8918, test acc: 0.8624\n",
      "loss: 1.3533395528793335, train acc: 0.8932\n",
      "loss: 1.1779798865318298, train acc: 0.8945\n",
      "loss: 1.138802707195282, train acc: 0.8921\n",
      "loss: 1.127213042974472, train acc: 0.8896\n",
      "loss: 1.1666994214057922, train acc: 0.8905\n",
      "loss: 1.0699012517929076, train acc: 0.8889\n",
      "loss: 1.0920465230941772, train acc: 0.8894\n",
      "loss: 1.0738668262958526, train acc: 0.8938\n",
      "epoch: 16, loss: 1.0207946300506592, train acc: 0.8938, test acc: 0.8597\n",
      "loss: 1.2912112474441528, train acc: 0.8927\n",
      "loss: 1.1877007603645324, train acc: 0.8949\n",
      "loss: 1.0816318929195403, train acc: 0.894\n",
      "loss: 1.1565094351768495, train acc: 0.8938\n",
      "loss: 1.1163247346878051, train acc: 0.8952\n",
      "loss: 1.10201455950737, train acc: 0.892\n",
      "loss: 1.091105204820633, train acc: 0.8922\n",
      "loss: 1.0543700873851776, train acc: 0.8943\n",
      "epoch: 17, loss: 0.8258228302001953, train acc: 0.8943, test acc: 0.8659\n",
      "loss: 1.1524815559387207, train acc: 0.8962\n",
      "loss: 1.1535065412521361, train acc: 0.8948\n",
      "loss: 1.1060986638069152, train acc: 0.8969\n",
      "loss: 1.151909863948822, train acc: 0.8949\n",
      "loss: 1.1360215902328492, train acc: 0.8929\n",
      "loss: 1.0927746295928955, train acc: 0.8923\n",
      "loss: 1.0896370828151702, train acc: 0.8944\n",
      "loss: 1.1125506997108459, train acc: 0.8938\n",
      "epoch: 18, loss: 1.1884949207305908, train acc: 0.8938, test acc: 0.8624\n",
      "loss: 1.17137610912323, train acc: 0.8951\n",
      "loss: 1.1918787360191345, train acc: 0.8941\n",
      "loss: 1.105213987827301, train acc: 0.8948\n",
      "loss: 1.1346609175205231, train acc: 0.8943\n",
      "loss: 1.1093703091144562, train acc: 0.8979\n",
      "loss: 1.0791723370552062, train acc: 0.8946\n",
      "loss: 1.080110001564026, train acc: 0.8948\n",
      "loss: 1.0696701407432556, train acc: 0.8951\n",
      "epoch: 19, loss: 1.1503804922103882, train acc: 0.8951, test acc: 0.8634\n",
      "loss: 1.047959327697754, train acc: 0.898\n",
      "loss: 1.1685893774032592, train acc: 0.8988\n",
      "loss: 1.0910650074481965, train acc: 0.8973\n",
      "loss: 1.0775374472141266, train acc: 0.8967\n",
      "loss: 1.0990091919898988, train acc: 0.8968\n",
      "loss: 1.0703530728816986, train acc: 0.8948\n",
      "loss: 1.1037696123123169, train acc: 0.8964\n",
      "loss: 1.1155920326709747, train acc: 0.8998\n",
      "epoch: 20, loss: 0.729628324508667, train acc: 0.8998, test acc: 0.8638\n",
      "loss: 1.179762363433838, train acc: 0.8972\n",
      "loss: 1.2474278092384339, train acc: 0.8995\n",
      "loss: 1.097102415561676, train acc: 0.8971\n",
      "loss: 1.1306708693504333, train acc: 0.8997\n",
      "loss: 1.077730792760849, train acc: 0.8993\n",
      "loss: 1.077723205089569, train acc: 0.898\n",
      "loss: 1.125519049167633, train acc: 0.9025\n",
      "loss: 1.1311115503311158, train acc: 0.9012\n",
      "epoch: 21, loss: 1.0842701196670532, train acc: 0.9012, test acc: 0.8648\n",
      "loss: 1.0005742311477661, train acc: 0.9\n",
      "loss: 1.1383354425430299, train acc: 0.8993\n",
      "loss: 1.0673052310943603, train acc: 0.9016\n",
      "loss: 1.1148881375789643, train acc: 0.9014\n",
      "loss: 1.1393407225608825, train acc: 0.8953\n",
      "loss: 1.0783878684043884, train acc: 0.901\n",
      "loss: 1.111191201210022, train acc: 0.9025\n",
      "loss: 1.0641593277454375, train acc: 0.9029\n",
      "epoch: 22, loss: 1.124174952507019, train acc: 0.9029, test acc: 0.8682\n",
      "loss: 1.0804119110107422, train acc: 0.9041\n",
      "loss: 1.120063066482544, train acc: 0.9001\n",
      "loss: 1.0814484119415284, train acc: 0.9019\n",
      "loss: 1.063924926519394, train acc: 0.9004\n",
      "loss: 1.075090718269348, train acc: 0.9021\n",
      "loss: 1.057869118452072, train acc: 0.899\n",
      "loss: 1.1012775123119354, train acc: 0.9024\n",
      "loss: 1.0576635956764222, train acc: 0.9056\n",
      "epoch: 23, loss: 0.927139937877655, train acc: 0.9056, test acc: 0.8688\n",
      "loss: 1.0856714248657227, train acc: 0.9051\n",
      "loss: 1.149676513671875, train acc: 0.9025\n",
      "loss: 1.0645778715610503, train acc: 0.9021\n",
      "loss: 1.125495308637619, train acc: 0.9035\n",
      "loss: 1.1302037119865418, train acc: 0.9015\n",
      "loss: 1.1116949796676636, train acc: 0.9054\n",
      "loss: 1.1034528493881226, train acc: 0.9032\n",
      "loss: 1.11859387755394, train acc: 0.8982\n",
      "epoch: 24, loss: 1.1779011487960815, train acc: 0.8982, test acc: 0.8642\n",
      "loss: 1.0228900909423828, train acc: 0.9008\n",
      "loss: 1.1267712116241455, train acc: 0.904\n",
      "loss: 1.111303973197937, train acc: 0.8983\n",
      "loss: 1.1260441541671753, train acc: 0.9003\n",
      "loss: 1.0681291341781616, train acc: 0.9015\n",
      "loss: 1.0999189794063569, train acc: 0.901\n",
      "loss: 1.104034560918808, train acc: 0.9048\n",
      "loss: 1.0770609080791473, train acc: 0.9056\n",
      "epoch: 25, loss: 1.412888526916504, train acc: 0.9056, test acc: 0.8659\n",
      "loss: 1.0140862464904785, train acc: 0.9039\n",
      "loss: 1.1724924921989441, train acc: 0.9037\n",
      "loss: 1.0961520433425904, train acc: 0.9034\n",
      "loss: 1.1041154503822326, train acc: 0.9035\n",
      "loss: 1.1030468463897705, train acc: 0.902\n",
      "loss: 1.0833635926246643, train acc: 0.9053\n",
      "loss: 1.0473730742931366, train acc: 0.9056\n",
      "loss: 1.1071823358535766, train acc: 0.9075\n",
      "epoch: 26, loss: 1.4907183647155762, train acc: 0.9075, test acc: 0.8695\n",
      "loss: 1.051339030265808, train acc: 0.9028\n",
      "loss: 1.1144027590751648, train acc: 0.9055\n",
      "loss: 1.060196876525879, train acc: 0.9047\n",
      "loss: 1.0751063346862793, train acc: 0.9057\n",
      "loss: 1.0806715846061707, train acc: 0.905\n",
      "loss: 1.0698000490665436, train acc: 0.9049\n",
      "loss: 1.0843181371688844, train acc: 0.9055\n",
      "loss: 1.081535428762436, train acc: 0.9064\n",
      "epoch: 27, loss: 0.9146060943603516, train acc: 0.9064, test acc: 0.8678\n",
      "loss: 1.1576987504959106, train acc: 0.9041\n",
      "loss: 1.1524558544158936, train acc: 0.9032\n",
      "loss: 1.084235918521881, train acc: 0.9032\n",
      "loss: 1.1067759931087493, train acc: 0.8995\n",
      "loss: 1.1126042127609252, train acc: 0.9016\n",
      "loss: 1.0206238150596618, train acc: 0.9054\n",
      "loss: 1.0831319272518158, train acc: 0.9062\n",
      "loss: 1.0605641603469849, train acc: 0.9083\n",
      "epoch: 28, loss: 0.6663618087768555, train acc: 0.9083, test acc: 0.8658\n",
      "loss: 1.2246183156967163, train acc: 0.9023\n",
      "loss: 1.191389000415802, train acc: 0.9067\n",
      "loss: 1.0866078019142151, train acc: 0.9045\n",
      "loss: 1.086513352394104, train acc: 0.9085\n",
      "loss: 1.0928755044937133, train acc: 0.9083\n",
      "loss: 1.0742631196975707, train acc: 0.9086\n",
      "loss: 1.0552947580814362, train acc: 0.9081\n",
      "loss: 1.116013503074646, train acc: 0.9069\n",
      "epoch: 29, loss: 1.3098262548446655, train acc: 0.9069, test acc: 0.8645\n",
      "loss: 1.166907548904419, train acc: 0.9047\n",
      "loss: 1.1149747312068938, train acc: 0.9065\n",
      "loss: 1.0780321776866912, train acc: 0.907\n",
      "loss: 1.0755431830883027, train acc: 0.9059\n",
      "loss: 1.0808324337005615, train acc: 0.9043\n",
      "loss: 1.045980805158615, train acc: 0.9088\n",
      "loss: 1.048793339729309, train acc: 0.9094\n",
      "loss: 1.0656086444854735, train acc: 0.9064\n",
      "epoch: 30, loss: 1.07701575756073, train acc: 0.9064, test acc: 0.8715\n",
      "loss: 1.05089271068573, train acc: 0.9111\n",
      "loss: 1.1389111161231995, train acc: 0.9103\n",
      "loss: 1.0674887657165528, train acc: 0.9088\n",
      "loss: 1.0796535193920136, train acc: 0.9114\n",
      "loss: 1.064581698179245, train acc: 0.907\n",
      "loss: 1.0487951993942262, train acc: 0.9087\n",
      "loss: 1.0853567123413086, train acc: 0.9099\n",
      "loss: 1.0490515112876893, train acc: 0.9078\n",
      "epoch: 31, loss: 0.9045209288597107, train acc: 0.9078, test acc: 0.8704\n",
      "loss: 0.9656515121459961, train acc: 0.908\n",
      "loss: 1.1613999128341674, train acc: 0.9097\n",
      "loss: 1.1205995798110961, train acc: 0.909\n",
      "loss: 1.1360854148864745, train acc: 0.9084\n",
      "loss: 1.0685517251491548, train acc: 0.9079\n",
      "loss: 1.046954870223999, train acc: 0.906\n",
      "loss: 1.0645238876342773, train acc: 0.9082\n",
      "loss: 1.0487550675868988, train acc: 0.9091\n",
      "epoch: 32, loss: 0.8713819980621338, train acc: 0.9091, test acc: 0.8692\n",
      "loss: 1.1572774648666382, train acc: 0.9079\n",
      "loss: 1.147251045703888, train acc: 0.901\n",
      "loss: 1.1034726619720459, train acc: 0.9027\n",
      "loss: 1.0975562810897828, train acc: 0.9047\n",
      "loss: 1.100627785921097, train acc: 0.9054\n",
      "loss: 1.0565085351467132, train acc: 0.9085\n",
      "loss: 1.038274222612381, train acc: 0.909\n",
      "loss: 1.0578085958957673, train acc: 0.9114\n",
      "epoch: 33, loss: 0.7533597946166992, train acc: 0.9114, test acc: 0.8706\n",
      "loss: 1.1707496643066406, train acc: 0.9085\n",
      "loss: 1.1413536608219146, train acc: 0.911\n",
      "loss: 1.0474001169204712, train acc: 0.9082\n",
      "loss: 1.0796165585517883, train acc: 0.9083\n",
      "loss: 1.0893618047237397, train acc: 0.9105\n",
      "loss: 1.010595041513443, train acc: 0.9096\n",
      "loss: 1.052192896604538, train acc: 0.9095\n",
      "loss: 1.034220176935196, train acc: 0.9123\n",
      "epoch: 34, loss: 0.8587207794189453, train acc: 0.9123, test acc: 0.8708\n",
      "loss: 1.0957571268081665, train acc: 0.9111\n",
      "loss: 1.1061165034770966, train acc: 0.9137\n",
      "loss: 1.0749323487281799, train acc: 0.9111\n",
      "loss: 1.0727956414222717, train acc: 0.9108\n",
      "loss: 1.06671262383461, train acc: 0.9091\n",
      "loss: 1.1049317598342896, train acc: 0.914\n",
      "loss: 1.0408820807933807, train acc: 0.9134\n",
      "loss: 1.1082413494586945, train acc: 0.9101\n",
      "epoch: 35, loss: 1.0217679738998413, train acc: 0.9101, test acc: 0.8712\n",
      "loss: 0.9828570485115051, train acc: 0.9119\n",
      "loss: 1.0841399312019349, train acc: 0.9117\n",
      "loss: 1.0544401943683623, train acc: 0.9126\n",
      "loss: 1.0799328625202178, train acc: 0.9106\n",
      "loss: 1.0865220785140992, train acc: 0.9077\n",
      "loss: 1.0357806444168092, train acc: 0.911\n",
      "loss: 1.0662623286247253, train acc: 0.9112\n",
      "loss: 1.0540244698524475, train acc: 0.9122\n",
      "epoch: 36, loss: 1.0491029024124146, train acc: 0.9122, test acc: 0.8681\n",
      "loss: 1.22433340549469, train acc: 0.9101\n",
      "loss: 1.0956465125083923, train acc: 0.9106\n",
      "loss: 1.045763510465622, train acc: 0.9079\n",
      "loss: 1.0595372557640075, train acc: 0.9097\n",
      "loss: 1.086554181575775, train acc: 0.9062\n",
      "loss: 1.098340678215027, train acc: 0.9128\n",
      "loss: 1.0478144764900208, train acc: 0.9088\n",
      "loss: 1.041030263900757, train acc: 0.9131\n",
      "epoch: 37, loss: 0.9118046164512634, train acc: 0.9131, test acc: 0.867\n",
      "loss: 1.0262904167175293, train acc: 0.9093\n",
      "loss: 1.1220613479614259, train acc: 0.914\n",
      "loss: 1.031160169839859, train acc: 0.91\n",
      "loss: 1.10438591837883, train acc: 0.9126\n",
      "loss: 1.0391975939273834, train acc: 0.9124\n",
      "loss: 1.0447325110435486, train acc: 0.9117\n",
      "loss: 1.0863665878772735, train acc: 0.9113\n",
      "loss: 1.0885912835597993, train acc: 0.9127\n",
      "epoch: 38, loss: 0.9538459181785583, train acc: 0.9127, test acc: 0.8712\n",
      "loss: 1.166441798210144, train acc: 0.9127\n",
      "loss: 1.1639845371246338, train acc: 0.9084\n",
      "loss: 1.051343560218811, train acc: 0.9124\n",
      "loss: 1.1046933770179748, train acc: 0.9088\n",
      "loss: 1.0883947730064392, train acc: 0.9085\n",
      "loss: 1.0827088534832001, train acc: 0.9126\n",
      "loss: 1.0752760767936707, train acc: 0.9105\n",
      "loss: 1.0360620379447938, train acc: 0.9136\n",
      "epoch: 39, loss: 1.054384469985962, train acc: 0.9136, test acc: 0.8682\n",
      "loss: 1.0047484636306763, train acc: 0.9119\n",
      "loss: 1.1048053920269012, train acc: 0.9111\n",
      "loss: 1.0817587018013, train acc: 0.9121\n",
      "loss: 1.0638318955898285, train acc: 0.9075\n",
      "loss: 1.0085948407649994, train acc: 0.9105\n",
      "loss: 1.0253574669361114, train acc: 0.9133\n",
      "loss: 1.0452527463436128, train acc: 0.9148\n",
      "loss: 1.0641345918178557, train acc: 0.9142\n",
      "epoch: 40, loss: 0.9656335711479187, train acc: 0.9142, test acc: 0.8676\n",
      "loss: 1.1156628131866455, train acc: 0.909\n",
      "loss: 1.084570550918579, train acc: 0.9129\n",
      "loss: 1.0645346462726593, train acc: 0.9101\n",
      "loss: 1.0730798602104188, train acc: 0.9136\n",
      "loss: 1.0314749002456665, train acc: 0.9116\n",
      "loss: 1.0264612793922425, train acc: 0.9149\n",
      "loss: 1.030999779701233, train acc: 0.9135\n",
      "loss: 1.1030633211135865, train acc: 0.9138\n",
      "epoch: 41, loss: 1.2943034172058105, train acc: 0.9138, test acc: 0.8725\n",
      "loss: 1.0168840885162354, train acc: 0.9136\n",
      "loss: 1.1068760395050048, train acc: 0.9142\n",
      "loss: 1.0397070944309235, train acc: 0.9106\n",
      "loss: 1.0739644169807434, train acc: 0.914\n",
      "loss: 1.0594810485839843, train acc: 0.9144\n",
      "loss: 1.028303176164627, train acc: 0.9124\n",
      "loss: 1.0754452168941497, train acc: 0.9149\n",
      "loss: 1.05859477519989, train acc: 0.9153\n",
      "epoch: 42, loss: 0.7487859129905701, train acc: 0.9153, test acc: 0.8716\n",
      "loss: 1.1887873411178589, train acc: 0.9164\n",
      "loss: 1.1117962956428529, train acc: 0.9156\n",
      "loss: 1.0563735008239745, train acc: 0.9122\n",
      "loss: 1.0897248208522796, train acc: 0.9114\n",
      "loss: 1.0238089919090272, train acc: 0.9095\n",
      "loss: 1.0255137920379638, train acc: 0.9126\n",
      "loss: 1.0771194040775298, train acc: 0.9142\n",
      "loss: 1.0514283061027527, train acc: 0.9154\n",
      "epoch: 43, loss: 1.138161063194275, train acc: 0.9154, test acc: 0.8727\n",
      "loss: 1.1206018924713135, train acc: 0.9152\n",
      "loss: 1.1518710613250733, train acc: 0.9134\n",
      "loss: 1.0584239780902862, train acc: 0.9123\n",
      "loss: 1.078616201877594, train acc: 0.9128\n",
      "loss: 1.0465509355068208, train acc: 0.9106\n",
      "loss: 0.9993540525436402, train acc: 0.9159\n",
      "loss: 1.0981437504291534, train acc: 0.9121\n",
      "loss: 1.062588483095169, train acc: 0.9136\n",
      "epoch: 44, loss: 0.7256494760513306, train acc: 0.9136, test acc: 0.8741\n",
      "loss: 1.0046387910842896, train acc: 0.9167\n",
      "loss: 1.0941527009010314, train acc: 0.9138\n",
      "loss: 1.0352068543434143, train acc: 0.917\n",
      "loss: 1.1008021235466003, train acc: 0.9176\n",
      "loss: 1.0524127721786498, train acc: 0.9137\n",
      "loss: 1.01987766623497, train acc: 0.916\n",
      "loss: 1.047180724143982, train acc: 0.9153\n",
      "loss: 1.0608420193195343, train acc: 0.9125\n",
      "epoch: 45, loss: 0.5511782765388489, train acc: 0.9125, test acc: 0.871\n",
      "loss: 1.0280297994613647, train acc: 0.9162\n",
      "loss: 1.089118629693985, train acc: 0.9149\n",
      "loss: 1.090002751350403, train acc: 0.9145\n",
      "loss: 1.0759941816329956, train acc: 0.9126\n",
      "loss: 1.003907698392868, train acc: 0.9163\n",
      "loss: 1.0293288946151733, train acc: 0.9139\n",
      "loss: 1.0449354469776153, train acc: 0.9135\n",
      "loss: 1.0557121455669403, train acc: 0.9164\n",
      "epoch: 46, loss: 0.8835035562515259, train acc: 0.9164, test acc: 0.8667\n",
      "loss: 0.9427844882011414, train acc: 0.9129\n",
      "loss: 1.1023154079914093, train acc: 0.9178\n",
      "loss: 1.0516900777816773, train acc: 0.9142\n",
      "loss: 1.0457481563091278, train acc: 0.9168\n",
      "loss: 1.0256402730941772, train acc: 0.9134\n",
      "loss: 1.0439419984817504, train acc: 0.915\n",
      "loss: 1.0396770656108856, train acc: 0.9141\n",
      "loss: 1.0247312366962433, train acc: 0.9149\n",
      "epoch: 47, loss: 0.9419088959693909, train acc: 0.9149, test acc: 0.8755\n",
      "loss: 1.0545510053634644, train acc: 0.9195\n",
      "loss: 1.127018976211548, train acc: 0.9139\n",
      "loss: 1.1363759100437165, train acc: 0.9154\n",
      "loss: 1.0542202591896057, train acc: 0.9159\n",
      "loss: 1.0495209693908691, train acc: 0.9169\n",
      "loss: 1.0699515521526337, train acc: 0.9189\n",
      "loss: 1.0299901247024537, train acc: 0.916\n",
      "loss: 1.086785465478897, train acc: 0.9162\n",
      "epoch: 48, loss: 0.603291928768158, train acc: 0.9162, test acc: 0.8725\n",
      "loss: 1.0243489742279053, train acc: 0.9154\n",
      "loss: 1.0995518088340759, train acc: 0.9134\n",
      "loss: 1.0317229449748992, train acc: 0.9155\n",
      "loss: 1.0611899077892304, train acc: 0.9126\n",
      "loss: 1.0908748626708984, train acc: 0.9111\n",
      "loss: 1.0120908737182617, train acc: 0.9168\n",
      "loss: 1.0370424926280974, train acc: 0.9162\n",
      "loss: 1.077385950088501, train acc: 0.9172\n",
      "epoch: 49, loss: 0.8742114305496216, train acc: 0.9172, test acc: 0.8699\n",
      "loss: 1.1228095293045044, train acc: 0.9124\n",
      "loss: 1.0963061094284057, train acc: 0.9193\n",
      "loss: 1.0032846927642822, train acc: 0.915\n",
      "loss: 1.04790318608284, train acc: 0.915\n",
      "loss: 1.0409083604812621, train acc: 0.913\n",
      "loss: 1.040893542766571, train acc: 0.916\n",
      "loss: 1.0700850009918212, train acc: 0.9173\n",
      "loss: 0.974656480550766, train acc: 0.9168\n",
      "epoch: 50, loss: 0.820968508720398, train acc: 0.9168, test acc: 0.8731\n",
      "loss: 1.0245859622955322, train acc: 0.918\n",
      "loss: 1.1026614367961884, train acc: 0.9123\n",
      "loss: 1.0627201676368714, train acc: 0.9132\n",
      "loss: 1.102159023284912, train acc: 0.9138\n",
      "loss: 1.0596664309501649, train acc: 0.9102\n",
      "loss: 1.018420046567917, train acc: 0.9142\n",
      "loss: 1.027439796924591, train acc: 0.9145\n",
      "loss: 1.02243230342865, train acc: 0.9138\n",
      "epoch: 51, loss: 1.170620322227478, train acc: 0.9138, test acc: 0.8696\n",
      "loss: 1.0104326009750366, train acc: 0.9158\n",
      "loss: 1.0420848906040192, train acc: 0.9129\n",
      "loss: 1.0316514730453492, train acc: 0.9173\n",
      "loss: 1.0522858381271363, train acc: 0.9142\n",
      "loss: 1.0253297626972198, train acc: 0.9165\n",
      "loss: 1.0319768071174622, train acc: 0.9182\n",
      "loss: 1.0686034977436065, train acc: 0.9174\n",
      "loss: 1.0103223860263824, train acc: 0.9176\n",
      "epoch: 52, loss: 0.9708954691886902, train acc: 0.9176, test acc: 0.8751\n",
      "loss: 1.2140735387802124, train acc: 0.9185\n",
      "loss: 1.1129213988780975, train acc: 0.9133\n",
      "loss: 1.0048610150814057, train acc: 0.9167\n",
      "loss: 1.032961016893387, train acc: 0.9177\n",
      "loss: 1.0345059394836427, train acc: 0.9182\n",
      "loss: 1.0650226771831512, train acc: 0.9154\n",
      "loss: 1.0547014951705933, train acc: 0.9128\n",
      "loss: 1.0358379185199738, train acc: 0.9157\n",
      "epoch: 53, loss: 0.7408359050750732, train acc: 0.9157, test acc: 0.8733\n",
      "loss: 0.9024216532707214, train acc: 0.9182\n",
      "loss: 1.0565067946910858, train acc: 0.9198\n",
      "loss: 1.0684346675872802, train acc: 0.9188\n",
      "loss: 1.1119961619377137, train acc: 0.9152\n",
      "loss: 1.0699627637863158, train acc: 0.9158\n",
      "loss: 1.0404889822006225, train acc: 0.9183\n",
      "loss: 1.0278116166591644, train acc: 0.9167\n",
      "loss: 1.0777429044246674, train acc: 0.9202\n",
      "epoch: 54, loss: 0.9548493027687073, train acc: 0.9202, test acc: 0.8747\n",
      "loss: 1.0937224626541138, train acc: 0.9191\n",
      "loss: 1.1501737117767334, train acc: 0.9201\n",
      "loss: 1.0603291630744933, train acc: 0.9169\n",
      "loss: 1.0852874040603637, train acc: 0.9181\n",
      "loss: 0.9900922477245331, train acc: 0.9189\n",
      "loss: 1.0606731116771697, train acc: 0.917\n",
      "loss: 1.0679090976715089, train acc: 0.9159\n",
      "loss: 1.1010398983955383, train acc: 0.9163\n",
      "epoch: 55, loss: 1.0439308881759644, train acc: 0.9163, test acc: 0.8727\n",
      "loss: 1.0532561540603638, train acc: 0.9177\n",
      "loss: 1.0665904700756073, train acc: 0.9162\n",
      "loss: 1.0412949860095977, train acc: 0.917\n",
      "loss: 1.0901491940021515, train acc: 0.9143\n",
      "loss: 1.0766179502010345, train acc: 0.9162\n",
      "loss: 1.001441442966461, train acc: 0.918\n",
      "loss: 1.0506868958473206, train acc: 0.9148\n",
      "loss: 1.041008287668228, train acc: 0.9198\n",
      "epoch: 56, loss: 0.8071518540382385, train acc: 0.9198, test acc: 0.8685\n",
      "loss: 1.0637911558151245, train acc: 0.9145\n",
      "loss: 1.089182847738266, train acc: 0.9182\n",
      "loss: 1.022946572303772, train acc: 0.9178\n",
      "loss: 1.060288143157959, train acc: 0.9182\n",
      "loss: 1.0660995662212371, train acc: 0.9163\n",
      "loss: 1.0074630558490754, train acc: 0.9179\n",
      "loss: 1.05066819190979, train acc: 0.9209\n",
      "loss: 1.0457092702388764, train acc: 0.9195\n",
      "epoch: 57, loss: 0.7748081088066101, train acc: 0.9195, test acc: 0.8731\n",
      "loss: 1.037509560585022, train acc: 0.9199\n",
      "loss: 1.064715975522995, train acc: 0.921\n",
      "loss: 1.0149071097373963, train acc: 0.9193\n",
      "loss: 1.0728106915950775, train acc: 0.9166\n",
      "loss: 1.07749103307724, train acc: 0.913\n",
      "loss: 0.9924300074577331, train acc: 0.921\n",
      "loss: 1.025210827589035, train acc: 0.9189\n",
      "loss: 1.0149190068244933, train acc: 0.9208\n",
      "epoch: 58, loss: 0.9043427109718323, train acc: 0.9208, test acc: 0.8721\n",
      "loss: 1.0818899869918823, train acc: 0.9184\n",
      "loss: 1.0761358499526978, train acc: 0.9181\n",
      "loss: 1.057014960050583, train acc: 0.9199\n",
      "loss: 1.108664482831955, train acc: 0.9185\n",
      "loss: 1.0596388697624206, train acc: 0.9184\n",
      "loss: 0.9970764100551606, train acc: 0.9147\n",
      "loss: 1.0841182112693786, train acc: 0.9175\n",
      "loss: 1.0453203320503235, train acc: 0.9196\n",
      "epoch: 59, loss: 0.9508348107337952, train acc: 0.9196, test acc: 0.8721\n",
      "loss: 0.955189049243927, train acc: 0.9181\n",
      "loss: 1.0789731919765473, train acc: 0.9195\n",
      "loss: 1.0017292022705078, train acc: 0.9164\n",
      "loss: 1.0512123644351958, train acc: 0.9178\n",
      "loss: 1.0348843812942505, train acc: 0.9178\n",
      "loss: 1.0070279657840728, train acc: 0.9189\n",
      "loss: 1.0294422924518585, train acc: 0.9183\n",
      "loss: 1.0332012176513672, train acc: 0.9163\n",
      "epoch: 60, loss: 0.650078296661377, train acc: 0.9163, test acc: 0.8711\n",
      "loss: 1.0074725151062012, train acc: 0.9175\n",
      "loss: 1.079620099067688, train acc: 0.9188\n",
      "loss: 1.0514589548110962, train acc: 0.9183\n",
      "loss: 1.0641954779624938, train acc: 0.9187\n",
      "loss: 1.0591077148914336, train acc: 0.9185\n",
      "loss: 0.9858254373073578, train acc: 0.9177\n",
      "loss: 0.987211960554123, train acc: 0.9164\n",
      "loss: 1.049274456501007, train acc: 0.9183\n",
      "epoch: 61, loss: 1.2051340341567993, train acc: 0.9183, test acc: 0.8725\n",
      "loss: 0.9059914946556091, train acc: 0.921\n",
      "loss: 1.0428282737731933, train acc: 0.9173\n",
      "loss: 1.029512196779251, train acc: 0.9174\n",
      "loss: 1.0464702606201173, train acc: 0.9187\n",
      "loss: 1.008508425951004, train acc: 0.9204\n",
      "loss: 0.9950523436069488, train acc: 0.9181\n",
      "loss: 1.0345679819583893, train acc: 0.9209\n",
      "loss: 1.0620936036109925, train acc: 0.9183\n",
      "epoch: 62, loss: 0.6079249382019043, train acc: 0.9183, test acc: 0.8748\n",
      "loss: 1.018062710762024, train acc: 0.9186\n",
      "loss: 1.0414301872253418, train acc: 0.9178\n",
      "loss: 1.032816517353058, train acc: 0.9178\n",
      "loss: 1.0597397089004517, train acc: 0.9175\n",
      "loss: 1.0143420338630675, train acc: 0.9184\n",
      "loss: 1.0084969639778136, train acc: 0.9167\n",
      "loss: 1.0344070792198181, train acc: 0.9182\n",
      "loss: 1.054269152879715, train acc: 0.9216\n",
      "epoch: 63, loss: 0.830516517162323, train acc: 0.9216, test acc: 0.8721\n",
      "loss: 0.966639518737793, train acc: 0.9208\n",
      "loss: 1.0422662913799285, train acc: 0.9201\n",
      "loss: 1.0260005652904511, train acc: 0.9199\n",
      "loss: 1.0580860733985902, train acc: 0.9162\n",
      "loss: 1.0175398945808412, train acc: 0.9196\n",
      "loss: 1.0153037011623383, train acc: 0.9196\n",
      "loss: 1.0304239332675933, train acc: 0.9157\n",
      "loss: 1.0047296285629272, train acc: 0.919\n",
      "epoch: 64, loss: 0.957421600818634, train acc: 0.919, test acc: 0.8714\n",
      "loss: 1.0418788194656372, train acc: 0.9173\n",
      "loss: 1.0732346296310424, train acc: 0.9185\n",
      "loss: 1.0136472046375276, train acc: 0.9177\n",
      "loss: 1.0631454050540925, train acc: 0.9185\n",
      "loss: 1.0379372775554656, train acc: 0.92\n",
      "loss: 1.032389223575592, train acc: 0.9211\n",
      "loss: 1.0474140703678132, train acc: 0.9191\n",
      "loss: 1.0027968466281891, train acc: 0.9211\n",
      "epoch: 65, loss: 0.9852052330970764, train acc: 0.9211, test acc: 0.8741\n",
      "loss: 1.0905427932739258, train acc: 0.9202\n",
      "loss: 1.1082218289375305, train acc: 0.9208\n",
      "loss: 0.9807786345481873, train acc: 0.9208\n",
      "loss: 1.0269187092781067, train acc: 0.9183\n",
      "loss: 1.0255585432052612, train acc: 0.9178\n",
      "loss: 1.0269910275936127, train acc: 0.9189\n",
      "loss: 1.0664794921875, train acc: 0.9222\n",
      "loss: 1.0357976853847504, train acc: 0.9213\n",
      "epoch: 66, loss: 1.0926536321640015, train acc: 0.9213, test acc: 0.8758\n",
      "loss: 0.9952952265739441, train acc: 0.9233\n",
      "loss: 1.1039792954921723, train acc: 0.9228\n",
      "loss: 1.0405483543872833, train acc: 0.9235\n",
      "loss: 1.0401241600513458, train acc: 0.9216\n",
      "loss: 1.045274293422699, train acc: 0.9196\n",
      "loss: 1.024367356300354, train acc: 0.9204\n",
      "loss: 1.0377303421497346, train acc: 0.9207\n",
      "loss: 1.059097445011139, train acc: 0.9215\n",
      "epoch: 67, loss: 0.6318566799163818, train acc: 0.9215, test acc: 0.8705\n",
      "loss: 1.1120506525039673, train acc: 0.9194\n",
      "loss: 1.1125729084014893, train acc: 0.9217\n",
      "loss: 1.0278936445713043, train acc: 0.9188\n",
      "loss: 1.0677659153938293, train acc: 0.9184\n",
      "loss: 1.020480704307556, train acc: 0.9178\n",
      "loss: 0.9946200489997864, train acc: 0.9218\n",
      "loss: 1.045064789056778, train acc: 0.9204\n",
      "loss: 1.0404217720031739, train acc: 0.9185\n",
      "epoch: 68, loss: 1.2413347959518433, train acc: 0.9185, test acc: 0.8733\n",
      "loss: 1.1995439529418945, train acc: 0.9206\n",
      "loss: 1.0838871002197266, train acc: 0.9186\n",
      "loss: 1.0646082639694214, train acc: 0.9169\n",
      "loss: 1.0295043587684631, train acc: 0.9185\n",
      "loss: 1.0724296033382417, train acc: 0.9206\n",
      "loss: 1.0172664880752564, train acc: 0.9183\n",
      "loss: 1.0681147038936616, train acc: 0.918\n",
      "loss: 1.037507027387619, train acc: 0.9181\n",
      "epoch: 69, loss: 0.8392629623413086, train acc: 0.9181, test acc: 0.8709\n",
      "loss: 1.0008512735366821, train acc: 0.92\n",
      "loss: 1.0736282229423524, train acc: 0.9207\n",
      "loss: 1.060356342792511, train acc: 0.9203\n",
      "loss: 1.0251191556453705, train acc: 0.9214\n",
      "loss: 1.0600357115268708, train acc: 0.9193\n",
      "loss: 1.0197726905345916, train acc: 0.9228\n",
      "loss: 1.019227397441864, train acc: 0.9221\n",
      "loss: 1.026903074979782, train acc: 0.9233\n",
      "epoch: 70, loss: 1.058318853378296, train acc: 0.9233, test acc: 0.8735\n",
      "loss: 0.9654156565666199, train acc: 0.9215\n",
      "loss: 1.1095875203609467, train acc: 0.9214\n",
      "loss: 1.0290037512779235, train acc: 0.9202\n",
      "loss: 1.0289163827896117, train acc: 0.9209\n",
      "loss: 1.0155994176864624, train acc: 0.9223\n",
      "loss: 1.005331027507782, train acc: 0.9224\n",
      "loss: 1.0550746738910675, train acc: 0.921\n",
      "loss: 1.0621910393238068, train acc: 0.9223\n",
      "epoch: 71, loss: 0.9384981393814087, train acc: 0.9223, test acc: 0.8714\n",
      "loss: 1.0806894302368164, train acc: 0.9185\n",
      "loss: 1.1119990289211272, train acc: 0.9226\n",
      "loss: 1.0331458449363708, train acc: 0.9207\n",
      "loss: 1.023871862888336, train acc: 0.9209\n",
      "loss: 0.9704799652099609, train acc: 0.921\n",
      "loss: 1.023557323217392, train acc: 0.9203\n",
      "loss: 1.052656465768814, train acc: 0.9197\n",
      "loss: 1.0225228309631347, train acc: 0.9228\n",
      "epoch: 72, loss: 0.7785037159919739, train acc: 0.9228, test acc: 0.8691\n",
      "loss: 1.0316907167434692, train acc: 0.9174\n",
      "loss: 1.0739675283432006, train acc: 0.9207\n",
      "loss: 1.0460669159889222, train acc: 0.92\n",
      "loss: 1.0531170606613158, train acc: 0.9201\n",
      "loss: 1.0175352931022643, train acc: 0.9188\n",
      "loss: 1.0496935725212098, train acc: 0.9208\n",
      "loss: 1.026675772666931, train acc: 0.9202\n",
      "loss: 1.0479129910469056, train acc: 0.9209\n",
      "epoch: 73, loss: 1.0405519008636475, train acc: 0.9209, test acc: 0.875\n",
      "loss: 1.1532927751541138, train acc: 0.9227\n",
      "loss: 1.0996706664562226, train acc: 0.9203\n",
      "loss: 1.0418573498725892, train acc: 0.9212\n",
      "loss: 1.0295029640197755, train acc: 0.9187\n",
      "loss: 1.0391887962818145, train acc: 0.9187\n",
      "loss: 1.0334899127483368, train acc: 0.9201\n",
      "loss: 0.9993548095226288, train acc: 0.9197\n",
      "loss: 1.0095255613327025, train acc: 0.9197\n",
      "epoch: 74, loss: 0.762417197227478, train acc: 0.9197, test acc: 0.8712\n",
      "loss: 1.2068157196044922, train acc: 0.9187\n",
      "loss: 1.0842382729053497, train acc: 0.9223\n",
      "loss: 1.0097812175750733, train acc: 0.9223\n",
      "loss: 1.0388810813426972, train acc: 0.9207\n",
      "loss: 1.0483747959136962, train acc: 0.9188\n",
      "loss: 1.0156300127506257, train acc: 0.9225\n",
      "loss: 1.0019464015960693, train acc: 0.925\n",
      "loss: 1.0737192809581757, train acc: 0.9198\n",
      "epoch: 75, loss: 1.3621861934661865, train acc: 0.9198, test acc: 0.8696\n",
      "loss: 1.1305032968521118, train acc: 0.9202\n",
      "loss: 1.097581398487091, train acc: 0.92\n",
      "loss: 1.037660503387451, train acc: 0.9228\n",
      "loss: 1.0473717212677003, train acc: 0.9219\n",
      "loss: 1.0515633344650268, train acc: 0.9194\n",
      "loss: 1.015866506099701, train acc: 0.9232\n",
      "loss: 1.0529891014099122, train acc: 0.9221\n",
      "loss: 1.001857477426529, train acc: 0.9212\n",
      "epoch: 76, loss: 1.0702614784240723, train acc: 0.9212, test acc: 0.8739\n",
      "loss: 1.0790878534317017, train acc: 0.9233\n",
      "loss: 1.0567637622356414, train acc: 0.923\n",
      "loss: 1.0186085462570191, train acc: 0.9217\n",
      "loss: 1.050181609392166, train acc: 0.9229\n",
      "loss: 1.0060217320919036, train acc: 0.9219\n",
      "loss: 0.9757094264030457, train acc: 0.9194\n",
      "loss: 1.0267011165618896, train acc: 0.9195\n",
      "loss: 1.0260306358337403, train acc: 0.9224\n",
      "epoch: 77, loss: 1.3550914525985718, train acc: 0.9224, test acc: 0.8712\n",
      "loss: 1.0610796213150024, train acc: 0.9204\n",
      "loss: 1.1047412872314453, train acc: 0.9204\n",
      "loss: 1.0321480989456178, train acc: 0.9231\n",
      "loss: 1.0718984663486482, train acc: 0.9207\n",
      "loss: 1.0279129922389985, train acc: 0.9199\n",
      "loss: 1.0276391863822938, train acc: 0.9239\n",
      "loss: 1.0315129697322845, train acc: 0.9212\n",
      "loss: 0.9612758040428162, train acc: 0.9197\n",
      "epoch: 78, loss: 0.7436758875846863, train acc: 0.9197, test acc: 0.8752\n",
      "loss: 1.138362169265747, train acc: 0.9245\n",
      "loss: 1.0882425844669341, train acc: 0.9242\n",
      "loss: 1.0409923255443574, train acc: 0.918\n",
      "loss: 1.0287711441516876, train acc: 0.9202\n",
      "loss: 1.0533273100852967, train acc: 0.9211\n",
      "loss: 1.0034077703952788, train acc: 0.9204\n",
      "loss: 1.0025729596614839, train acc: 0.9223\n",
      "loss: 0.9578617632389068, train acc: 0.9221\n",
      "epoch: 79, loss: 0.7278090119361877, train acc: 0.9221, test acc: 0.8747\n",
      "#####training and testing end with K:10, P:0.5######\n",
      "#####training and testing start with K:10, P:1######\n",
      "loss: 2.3760550022125244, train acc: 0.119\n",
      "loss: 2.212286138534546, train acc: 0.2281\n",
      "loss: 1.957291316986084, train acc: 0.3696\n",
      "loss: 1.6791118383407593, train acc: 0.5804\n",
      "loss: 1.40098637342453, train acc: 0.6546\n",
      "loss: 1.2187862634658813, train acc: 0.7371\n",
      "loss: 1.0068105220794679, train acc: 0.7957\n",
      "loss: 0.8490115821361541, train acc: 0.8224\n",
      "epoch: 0, loss: 0.61857670545578, train acc: 0.8224, test acc: 0.8413\n",
      "loss: 0.63008713722229, train acc: 0.8409\n",
      "loss: 0.6860821783542633, train acc: 0.8527\n",
      "loss: 0.6505602061748504, train acc: 0.8625\n",
      "loss: 0.5808851599693299, train acc: 0.8684\n",
      "loss: 0.5145071148872375, train acc: 0.8675\n",
      "loss: 0.4965069770812988, train acc: 0.8738\n",
      "loss: 0.45136553049087524, train acc: 0.8817\n",
      "loss: 0.43299056887626647, train acc: 0.8809\n",
      "epoch: 1, loss: 0.3038289546966553, train acc: 0.8809, test acc: 0.882\n",
      "loss: 0.33554741740226746, train acc: 0.8862\n",
      "loss: 0.4311768561601639, train acc: 0.8906\n",
      "loss: 0.45277694463729856, train acc: 0.8925\n",
      "loss: 0.4215406745672226, train acc: 0.897\n",
      "loss: 0.389314541220665, train acc: 0.8933\n",
      "loss: 0.38271255791187286, train acc: 0.8976\n",
      "loss: 0.35022458136081697, train acc: 0.9015\n",
      "loss: 0.34403064250946047, train acc: 0.9009\n",
      "epoch: 2, loss: 0.19904416799545288, train acc: 0.9009, test acc: 0.8946\n",
      "loss: 0.25579383969306946, train acc: 0.9027\n",
      "loss: 0.36030434668064115, train acc: 0.9046\n",
      "loss: 0.3848343759775162, train acc: 0.9047\n",
      "loss: 0.36197523921728136, train acc: 0.9093\n",
      "loss: 0.34148754477500914, train acc: 0.9061\n",
      "loss: 0.3349943280220032, train acc: 0.9099\n",
      "loss: 0.30440898388624194, train acc: 0.91\n",
      "loss: 0.3036811023950577, train acc: 0.9111\n",
      "epoch: 3, loss: 0.1477067470550537, train acc: 0.9111, test acc: 0.8994\n",
      "loss: 0.21573127806186676, train acc: 0.9119\n",
      "loss: 0.32329341024160385, train acc: 0.9134\n",
      "loss: 0.34703690409660337, train acc: 0.9131\n",
      "loss: 0.32832269966602323, train acc: 0.9167\n",
      "loss: 0.3133701726794243, train acc: 0.9135\n",
      "loss: 0.3075252532958984, train acc: 0.9176\n",
      "loss: 0.2772213488817215, train acc: 0.918\n",
      "loss: 0.2794258326292038, train acc: 0.9167\n",
      "epoch: 4, loss: 0.11821020394563675, train acc: 0.9167, test acc: 0.9011\n",
      "loss: 0.19109389185905457, train acc: 0.9184\n",
      "loss: 0.2998731449246407, train acc: 0.9184\n",
      "loss: 0.3225422531366348, train acc: 0.9188\n",
      "loss: 0.30534292161464693, train acc: 0.9224\n",
      "loss: 0.2937054142355919, train acc: 0.9167\n",
      "loss: 0.2887580692768097, train acc: 0.9222\n",
      "loss: 0.2584579288959503, train acc: 0.9224\n",
      "loss: 0.2628966823220253, train acc: 0.9216\n",
      "epoch: 5, loss: 0.09626311808824539, train acc: 0.9216, test acc: 0.9039\n",
      "loss: 0.1730356216430664, train acc: 0.9221\n",
      "loss: 0.28234497606754305, train acc: 0.9226\n",
      "loss: 0.3046363264322281, train acc: 0.9235\n",
      "loss: 0.28841590136289597, train acc: 0.9262\n",
      "loss: 0.2791328340768814, train acc: 0.9203\n",
      "loss: 0.2753480687737465, train acc: 0.9258\n",
      "loss: 0.24452926069498063, train acc: 0.9262\n",
      "loss: 0.25074995011091233, train acc: 0.9251\n",
      "epoch: 6, loss: 0.08083301782608032, train acc: 0.9251, test acc: 0.9052\n",
      "loss: 0.15894968807697296, train acc: 0.9268\n",
      "loss: 0.26942266523838043, train acc: 0.9266\n",
      "loss: 0.2908435553312302, train acc: 0.9271\n",
      "loss: 0.2746809184551239, train acc: 0.9293\n",
      "loss: 0.267147858440876, train acc: 0.9245\n",
      "loss: 0.2640319004654884, train acc: 0.9292\n",
      "loss: 0.23333700299263, train acc: 0.9302\n",
      "loss: 0.24097128063440323, train acc: 0.9284\n",
      "epoch: 7, loss: 0.06888066232204437, train acc: 0.9284, test acc: 0.9065\n",
      "loss: 0.14864581823349, train acc: 0.9301\n",
      "loss: 0.25899345576763155, train acc: 0.9292\n",
      "loss: 0.2796345457434654, train acc: 0.9299\n",
      "loss: 0.2639119565486908, train acc: 0.9317\n",
      "loss: 0.2572106897830963, train acc: 0.9265\n",
      "loss: 0.2547344669699669, train acc: 0.9321\n",
      "loss: 0.22393118739128112, train acc: 0.9317\n",
      "loss: 0.23255862593650817, train acc: 0.9316\n",
      "epoch: 8, loss: 0.05900772288441658, train acc: 0.9316, test acc: 0.9069\n",
      "loss: 0.1405923068523407, train acc: 0.9325\n",
      "loss: 0.25031598657369614, train acc: 0.9317\n",
      "loss: 0.26973326653242113, train acc: 0.9317\n",
      "loss: 0.254706771671772, train acc: 0.9336\n",
      "loss: 0.24824209064245223, train acc: 0.9291\n",
      "loss: 0.24628671407699584, train acc: 0.9346\n",
      "loss: 0.21608397513628005, train acc: 0.9332\n",
      "loss: 0.22548231184482576, train acc: 0.9331\n",
      "epoch: 9, loss: 0.051604606211185455, train acc: 0.9331, test acc: 0.9078\n",
      "loss: 0.13352516293525696, train acc: 0.9346\n",
      "loss: 0.24294668883085252, train acc: 0.9341\n",
      "loss: 0.26112420707941053, train acc: 0.9349\n",
      "loss: 0.2467044934630394, train acc: 0.9354\n",
      "loss: 0.2407655581831932, train acc: 0.9314\n",
      "loss: 0.23905885070562363, train acc: 0.9355\n",
      "loss: 0.20909408181905748, train acc: 0.9359\n",
      "loss: 0.21956483125686646, train acc: 0.935\n",
      "epoch: 10, loss: 0.04597027972340584, train acc: 0.935, test acc: 0.9089\n",
      "loss: 0.1281367838382721, train acc: 0.9366\n",
      "loss: 0.23663332164287568, train acc: 0.9358\n",
      "loss: 0.25360364764928817, train acc: 0.9359\n",
      "loss: 0.23984726071357726, train acc: 0.9388\n",
      "loss: 0.23336423933506012, train acc: 0.9326\n",
      "loss: 0.2325585588812828, train acc: 0.9374\n",
      "loss: 0.20306058079004288, train acc: 0.9379\n",
      "loss: 0.21419721841812134, train acc: 0.9376\n",
      "epoch: 11, loss: 0.04130125418305397, train acc: 0.9376, test acc: 0.9101\n",
      "loss: 0.1236901506781578, train acc: 0.938\n",
      "loss: 0.23109242096543312, train acc: 0.9367\n",
      "loss: 0.24717519134283067, train acc: 0.9381\n",
      "loss: 0.23383208364248276, train acc: 0.9406\n",
      "loss: 0.22677502483129502, train acc: 0.9353\n",
      "loss: 0.22667953819036485, train acc: 0.9384\n",
      "loss: 0.19746991693973542, train acc: 0.9394\n",
      "loss: 0.20968954563140868, train acc: 0.9392\n",
      "epoch: 12, loss: 0.03685646131634712, train acc: 0.9392, test acc: 0.9114\n",
      "loss: 0.11980906128883362, train acc: 0.9391\n",
      "loss: 0.22579171508550644, train acc: 0.9384\n",
      "loss: 0.2410778820514679, train acc: 0.9393\n",
      "loss: 0.22842712104320526, train acc: 0.9416\n",
      "loss: 0.22053831666707993, train acc: 0.9364\n",
      "loss: 0.22163429111242294, train acc: 0.9398\n",
      "loss: 0.19259555339813234, train acc: 0.9412\n",
      "loss: 0.2053752526640892, train acc: 0.9413\n",
      "epoch: 13, loss: 0.03330260515213013, train acc: 0.9413, test acc: 0.912\n",
      "loss: 0.11645907908678055, train acc: 0.9404\n",
      "loss: 0.22084981128573417, train acc: 0.9401\n",
      "loss: 0.2353854477405548, train acc: 0.9407\n",
      "loss: 0.22343388199806213, train acc: 0.9424\n",
      "loss: 0.21496346592903137, train acc: 0.9377\n",
      "loss: 0.21644485518336296, train acc: 0.9408\n",
      "loss: 0.187753888964653, train acc: 0.9431\n",
      "loss: 0.20129453092813493, train acc: 0.9425\n",
      "epoch: 14, loss: 0.030527658760547638, train acc: 0.9425, test acc: 0.9124\n",
      "loss: 0.11402741819620132, train acc: 0.9418\n",
      "loss: 0.21672450229525567, train acc: 0.9416\n",
      "loss: 0.23015666455030442, train acc: 0.9417\n",
      "loss: 0.2188494920730591, train acc: 0.9439\n",
      "loss: 0.20913151353597642, train acc: 0.9386\n",
      "loss: 0.21206340789794922, train acc: 0.9418\n",
      "loss: 0.18325001299381255, train acc: 0.9438\n",
      "loss: 0.19763170182704926, train acc: 0.9433\n",
      "epoch: 15, loss: 0.02836143597960472, train acc: 0.9433, test acc: 0.9119\n",
      "loss: 0.11134418845176697, train acc: 0.9431\n",
      "loss: 0.21228224635124207, train acc: 0.9424\n",
      "loss: 0.22544492930173873, train acc: 0.943\n",
      "loss: 0.21438802108168603, train acc: 0.9452\n",
      "loss: 0.20379498898983, train acc: 0.94\n",
      "loss: 0.20770986527204513, train acc: 0.9426\n",
      "loss: 0.17929310649633406, train acc: 0.9458\n",
      "loss: 0.19425360411405562, train acc: 0.9445\n",
      "epoch: 16, loss: 0.025852767750620842, train acc: 0.9445, test acc: 0.9122\n",
      "loss: 0.10921023041009903, train acc: 0.9447\n",
      "loss: 0.20845934972167016, train acc: 0.9434\n",
      "loss: 0.22105223685503006, train acc: 0.9441\n",
      "loss: 0.21019267439842224, train acc: 0.9471\n",
      "loss: 0.19873876124620438, train acc: 0.9408\n",
      "loss: 0.20364566743373871, train acc: 0.9432\n",
      "loss: 0.17527777776122094, train acc: 0.9469\n",
      "loss: 0.19088822081685067, train acc: 0.9457\n",
      "epoch: 17, loss: 0.023826083168387413, train acc: 0.9457, test acc: 0.9128\n",
      "loss: 0.10698777437210083, train acc: 0.946\n",
      "loss: 0.20467251241207124, train acc: 0.9447\n",
      "loss: 0.21676560938358308, train acc: 0.9451\n",
      "loss: 0.20643604025244713, train acc: 0.9478\n",
      "loss: 0.19402194395661354, train acc: 0.9418\n",
      "loss: 0.19931461066007614, train acc: 0.9446\n",
      "loss: 0.17148632928729057, train acc: 0.9481\n",
      "loss: 0.18758229836821555, train acc: 0.9471\n",
      "epoch: 18, loss: 0.02220359444618225, train acc: 0.9471, test acc: 0.9128\n",
      "loss: 0.10517340898513794, train acc: 0.9475\n",
      "loss: 0.20110255479812622, train acc: 0.9456\n",
      "loss: 0.21285078376531602, train acc: 0.946\n",
      "loss: 0.20264558643102645, train acc: 0.949\n",
      "loss: 0.18993092626333236, train acc: 0.9426\n",
      "loss: 0.19563660770654678, train acc: 0.9455\n",
      "loss: 0.1683820180594921, train acc: 0.949\n",
      "loss: 0.18430826738476752, train acc: 0.9479\n",
      "epoch: 19, loss: 0.02080310694873333, train acc: 0.9479, test acc: 0.9124\n",
      "loss: 0.10364886373281479, train acc: 0.9488\n",
      "loss: 0.1979094199836254, train acc: 0.9463\n",
      "loss: 0.20893213152885437, train acc: 0.9466\n",
      "loss: 0.1991523928940296, train acc: 0.9495\n",
      "loss: 0.18575238287448884, train acc: 0.944\n",
      "loss: 0.19158194810152054, train acc: 0.9467\n",
      "loss: 0.1647893764078617, train acc: 0.95\n",
      "loss: 0.18143104985356331, train acc: 0.9492\n",
      "epoch: 20, loss: 0.019659757614135742, train acc: 0.9492, test acc: 0.9124\n",
      "loss: 0.10180813819169998, train acc: 0.9493\n",
      "loss: 0.1945405974984169, train acc: 0.9474\n",
      "loss: 0.20489659458398818, train acc: 0.9475\n",
      "loss: 0.19544614627957344, train acc: 0.9501\n",
      "loss: 0.18162633553147317, train acc: 0.9444\n",
      "loss: 0.18802021220326423, train acc: 0.9474\n",
      "loss: 0.16180355101823807, train acc: 0.9506\n",
      "loss: 0.17844338417053224, train acc: 0.9501\n",
      "epoch: 21, loss: 0.01868845522403717, train acc: 0.9501, test acc: 0.9125\n",
      "loss: 0.10078664124011993, train acc: 0.9505\n",
      "loss: 0.1916128106415272, train acc: 0.9487\n",
      "loss: 0.20156627371907235, train acc: 0.9492\n",
      "loss: 0.1918761819601059, train acc: 0.9517\n",
      "loss: 0.17758803814649582, train acc: 0.9453\n",
      "loss: 0.18465472832322122, train acc: 0.9485\n",
      "loss: 0.15850932747125626, train acc: 0.9518\n",
      "loss: 0.17548595294356345, train acc: 0.9516\n",
      "epoch: 22, loss: 0.017843138426542282, train acc: 0.9516, test acc: 0.912\n",
      "loss: 0.09971265494823456, train acc: 0.9511\n",
      "loss: 0.18861374855041504, train acc: 0.9492\n",
      "loss: 0.19804830700159073, train acc: 0.9502\n",
      "loss: 0.1885114885866642, train acc: 0.9526\n",
      "loss: 0.17391065061092376, train acc: 0.9463\n",
      "loss: 0.18166477233171463, train acc: 0.9498\n",
      "loss: 0.15546129941940307, train acc: 0.9531\n",
      "loss: 0.17261014953255654, train acc: 0.9524\n",
      "epoch: 23, loss: 0.01722639799118042, train acc: 0.9524, test acc: 0.9122\n",
      "loss: 0.09875714033842087, train acc: 0.952\n",
      "loss: 0.18576350510120393, train acc: 0.9495\n",
      "loss: 0.19461629763245583, train acc: 0.9509\n",
      "loss: 0.18486659973859787, train acc: 0.9537\n",
      "loss: 0.1702904559671879, train acc: 0.9473\n",
      "loss: 0.1783745341002941, train acc: 0.9506\n",
      "loss: 0.15254810750484465, train acc: 0.9538\n",
      "loss: 0.16983763948082925, train acc: 0.9524\n",
      "epoch: 24, loss: 0.016160741448402405, train acc: 0.9524, test acc: 0.9124\n",
      "loss: 0.0977577343583107, train acc: 0.9526\n",
      "loss: 0.18300724253058434, train acc: 0.9505\n",
      "loss: 0.19112650603055953, train acc: 0.9517\n",
      "loss: 0.18196480572223664, train acc: 0.9544\n",
      "loss: 0.16696227937936783, train acc: 0.9485\n",
      "loss: 0.1751023568212986, train acc: 0.9515\n",
      "loss: 0.15011833235621452, train acc: 0.9547\n",
      "loss: 0.16714796423912048, train acc: 0.9534\n",
      "epoch: 25, loss: 0.015281599014997482, train acc: 0.9534, test acc: 0.9128\n",
      "loss: 0.09732378274202347, train acc: 0.9538\n",
      "loss: 0.18050969988107682, train acc: 0.9516\n",
      "loss: 0.18800364062190056, train acc: 0.9526\n",
      "loss: 0.1789445973932743, train acc: 0.9555\n",
      "loss: 0.16354961916804314, train acc: 0.9495\n",
      "loss: 0.17179698199033738, train acc: 0.9526\n",
      "loss: 0.14743517264723777, train acc: 0.9545\n",
      "loss: 0.1648592509329319, train acc: 0.9543\n",
      "epoch: 26, loss: 0.014903361909091473, train acc: 0.9543, test acc: 0.9127\n",
      "loss: 0.09611322730779648, train acc: 0.9547\n",
      "loss: 0.17821263149380684, train acc: 0.9524\n",
      "loss: 0.18498900160193443, train acc: 0.9533\n",
      "loss: 0.17582421377301216, train acc: 0.9566\n",
      "loss: 0.16076522320508957, train acc: 0.9505\n",
      "loss: 0.16909046843647957, train acc: 0.9535\n",
      "loss: 0.14497635513544083, train acc: 0.9551\n",
      "loss: 0.16259411126375198, train acc: 0.9553\n",
      "epoch: 27, loss: 0.014526238664984703, train acc: 0.9553, test acc: 0.9124\n",
      "loss: 0.09482743591070175, train acc: 0.9552\n",
      "loss: 0.17604896649718285, train acc: 0.953\n",
      "loss: 0.18178317844867706, train acc: 0.9544\n",
      "loss: 0.17291573360562323, train acc: 0.9575\n",
      "loss: 0.15785053074359895, train acc: 0.9514\n",
      "loss: 0.16626998856663705, train acc: 0.9541\n",
      "loss: 0.1423978827893734, train acc: 0.9555\n",
      "loss: 0.16023289784789085, train acc: 0.9557\n",
      "epoch: 28, loss: 0.013738743960857391, train acc: 0.9557, test acc: 0.912\n",
      "loss: 0.09436292946338654, train acc: 0.9555\n",
      "loss: 0.17382278218865393, train acc: 0.9538\n",
      "loss: 0.17931481823325157, train acc: 0.9547\n",
      "loss: 0.16995766162872314, train acc: 0.9579\n",
      "loss: 0.1550428457558155, train acc: 0.9518\n",
      "loss: 0.16339508220553398, train acc: 0.9548\n",
      "loss: 0.13987650573253632, train acc: 0.9563\n",
      "loss: 0.15779221430420876, train acc: 0.9564\n",
      "epoch: 29, loss: 0.013137739151716232, train acc: 0.9564, test acc: 0.9121\n",
      "loss: 0.09313677996397018, train acc: 0.9564\n",
      "loss: 0.17186087593436242, train acc: 0.9546\n",
      "loss: 0.17668571844696998, train acc: 0.9553\n",
      "loss: 0.16740304082632065, train acc: 0.9586\n",
      "loss: 0.15243568643927574, train acc: 0.9529\n",
      "loss: 0.16055998057127, train acc: 0.9555\n",
      "loss: 0.1379832483828068, train acc: 0.9568\n",
      "loss: 0.15553106218576432, train acc: 0.958\n",
      "epoch: 30, loss: 0.012568771839141846, train acc: 0.958, test acc: 0.9122\n",
      "loss: 0.09308004379272461, train acc: 0.957\n",
      "loss: 0.16986374482512473, train acc: 0.9549\n",
      "loss: 0.1744043216109276, train acc: 0.9562\n",
      "loss: 0.16460673734545708, train acc: 0.959\n",
      "loss: 0.14977224096655845, train acc: 0.9534\n",
      "loss: 0.15804534405469894, train acc: 0.9565\n",
      "loss: 0.13559066131711006, train acc: 0.9575\n",
      "loss: 0.1535846509039402, train acc: 0.9583\n",
      "epoch: 31, loss: 0.012104861438274384, train acc: 0.9583, test acc: 0.9113\n",
      "loss: 0.09215907007455826, train acc: 0.9573\n",
      "loss: 0.16775499880313874, train acc: 0.9557\n",
      "loss: 0.17195040509104728, train acc: 0.957\n",
      "loss: 0.1622622214257717, train acc: 0.9597\n",
      "loss: 0.14732986465096473, train acc: 0.9544\n",
      "loss: 0.155446457862854, train acc: 0.9567\n",
      "loss: 0.13344158604741096, train acc: 0.9585\n",
      "loss: 0.15128714293241502, train acc: 0.9594\n",
      "epoch: 32, loss: 0.011606210842728615, train acc: 0.9594, test acc: 0.9119\n",
      "loss: 0.09114918112754822, train acc: 0.9579\n",
      "loss: 0.16592446267604827, train acc: 0.9564\n",
      "loss: 0.1694887489080429, train acc: 0.9576\n",
      "loss: 0.15972817689180374, train acc: 0.9604\n",
      "loss: 0.14502375274896623, train acc: 0.9547\n",
      "loss: 0.1526345990598202, train acc: 0.9578\n",
      "loss: 0.13125307857990265, train acc: 0.9588\n",
      "loss: 0.1493231698870659, train acc: 0.96\n",
      "epoch: 33, loss: 0.011091272346675396, train acc: 0.96, test acc: 0.911\n",
      "loss: 0.09137469530105591, train acc: 0.9586\n",
      "loss: 0.16370567455887794, train acc: 0.9564\n",
      "loss: 0.1675246112048626, train acc: 0.9589\n",
      "loss: 0.15729003995656968, train acc: 0.9611\n",
      "loss: 0.14298344776034355, train acc: 0.9547\n",
      "loss: 0.1504499062895775, train acc: 0.9584\n",
      "loss: 0.1295230954885483, train acc: 0.9593\n",
      "loss: 0.14675526395440103, train acc: 0.9605\n",
      "epoch: 34, loss: 0.01070774719119072, train acc: 0.9605, test acc: 0.9105\n",
      "loss: 0.09017613530158997, train acc: 0.9594\n",
      "loss: 0.16168263107538222, train acc: 0.9572\n",
      "loss: 0.16519950628280639, train acc: 0.9592\n",
      "loss: 0.15497977212071418, train acc: 0.9624\n",
      "loss: 0.14069809913635253, train acc: 0.9555\n",
      "loss: 0.1482849396765232, train acc: 0.959\n",
      "loss: 0.1274310104548931, train acc: 0.9599\n",
      "loss: 0.1449533611536026, train acc: 0.9614\n",
      "epoch: 35, loss: 0.010507182218134403, train acc: 0.9614, test acc: 0.9107\n",
      "loss: 0.08979551494121552, train acc: 0.9603\n",
      "loss: 0.15992335900664328, train acc: 0.9576\n",
      "loss: 0.16291816532611847, train acc: 0.9592\n",
      "loss: 0.15296795070171357, train acc: 0.963\n",
      "loss: 0.13875583857297896, train acc: 0.9566\n",
      "loss: 0.14567595794796945, train acc: 0.9596\n",
      "loss: 0.12536854445934295, train acc: 0.9604\n",
      "loss: 0.14296151995658873, train acc: 0.9619\n",
      "epoch: 36, loss: 0.010099358856678009, train acc: 0.9619, test acc: 0.9108\n",
      "loss: 0.0892271175980568, train acc: 0.9609\n",
      "loss: 0.15790850073099136, train acc: 0.9579\n",
      "loss: 0.16108906343579293, train acc: 0.96\n",
      "loss: 0.15070613846182823, train acc: 0.963\n",
      "loss: 0.13647713139653206, train acc: 0.9573\n",
      "loss: 0.1433201588690281, train acc: 0.9599\n",
      "loss: 0.12374147474765777, train acc: 0.9613\n",
      "loss: 0.1408148854970932, train acc: 0.9627\n",
      "epoch: 37, loss: 0.009631304070353508, train acc: 0.9627, test acc: 0.9111\n",
      "loss: 0.08884365111589432, train acc: 0.9621\n",
      "loss: 0.1561824105679989, train acc: 0.9593\n",
      "loss: 0.15951945409178733, train acc: 0.9603\n",
      "loss: 0.1488421343266964, train acc: 0.9638\n",
      "loss: 0.13447200283408164, train acc: 0.9582\n",
      "loss: 0.14111190438270568, train acc: 0.9604\n",
      "loss: 0.12206496745347976, train acc: 0.9612\n",
      "loss: 0.13902235329151152, train acc: 0.9631\n",
      "epoch: 38, loss: 0.009397572837769985, train acc: 0.9631, test acc: 0.9106\n",
      "loss: 0.08770014345645905, train acc: 0.9625\n",
      "loss: 0.15441180542111396, train acc: 0.9598\n",
      "loss: 0.15717642605304719, train acc: 0.9608\n",
      "loss: 0.1469491071999073, train acc: 0.9636\n",
      "loss: 0.13286357298493384, train acc: 0.9588\n",
      "loss: 0.13905422315001487, train acc: 0.961\n",
      "loss: 0.12010341063141823, train acc: 0.9623\n",
      "loss: 0.136689592897892, train acc: 0.9636\n",
      "epoch: 39, loss: 0.008968564681708813, train acc: 0.9636, test acc: 0.9109\n",
      "loss: 0.08678504079580307, train acc: 0.9631\n",
      "loss: 0.15251993760466576, train acc: 0.9603\n",
      "loss: 0.1551634728908539, train acc: 0.9612\n",
      "loss: 0.14502063617110253, train acc: 0.9645\n",
      "loss: 0.13114374056458472, train acc: 0.9594\n",
      "loss: 0.13680739104747772, train acc: 0.9612\n",
      "loss: 0.11853233873844146, train acc: 0.9626\n",
      "loss: 0.13499488532543183, train acc: 0.9642\n",
      "epoch: 40, loss: 0.008740563876926899, train acc: 0.9642, test acc: 0.9107\n",
      "loss: 0.08659538626670837, train acc: 0.9637\n",
      "loss: 0.15071230828762056, train acc: 0.9609\n",
      "loss: 0.15329472199082375, train acc: 0.962\n",
      "loss: 0.14325760900974274, train acc: 0.9644\n",
      "loss: 0.12922561317682266, train acc: 0.9604\n",
      "loss: 0.13478977978229523, train acc: 0.9617\n",
      "loss: 0.11711673215031623, train acc: 0.9633\n",
      "loss: 0.13309212550520896, train acc: 0.9644\n",
      "epoch: 41, loss: 0.008565781638026237, train acc: 0.9644, test acc: 0.91\n",
      "loss: 0.08647629618644714, train acc: 0.9649\n",
      "loss: 0.1492149882018566, train acc: 0.9613\n",
      "loss: 0.15196264609694482, train acc: 0.962\n",
      "loss: 0.141225104033947, train acc: 0.9653\n",
      "loss: 0.12733341827988626, train acc: 0.9609\n",
      "loss: 0.13277637511491774, train acc: 0.9624\n",
      "loss: 0.1152119480073452, train acc: 0.9636\n",
      "loss: 0.13167300820350647, train acc: 0.9647\n",
      "epoch: 42, loss: 0.00815512239933014, train acc: 0.9647, test acc: 0.9101\n",
      "loss: 0.08516547828912735, train acc: 0.9654\n",
      "loss: 0.14738964661955833, train acc: 0.9621\n",
      "loss: 0.14976392909884453, train acc: 0.9619\n",
      "loss: 0.1397176817059517, train acc: 0.9652\n",
      "loss: 0.12575719952583314, train acc: 0.9615\n",
      "loss: 0.13105201348662376, train acc: 0.9628\n",
      "loss: 0.11415734589099884, train acc: 0.9641\n",
      "loss: 0.1295902080833912, train acc: 0.9652\n",
      "epoch: 43, loss: 0.008005432784557343, train acc: 0.9652, test acc: 0.9096\n",
      "loss: 0.08581593632698059, train acc: 0.9654\n",
      "loss: 0.14593881294131278, train acc: 0.9626\n",
      "loss: 0.14823443293571473, train acc: 0.9634\n",
      "loss: 0.13752344362437724, train acc: 0.9661\n",
      "loss: 0.12389373406767845, train acc: 0.9617\n",
      "loss: 0.12905509471893312, train acc: 0.9635\n",
      "loss: 0.11227600276470184, train acc: 0.9644\n",
      "loss: 0.12770941704511643, train acc: 0.9652\n",
      "epoch: 44, loss: 0.007583033759146929, train acc: 0.9652, test acc: 0.9096\n",
      "loss: 0.08541730046272278, train acc: 0.9657\n",
      "loss: 0.14425625130534173, train acc: 0.9628\n",
      "loss: 0.14645667374134064, train acc: 0.9636\n",
      "loss: 0.13570279330015184, train acc: 0.9666\n",
      "loss: 0.12227606102824211, train acc: 0.9618\n",
      "loss: 0.1274130530655384, train acc: 0.9639\n",
      "loss: 0.11091186329722405, train acc: 0.9651\n",
      "loss: 0.12614451795816423, train acc: 0.9665\n",
      "epoch: 45, loss: 0.00754522392526269, train acc: 0.9665, test acc: 0.9094\n",
      "loss: 0.0845041275024414, train acc: 0.9661\n",
      "loss: 0.14271557405591012, train acc: 0.9639\n",
      "loss: 0.14467759504914285, train acc: 0.9634\n",
      "loss: 0.13440416157245635, train acc: 0.9671\n",
      "loss: 0.12079596817493439, train acc: 0.9621\n",
      "loss: 0.1254499688744545, train acc: 0.9646\n",
      "loss: 0.10940852463245392, train acc: 0.9655\n",
      "loss: 0.12449825778603554, train acc: 0.9664\n",
      "epoch: 46, loss: 0.007229277398437262, train acc: 0.9664, test acc: 0.9088\n",
      "loss: 0.08398779481649399, train acc: 0.9662\n",
      "loss: 0.14127973094582558, train acc: 0.9646\n",
      "loss: 0.142984551936388, train acc: 0.9642\n",
      "loss: 0.13250180520117283, train acc: 0.9672\n",
      "loss: 0.11892827488481998, train acc: 0.9626\n",
      "loss: 0.12379215210676194, train acc: 0.9644\n",
      "loss: 0.10792051032185554, train acc: 0.9659\n",
      "loss: 0.12291138991713524, train acc: 0.9674\n",
      "epoch: 47, loss: 0.007025990169495344, train acc: 0.9674, test acc: 0.9085\n",
      "loss: 0.08327433466911316, train acc: 0.9673\n",
      "loss: 0.13948653563857077, train acc: 0.9643\n",
      "loss: 0.14125831574201583, train acc: 0.964\n",
      "loss: 0.13110849894583226, train acc: 0.9671\n",
      "loss: 0.11765449568629265, train acc: 0.9632\n",
      "loss: 0.12187447398900986, train acc: 0.9652\n",
      "loss: 0.10693102926015854, train acc: 0.9668\n",
      "loss: 0.12103582695126533, train acc: 0.9671\n",
      "epoch: 48, loss: 0.006862488575279713, train acc: 0.9671, test acc: 0.9084\n",
      "loss: 0.08297540992498398, train acc: 0.9678\n",
      "loss: 0.1382987305521965, train acc: 0.965\n",
      "loss: 0.14003375098109244, train acc: 0.9645\n",
      "loss: 0.12970655858516694, train acc: 0.9673\n",
      "loss: 0.11599063202738762, train acc: 0.9633\n",
      "loss: 0.12023128047585488, train acc: 0.9658\n",
      "loss: 0.10540558770298958, train acc: 0.9665\n",
      "loss: 0.11955515965819359, train acc: 0.9678\n",
      "epoch: 49, loss: 0.006710048299282789, train acc: 0.9678, test acc: 0.9082\n",
      "loss: 0.08196942508220673, train acc: 0.9679\n",
      "loss: 0.13684387654066085, train acc: 0.9652\n",
      "loss: 0.13839929103851317, train acc: 0.9654\n",
      "loss: 0.12793943621218204, train acc: 0.9679\n",
      "loss: 0.11471016556024552, train acc: 0.9638\n",
      "loss: 0.11842747926712036, train acc: 0.9661\n",
      "loss: 0.1039573960006237, train acc: 0.967\n",
      "loss: 0.11776812821626663, train acc: 0.9683\n",
      "epoch: 50, loss: 0.006363078020513058, train acc: 0.9683, test acc: 0.9074\n",
      "loss: 0.08115881681442261, train acc: 0.9682\n",
      "loss: 0.1350199557840824, train acc: 0.9656\n",
      "loss: 0.1369003776460886, train acc: 0.9654\n",
      "loss: 0.12673416808247567, train acc: 0.9673\n",
      "loss: 0.11276369206607342, train acc: 0.9643\n",
      "loss: 0.11680075451731682, train acc: 0.9668\n",
      "loss: 0.10285051167011261, train acc: 0.967\n",
      "loss: 0.11651557460427284, train acc: 0.9685\n",
      "epoch: 51, loss: 0.006355153396725655, train acc: 0.9685, test acc: 0.9077\n",
      "loss: 0.081466443836689, train acc: 0.9691\n",
      "loss: 0.1336612172424793, train acc: 0.9652\n",
      "loss: 0.13483961783349513, train acc: 0.9659\n",
      "loss: 0.12484948076307774, train acc: 0.9677\n",
      "loss: 0.11170954890549183, train acc: 0.9643\n",
      "loss: 0.11538245007395745, train acc: 0.9668\n",
      "loss: 0.10155485197901726, train acc: 0.9675\n",
      "loss: 0.11519664376974106, train acc: 0.9691\n",
      "epoch: 52, loss: 0.0060598161071538925, train acc: 0.9691, test acc: 0.9072\n",
      "loss: 0.08099272847175598, train acc: 0.969\n",
      "loss: 0.13229709006845952, train acc: 0.9663\n",
      "loss: 0.13397758826613426, train acc: 0.9658\n",
      "loss: 0.12364739663898945, train acc: 0.9679\n",
      "loss: 0.11004914417862892, train acc: 0.9651\n",
      "loss: 0.11376854553818702, train acc: 0.9674\n",
      "loss: 0.10016708225011825, train acc: 0.9677\n",
      "loss: 0.11355494484305381, train acc: 0.9696\n",
      "epoch: 53, loss: 0.005969978403300047, train acc: 0.9696, test acc: 0.9074\n",
      "loss: 0.08078041672706604, train acc: 0.9701\n",
      "loss: 0.13045793436467648, train acc: 0.9664\n",
      "loss: 0.13245580680668353, train acc: 0.9665\n",
      "loss: 0.12178484499454498, train acc: 0.9678\n",
      "loss: 0.10892602428793907, train acc: 0.9653\n",
      "loss: 0.11224143132567406, train acc: 0.9678\n",
      "loss: 0.09921097010374069, train acc: 0.9682\n",
      "loss: 0.11202723607420921, train acc: 0.9705\n",
      "epoch: 54, loss: 0.005791084840893745, train acc: 0.9705, test acc: 0.9072\n",
      "loss: 0.0795513316988945, train acc: 0.97\n",
      "loss: 0.1290267962962389, train acc: 0.9664\n",
      "loss: 0.13067844323813915, train acc: 0.9664\n",
      "loss: 0.12063940912485123, train acc: 0.9685\n",
      "loss: 0.10763644687831402, train acc: 0.9653\n",
      "loss: 0.11083498150110245, train acc: 0.9683\n",
      "loss: 0.09799679219722748, train acc: 0.9691\n",
      "loss: 0.11069255843758583, train acc: 0.9701\n",
      "epoch: 55, loss: 0.005552394315600395, train acc: 0.9701, test acc: 0.9068\n",
      "loss: 0.07969053834676743, train acc: 0.9704\n",
      "loss: 0.12754537984728814, train acc: 0.9677\n",
      "loss: 0.12953901961445807, train acc: 0.9671\n",
      "loss: 0.11912705600261689, train acc: 0.9688\n",
      "loss: 0.10636093467473984, train acc: 0.9661\n",
      "loss: 0.10920942574739456, train acc: 0.9683\n",
      "loss: 0.09661081284284592, train acc: 0.9695\n",
      "loss: 0.10895430259406566, train acc: 0.9704\n",
      "epoch: 56, loss: 0.005363564472645521, train acc: 0.9704, test acc: 0.9069\n",
      "loss: 0.07855767011642456, train acc: 0.9706\n",
      "loss: 0.12610331922769547, train acc: 0.9678\n",
      "loss: 0.12792686112225055, train acc: 0.9673\n",
      "loss: 0.1176934689283371, train acc: 0.9685\n",
      "loss: 0.10505231097340584, train acc: 0.9663\n",
      "loss: 0.10787793099880219, train acc: 0.9693\n",
      "loss: 0.09556426405906678, train acc: 0.9695\n",
      "loss: 0.10730095468461513, train acc: 0.9706\n",
      "epoch: 57, loss: 0.005338593386113644, train acc: 0.9706, test acc: 0.9069\n",
      "loss: 0.07717284560203552, train acc: 0.9708\n",
      "loss: 0.12455589547753335, train acc: 0.9678\n",
      "loss: 0.12712760604918003, train acc: 0.9667\n",
      "loss: 0.11659138090908527, train acc: 0.9693\n",
      "loss: 0.10405389070510865, train acc: 0.9665\n",
      "loss: 0.10678098052740097, train acc: 0.9693\n",
      "loss: 0.09427155256271362, train acc: 0.9703\n",
      "loss: 0.10656613148748875, train acc: 0.9708\n",
      "epoch: 58, loss: 0.005127371288836002, train acc: 0.9708, test acc: 0.9067\n",
      "loss: 0.07766959816217422, train acc: 0.9711\n",
      "loss: 0.12305176146328449, train acc: 0.9689\n",
      "loss: 0.1252566620707512, train acc: 0.9682\n",
      "loss: 0.115040822327137, train acc: 0.9689\n",
      "loss: 0.10295965895056725, train acc: 0.9669\n",
      "loss: 0.10516854636371135, train acc: 0.97\n",
      "loss: 0.09310683831572533, train acc: 0.9703\n",
      "loss: 0.10456332676112652, train acc: 0.9711\n",
      "epoch: 59, loss: 0.005056461319327354, train acc: 0.9711, test acc: 0.9064\n",
      "loss: 0.07575436681509018, train acc: 0.9711\n",
      "loss: 0.12160619571805001, train acc: 0.9691\n",
      "loss: 0.12426671907305717, train acc: 0.9675\n",
      "loss: 0.114051129296422, train acc: 0.9697\n",
      "loss: 0.1019008681178093, train acc: 0.9671\n",
      "loss: 0.10416415333747864, train acc: 0.9699\n",
      "loss: 0.09234868064522743, train acc: 0.9704\n",
      "loss: 0.1034356776624918, train acc: 0.972\n",
      "epoch: 60, loss: 0.004783335141837597, train acc: 0.972, test acc: 0.9066\n",
      "loss: 0.07673875987529755, train acc: 0.9717\n",
      "loss: 0.11990466453135014, train acc: 0.9694\n",
      "loss: 0.12252985164523125, train acc: 0.9682\n",
      "loss: 0.11214703321456909, train acc: 0.97\n",
      "loss: 0.10077410116791725, train acc: 0.9677\n",
      "loss: 0.10271022282540798, train acc: 0.9704\n",
      "loss: 0.09087281823158264, train acc: 0.9707\n",
      "loss: 0.10240271463990211, train acc: 0.9719\n",
      "epoch: 61, loss: 0.004679111763834953, train acc: 0.9719, test acc: 0.9061\n",
      "loss: 0.07469784468412399, train acc: 0.9723\n",
      "loss: 0.11860136352479458, train acc: 0.9699\n",
      "loss: 0.12165458016097545, train acc: 0.9683\n",
      "loss: 0.11104247197508813, train acc: 0.9706\n",
      "loss: 0.09918361231684684, train acc: 0.968\n",
      "loss: 0.10148564390838147, train acc: 0.9708\n",
      "loss: 0.09019133001565934, train acc: 0.9704\n",
      "loss: 0.10089535191655159, train acc: 0.9723\n",
      "epoch: 62, loss: 0.004685977473855019, train acc: 0.9723, test acc: 0.9058\n",
      "loss: 0.07521852105855942, train acc: 0.9729\n",
      "loss: 0.11740496158599853, train acc: 0.9701\n",
      "loss: 0.1205597847700119, train acc: 0.9681\n",
      "loss: 0.10993276238441467, train acc: 0.9707\n",
      "loss: 0.09820020087063312, train acc: 0.9687\n",
      "loss: 0.1001256588846445, train acc: 0.9712\n",
      "loss: 0.08881480842828751, train acc: 0.9702\n",
      "loss: 0.09976014830172061, train acc: 0.9725\n",
      "epoch: 63, loss: 0.00438906904309988, train acc: 0.9725, test acc: 0.9054\n",
      "loss: 0.07374563813209534, train acc: 0.9727\n",
      "loss: 0.11603631153702736, train acc: 0.9703\n",
      "loss: 0.11890015341341495, train acc: 0.9687\n",
      "loss: 0.10864319615066051, train acc: 0.9711\n",
      "loss: 0.09742610491812229, train acc: 0.9694\n",
      "loss: 0.09890885129570962, train acc: 0.9715\n",
      "loss: 0.08793119676411151, train acc: 0.9707\n",
      "loss: 0.0984109453856945, train acc: 0.973\n",
      "epoch: 64, loss: 0.004343814216554165, train acc: 0.973, test acc: 0.9058\n",
      "loss: 0.07416162639856339, train acc: 0.9734\n",
      "loss: 0.11469743363559246, train acc: 0.9705\n",
      "loss: 0.11804849803447723, train acc: 0.9689\n",
      "loss: 0.1072290051728487, train acc: 0.9722\n",
      "loss: 0.09597943648695946, train acc: 0.9691\n",
      "loss: 0.09786526076495647, train acc: 0.9715\n",
      "loss: 0.08625231795012951, train acc: 0.9709\n",
      "loss: 0.09709478095173836, train acc: 0.9732\n",
      "epoch: 65, loss: 0.004311143886297941, train acc: 0.9732, test acc: 0.9057\n",
      "loss: 0.07318628579378128, train acc: 0.9738\n",
      "loss: 0.11347860656678677, train acc: 0.9715\n",
      "loss: 0.11662541590631008, train acc: 0.9684\n",
      "loss: 0.10616814382374287, train acc: 0.9724\n",
      "loss: 0.09506263881921768, train acc: 0.9697\n",
      "loss: 0.09651302993297577, train acc: 0.9717\n",
      "loss: 0.08582618907094001, train acc: 0.9718\n",
      "loss: 0.09593773037195205, train acc: 0.9739\n",
      "epoch: 66, loss: 0.0041748094372451305, train acc: 0.9739, test acc: 0.9053\n",
      "loss: 0.07275696843862534, train acc: 0.9736\n",
      "loss: 0.11176108047366143, train acc: 0.9721\n",
      "loss: 0.11539652831852436, train acc: 0.9688\n",
      "loss: 0.10507747791707515, train acc: 0.9725\n",
      "loss: 0.09423348791897297, train acc: 0.9699\n",
      "loss: 0.09537425488233567, train acc: 0.972\n",
      "loss: 0.08436247371137143, train acc: 0.9719\n",
      "loss: 0.09466219022870064, train acc: 0.9737\n",
      "epoch: 67, loss: 0.003945542965084314, train acc: 0.9737, test acc: 0.9045\n",
      "loss: 0.07290971279144287, train acc: 0.974\n",
      "loss: 0.11074147485196591, train acc: 0.9721\n",
      "loss: 0.11461811028420925, train acc: 0.9692\n",
      "loss: 0.103830386698246, train acc: 0.9728\n",
      "loss: 0.09294823482632637, train acc: 0.9707\n",
      "loss: 0.09418320916593075, train acc: 0.9724\n",
      "loss: 0.08349568732082843, train acc: 0.9722\n",
      "loss: 0.09333927817642688, train acc: 0.9738\n",
      "epoch: 68, loss: 0.003977843094617128, train acc: 0.9738, test acc: 0.9054\n",
      "loss: 0.07074711471796036, train acc: 0.9739\n",
      "loss: 0.10909797400236129, train acc: 0.9728\n",
      "loss: 0.11319413296878338, train acc: 0.9693\n",
      "loss: 0.10269098207354546, train acc: 0.9726\n",
      "loss: 0.09208252020180226, train acc: 0.9713\n",
      "loss: 0.09324705339968205, train acc: 0.9724\n",
      "loss: 0.08266493156552315, train acc: 0.9722\n",
      "loss: 0.09235471114516258, train acc: 0.974\n",
      "epoch: 69, loss: 0.003919400740414858, train acc: 0.974, test acc: 0.9043\n",
      "loss: 0.07090435922145844, train acc: 0.9747\n",
      "loss: 0.1080044873058796, train acc: 0.9727\n",
      "loss: 0.11213343180716037, train acc: 0.9696\n",
      "loss: 0.10154868066310882, train acc: 0.9731\n",
      "loss: 0.09079186096787453, train acc: 0.9714\n",
      "loss: 0.09214431121945381, train acc: 0.9728\n",
      "loss: 0.08153770454227924, train acc: 0.9723\n",
      "loss: 0.09105911403894425, train acc: 0.9745\n",
      "epoch: 70, loss: 0.0037588237319141626, train acc: 0.9745, test acc: 0.905\n",
      "loss: 0.07015416026115417, train acc: 0.9745\n",
      "loss: 0.10661174580454827, train acc: 0.9728\n",
      "loss: 0.11124689802527428, train acc: 0.9701\n",
      "loss: 0.10029342360794544, train acc: 0.974\n",
      "loss: 0.08995025120675564, train acc: 0.9712\n",
      "loss: 0.09121884480118751, train acc: 0.9729\n",
      "loss: 0.08045844361186028, train acc: 0.9724\n",
      "loss: 0.09021851308643818, train acc: 0.9741\n",
      "epoch: 71, loss: 0.0035639165434986353, train acc: 0.9741, test acc: 0.9044\n",
      "loss: 0.06959359347820282, train acc: 0.9749\n",
      "loss: 0.10575781799852849, train acc: 0.9733\n",
      "loss: 0.10987245738506317, train acc: 0.9699\n",
      "loss: 0.09937979727983474, train acc: 0.9739\n",
      "loss: 0.08881882019340992, train acc: 0.9715\n",
      "loss: 0.09023151993751526, train acc: 0.9734\n",
      "loss: 0.07976103015244007, train acc: 0.9729\n",
      "loss: 0.08917436487972737, train acc: 0.9747\n",
      "epoch: 72, loss: 0.003663817420601845, train acc: 0.9747, test acc: 0.9039\n",
      "loss: 0.06881174445152283, train acc: 0.9751\n",
      "loss: 0.10416140928864479, train acc: 0.9734\n",
      "loss: 0.10916305631399155, train acc: 0.9704\n",
      "loss: 0.0979646448045969, train acc: 0.9742\n",
      "loss: 0.08808402605354786, train acc: 0.9716\n",
      "loss: 0.08909288384020328, train acc: 0.9733\n",
      "loss: 0.07849166058003902, train acc: 0.9729\n",
      "loss: 0.08797323368489743, train acc: 0.9749\n",
      "epoch: 73, loss: 0.003485493129119277, train acc: 0.9749, test acc: 0.904\n",
      "loss: 0.06785126030445099, train acc: 0.9756\n",
      "loss: 0.10293012037873268, train acc: 0.9735\n",
      "loss: 0.10856032371520996, train acc: 0.97\n",
      "loss: 0.09723197594285012, train acc: 0.9744\n",
      "loss: 0.08685680702328683, train acc: 0.9718\n",
      "loss: 0.08788659647107125, train acc: 0.9739\n",
      "loss: 0.0781662940979004, train acc: 0.9735\n",
      "loss: 0.08629516921937466, train acc: 0.9754\n",
      "epoch: 74, loss: 0.0034317555837333202, train acc: 0.9754, test acc: 0.903\n",
      "loss: 0.0668187364935875, train acc: 0.9762\n",
      "loss: 0.10180690884590149, train acc: 0.9737\n",
      "loss: 0.10697653852403163, train acc: 0.9708\n",
      "loss: 0.09609876200556755, train acc: 0.9749\n",
      "loss: 0.08612021952867507, train acc: 0.9721\n",
      "loss: 0.08713370822370052, train acc: 0.9748\n",
      "loss: 0.07663759291172027, train acc: 0.9734\n",
      "loss: 0.08575747944414616, train acc: 0.9753\n",
      "epoch: 75, loss: 0.0034040219616144896, train acc: 0.9753, test acc: 0.9031\n",
      "loss: 0.06649548560380936, train acc: 0.9765\n",
      "loss: 0.10047650448977948, train acc: 0.9738\n",
      "loss: 0.10629801377654076, train acc: 0.9715\n",
      "loss: 0.09495779201388359, train acc: 0.9749\n",
      "loss: 0.08486294709146022, train acc: 0.9721\n",
      "loss: 0.08596224375069142, train acc: 0.9746\n",
      "loss: 0.0760431133210659, train acc: 0.9737\n",
      "loss: 0.08453822992742062, train acc: 0.9756\n",
      "epoch: 76, loss: 0.0032774191349744797, train acc: 0.9756, test acc: 0.9025\n",
      "loss: 0.06605739891529083, train acc: 0.977\n",
      "loss: 0.09927378334105015, train acc: 0.9743\n",
      "loss: 0.10526524335145951, train acc: 0.9714\n",
      "loss: 0.09388118758797645, train acc: 0.9745\n",
      "loss: 0.08414998725056648, train acc: 0.9726\n",
      "loss: 0.08471927754580974, train acc: 0.9749\n",
      "loss: 0.07500566095113755, train acc: 0.9739\n",
      "loss: 0.0835044365376234, train acc: 0.9757\n",
      "epoch: 77, loss: 0.0032966542057693005, train acc: 0.9757, test acc: 0.9029\n",
      "loss: 0.06523419171571732, train acc: 0.977\n",
      "loss: 0.09841434843838215, train acc: 0.9744\n",
      "loss: 0.10391585603356361, train acc: 0.9717\n",
      "loss: 0.09291926138103009, train acc: 0.975\n",
      "loss: 0.08326460048556328, train acc: 0.9724\n",
      "loss: 0.08395230621099473, train acc: 0.9754\n",
      "loss: 0.07412121035158634, train acc: 0.9745\n",
      "loss: 0.08259911015629769, train acc: 0.9764\n",
      "epoch: 78, loss: 0.0031410176306962967, train acc: 0.9764, test acc: 0.9027\n",
      "loss: 0.06528676301240921, train acc: 0.977\n",
      "loss: 0.09722741171717644, train acc: 0.9743\n",
      "loss: 0.1034114494919777, train acc: 0.9721\n",
      "loss: 0.09211199060082435, train acc: 0.9756\n",
      "loss: 0.08224701955914497, train acc: 0.9729\n",
      "loss: 0.08313883915543556, train acc: 0.9759\n",
      "loss: 0.07330498397350312, train acc: 0.9743\n",
      "loss: 0.08161109350621701, train acc: 0.9766\n",
      "epoch: 79, loss: 0.003048182697966695, train acc: 0.9766, test acc: 0.9018\n",
      "#####training and testing end with K:10, P:1######\n",
      "#####training and testing start with K:20, P:0.1######\n",
      "loss: 2.3594040870666504, train acc: 0.1027\n",
      "loss: 2.0804262280464174, train acc: 0.5444\n",
      "loss: 1.5804366230964662, train acc: 0.6685\n",
      "loss: 1.2059751987457275, train acc: 0.7655\n",
      "loss: 0.9729364573955536, train acc: 0.8261\n",
      "loss: 0.811994981765747, train acc: 0.8508\n",
      "loss: 0.7384802877902985, train acc: 0.8669\n",
      "loss: 0.6776759326457977, train acc: 0.8744\n",
      "epoch: 0, loss: 0.3920840919017792, train acc: 0.8744, test acc: 0.8764\n",
      "loss: 0.634027361869812, train acc: 0.884\n",
      "loss: 0.5608825802803039, train acc: 0.8828\n",
      "loss: 0.5537958681583405, train acc: 0.8938\n",
      "loss: 0.5280612021684646, train acc: 0.8865\n",
      "loss: 0.48026318550109864, train acc: 0.8955\n",
      "loss: 0.482132488489151, train acc: 0.9023\n",
      "loss: 0.4901919990777969, train acc: 0.9057\n",
      "loss: 0.4868157297372818, train acc: 0.9053\n",
      "epoch: 1, loss: 0.2657855153083801, train acc: 0.9053, test acc: 0.9005\n",
      "loss: 0.40609702467918396, train acc: 0.9084\n",
      "loss: 0.40385221838951113, train acc: 0.9079\n",
      "loss: 0.43044683039188386, train acc: 0.911\n",
      "loss: 0.4325400561094284, train acc: 0.9054\n",
      "loss: 0.38477015793323516, train acc: 0.9095\n",
      "loss: 0.3882580816745758, train acc: 0.9135\n",
      "loss: 0.4217389553785324, train acc: 0.9165\n",
      "loss: 0.40356734991073606, train acc: 0.9162\n",
      "epoch: 2, loss: 0.25856733322143555, train acc: 0.9162, test acc: 0.9069\n",
      "loss: 0.3257341980934143, train acc: 0.9197\n",
      "loss: 0.36297533214092254, train acc: 0.9195\n",
      "loss: 0.38987685143947604, train acc: 0.9187\n",
      "loss: 0.37674733251333237, train acc: 0.9152\n",
      "loss: 0.36314711570739744, train acc: 0.9199\n",
      "loss: 0.3654229998588562, train acc: 0.9239\n",
      "loss: 0.38100938200950624, train acc: 0.9232\n",
      "loss: 0.3769454568624496, train acc: 0.9256\n",
      "epoch: 3, loss: 0.09979356825351715, train acc: 0.9256, test acc: 0.9125\n",
      "loss: 0.34919753670692444, train acc: 0.9291\n",
      "loss: 0.3359293773770332, train acc: 0.9281\n",
      "loss: 0.35939808189868927, train acc: 0.9271\n",
      "loss: 0.3493767067790031, train acc: 0.9256\n",
      "loss: 0.31714619845151903, train acc: 0.9303\n",
      "loss: 0.34402723908424376, train acc: 0.9277\n",
      "loss: 0.33399429023265836, train acc: 0.9299\n",
      "loss: 0.3616117686033249, train acc: 0.929\n",
      "epoch: 4, loss: 0.1246928945183754, train acc: 0.929, test acc: 0.9162\n",
      "loss: 0.3149261772632599, train acc: 0.9338\n",
      "loss: 0.2990058645606041, train acc: 0.9332\n",
      "loss: 0.3381548643112183, train acc: 0.9313\n",
      "loss: 0.3318853974342346, train acc: 0.9271\n",
      "loss: 0.2962790533900261, train acc: 0.9339\n",
      "loss: 0.31372126638889314, train acc: 0.9326\n",
      "loss: 0.32485159039497374, train acc: 0.9338\n",
      "loss: 0.33455789387226104, train acc: 0.9356\n",
      "epoch: 5, loss: 0.10952897369861603, train acc: 0.9356, test acc: 0.9161\n",
      "loss: 0.3461925685405731, train acc: 0.9387\n",
      "loss: 0.3036469489336014, train acc: 0.9372\n",
      "loss: 0.32048315554857254, train acc: 0.9365\n",
      "loss: 0.3154182225465775, train acc: 0.9349\n",
      "loss: 0.27004400342702867, train acc: 0.938\n",
      "loss: 0.2996174097061157, train acc: 0.9376\n",
      "loss: 0.30424813032150266, train acc: 0.9385\n",
      "loss: 0.315302374958992, train acc: 0.9404\n",
      "epoch: 6, loss: 0.15181955695152283, train acc: 0.9404, test acc: 0.9204\n",
      "loss: 0.3026013970375061, train acc: 0.9413\n",
      "loss: 0.2738586962223053, train acc: 0.9412\n",
      "loss: 0.3232573062181473, train acc: 0.941\n",
      "loss: 0.26954240202903745, train acc: 0.938\n",
      "loss: 0.28060677349567414, train acc: 0.9423\n",
      "loss: 0.29218811690807345, train acc: 0.9415\n",
      "loss: 0.30975563675165174, train acc: 0.9431\n",
      "loss: 0.29551787972450255, train acc: 0.9409\n",
      "epoch: 7, loss: 0.07755804806947708, train acc: 0.9409, test acc: 0.9205\n",
      "loss: 0.2746894955635071, train acc: 0.9442\n",
      "loss: 0.2629771426320076, train acc: 0.9426\n",
      "loss: 0.3082409739494324, train acc: 0.9437\n",
      "loss: 0.2882508859038353, train acc: 0.9419\n",
      "loss: 0.25519929677248, train acc: 0.9447\n",
      "loss: 0.24893728643655777, train acc: 0.9438\n",
      "loss: 0.2927461892366409, train acc: 0.9452\n",
      "loss: 0.28599428832530976, train acc: 0.9446\n",
      "epoch: 8, loss: 0.047827187925577164, train acc: 0.9446, test acc: 0.9241\n",
      "loss: 0.26591017842292786, train acc: 0.9482\n",
      "loss: 0.2445705398917198, train acc: 0.9472\n",
      "loss: 0.29354845732450485, train acc: 0.9468\n",
      "loss: 0.26015097796916964, train acc: 0.9467\n",
      "loss: 0.2412881150841713, train acc: 0.948\n",
      "loss: 0.26655309945344924, train acc: 0.9468\n",
      "loss: 0.28141189068555833, train acc: 0.9477\n",
      "loss: 0.26308498382568357, train acc: 0.948\n",
      "epoch: 9, loss: 0.12725752592086792, train acc: 0.948, test acc: 0.9259\n",
      "loss: 0.23322254419326782, train acc: 0.9507\n",
      "loss: 0.23000311553478242, train acc: 0.9491\n",
      "loss: 0.2872007802128792, train acc: 0.9499\n",
      "loss: 0.2598048895597458, train acc: 0.9474\n",
      "loss: 0.2201196402311325, train acc: 0.9506\n",
      "loss: 0.24568094164133072, train acc: 0.9517\n",
      "loss: 0.26008773893117904, train acc: 0.9511\n",
      "loss: 0.26508415788412093, train acc: 0.9526\n",
      "epoch: 10, loss: 0.056130267679691315, train acc: 0.9526, test acc: 0.9245\n",
      "loss: 0.2750512361526489, train acc: 0.9532\n",
      "loss: 0.2394556298851967, train acc: 0.9513\n",
      "loss: 0.2769977867603302, train acc: 0.9541\n",
      "loss: 0.2573801979422569, train acc: 0.9504\n",
      "loss: 0.22152759432792662, train acc: 0.9531\n",
      "loss: 0.23424785137176513, train acc: 0.9528\n",
      "loss: 0.26600769311189654, train acc: 0.9541\n",
      "loss: 0.26379109770059583, train acc: 0.953\n",
      "epoch: 11, loss: 0.108828604221344, train acc: 0.953, test acc: 0.9258\n",
      "loss: 0.2663653492927551, train acc: 0.9548\n",
      "loss: 0.23796509504318236, train acc: 0.9523\n",
      "loss: 0.2734148308634758, train acc: 0.956\n",
      "loss: 0.232605642080307, train acc: 0.9524\n",
      "loss: 0.19781709983944892, train acc: 0.9556\n",
      "loss: 0.2399301514029503, train acc: 0.9528\n",
      "loss: 0.2429196149110794, train acc: 0.9537\n",
      "loss: 0.2565142884850502, train acc: 0.9543\n",
      "epoch: 12, loss: 0.02074762061238289, train acc: 0.9543, test acc: 0.9265\n",
      "loss: 0.21913057565689087, train acc: 0.9583\n",
      "loss: 0.2139226973056793, train acc: 0.9557\n",
      "loss: 0.262321874499321, train acc: 0.9568\n",
      "loss: 0.21906844303011894, train acc: 0.9508\n",
      "loss: 0.21131957620382308, train acc: 0.9569\n",
      "loss: 0.2251705303788185, train acc: 0.957\n",
      "loss: 0.2414233162999153, train acc: 0.9556\n",
      "loss: 0.22307090908288957, train acc: 0.9585\n",
      "epoch: 13, loss: 0.024624768644571304, train acc: 0.9585, test acc: 0.9256\n",
      "loss: 0.3096315860748291, train acc: 0.9585\n",
      "loss: 0.230526402592659, train acc: 0.9574\n",
      "loss: 0.2522568076848984, train acc: 0.9563\n",
      "loss: 0.23464464545249938, train acc: 0.9534\n",
      "loss: 0.20116475075483323, train acc: 0.9573\n",
      "loss: 0.2148078352212906, train acc: 0.9586\n",
      "loss: 0.22238629311323166, train acc: 0.9587\n",
      "loss: 0.22425188720226288, train acc: 0.9587\n",
      "epoch: 14, loss: 0.050379879772663116, train acc: 0.9587, test acc: 0.9288\n",
      "loss: 0.21928979456424713, train acc: 0.9611\n",
      "loss: 0.194966821372509, train acc: 0.9601\n",
      "loss: 0.2567716747522354, train acc: 0.9602\n",
      "loss: 0.19800313413143159, train acc: 0.954\n",
      "loss: 0.19315492063760759, train acc: 0.9599\n",
      "loss: 0.22291496247053147, train acc: 0.9602\n",
      "loss: 0.22703203707933425, train acc: 0.9613\n",
      "loss: 0.22056813910603523, train acc: 0.9622\n",
      "epoch: 15, loss: 0.03786854073405266, train acc: 0.9622, test acc: 0.9287\n",
      "loss: 0.2077466994524002, train acc: 0.9625\n",
      "loss: 0.20206717401742935, train acc: 0.9614\n",
      "loss: 0.23916493505239486, train acc: 0.9621\n",
      "loss: 0.20504415780305862, train acc: 0.9581\n",
      "loss: 0.20868190079927446, train acc: 0.9613\n",
      "loss: 0.21533362418413163, train acc: 0.9616\n",
      "loss: 0.21850887387990953, train acc: 0.9632\n",
      "loss: 0.22291375398635865, train acc: 0.9627\n",
      "epoch: 16, loss: 0.0658496767282486, train acc: 0.9627, test acc: 0.9302\n",
      "loss: 0.2474983036518097, train acc: 0.9637\n",
      "loss: 0.18899860605597496, train acc: 0.9613\n",
      "loss: 0.2389451451599598, train acc: 0.9634\n",
      "loss: 0.20198142379522324, train acc: 0.9613\n",
      "loss: 0.17510685175657273, train acc: 0.9629\n",
      "loss: 0.20439144521951674, train acc: 0.962\n",
      "loss: 0.23089235574007033, train acc: 0.9619\n",
      "loss: 0.20834963768720627, train acc: 0.9637\n",
      "epoch: 17, loss: 0.01345872227102518, train acc: 0.9637, test acc: 0.9302\n",
      "loss: 0.2417474240064621, train acc: 0.9653\n",
      "loss: 0.17961181774735452, train acc: 0.9635\n",
      "loss: 0.2218001216650009, train acc: 0.9643\n",
      "loss: 0.19121002331376075, train acc: 0.9608\n",
      "loss: 0.17488046288490294, train acc: 0.963\n",
      "loss: 0.19452418833971025, train acc: 0.9638\n",
      "loss: 0.2076419860124588, train acc: 0.9658\n",
      "loss: 0.1887570858001709, train acc: 0.9654\n",
      "epoch: 18, loss: 0.019678855314850807, train acc: 0.9654, test acc: 0.9311\n",
      "loss: 0.1898646056652069, train acc: 0.9669\n",
      "loss: 0.18054158762097358, train acc: 0.9659\n",
      "loss: 0.22017478197813034, train acc: 0.9658\n",
      "loss: 0.186916434019804, train acc: 0.9638\n",
      "loss: 0.16475673988461495, train acc: 0.9639\n",
      "loss: 0.18386012464761733, train acc: 0.9656\n",
      "loss: 0.20288999676704406, train acc: 0.967\n",
      "loss: 0.1872356615960598, train acc: 0.9659\n",
      "epoch: 19, loss: 0.01515298243612051, train acc: 0.9659, test acc: 0.931\n",
      "loss: 0.2093619406223297, train acc: 0.9674\n",
      "loss: 0.17329295575618744, train acc: 0.9676\n",
      "loss: 0.23317495882511138, train acc: 0.9663\n",
      "loss: 0.16928367987275122, train acc: 0.9645\n",
      "loss: 0.16731688007712364, train acc: 0.966\n",
      "loss: 0.18182364404201506, train acc: 0.9663\n",
      "loss: 0.2014535076916218, train acc: 0.9667\n",
      "loss: 0.19432414025068284, train acc: 0.9675\n",
      "epoch: 20, loss: 0.018500858917832375, train acc: 0.9675, test acc: 0.9302\n",
      "loss: 0.20806382596492767, train acc: 0.968\n",
      "loss: 0.17534220069646836, train acc: 0.9676\n",
      "loss: 0.2106665015220642, train acc: 0.9685\n",
      "loss: 0.18845685496926307, train acc: 0.9652\n",
      "loss: 0.16842446476221085, train acc: 0.9668\n",
      "loss: 0.18107312619686128, train acc: 0.9665\n",
      "loss: 0.18229580968618392, train acc: 0.9681\n",
      "loss: 0.2019558772444725, train acc: 0.9689\n",
      "epoch: 21, loss: 0.02890745922923088, train acc: 0.9689, test acc: 0.9315\n",
      "loss: 0.21329233050346375, train acc: 0.9701\n",
      "loss: 0.1636988863348961, train acc: 0.969\n",
      "loss: 0.20990617275238038, train acc: 0.9708\n",
      "loss: 0.17880491241812707, train acc: 0.9671\n",
      "loss: 0.1597124643623829, train acc: 0.9672\n",
      "loss: 0.1682645082473755, train acc: 0.9687\n",
      "loss: 0.17893549129366876, train acc: 0.9673\n",
      "loss: 0.19399155974388121, train acc: 0.97\n",
      "epoch: 22, loss: 0.017584452405571938, train acc: 0.97, test acc: 0.9309\n",
      "loss: 0.22823937237262726, train acc: 0.9701\n",
      "loss: 0.17437002658843995, train acc: 0.9675\n",
      "loss: 0.20850224122405053, train acc: 0.9707\n",
      "loss: 0.17103326320648193, train acc: 0.9645\n",
      "loss: 0.14773759841918946, train acc: 0.9678\n",
      "loss: 0.17266697213053703, train acc: 0.9702\n",
      "loss: 0.16925525814294815, train acc: 0.9719\n",
      "loss: 0.18349692821502686, train acc: 0.9712\n",
      "epoch: 23, loss: 0.060918476432561874, train acc: 0.9712, test acc: 0.9312\n",
      "loss: 0.18316447734832764, train acc: 0.9715\n",
      "loss: 0.15606299564242362, train acc: 0.9711\n",
      "loss: 0.19251630008220671, train acc: 0.9726\n",
      "loss: 0.162064179033041, train acc: 0.9686\n",
      "loss: 0.16035928949713707, train acc: 0.9704\n",
      "loss: 0.1581011436879635, train acc: 0.9707\n",
      "loss: 0.18284668549895286, train acc: 0.9713\n",
      "loss: 0.17051801458001137, train acc: 0.9734\n",
      "epoch: 24, loss: 0.0115898372605443, train acc: 0.9734, test acc: 0.9307\n",
      "loss: 0.20977866649627686, train acc: 0.9737\n",
      "loss: 0.15525201037526132, train acc: 0.972\n",
      "loss: 0.20940093249082564, train acc: 0.9722\n",
      "loss: 0.15722547098994255, train acc: 0.9672\n",
      "loss: 0.146588534116745, train acc: 0.9703\n",
      "loss: 0.1617148295044899, train acc: 0.9739\n",
      "loss: 0.17373499870300294, train acc: 0.9718\n",
      "loss: 0.18892055228352547, train acc: 0.9738\n",
      "epoch: 25, loss: 0.026656029745936394, train acc: 0.9738, test acc: 0.9313\n",
      "loss: 0.19427064061164856, train acc: 0.9729\n",
      "loss: 0.15544373691082, train acc: 0.9737\n",
      "loss: 0.18882380053400993, train acc: 0.9756\n",
      "loss: 0.15258229225873948, train acc: 0.9675\n",
      "loss: 0.1509128712117672, train acc: 0.9737\n",
      "loss: 0.1622077740728855, train acc: 0.9736\n",
      "loss: 0.16383154317736626, train acc: 0.9742\n",
      "loss: 0.1714327096939087, train acc: 0.9753\n",
      "epoch: 26, loss: 0.0628843754529953, train acc: 0.9753, test acc: 0.9325\n",
      "loss: 0.20801468193531036, train acc: 0.9738\n",
      "loss: 0.15253884121775627, train acc: 0.9734\n",
      "loss: 0.1877908080816269, train acc: 0.9747\n",
      "loss: 0.16003119722008705, train acc: 0.9662\n",
      "loss: 0.1526229590177536, train acc: 0.9743\n",
      "loss: 0.16136059165000916, train acc: 0.9743\n",
      "loss: 0.15699025020003318, train acc: 0.9725\n",
      "loss: 0.17873913571238517, train acc: 0.9751\n",
      "epoch: 27, loss: 0.020303940400481224, train acc: 0.9751, test acc: 0.9313\n",
      "loss: 0.19572988152503967, train acc: 0.9758\n",
      "loss: 0.15312876999378205, train acc: 0.9737\n",
      "loss: 0.18495487794280052, train acc: 0.9761\n",
      "loss: 0.15384726226329803, train acc: 0.9712\n",
      "loss: 0.1335366044193506, train acc: 0.9729\n",
      "loss: 0.16261674240231513, train acc: 0.9748\n",
      "loss: 0.1510520361363888, train acc: 0.9755\n",
      "loss: 0.15998009070754052, train acc: 0.9767\n",
      "epoch: 28, loss: 0.027521207928657532, train acc: 0.9767, test acc: 0.9315\n",
      "loss: 0.1953890025615692, train acc: 0.9757\n",
      "loss: 0.14234983511269092, train acc: 0.9758\n",
      "loss: 0.17861991077661515, train acc: 0.9769\n",
      "loss: 0.1736268252134323, train acc: 0.972\n",
      "loss: 0.1378691166639328, train acc: 0.9735\n",
      "loss: 0.1526064917445183, train acc: 0.975\n",
      "loss: 0.14886953309178352, train acc: 0.973\n",
      "loss: 0.16146236062049865, train acc: 0.9767\n",
      "epoch: 29, loss: 0.039195895195007324, train acc: 0.9767, test acc: 0.9321\n",
      "loss: 0.14409324526786804, train acc: 0.9784\n",
      "loss: 0.13828358799219131, train acc: 0.9769\n",
      "loss: 0.18006108105182647, train acc: 0.9784\n",
      "loss: 0.1394767016172409, train acc: 0.9726\n",
      "loss: 0.1333352118730545, train acc: 0.9757\n",
      "loss: 0.16637119129300118, train acc: 0.976\n",
      "loss: 0.1525106430053711, train acc: 0.9753\n",
      "loss: 0.16215713918209076, train acc: 0.977\n",
      "epoch: 30, loss: 0.022187840193510056, train acc: 0.977, test acc: 0.9335\n",
      "loss: 0.1506103277206421, train acc: 0.9792\n",
      "loss: 0.1292266681790352, train acc: 0.9768\n",
      "loss: 0.17921457439661026, train acc: 0.9789\n",
      "loss: 0.13573771491646766, train acc: 0.9764\n",
      "loss: 0.13694558814167976, train acc: 0.977\n",
      "loss: 0.1453087843954563, train acc: 0.9777\n",
      "loss: 0.15814720168709756, train acc: 0.9776\n",
      "loss: 0.14006013348698615, train acc: 0.9791\n",
      "epoch: 31, loss: 0.0647510439157486, train acc: 0.9791, test acc: 0.9302\n",
      "loss: 0.2026403695344925, train acc: 0.977\n",
      "loss: 0.14268930554389953, train acc: 0.9785\n",
      "loss: 0.16930695325136186, train acc: 0.9791\n",
      "loss: 0.1421332374215126, train acc: 0.9731\n",
      "loss: 0.11997233405709266, train acc: 0.9781\n",
      "loss: 0.1473277248442173, train acc: 0.9788\n",
      "loss: 0.15208194628357888, train acc: 0.978\n",
      "loss: 0.16130989342927932, train acc: 0.9809\n",
      "epoch: 32, loss: 0.014854928478598595, train acc: 0.9809, test acc: 0.9299\n",
      "loss: 0.22048798203468323, train acc: 0.9801\n",
      "loss: 0.13860389590263367, train acc: 0.9783\n",
      "loss: 0.16042837128043175, train acc: 0.9805\n",
      "loss: 0.13908841758966445, train acc: 0.9734\n",
      "loss: 0.1253414936363697, train acc: 0.9772\n",
      "loss: 0.13776227012276648, train acc: 0.9791\n",
      "loss: 0.1447545327246189, train acc: 0.9762\n",
      "loss: 0.14486925825476646, train acc: 0.9795\n",
      "epoch: 33, loss: 0.017494041472673416, train acc: 0.9795, test acc: 0.9285\n",
      "loss: 0.2054814100265503, train acc: 0.9807\n",
      "loss: 0.13811369612812996, train acc: 0.9821\n",
      "loss: 0.15688555613160132, train acc: 0.9801\n",
      "loss: 0.1403149850666523, train acc: 0.9797\n",
      "loss: 0.12374164015054703, train acc: 0.9773\n",
      "loss: 0.14498088844120502, train acc: 0.9805\n",
      "loss: 0.1457184374332428, train acc: 0.9791\n",
      "loss: 0.1426227793097496, train acc: 0.9817\n",
      "epoch: 34, loss: 0.06213289126753807, train acc: 0.9817, test acc: 0.9328\n",
      "loss: 0.18553230166435242, train acc: 0.9818\n",
      "loss: 0.12452686950564384, train acc: 0.9822\n",
      "loss: 0.15195938050746918, train acc: 0.9805\n",
      "loss: 0.12138582170009612, train acc: 0.978\n",
      "loss: 0.1343804683536291, train acc: 0.9786\n",
      "loss: 0.13689837083220482, train acc: 0.9805\n",
      "loss: 0.13273575603961946, train acc: 0.9789\n",
      "loss: 0.14593935646116735, train acc: 0.9817\n",
      "epoch: 35, loss: 0.013189851306378841, train acc: 0.9817, test acc: 0.9308\n",
      "loss: 0.16993021965026855, train acc: 0.9817\n",
      "loss: 0.1383478619158268, train acc: 0.9824\n",
      "loss: 0.14790966510772705, train acc: 0.9827\n",
      "loss: 0.13641005456447602, train acc: 0.979\n",
      "loss: 0.11362664103507995, train acc: 0.981\n",
      "loss: 0.1300010085105896, train acc: 0.9809\n",
      "loss: 0.12907849624752998, train acc: 0.9815\n",
      "loss: 0.15099600329995155, train acc: 0.9826\n",
      "epoch: 36, loss: 0.00828075036406517, train acc: 0.9826, test acc: 0.9325\n",
      "loss: 0.16947387158870697, train acc: 0.9821\n",
      "loss: 0.12106954455375671, train acc: 0.9836\n",
      "loss: 0.15964309722185135, train acc: 0.9838\n",
      "loss: 0.1329638659954071, train acc: 0.9794\n",
      "loss: 0.10849795639514923, train acc: 0.9826\n",
      "loss: 0.13550185784697533, train acc: 0.9809\n",
      "loss: 0.12099638357758521, train acc: 0.9795\n",
      "loss: 0.135408079251647, train acc: 0.9818\n",
      "epoch: 37, loss: 0.030082378536462784, train acc: 0.9818, test acc: 0.9309\n",
      "loss: 0.1344100385904312, train acc: 0.9834\n",
      "loss: 0.11178500056266785, train acc: 0.9838\n",
      "loss: 0.1614588402211666, train acc: 0.9839\n",
      "loss: 0.13149569407105446, train acc: 0.9779\n",
      "loss: 0.12039110064506531, train acc: 0.9818\n",
      "loss: 0.1346510261297226, train acc: 0.9812\n",
      "loss: 0.13725185096263887, train acc: 0.9808\n",
      "loss: 0.13991605043411254, train acc: 0.9834\n",
      "epoch: 38, loss: 0.025039682164788246, train acc: 0.9834, test acc: 0.931\n",
      "loss: 0.14671280980110168, train acc: 0.9833\n",
      "loss: 0.12507329285144805, train acc: 0.9841\n",
      "loss: 0.15928353890776634, train acc: 0.9843\n",
      "loss: 0.11443863324820995, train acc: 0.9786\n",
      "loss: 0.12656468003988267, train acc: 0.9801\n",
      "loss: 0.1523098938167095, train acc: 0.9824\n",
      "loss: 0.13180598989129066, train acc: 0.9827\n",
      "loss: 0.1241323173046112, train acc: 0.9847\n",
      "epoch: 39, loss: 0.034448254853487015, train acc: 0.9847, test acc: 0.9323\n",
      "loss: 0.12491617351770401, train acc: 0.9847\n",
      "loss: 0.11163498647511005, train acc: 0.984\n",
      "loss: 0.1610642969608307, train acc: 0.9849\n",
      "loss: 0.12418237328529358, train acc: 0.9825\n",
      "loss: 0.1054163035005331, train acc: 0.9813\n",
      "loss: 0.14058029875159264, train acc: 0.9831\n",
      "loss: 0.1396808184683323, train acc: 0.983\n",
      "loss: 0.13943301662802696, train acc: 0.983\n",
      "epoch: 40, loss: 0.012989328242838383, train acc: 0.983, test acc: 0.9303\n",
      "loss: 0.1710866391658783, train acc: 0.9836\n",
      "loss: 0.12567155323922635, train acc: 0.9847\n",
      "loss: 0.13935521245002747, train acc: 0.984\n",
      "loss: 0.12639451697468757, train acc: 0.9792\n",
      "loss: 0.12911061495542525, train acc: 0.9812\n",
      "loss: 0.12083395794034005, train acc: 0.9824\n",
      "loss: 0.13648668453097343, train acc: 0.9808\n",
      "loss: 0.13547851108014583, train acc: 0.9849\n",
      "epoch: 41, loss: 0.005726819392293692, train acc: 0.9849, test acc: 0.9317\n",
      "loss: 0.12279175221920013, train acc: 0.9839\n",
      "loss: 0.09134988635778427, train acc: 0.9854\n",
      "loss: 0.13950111865997314, train acc: 0.9855\n",
      "loss: 0.11012134477496147, train acc: 0.9801\n",
      "loss: 0.11694711595773696, train acc: 0.9837\n",
      "loss: 0.12435107380151748, train acc: 0.984\n",
      "loss: 0.13267264924943448, train acc: 0.9839\n",
      "loss: 0.11979958266019822, train acc: 0.9846\n",
      "epoch: 42, loss: 0.013131249696016312, train acc: 0.9846, test acc: 0.9304\n",
      "loss: 0.10111277550458908, train acc: 0.9851\n",
      "loss: 0.10839118584990501, train acc: 0.9856\n",
      "loss: 0.1423313409090042, train acc: 0.9853\n",
      "loss: 0.11415455564856529, train acc: 0.9801\n",
      "loss: 0.11137124970555305, train acc: 0.9849\n",
      "loss: 0.11148190833628177, train acc: 0.9846\n",
      "loss: 0.12624650560319423, train acc: 0.9857\n",
      "loss: 0.12216722406446934, train acc: 0.9871\n",
      "epoch: 43, loss: 0.013400758616626263, train acc: 0.9871, test acc: 0.9323\n",
      "loss: 0.10141928493976593, train acc: 0.9864\n",
      "loss: 0.11580352298915386, train acc: 0.9868\n",
      "loss: 0.147425851598382, train acc: 0.9865\n",
      "loss: 0.10755507908761501, train acc: 0.9806\n",
      "loss: 0.11693115383386612, train acc: 0.9831\n",
      "loss: 0.1242690771818161, train acc: 0.9856\n",
      "loss: 0.11257358938455582, train acc: 0.9861\n",
      "loss: 0.13251012563705444, train acc: 0.9865\n",
      "epoch: 44, loss: 0.01819535158574581, train acc: 0.9865, test acc: 0.9312\n",
      "loss: 0.14695560932159424, train acc: 0.9852\n",
      "loss: 0.09422502890229226, train acc: 0.987\n",
      "loss: 0.12905937433242798, train acc: 0.9869\n",
      "loss: 0.12465076521039009, train acc: 0.9842\n",
      "loss: 0.10275926366448403, train acc: 0.9822\n",
      "loss: 0.11298608034849167, train acc: 0.9861\n",
      "loss: 0.11496487185359001, train acc: 0.9844\n",
      "loss: 0.12222607992589474, train acc: 0.9862\n",
      "epoch: 45, loss: 0.006377498619258404, train acc: 0.9862, test acc: 0.9312\n",
      "loss: 0.14587368071079254, train acc: 0.9876\n",
      "loss: 0.10260016247630119, train acc: 0.9863\n",
      "loss: 0.1312828239053488, train acc: 0.9877\n",
      "loss: 0.11124937236309052, train acc: 0.9843\n",
      "loss: 0.10553538724780083, train acc: 0.9842\n",
      "loss: 0.1125006303191185, train acc: 0.9843\n",
      "loss: 0.12822696566581726, train acc: 0.9847\n",
      "loss: 0.11813520267605782, train acc: 0.9875\n",
      "epoch: 46, loss: 0.0529552586376667, train acc: 0.9875, test acc: 0.932\n",
      "loss: 0.1358337700366974, train acc: 0.9871\n",
      "loss: 0.10188924372196198, train acc: 0.9866\n",
      "loss: 0.12009049281477928, train acc: 0.9873\n",
      "loss: 0.10787106938660145, train acc: 0.9796\n",
      "loss: 0.10510650426149368, train acc: 0.9846\n",
      "loss: 0.11883497685194015, train acc: 0.9867\n",
      "loss: 0.10599052384495736, train acc: 0.9852\n",
      "loss: 0.13729614950716496, train acc: 0.9882\n",
      "epoch: 47, loss: 0.007877711206674576, train acc: 0.9882, test acc: 0.9288\n",
      "loss: 0.12805108726024628, train acc: 0.9858\n",
      "loss: 0.10565164946019649, train acc: 0.9885\n",
      "loss: 0.12529442608356475, train acc: 0.9891\n",
      "loss: 0.10914023630321026, train acc: 0.9861\n",
      "loss: 0.08424724116921425, train acc: 0.9852\n",
      "loss: 0.13268231749534606, train acc: 0.986\n",
      "loss: 0.11664382554590702, train acc: 0.9878\n",
      "loss: 0.12149550206959248, train acc: 0.989\n",
      "epoch: 48, loss: 0.04249249026179314, train acc: 0.989, test acc: 0.9316\n",
      "loss: 0.10735935717821121, train acc: 0.9878\n",
      "loss: 0.09082293957471847, train acc: 0.9877\n",
      "loss: 0.1308654122054577, train acc: 0.9871\n",
      "loss: 0.10933367349207401, train acc: 0.984\n",
      "loss: 0.10054842606186867, train acc: 0.9855\n",
      "loss: 0.1145558513700962, train acc: 0.9871\n",
      "loss: 0.1269444204866886, train acc: 0.9879\n",
      "loss: 0.10512556992471218, train acc: 0.987\n",
      "epoch: 49, loss: 0.008917068131268024, train acc: 0.987, test acc: 0.9315\n",
      "loss: 0.14442062377929688, train acc: 0.9891\n",
      "loss: 0.09213567972183227, train acc: 0.9889\n",
      "loss: 0.12392452545464039, train acc: 0.9889\n",
      "loss: 0.1005807239562273, train acc: 0.9865\n",
      "loss: 0.08750352226197719, train acc: 0.9858\n",
      "loss: 0.10603785887360573, train acc: 0.9878\n",
      "loss: 0.11976262256503105, train acc: 0.9868\n",
      "loss: 0.11939951926469802, train acc: 0.9888\n",
      "epoch: 50, loss: 0.025233278051018715, train acc: 0.9888, test acc: 0.9314\n",
      "loss: 0.10943533480167389, train acc: 0.9873\n",
      "loss: 0.10664307549595833, train acc: 0.9889\n",
      "loss: 0.11475629359483719, train acc: 0.9888\n",
      "loss: 0.11366270333528519, train acc: 0.9886\n",
      "loss: 0.08895139507949353, train acc: 0.9894\n",
      "loss: 0.11527963690459728, train acc: 0.9886\n",
      "loss: 0.12109030671417713, train acc: 0.988\n",
      "loss: 0.10723094530403614, train acc: 0.9895\n",
      "epoch: 51, loss: 0.03521057218313217, train acc: 0.9895, test acc: 0.9306\n",
      "loss: 0.19125714898109436, train acc: 0.9877\n",
      "loss: 0.10607507936656475, train acc: 0.9892\n",
      "loss: 0.11271071508526802, train acc: 0.9882\n",
      "loss: 0.1102441180497408, train acc: 0.9885\n",
      "loss: 0.11738912016153336, train acc: 0.9891\n",
      "loss: 0.10703711919486522, train acc: 0.988\n",
      "loss: 0.11027630306780338, train acc: 0.9876\n",
      "loss: 0.0992729313671589, train acc: 0.9872\n",
      "epoch: 52, loss: 0.021563466638326645, train acc: 0.9872, test acc: 0.9296\n",
      "loss: 0.12046574801206589, train acc: 0.989\n",
      "loss: 0.08711521252989769, train acc: 0.9912\n",
      "loss: 0.1192552961409092, train acc: 0.9902\n",
      "loss: 0.10560768581926823, train acc: 0.9877\n",
      "loss: 0.10581015273928643, train acc: 0.985\n",
      "loss: 0.1147843535989523, train acc: 0.9908\n",
      "loss: 0.10713444799184799, train acc: 0.9866\n",
      "loss: 0.11068400591611863, train acc: 0.9894\n",
      "epoch: 53, loss: 0.01283726654946804, train acc: 0.9894, test acc: 0.9305\n",
      "loss: 0.10330966114997864, train acc: 0.9887\n",
      "loss: 0.1057413749396801, train acc: 0.991\n",
      "loss: 0.12747639119625093, train acc: 0.9909\n",
      "loss: 0.0955301159992814, train acc: 0.9882\n",
      "loss: 0.09194279089570045, train acc: 0.9892\n",
      "loss: 0.11535084545612335, train acc: 0.9904\n",
      "loss: 0.11100348271429539, train acc: 0.9896\n",
      "loss: 0.10670115277171136, train acc: 0.9895\n",
      "epoch: 54, loss: 0.03693511709570885, train acc: 0.9895, test acc: 0.9297\n",
      "loss: 0.1226935163140297, train acc: 0.9872\n",
      "loss: 0.08659792318940163, train acc: 0.9909\n",
      "loss: 0.10047398023307323, train acc: 0.9911\n",
      "loss: 0.1042140431702137, train acc: 0.9888\n",
      "loss: 0.10268520563840866, train acc: 0.986\n",
      "loss: 0.10242004208266735, train acc: 0.9908\n",
      "loss: 0.09892452917993069, train acc: 0.9886\n",
      "loss: 0.11242079734802246, train acc: 0.9902\n",
      "epoch: 55, loss: 0.00851372815668583, train acc: 0.9902, test acc: 0.9293\n",
      "loss: 0.10128914564847946, train acc: 0.9902\n",
      "loss: 0.10060433819890022, train acc: 0.9916\n",
      "loss: 0.10850696004927159, train acc: 0.9908\n",
      "loss: 0.09190266281366348, train acc: 0.9902\n",
      "loss: 0.08684399351477623, train acc: 0.9889\n",
      "loss: 0.09494984075427056, train acc: 0.9887\n",
      "loss: 0.10889279916882515, train acc: 0.9897\n",
      "loss: 0.08717929422855378, train acc: 0.9905\n",
      "epoch: 56, loss: 0.015709232538938522, train acc: 0.9905, test acc: 0.9288\n",
      "loss: 0.1118328645825386, train acc: 0.9906\n",
      "loss: 0.08351256027817726, train acc: 0.9911\n",
      "loss: 0.12489754818379879, train acc: 0.9906\n",
      "loss: 0.09800870157778263, train acc: 0.9886\n",
      "loss: 0.08679668195545673, train acc: 0.9877\n",
      "loss: 0.09807700663805008, train acc: 0.9908\n",
      "loss: 0.10085221715271472, train acc: 0.989\n",
      "loss: 0.10762405060231686, train acc: 0.9904\n",
      "epoch: 57, loss: 0.0054468512535095215, train acc: 0.9904, test acc: 0.9293\n",
      "loss: 0.07871080189943314, train acc: 0.9891\n",
      "loss: 0.09631817042827606, train acc: 0.992\n",
      "loss: 0.11212304458022118, train acc: 0.9905\n",
      "loss: 0.09033040702342987, train acc: 0.9899\n",
      "loss: 0.07272153086960316, train acc: 0.9875\n",
      "loss: 0.093514509126544, train acc: 0.9902\n",
      "loss: 0.09525795876979828, train acc: 0.9889\n",
      "loss: 0.0974723070859909, train acc: 0.9918\n",
      "epoch: 58, loss: 0.016552265733480453, train acc: 0.9918, test acc: 0.9287\n",
      "loss: 0.13024692237377167, train acc: 0.9904\n",
      "loss: 0.09427872486412525, train acc: 0.9919\n",
      "loss: 0.12972409129142762, train acc: 0.9911\n",
      "loss: 0.09107214398682117, train acc: 0.9896\n",
      "loss: 0.09756763763725758, train acc: 0.988\n",
      "loss: 0.10847672782838344, train acc: 0.9919\n",
      "loss: 0.09047037214040757, train acc: 0.9909\n",
      "loss: 0.11576793380081654, train acc: 0.9914\n",
      "epoch: 59, loss: 0.008230296894907951, train acc: 0.9914, test acc: 0.9297\n",
      "loss: 0.09039584547281265, train acc: 0.9907\n",
      "loss: 0.08603954128921032, train acc: 0.9917\n",
      "loss: 0.10720352455973625, train acc: 0.9905\n",
      "loss: 0.1096790798008442, train acc: 0.9884\n",
      "loss: 0.0855063445866108, train acc: 0.989\n",
      "loss: 0.10004281252622604, train acc: 0.9917\n",
      "loss: 0.10075970329344272, train acc: 0.9908\n",
      "loss: 0.08464336842298507, train acc: 0.9909\n",
      "epoch: 60, loss: 0.015920093283057213, train acc: 0.9909, test acc: 0.9303\n",
      "loss: 0.12123437970876694, train acc: 0.9891\n",
      "loss: 0.0829957339912653, train acc: 0.992\n",
      "loss: 0.09933774434030056, train acc: 0.9907\n",
      "loss: 0.08590677864849568, train acc: 0.9916\n",
      "loss: 0.07793241925537586, train acc: 0.9911\n",
      "loss: 0.11005975343286992, train acc: 0.9911\n",
      "loss: 0.0983414489775896, train acc: 0.9892\n",
      "loss: 0.12315998151898384, train acc: 0.9922\n",
      "epoch: 61, loss: 0.04669538512825966, train acc: 0.9922, test acc: 0.9282\n",
      "loss: 0.09595349431037903, train acc: 0.9911\n",
      "loss: 0.07094368953257799, train acc: 0.9923\n",
      "loss: 0.10786842256784439, train acc: 0.9919\n",
      "loss: 0.07871531508862972, train acc: 0.987\n",
      "loss: 0.08858638629317284, train acc: 0.9888\n",
      "loss: 0.10990793853998185, train acc: 0.9879\n",
      "loss: 0.0923480212688446, train acc: 0.9919\n",
      "loss: 0.09301688447594643, train acc: 0.9921\n",
      "epoch: 62, loss: 0.022736897692084312, train acc: 0.9921, test acc: 0.929\n",
      "loss: 0.0669967457652092, train acc: 0.9919\n",
      "loss: 0.06860393360257148, train acc: 0.9931\n",
      "loss: 0.0873635996133089, train acc: 0.9911\n",
      "loss: 0.09241320043802262, train acc: 0.9922\n",
      "loss: 0.08370082117617131, train acc: 0.9874\n",
      "loss: 0.08474072292447091, train acc: 0.9903\n",
      "loss: 0.08960909992456437, train acc: 0.9919\n",
      "loss: 0.10305035598576069, train acc: 0.9925\n",
      "epoch: 63, loss: 0.043156277388334274, train acc: 0.9925, test acc: 0.9279\n",
      "loss: 0.15813776850700378, train acc: 0.9902\n",
      "loss: 0.09964861385524273, train acc: 0.9912\n",
      "loss: 0.09208684675395488, train acc: 0.9924\n",
      "loss: 0.07345025390386581, train acc: 0.9917\n",
      "loss: 0.09230516403913498, train acc: 0.9885\n",
      "loss: 0.1021461371332407, train acc: 0.9919\n",
      "loss: 0.08835906460881233, train acc: 0.9899\n",
      "loss: 0.09718841463327407, train acc: 0.9924\n",
      "epoch: 64, loss: 0.003874568035826087, train acc: 0.9924, test acc: 0.9292\n",
      "loss: 0.1911477893590927, train acc: 0.9908\n",
      "loss: 0.10021568797528743, train acc: 0.9922\n",
      "loss: 0.0954401034861803, train acc: 0.9915\n",
      "loss: 0.09238843806087971, train acc: 0.9865\n",
      "loss: 0.09089010246098042, train acc: 0.9901\n",
      "loss: 0.09058609530329705, train acc: 0.9897\n",
      "loss: 0.10335931815207004, train acc: 0.9919\n",
      "loss: 0.09877649433910847, train acc: 0.9918\n",
      "epoch: 65, loss: 0.004634405020624399, train acc: 0.9918, test acc: 0.9254\n",
      "loss: 0.14173945784568787, train acc: 0.9891\n",
      "loss: 0.08330879807472229, train acc: 0.9922\n",
      "loss: 0.10128436014056205, train acc: 0.9914\n",
      "loss: 0.0898701649159193, train acc: 0.99\n",
      "loss: 0.08035631701350213, train acc: 0.9882\n",
      "loss: 0.0867372214794159, train acc: 0.9899\n",
      "loss: 0.09556310027837753, train acc: 0.9919\n",
      "loss: 0.102027178555727, train acc: 0.9934\n",
      "epoch: 66, loss: 0.02007007785141468, train acc: 0.9934, test acc: 0.9301\n",
      "loss: 0.16822437942028046, train acc: 0.9935\n",
      "loss: 0.08692647702991962, train acc: 0.9938\n",
      "loss: 0.11205940023064613, train acc: 0.9934\n",
      "loss: 0.09617053382098675, train acc: 0.9927\n",
      "loss: 0.09421530999243259, train acc: 0.9906\n",
      "loss: 0.09726455733180046, train acc: 0.9907\n",
      "loss: 0.0907336413860321, train acc: 0.992\n",
      "loss: 0.11399343572556972, train acc: 0.9931\n",
      "epoch: 67, loss: 0.004495471250265837, train acc: 0.9931, test acc: 0.9287\n",
      "loss: 0.1353311538696289, train acc: 0.9911\n",
      "loss: 0.08161523789167405, train acc: 0.993\n",
      "loss: 0.0902236431837082, train acc: 0.993\n",
      "loss: 0.09700403995811939, train acc: 0.9907\n",
      "loss: 0.08393867984414101, train acc: 0.9894\n",
      "loss: 0.08185921907424927, train acc: 0.9922\n",
      "loss: 0.1039111353456974, train acc: 0.9923\n",
      "loss: 0.11642969995737076, train acc: 0.9926\n",
      "epoch: 68, loss: 0.019840378314256668, train acc: 0.9926, test acc: 0.9307\n",
      "loss: 0.08848464488983154, train acc: 0.9929\n",
      "loss: 0.0791287187486887, train acc: 0.992\n",
      "loss: 0.09815476834774017, train acc: 0.9931\n",
      "loss: 0.06973727438598872, train acc: 0.9907\n",
      "loss: 0.08391517177224159, train acc: 0.9893\n",
      "loss: 0.06862393692135811, train acc: 0.9922\n",
      "loss: 0.08777406029403209, train acc: 0.9936\n",
      "loss: 0.09677164256572723, train acc: 0.9932\n",
      "epoch: 69, loss: 0.004186945501714945, train acc: 0.9932, test acc: 0.9308\n",
      "loss: 0.11293941736221313, train acc: 0.9921\n",
      "loss: 0.07836126014590264, train acc: 0.9939\n",
      "loss: 0.0905192781239748, train acc: 0.994\n",
      "loss: 0.0796585340052843, train acc: 0.9915\n",
      "loss: 0.09436954706907272, train acc: 0.9919\n",
      "loss: 0.08425426334142685, train acc: 0.9928\n",
      "loss: 0.09308964647352695, train acc: 0.9925\n",
      "loss: 0.08842024020850658, train acc: 0.9925\n",
      "epoch: 70, loss: 0.09284065663814545, train acc: 0.9925, test acc: 0.9292\n",
      "loss: 0.1132955551147461, train acc: 0.9929\n",
      "loss: 0.07782054357230664, train acc: 0.9922\n",
      "loss: 0.10422405935823917, train acc: 0.9936\n",
      "loss: 0.08009382523596287, train acc: 0.994\n",
      "loss: 0.07745347507297992, train acc: 0.9927\n",
      "loss: 0.08443179018795491, train acc: 0.9919\n",
      "loss: 0.08732839189469814, train acc: 0.9925\n",
      "loss: 0.09470072574913502, train acc: 0.9936\n",
      "epoch: 71, loss: 0.0014851349405944347, train acc: 0.9936, test acc: 0.9282\n",
      "loss: 0.09170504659414291, train acc: 0.9922\n",
      "loss: 0.0832657802850008, train acc: 0.9934\n",
      "loss: 0.0997113548219204, train acc: 0.9937\n",
      "loss: 0.062450379319489005, train acc: 0.9932\n",
      "loss: 0.07615548148751258, train acc: 0.9887\n",
      "loss: 0.08355361334979534, train acc: 0.9926\n",
      "loss: 0.08790374100208283, train acc: 0.994\n",
      "loss: 0.08167686276137828, train acc: 0.9928\n",
      "epoch: 72, loss: 0.055735994130373, train acc: 0.9928, test acc: 0.9291\n",
      "loss: 0.08835397660732269, train acc: 0.9936\n",
      "loss: 0.06884441282600165, train acc: 0.9937\n",
      "loss: 0.08889821637421846, train acc: 0.994\n",
      "loss: 0.07976587042212487, train acc: 0.9938\n",
      "loss: 0.0711184624582529, train acc: 0.992\n",
      "loss: 0.09599424824118614, train acc: 0.9907\n",
      "loss: 0.08429699782282114, train acc: 0.9924\n",
      "loss: 0.09581825844943523, train acc: 0.9931\n",
      "epoch: 73, loss: 0.017038913443684578, train acc: 0.9931, test acc: 0.9276\n",
      "loss: 0.10776570439338684, train acc: 0.9933\n",
      "loss: 0.06896008849143982, train acc: 0.9929\n",
      "loss: 0.07821696065366268, train acc: 0.9936\n",
      "loss: 0.08753296211361886, train acc: 0.9935\n",
      "loss: 0.07707132250070572, train acc: 0.9913\n",
      "loss: 0.07915470208972693, train acc: 0.9937\n",
      "loss: 0.09059837535023689, train acc: 0.9929\n",
      "loss: 0.09661819338798523, train acc: 0.9925\n",
      "epoch: 74, loss: 0.008680079132318497, train acc: 0.9925, test acc: 0.9291\n",
      "loss: 0.08349931985139847, train acc: 0.9943\n",
      "loss: 0.0794955924153328, train acc: 0.9942\n",
      "loss: 0.08251125551760197, train acc: 0.995\n",
      "loss: 0.06668843915686011, train acc: 0.9923\n",
      "loss: 0.08684685155749321, train acc: 0.9921\n",
      "loss: 0.06652842927724123, train acc: 0.9927\n",
      "loss: 0.09527351073920727, train acc: 0.994\n",
      "loss: 0.07461718320846558, train acc: 0.995\n",
      "epoch: 75, loss: 0.11616271734237671, train acc: 0.995, test acc: 0.9301\n",
      "loss: 0.09228458255529404, train acc: 0.9951\n",
      "loss: 0.07400748021900654, train acc: 0.9934\n",
      "loss: 0.09090742245316505, train acc: 0.9935\n",
      "loss: 0.08970407582819462, train acc: 0.9924\n",
      "loss: 0.06611268948763609, train acc: 0.9928\n",
      "loss: 0.07159793712198734, train acc: 0.9926\n",
      "loss: 0.08929207064211368, train acc: 0.9938\n",
      "loss: 0.07870719023048878, train acc: 0.995\n",
      "epoch: 76, loss: 0.002853481797501445, train acc: 0.995, test acc: 0.9281\n",
      "loss: 0.08828291296958923, train acc: 0.9945\n",
      "loss: 0.082211335003376, train acc: 0.9941\n",
      "loss: 0.09692981094121933, train acc: 0.9951\n",
      "loss: 0.076179014518857, train acc: 0.9931\n",
      "loss: 0.07301735803484917, train acc: 0.9935\n",
      "loss: 0.09005845785140991, train acc: 0.9934\n",
      "loss: 0.08859391771256923, train acc: 0.9942\n",
      "loss: 0.08751152567565441, train acc: 0.9947\n",
      "epoch: 77, loss: 0.008406377397477627, train acc: 0.9947, test acc: 0.931\n",
      "loss: 0.06534772366285324, train acc: 0.9948\n",
      "loss: 0.06939740218222142, train acc: 0.9953\n",
      "loss: 0.08537512682378293, train acc: 0.9958\n",
      "loss: 0.070787076279521, train acc: 0.9959\n",
      "loss: 0.06825566440820693, train acc: 0.9902\n",
      "loss: 0.07889791131019593, train acc: 0.9932\n",
      "loss: 0.08748666904866695, train acc: 0.9938\n",
      "loss: 0.08712629005312919, train acc: 0.9941\n",
      "epoch: 78, loss: 0.004696284420788288, train acc: 0.9941, test acc: 0.9278\n",
      "loss: 0.11565394699573517, train acc: 0.9919\n",
      "loss: 0.09628070816397667, train acc: 0.9922\n",
      "loss: 0.08730080761015416, train acc: 0.9946\n",
      "loss: 0.06917289085686207, train acc: 0.9932\n",
      "loss: 0.060022320970892905, train acc: 0.9927\n",
      "loss: 0.08313529714941978, train acc: 0.9942\n",
      "loss: 0.08230979815125465, train acc: 0.9953\n",
      "loss: 0.08876235783100128, train acc: 0.9934\n",
      "epoch: 79, loss: 0.018353136256337166, train acc: 0.9934, test acc: 0.928\n",
      "#####training and testing end with K:20, P:0.1######\n",
      "#####training and testing start with K:20, P:0.5######\n",
      "loss: 2.344951868057251, train acc: 0.1567\n",
      "loss: 2.193691444396973, train acc: 0.3783\n",
      "loss: 1.9531484723091126, train acc: 0.5342\n",
      "loss: 1.737277913093567, train acc: 0.6298\n",
      "loss: 1.5470762848854065, train acc: 0.7334\n",
      "loss: 1.428947925567627, train acc: 0.7782\n",
      "loss: 1.2911894559860229, train acc: 0.797\n",
      "loss: 1.2373045086860657, train acc: 0.827\n",
      "epoch: 0, loss: 1.0827807188034058, train acc: 0.827, test acc: 0.8384\n",
      "loss: 1.212923288345337, train acc: 0.842\n",
      "loss: 1.1056155920028687, train acc: 0.8381\n",
      "loss: 1.084629237651825, train acc: 0.8531\n",
      "loss: 1.0606304705142975, train acc: 0.8605\n",
      "loss: 1.0050146281719208, train acc: 0.8712\n",
      "loss: 0.9713209331035614, train acc: 0.8746\n",
      "loss: 0.9513960003852844, train acc: 0.8794\n",
      "loss: 0.9510498344898224, train acc: 0.8841\n",
      "epoch: 1, loss: 0.7279256582260132, train acc: 0.8841, test acc: 0.8785\n",
      "loss: 0.9005011916160583, train acc: 0.8838\n",
      "loss: 0.9313468158245086, train acc: 0.8812\n",
      "loss: 0.8733207762241364, train acc: 0.8892\n",
      "loss: 0.8894438862800598, train acc: 0.892\n",
      "loss: 0.8921348273754119, train acc: 0.8952\n",
      "loss: 0.8450157284736634, train acc: 0.8954\n",
      "loss: 0.813503521680832, train acc: 0.8977\n",
      "loss: 0.807046091556549, train acc: 0.8981\n",
      "epoch: 2, loss: 0.4289180338382721, train acc: 0.8981, test acc: 0.8915\n",
      "loss: 0.7619720697402954, train acc: 0.9007\n",
      "loss: 0.7943397998809815, train acc: 0.9007\n",
      "loss: 0.8329215943813324, train acc: 0.8983\n",
      "loss: 0.8388017177581787, train acc: 0.9035\n",
      "loss: 0.7949065685272216, train acc: 0.9054\n",
      "loss: 0.8035959124565124, train acc: 0.9074\n",
      "loss: 0.7881778359413147, train acc: 0.906\n",
      "loss: 0.8040731430053711, train acc: 0.9086\n",
      "epoch: 3, loss: 0.7250833511352539, train acc: 0.9086, test acc: 0.895\n",
      "loss: 0.7663849592208862, train acc: 0.9069\n",
      "loss: 0.7816618025302887, train acc: 0.9053\n",
      "loss: 0.7702698886394501, train acc: 0.9087\n",
      "loss: 0.7711283445358277, train acc: 0.9098\n",
      "loss: 0.7824596703052521, train acc: 0.9114\n",
      "loss: 0.7669666409492493, train acc: 0.9139\n",
      "loss: 0.7495425581932068, train acc: 0.9109\n",
      "loss: 0.7517339587211609, train acc: 0.9119\n",
      "epoch: 4, loss: 0.5690196752548218, train acc: 0.9119, test acc: 0.8972\n",
      "loss: 0.7210103273391724, train acc: 0.9107\n",
      "loss: 0.7441475987434387, train acc: 0.9098\n",
      "loss: 0.7739078998565674, train acc: 0.9137\n",
      "loss: 0.7497113585472107, train acc: 0.9136\n",
      "loss: 0.7376504778862, train acc: 0.9128\n",
      "loss: 0.7609826803207398, train acc: 0.9169\n",
      "loss: 0.7042264223098755, train acc: 0.9177\n",
      "loss: 0.7410614371299744, train acc: 0.9158\n",
      "epoch: 5, loss: 0.6092436909675598, train acc: 0.9158, test acc: 0.9001\n",
      "loss: 0.5541924834251404, train acc: 0.9159\n",
      "loss: 0.7081042408943177, train acc: 0.9171\n",
      "loss: 0.7208623647689819, train acc: 0.9177\n",
      "loss: 0.7173315465450287, train acc: 0.9182\n",
      "loss: 0.6977320551872254, train acc: 0.9231\n",
      "loss: 0.6986557871103287, train acc: 0.9191\n",
      "loss: 0.6804503858089447, train acc: 0.919\n",
      "loss: 0.662431252002716, train acc: 0.9209\n",
      "epoch: 6, loss: 0.6263632774353027, train acc: 0.9209, test acc: 0.9035\n",
      "loss: 0.653992235660553, train acc: 0.92\n",
      "loss: 0.6820671617984772, train acc: 0.9182\n",
      "loss: 0.680222338438034, train acc: 0.9216\n",
      "loss: 0.717436021566391, train acc: 0.9203\n",
      "loss: 0.7298497140407563, train acc: 0.9237\n",
      "loss: 0.7264043152332306, train acc: 0.9245\n",
      "loss: 0.7077111542224884, train acc: 0.9242\n",
      "loss: 0.6470895230770111, train acc: 0.9218\n",
      "epoch: 7, loss: 0.4129357635974884, train acc: 0.9218, test acc: 0.9056\n",
      "loss: 0.5450381636619568, train acc: 0.9239\n",
      "loss: 0.669627982378006, train acc: 0.9231\n",
      "loss: 0.6776151359081268, train acc: 0.9235\n",
      "loss: 0.6854699671268463, train acc: 0.923\n",
      "loss: 0.7153853237628937, train acc: 0.9248\n",
      "loss: 0.6760881006717682, train acc: 0.9264\n",
      "loss: 0.6665182709693909, train acc: 0.9263\n",
      "loss: 0.6656513988971711, train acc: 0.9257\n",
      "epoch: 8, loss: 0.36049774289131165, train acc: 0.9257, test acc: 0.906\n",
      "loss: 0.6450207829475403, train acc: 0.9257\n",
      "loss: 0.6498180329799652, train acc: 0.9262\n",
      "loss: 0.666258579492569, train acc: 0.9277\n",
      "loss: 0.7118631780147553, train acc: 0.9246\n",
      "loss: 0.6880121946334838, train acc: 0.9305\n",
      "loss: 0.6461224675178527, train acc: 0.9284\n",
      "loss: 0.6081781685352325, train acc: 0.9283\n",
      "loss: 0.6628634095191955, train acc: 0.9281\n",
      "epoch: 9, loss: 0.46650299429893494, train acc: 0.9281, test acc: 0.9049\n",
      "loss: 0.5495513081550598, train acc: 0.9277\n",
      "loss: 0.6349383890628815, train acc: 0.927\n",
      "loss: 0.6829099416732788, train acc: 0.9283\n",
      "loss: 0.683906489610672, train acc: 0.9278\n",
      "loss: 0.6503305554389953, train acc: 0.928\n",
      "loss: 0.6763386607170105, train acc: 0.9285\n",
      "loss: 0.6166422843933106, train acc: 0.929\n",
      "loss: 0.6615263104438782, train acc: 0.9302\n",
      "epoch: 10, loss: 0.3090571165084839, train acc: 0.9302, test acc: 0.9081\n",
      "loss: 0.5349538922309875, train acc: 0.9323\n",
      "loss: 0.6933419287204743, train acc: 0.9296\n",
      "loss: 0.6507546305656433, train acc: 0.9323\n",
      "loss: 0.6683672726154327, train acc: 0.93\n",
      "loss: 0.684203028678894, train acc: 0.9303\n",
      "loss: 0.6520798325538635, train acc: 0.9312\n",
      "loss: 0.6570889294147492, train acc: 0.9299\n",
      "loss: 0.626181697845459, train acc: 0.9304\n",
      "epoch: 11, loss: 0.6730502247810364, train acc: 0.9304, test acc: 0.905\n",
      "loss: 0.5807808041572571, train acc: 0.9307\n",
      "loss: 0.6300000965595245, train acc: 0.9299\n",
      "loss: 0.6407582998275757, train acc: 0.9287\n",
      "loss: 0.6929325699806214, train acc: 0.9296\n",
      "loss: 0.6701454997062684, train acc: 0.9308\n",
      "loss: 0.6690026640892028, train acc: 0.9329\n",
      "loss: 0.6189992040395736, train acc: 0.9325\n",
      "loss: 0.5914048671722412, train acc: 0.9332\n",
      "epoch: 12, loss: 0.5841041207313538, train acc: 0.9332, test acc: 0.9102\n",
      "loss: 0.6025848984718323, train acc: 0.9319\n",
      "loss: 0.6294321894645691, train acc: 0.9288\n",
      "loss: 0.6085392624139786, train acc: 0.9321\n",
      "loss: 0.7165827214717865, train acc: 0.9322\n",
      "loss: 0.6309442937374115, train acc: 0.931\n",
      "loss: 0.64918652176857, train acc: 0.9334\n",
      "loss: 0.6279109418392181, train acc: 0.9288\n",
      "loss: 0.6107855707406997, train acc: 0.9308\n",
      "epoch: 13, loss: 0.540827751159668, train acc: 0.9308, test acc: 0.9054\n",
      "loss: 0.6199721693992615, train acc: 0.9318\n",
      "loss: 0.6152596473693848, train acc: 0.9324\n",
      "loss: 0.6609080016613007, train acc: 0.9321\n",
      "loss: 0.6265143871307373, train acc: 0.9308\n",
      "loss: 0.641527259349823, train acc: 0.9335\n",
      "loss: 0.6376345962285995, train acc: 0.9339\n",
      "loss: 0.6187972158193589, train acc: 0.9302\n",
      "loss: 0.6313520461320877, train acc: 0.933\n",
      "epoch: 14, loss: 0.5433055758476257, train acc: 0.933, test acc: 0.909\n",
      "loss: 0.6286572217941284, train acc: 0.9325\n",
      "loss: 0.6212725281715393, train acc: 0.933\n",
      "loss: 0.6721840560436249, train acc: 0.9332\n",
      "loss: 0.6268783748149872, train acc: 0.9343\n",
      "loss: 0.6330648899078369, train acc: 0.9363\n",
      "loss: 0.6111868351697922, train acc: 0.9377\n",
      "loss: 0.6483383774757385, train acc: 0.9372\n",
      "loss: 0.6142790913581848, train acc: 0.9356\n",
      "epoch: 15, loss: 0.43481701612472534, train acc: 0.9356, test acc: 0.9107\n",
      "loss: 0.5236031413078308, train acc: 0.9346\n",
      "loss: 0.6272288680076599, train acc: 0.9361\n",
      "loss: 0.577313369512558, train acc: 0.9362\n",
      "loss: 0.6255624711513519, train acc: 0.9356\n",
      "loss: 0.6092169910669327, train acc: 0.9381\n",
      "loss: 0.6335124939680099, train acc: 0.9383\n",
      "loss: 0.6126922309398651, train acc: 0.939\n",
      "loss: 0.5822869777679444, train acc: 0.9345\n",
      "epoch: 16, loss: 0.34721076488494873, train acc: 0.9345, test acc: 0.9099\n",
      "loss: 0.7211489677429199, train acc: 0.937\n",
      "loss: 0.6252426505088806, train acc: 0.9348\n",
      "loss: 0.6190280795097352, train acc: 0.9369\n",
      "loss: 0.6778140723705292, train acc: 0.9348\n",
      "loss: 0.6385923504829407, train acc: 0.9383\n",
      "loss: 0.632742977142334, train acc: 0.9398\n",
      "loss: 0.6385253727436065, train acc: 0.938\n",
      "loss: 0.6099298626184464, train acc: 0.9384\n",
      "epoch: 17, loss: 0.20854178071022034, train acc: 0.9384, test acc: 0.9087\n",
      "loss: 0.5961533188819885, train acc: 0.9349\n",
      "loss: 0.5858833611011505, train acc: 0.9357\n",
      "loss: 0.6073397517204284, train acc: 0.9385\n",
      "loss: 0.6139260292053222, train acc: 0.9365\n",
      "loss: 0.6267563879489899, train acc: 0.938\n",
      "loss: 0.5949822545051575, train acc: 0.9402\n",
      "loss: 0.5859121382236481, train acc: 0.9396\n",
      "loss: 0.5801826655864716, train acc: 0.9388\n",
      "epoch: 18, loss: 0.2221595197916031, train acc: 0.9388, test acc: 0.9103\n",
      "loss: 0.5749646425247192, train acc: 0.9392\n",
      "loss: 0.5701646238565445, train acc: 0.9384\n",
      "loss: 0.6114408284425735, train acc: 0.9387\n",
      "loss: 0.6205095469951629, train acc: 0.939\n",
      "loss: 0.5992253422737122, train acc: 0.9391\n",
      "loss: 0.6233654081821441, train acc: 0.9402\n",
      "loss: 0.6250694811344146, train acc: 0.9407\n",
      "loss: 0.5818398654460907, train acc: 0.9377\n",
      "epoch: 19, loss: 0.19626468420028687, train acc: 0.9377, test acc: 0.9103\n",
      "loss: 0.5102987289428711, train acc: 0.937\n",
      "loss: 0.5713014662265777, train acc: 0.9383\n",
      "loss: 0.6006448566913605, train acc: 0.9402\n",
      "loss: 0.6310602605342865, train acc: 0.94\n",
      "loss: 0.5978124290704727, train acc: 0.9399\n",
      "loss: 0.6023882299661636, train acc: 0.9421\n",
      "loss: 0.5774634540081024, train acc: 0.9413\n",
      "loss: 0.5873346000909805, train acc: 0.9406\n",
      "epoch: 20, loss: 0.35889843106269836, train acc: 0.9406, test acc: 0.9117\n",
      "loss: 0.5774919986724854, train acc: 0.9384\n",
      "loss: 0.5815149962902069, train acc: 0.938\n",
      "loss: 0.5608963251113892, train acc: 0.9398\n",
      "loss: 0.6214850604534149, train acc: 0.9382\n",
      "loss: 0.5796674758195877, train acc: 0.9404\n",
      "loss: 0.6174108028411865, train acc: 0.9417\n",
      "loss: 0.6524417579174042, train acc: 0.9399\n",
      "loss: 0.5593596428632737, train acc: 0.9406\n",
      "epoch: 21, loss: 0.21243636310100555, train acc: 0.9406, test acc: 0.9107\n",
      "loss: 0.519349217414856, train acc: 0.9411\n",
      "loss: 0.5882171809673309, train acc: 0.9407\n",
      "loss: 0.5829335510730743, train acc: 0.9392\n",
      "loss: 0.628273269534111, train acc: 0.9392\n",
      "loss: 0.6198882162570953, train acc: 0.941\n",
      "loss: 0.5734096050262452, train acc: 0.9414\n",
      "loss: 0.5923858553171157, train acc: 0.9423\n",
      "loss: 0.5772386431694031, train acc: 0.9414\n",
      "epoch: 22, loss: 0.3756089210510254, train acc: 0.9414, test acc: 0.9108\n",
      "loss: 0.6235988140106201, train acc: 0.9408\n",
      "loss: 0.5919978439807891, train acc: 0.9417\n",
      "loss: 0.5692436456680298, train acc: 0.9428\n",
      "loss: 0.5958516508340835, train acc: 0.941\n",
      "loss: 0.560818886756897, train acc: 0.9405\n",
      "loss: 0.6138944089412689, train acc: 0.9444\n",
      "loss: 0.6114360213279724, train acc: 0.9426\n",
      "loss: 0.5734902173280716, train acc: 0.9427\n",
      "epoch: 23, loss: 0.30571991205215454, train acc: 0.9427, test acc: 0.9139\n",
      "loss: 0.602595329284668, train acc: 0.9425\n",
      "loss: 0.5799501419067383, train acc: 0.941\n",
      "loss: 0.5558335512876511, train acc: 0.9409\n",
      "loss: 0.6181817024946212, train acc: 0.9408\n",
      "loss: 0.5476412981748581, train acc: 0.9423\n",
      "loss: 0.5888851106166839, train acc: 0.9454\n",
      "loss: 0.6094531148672104, train acc: 0.9431\n",
      "loss: 0.5784202188253402, train acc: 0.9424\n",
      "epoch: 24, loss: 0.3401365578174591, train acc: 0.9424, test acc: 0.913\n",
      "loss: 0.5410379767417908, train acc: 0.9429\n",
      "loss: 0.542451411485672, train acc: 0.9415\n",
      "loss: 0.5678549170494079, train acc: 0.9403\n",
      "loss: 0.6268659949302673, train acc: 0.9389\n",
      "loss: 0.6286984801292419, train acc: 0.9449\n",
      "loss: 0.6149093508720398, train acc: 0.946\n",
      "loss: 0.599590402841568, train acc: 0.9413\n",
      "loss: 0.5637588500976562, train acc: 0.9429\n",
      "epoch: 25, loss: 0.5316241383552551, train acc: 0.9429, test acc: 0.9114\n",
      "loss: 0.6046022176742554, train acc: 0.9422\n",
      "loss: 0.5723481863737107, train acc: 0.9422\n",
      "loss: 0.5523020595312118, train acc: 0.9436\n",
      "loss: 0.5805422842502594, train acc: 0.9435\n",
      "loss: 0.6031992554664611, train acc: 0.9428\n",
      "loss: 0.5858174920082092, train acc: 0.9454\n",
      "loss: 0.5705386102199554, train acc: 0.9425\n",
      "loss: 0.5530153214931488, train acc: 0.9434\n",
      "epoch: 26, loss: 0.23780333995819092, train acc: 0.9434, test acc: 0.9131\n",
      "loss: 0.4346122145652771, train acc: 0.9413\n",
      "loss: 0.5697349518537521, train acc: 0.9434\n",
      "loss: 0.5659112751483917, train acc: 0.9432\n",
      "loss: 0.5945487976074219, train acc: 0.9427\n",
      "loss: 0.5593723207712173, train acc: 0.9464\n",
      "loss: 0.5783532828092575, train acc: 0.9464\n",
      "loss: 0.5802589297294617, train acc: 0.945\n",
      "loss: 0.5556078940629959, train acc: 0.9441\n",
      "epoch: 27, loss: 0.6022749543190002, train acc: 0.9441, test acc: 0.9143\n",
      "loss: 0.6643825769424438, train acc: 0.9446\n",
      "loss: 0.559940230846405, train acc: 0.9452\n",
      "loss: 0.5519241452217102, train acc: 0.9456\n",
      "loss: 0.6052779912948608, train acc: 0.9447\n",
      "loss: 0.5969102084636688, train acc: 0.9462\n",
      "loss: 0.577274152636528, train acc: 0.9469\n",
      "loss: 0.5573473334312439, train acc: 0.9451\n",
      "loss: 0.5514999836683273, train acc: 0.9455\n",
      "epoch: 28, loss: 0.21618473529815674, train acc: 0.9455, test acc: 0.9137\n",
      "loss: 0.5450785160064697, train acc: 0.9452\n",
      "loss: 0.5099918127059937, train acc: 0.9447\n",
      "loss: 0.5439704835414887, train acc: 0.9442\n",
      "loss: 0.6330927908420563, train acc: 0.9463\n",
      "loss: 0.6188970923423767, train acc: 0.947\n",
      "loss: 0.5706456542015076, train acc: 0.9479\n",
      "loss: 0.5479045510292053, train acc: 0.9465\n",
      "loss: 0.56693294942379, train acc: 0.9459\n",
      "epoch: 29, loss: 0.1959993839263916, train acc: 0.9459, test acc: 0.9119\n",
      "loss: 0.5315582156181335, train acc: 0.9474\n",
      "loss: 0.5732075899839402, train acc: 0.9463\n",
      "loss: 0.54677694439888, train acc: 0.9452\n",
      "loss: 0.605547958612442, train acc: 0.9465\n",
      "loss: 0.5652690798044204, train acc: 0.9463\n",
      "loss: 0.5522059738636017, train acc: 0.9479\n",
      "loss: 0.5588983654975891, train acc: 0.9482\n",
      "loss: 0.5130083560943604, train acc: 0.9462\n",
      "epoch: 30, loss: 0.3549969792366028, train acc: 0.9462, test acc: 0.9126\n",
      "loss: 0.4557570219039917, train acc: 0.946\n",
      "loss: 0.5405918657779694, train acc: 0.9456\n",
      "loss: 0.5132447123527527, train acc: 0.9455\n",
      "loss: 0.5873247921466828, train acc: 0.9462\n",
      "loss: 0.5483906358480454, train acc: 0.9473\n",
      "loss: 0.5771447211503983, train acc: 0.9481\n",
      "loss: 0.59205561876297, train acc: 0.9475\n",
      "loss: 0.566637921333313, train acc: 0.9461\n",
      "epoch: 31, loss: 0.1730937510728836, train acc: 0.9461, test acc: 0.9134\n",
      "loss: 0.4759318232536316, train acc: 0.9468\n",
      "loss: 0.519231840968132, train acc: 0.9463\n",
      "loss: 0.5550226390361785, train acc: 0.9454\n",
      "loss: 0.6057213306427002, train acc: 0.9441\n",
      "loss: 0.5677255749702453, train acc: 0.9463\n",
      "loss: 0.5396497845649719, train acc: 0.9482\n",
      "loss: 0.5702065050601959, train acc: 0.9462\n",
      "loss: 0.5306226044893265, train acc: 0.9465\n",
      "epoch: 32, loss: 0.30221790075302124, train acc: 0.9465, test acc: 0.9125\n",
      "loss: 0.6710624098777771, train acc: 0.9454\n",
      "loss: 0.5720246732234955, train acc: 0.9472\n",
      "loss: 0.5407211363315583, train acc: 0.9464\n",
      "loss: 0.5992504268884659, train acc: 0.9459\n",
      "loss: 0.5045247942209243, train acc: 0.948\n",
      "loss: 0.5695399910211563, train acc: 0.9494\n",
      "loss: 0.5141171753406525, train acc: 0.9463\n",
      "loss: 0.5635969787836075, train acc: 0.9469\n",
      "epoch: 33, loss: 0.19970956444740295, train acc: 0.9469, test acc: 0.9105\n",
      "loss: 0.5083097815513611, train acc: 0.9487\n",
      "loss: 0.5660523772239685, train acc: 0.946\n",
      "loss: 0.5555131018161774, train acc: 0.9476\n",
      "loss: 0.5622764587402344, train acc: 0.9483\n",
      "loss: 0.5447664648294449, train acc: 0.9463\n",
      "loss: 0.5350295156240463, train acc: 0.9494\n",
      "loss: 0.5386366128921509, train acc: 0.9473\n",
      "loss: 0.5137835085391999, train acc: 0.9478\n",
      "epoch: 34, loss: 0.37830960750579834, train acc: 0.9478, test acc: 0.9129\n",
      "loss: 0.4784759283065796, train acc: 0.9471\n",
      "loss: 0.5378059953451156, train acc: 0.944\n",
      "loss: 0.5835046023130417, train acc: 0.9472\n",
      "loss: 0.5417547911405564, train acc: 0.9456\n",
      "loss: 0.5674011558294296, train acc: 0.9467\n",
      "loss: 0.5794685274362564, train acc: 0.9465\n",
      "loss: 0.5538124352693558, train acc: 0.9478\n",
      "loss: 0.5452023565769195, train acc: 0.9478\n",
      "epoch: 35, loss: 0.2378326952457428, train acc: 0.9478, test acc: 0.9125\n",
      "loss: 0.5239169597625732, train acc: 0.9464\n",
      "loss: 0.547637665271759, train acc: 0.9451\n",
      "loss: 0.535539773106575, train acc: 0.9452\n",
      "loss: 0.5695668965578079, train acc: 0.9465\n",
      "loss: 0.5364803314208985, train acc: 0.9495\n",
      "loss: 0.5829206138849259, train acc: 0.9486\n",
      "loss: 0.5411351054906846, train acc: 0.9474\n",
      "loss: 0.5201039642095566, train acc: 0.9484\n",
      "epoch: 36, loss: 0.2994960844516754, train acc: 0.9484, test acc: 0.9134\n",
      "loss: 0.580375611782074, train acc: 0.9479\n",
      "loss: 0.5168894708156586, train acc: 0.9469\n",
      "loss: 0.5368000358343125, train acc: 0.9488\n",
      "loss: 0.6057254284620285, train acc: 0.949\n",
      "loss: 0.5380225598812103, train acc: 0.9497\n",
      "loss: 0.532593908905983, train acc: 0.9502\n",
      "loss: 0.5548956274986268, train acc: 0.9485\n",
      "loss: 0.5267063587903976, train acc: 0.9487\n",
      "epoch: 37, loss: 0.4076560437679291, train acc: 0.9487, test acc: 0.915\n",
      "loss: 0.5698027014732361, train acc: 0.9502\n",
      "loss: 0.5647413015365601, train acc: 0.9483\n",
      "loss: 0.5552031457424164, train acc: 0.9482\n",
      "loss: 0.5672411322593689, train acc: 0.9466\n",
      "loss: 0.5461663603782654, train acc: 0.9498\n",
      "loss: 0.5298437416553498, train acc: 0.9521\n",
      "loss: 0.5682153165340423, train acc: 0.9519\n",
      "loss: 0.5382622420787812, train acc: 0.9496\n",
      "epoch: 38, loss: 0.301812082529068, train acc: 0.9496, test acc: 0.9126\n",
      "loss: 0.51854008436203, train acc: 0.9496\n",
      "loss: 0.5395704627037048, train acc: 0.9503\n",
      "loss: 0.5375768721103669, train acc: 0.9519\n",
      "loss: 0.5290599495172501, train acc: 0.9513\n",
      "loss: 0.5191252648830413, train acc: 0.952\n",
      "loss: 0.5146361917257309, train acc: 0.9532\n",
      "loss: 0.5484303802251815, train acc: 0.9525\n",
      "loss: 0.5081326276063919, train acc: 0.953\n",
      "epoch: 39, loss: 0.2167893946170807, train acc: 0.953, test acc: 0.9128\n",
      "loss: 0.5720085501670837, train acc: 0.9502\n",
      "loss: 0.5595431923866272, train acc: 0.9513\n",
      "loss: 0.5498904675245285, train acc: 0.9518\n",
      "loss: 0.5476847350597381, train acc: 0.9513\n",
      "loss: 0.5291343778371811, train acc: 0.9514\n",
      "loss: 0.5378139197826386, train acc: 0.9506\n",
      "loss: 0.5736116856336594, train acc: 0.9502\n",
      "loss: 0.5177800327539444, train acc: 0.9512\n",
      "epoch: 40, loss: 0.17657780647277832, train acc: 0.9512, test acc: 0.9092\n",
      "loss: 0.37904876470565796, train acc: 0.9485\n",
      "loss: 0.5130634427070617, train acc: 0.9522\n",
      "loss: 0.52425257563591, train acc: 0.9506\n",
      "loss: 0.5814892679452897, train acc: 0.95\n",
      "loss: 0.5619409859180451, train acc: 0.9519\n",
      "loss: 0.5146895259618759, train acc: 0.9534\n",
      "loss: 0.5491478502750397, train acc: 0.9538\n",
      "loss: 0.4916804313659668, train acc: 0.9524\n",
      "epoch: 41, loss: 0.07960522919893265, train acc: 0.9524, test acc: 0.9143\n",
      "loss: 0.45320311188697815, train acc: 0.953\n",
      "loss: 0.5092942774295807, train acc: 0.9511\n",
      "loss: 0.5274155110120773, train acc: 0.9505\n",
      "loss: 0.5529677212238312, train acc: 0.9513\n",
      "loss: 0.5300286501646042, train acc: 0.9527\n",
      "loss: 0.5270555078983307, train acc: 0.9515\n",
      "loss: 0.5601175457239151, train acc: 0.9517\n",
      "loss: 0.4987838089466095, train acc: 0.9533\n",
      "epoch: 42, loss: 0.2883278429508209, train acc: 0.9533, test acc: 0.914\n",
      "loss: 0.4962388277053833, train acc: 0.9528\n",
      "loss: 0.5166878968477249, train acc: 0.9531\n",
      "loss: 0.5162218093872071, train acc: 0.9539\n",
      "loss: 0.5409408837556839, train acc: 0.951\n",
      "loss: 0.5218260616064072, train acc: 0.9548\n",
      "loss: 0.4901231586933136, train acc: 0.9544\n",
      "loss: 0.5340033024549484, train acc: 0.9534\n",
      "loss: 0.550954407453537, train acc: 0.9526\n",
      "epoch: 43, loss: 0.2552882134914398, train acc: 0.9526, test acc: 0.9126\n",
      "loss: 0.40346068143844604, train acc: 0.9539\n",
      "loss: 0.4994975060224533, train acc: 0.9536\n",
      "loss: 0.5277980357408524, train acc: 0.9534\n",
      "loss: 0.5124752938747406, train acc: 0.9541\n",
      "loss: 0.5052068263292313, train acc: 0.9562\n",
      "loss: 0.5487571448087692, train acc: 0.9556\n",
      "loss: 0.5261406272649765, train acc: 0.9531\n",
      "loss: 0.5078463166952133, train acc: 0.9545\n",
      "epoch: 44, loss: 0.46235665678977966, train acc: 0.9545, test acc: 0.9115\n",
      "loss: 0.4122954308986664, train acc: 0.9533\n",
      "loss: 0.5134996116161347, train acc: 0.9497\n",
      "loss: 0.5515515953302383, train acc: 0.9521\n",
      "loss: 0.5632543742656708, train acc: 0.9517\n",
      "loss: 0.5294372767210007, train acc: 0.9536\n",
      "loss: 0.5105639636516571, train acc: 0.9546\n",
      "loss: 0.5388721555471421, train acc: 0.9542\n",
      "loss: 0.4974823027849197, train acc: 0.9542\n",
      "epoch: 45, loss: 0.21585798263549805, train acc: 0.9542, test acc: 0.913\n",
      "loss: 0.4509691298007965, train acc: 0.954\n",
      "loss: 0.5305058091878891, train acc: 0.9547\n",
      "loss: 0.5644511580467224, train acc: 0.9534\n",
      "loss: 0.5505752086639404, train acc: 0.9549\n",
      "loss: 0.5679830253124237, train acc: 0.9551\n",
      "loss: 0.5012840121984482, train acc: 0.9559\n",
      "loss: 0.5624957710504532, train acc: 0.9552\n",
      "loss: 0.5014829248189926, train acc: 0.9533\n",
      "epoch: 46, loss: 0.1777305155992508, train acc: 0.9533, test acc: 0.9171\n",
      "loss: 0.5449384450912476, train acc: 0.9555\n",
      "loss: 0.5176199346780777, train acc: 0.9552\n",
      "loss: 0.5059939831495285, train acc: 0.9536\n",
      "loss: 0.5124572038650512, train acc: 0.9543\n",
      "loss: 0.5377559214830399, train acc: 0.9553\n",
      "loss: 0.5437784105539322, train acc: 0.9555\n",
      "loss: 0.5528507977724075, train acc: 0.9553\n",
      "loss: 0.5006263166666031, train acc: 0.9547\n",
      "epoch: 47, loss: 0.3968696892261505, train acc: 0.9547, test acc: 0.9137\n",
      "loss: 0.43428343534469604, train acc: 0.9526\n",
      "loss: 0.5380637407302856, train acc: 0.9525\n",
      "loss: 0.527990511059761, train acc: 0.9551\n",
      "loss: 0.5650205612182617, train acc: 0.9561\n",
      "loss: 0.5340912759304046, train acc: 0.9557\n",
      "loss: 0.5028653860092163, train acc: 0.9543\n",
      "loss: 0.5194354772567749, train acc: 0.9539\n",
      "loss: 0.5261001974344254, train acc: 0.956\n",
      "epoch: 48, loss: 0.2778114378452301, train acc: 0.956, test acc: 0.916\n",
      "loss: 0.49639636278152466, train acc: 0.9562\n",
      "loss: 0.49992250502109525, train acc: 0.9556\n",
      "loss: 0.48771803379058837, train acc: 0.9577\n",
      "loss: 0.5495829313993454, train acc: 0.956\n",
      "loss: 0.5090843200683594, train acc: 0.9556\n",
      "loss: 0.5228827625513077, train acc: 0.9561\n",
      "loss: 0.5242714196443558, train acc: 0.9556\n",
      "loss: 0.5382694572210311, train acc: 0.9576\n",
      "epoch: 49, loss: 0.21011380851268768, train acc: 0.9576, test acc: 0.9149\n",
      "loss: 0.4813460409641266, train acc: 0.9561\n",
      "loss: 0.4953045457601547, train acc: 0.9574\n",
      "loss: 0.49855085611343386, train acc: 0.9531\n",
      "loss: 0.5524410635232926, train acc: 0.9565\n",
      "loss: 0.5234961360692978, train acc: 0.9545\n",
      "loss: 0.4949853390455246, train acc: 0.9556\n",
      "loss: 0.5793563783168793, train acc: 0.9525\n",
      "loss: 0.5042517513036728, train acc: 0.9568\n",
      "epoch: 50, loss: 0.2159225195646286, train acc: 0.9568, test acc: 0.9166\n",
      "loss: 0.409037709236145, train acc: 0.9583\n",
      "loss: 0.5046645283699036, train acc: 0.9572\n",
      "loss: 0.47737363874912264, train acc: 0.9562\n",
      "loss: 0.5567715853452683, train acc: 0.9563\n",
      "loss: 0.5044165432453156, train acc: 0.9554\n",
      "loss: 0.5082332402467727, train acc: 0.9578\n",
      "loss: 0.48306516408920286, train acc: 0.9565\n",
      "loss: 0.5214354813098907, train acc: 0.9554\n",
      "epoch: 51, loss: 0.4134560227394104, train acc: 0.9554, test acc: 0.9148\n",
      "loss: 0.46206238865852356, train acc: 0.958\n",
      "loss: 0.5260681003332138, train acc: 0.9582\n",
      "loss: 0.5359082847833634, train acc: 0.9553\n",
      "loss: 0.570710152387619, train acc: 0.9562\n",
      "loss: 0.4827217787504196, train acc: 0.9558\n",
      "loss: 0.5195047974586486, train acc: 0.9595\n",
      "loss: 0.5641356110572815, train acc: 0.9563\n",
      "loss: 0.5238612562417984, train acc: 0.9576\n",
      "epoch: 52, loss: 0.44951486587524414, train acc: 0.9576, test acc: 0.9138\n",
      "loss: 0.4341120719909668, train acc: 0.9537\n",
      "loss: 0.5118409663438797, train acc: 0.9569\n",
      "loss: 0.5247391492128373, train acc: 0.9567\n",
      "loss: 0.5506037592887878, train acc: 0.9573\n",
      "loss: 0.5105350881814956, train acc: 0.9574\n",
      "loss: 0.5141198396682739, train acc: 0.9583\n",
      "loss: 0.5159180700778961, train acc: 0.9566\n",
      "loss: 0.528463214635849, train acc: 0.9554\n",
      "epoch: 53, loss: 0.398137629032135, train acc: 0.9554, test acc: 0.9158\n",
      "loss: 0.44528090953826904, train acc: 0.9558\n",
      "loss: 0.53188816010952, train acc: 0.9567\n",
      "loss: 0.5092212378978729, train acc: 0.9576\n",
      "loss: 0.5634886234998703, train acc: 0.9579\n",
      "loss: 0.5149884045124054, train acc: 0.9579\n",
      "loss: 0.5090569376945495, train acc: 0.9575\n",
      "loss: 0.5309827238321304, train acc: 0.9573\n",
      "loss: 0.5130873113870621, train acc: 0.9594\n",
      "epoch: 54, loss: 0.10913726687431335, train acc: 0.9594, test acc: 0.9132\n",
      "loss: 0.4294126033782959, train acc: 0.959\n",
      "loss: 0.5127248942852021, train acc: 0.9573\n",
      "loss: 0.4934413105249405, train acc: 0.9598\n",
      "loss: 0.5577236175537109, train acc: 0.9584\n",
      "loss: 0.5299774289131165, train acc: 0.9584\n",
      "loss: 0.47808296978473663, train acc: 0.9599\n",
      "loss: 0.5365581810474396, train acc: 0.9582\n",
      "loss: 0.4856711596250534, train acc: 0.959\n",
      "epoch: 55, loss: 0.2684072256088257, train acc: 0.959, test acc: 0.9136\n",
      "loss: 0.4868523180484772, train acc: 0.9555\n",
      "loss: 0.4813543200492859, train acc: 0.9567\n",
      "loss: 0.5081466615200043, train acc: 0.9585\n",
      "loss: 0.5282968312501908, train acc: 0.9582\n",
      "loss: 0.4969248116016388, train acc: 0.9577\n",
      "loss: 0.48146678805351256, train acc: 0.9586\n",
      "loss: 0.5081838876008987, train acc: 0.9594\n",
      "loss: 0.44230322539806366, train acc: 0.9604\n",
      "epoch: 56, loss: 0.36702749133110046, train acc: 0.9604, test acc: 0.9159\n",
      "loss: 0.4047706127166748, train acc: 0.9593\n",
      "loss: 0.48090500831604005, train acc: 0.9567\n",
      "loss: 0.5079395234584808, train acc: 0.9605\n",
      "loss: 0.5278405755758285, train acc: 0.9585\n",
      "loss: 0.5210407823324203, train acc: 0.9583\n",
      "loss: 0.5471938282251358, train acc: 0.9583\n",
      "loss: 0.50670305788517, train acc: 0.9568\n",
      "loss: 0.4767172962427139, train acc: 0.9601\n",
      "epoch: 57, loss: 0.1710825264453888, train acc: 0.9601, test acc: 0.9152\n",
      "loss: 0.4440478980541229, train acc: 0.959\n",
      "loss: 0.45530945956707003, train acc: 0.9575\n",
      "loss: 0.49851947128772733, train acc: 0.9595\n",
      "loss: 0.5194612205028534, train acc: 0.9576\n",
      "loss: 0.5104624211788178, train acc: 0.9591\n",
      "loss: 0.5144100278615952, train acc: 0.9586\n",
      "loss: 0.5424305617809295, train acc: 0.9591\n",
      "loss: 0.48335826098918916, train acc: 0.96\n",
      "epoch: 58, loss: 0.3997904062271118, train acc: 0.96, test acc: 0.9143\n",
      "loss: 0.42004770040512085, train acc: 0.9582\n",
      "loss: 0.4882124841213226, train acc: 0.9558\n",
      "loss: 0.5031405389308929, train acc: 0.9597\n",
      "loss: 0.5458525717258453, train acc: 0.9588\n",
      "loss: 0.5325316488742828, train acc: 0.9594\n",
      "loss: 0.5121214151382446, train acc: 0.9588\n",
      "loss: 0.5367358952760697, train acc: 0.9583\n",
      "loss: 0.5139612525701522, train acc: 0.959\n",
      "epoch: 59, loss: 0.27777227759361267, train acc: 0.959, test acc: 0.9144\n",
      "loss: 0.48350855708122253, train acc: 0.9594\n",
      "loss: 0.4762457817792892, train acc: 0.9601\n",
      "loss: 0.5085168063640595, train acc: 0.9588\n",
      "loss: 0.5167542517185211, train acc: 0.959\n",
      "loss: 0.535650584101677, train acc: 0.9592\n",
      "loss: 0.4805789262056351, train acc: 0.9605\n",
      "loss: 0.5239791601896286, train acc: 0.9598\n",
      "loss: 0.5130195379257202, train acc: 0.9608\n",
      "epoch: 60, loss: 0.45555055141448975, train acc: 0.9608, test acc: 0.916\n",
      "loss: 0.4285321831703186, train acc: 0.9616\n",
      "loss: 0.481889483332634, train acc: 0.9582\n",
      "loss: 0.4749659180641174, train acc: 0.9591\n",
      "loss: 0.589692223072052, train acc: 0.9601\n",
      "loss: 0.5123548805713654, train acc: 0.9572\n",
      "loss: 0.5166528224945068, train acc: 0.9629\n",
      "loss: 0.550054308772087, train acc: 0.957\n",
      "loss: 0.5287176102399826, train acc: 0.9615\n",
      "epoch: 61, loss: 0.13902512192726135, train acc: 0.9615, test acc: 0.9141\n",
      "loss: 0.39213618636131287, train acc: 0.9603\n",
      "loss: 0.5010258376598358, train acc: 0.9609\n",
      "loss: 0.4756960988044739, train acc: 0.9592\n",
      "loss: 0.5329746752977371, train acc: 0.9606\n",
      "loss: 0.5175671190023422, train acc: 0.9594\n",
      "loss: 0.49175967276096344, train acc: 0.9618\n",
      "loss: 0.5042515307664871, train acc: 0.9624\n",
      "loss: 0.46931617259979247, train acc: 0.9619\n",
      "epoch: 62, loss: 0.1333714723587036, train acc: 0.9619, test acc: 0.9152\n",
      "loss: 0.5224153995513916, train acc: 0.9605\n",
      "loss: 0.49654134511947634, train acc: 0.9596\n",
      "loss: 0.500996133685112, train acc: 0.9623\n",
      "loss: 0.524910444021225, train acc: 0.9635\n",
      "loss: 0.4961568534374237, train acc: 0.9619\n",
      "loss: 0.49713202118873595, train acc: 0.9624\n",
      "loss: 0.5128958612680435, train acc: 0.9603\n",
      "loss: 0.47563706934452055, train acc: 0.9612\n",
      "epoch: 63, loss: 0.122501440346241, train acc: 0.9612, test acc: 0.9147\n",
      "loss: 0.521952748298645, train acc: 0.9604\n",
      "loss: 0.49958272874355314, train acc: 0.9594\n",
      "loss: 0.5191479444503784, train acc: 0.9595\n",
      "loss: 0.5172926425933838, train acc: 0.9582\n",
      "loss: 0.5248556286096573, train acc: 0.9604\n",
      "loss: 0.48550253808498384, train acc: 0.9612\n",
      "loss: 0.4996477484703064, train acc: 0.9591\n",
      "loss: 0.4860728085041046, train acc: 0.9625\n",
      "epoch: 64, loss: 0.07977868616580963, train acc: 0.9625, test acc: 0.9156\n",
      "loss: 0.4018687903881073, train acc: 0.9616\n",
      "loss: 0.4596536248922348, train acc: 0.9609\n",
      "loss: 0.4831379294395447, train acc: 0.9608\n",
      "loss: 0.5204652428627015, train acc: 0.9609\n",
      "loss: 0.4585784047842026, train acc: 0.9608\n",
      "loss: 0.493820858001709, train acc: 0.9628\n",
      "loss: 0.4931584268808365, train acc: 0.9622\n",
      "loss: 0.4795579046010971, train acc: 0.9612\n",
      "epoch: 65, loss: 0.35121068358421326, train acc: 0.9612, test acc: 0.9154\n",
      "loss: 0.4323449730873108, train acc: 0.9618\n",
      "loss: 0.4896340638399124, train acc: 0.958\n",
      "loss: 0.5009078413248063, train acc: 0.9588\n",
      "loss: 0.5559634625911712, train acc: 0.957\n",
      "loss: 0.524156454205513, train acc: 0.9603\n",
      "loss: 0.4820686519145966, train acc: 0.9605\n",
      "loss: 0.5344015240669251, train acc: 0.9611\n",
      "loss: 0.483479243516922, train acc: 0.9594\n",
      "epoch: 66, loss: 0.23867611587047577, train acc: 0.9594, test acc: 0.9133\n",
      "loss: 0.43944329023361206, train acc: 0.9612\n",
      "loss: 0.5015796512365341, train acc: 0.9585\n",
      "loss: 0.46050364077091216, train acc: 0.9585\n",
      "loss: 0.5199051082134247, train acc: 0.9596\n",
      "loss: 0.5078055918216705, train acc: 0.9625\n",
      "loss: 0.4841996878385544, train acc: 0.9629\n",
      "loss: 0.5191043436527252, train acc: 0.9626\n",
      "loss: 0.5229382395744324, train acc: 0.9623\n",
      "epoch: 67, loss: 0.34556692838668823, train acc: 0.9623, test acc: 0.9145\n",
      "loss: 0.4628211259841919, train acc: 0.9617\n",
      "loss: 0.5013299226760864, train acc: 0.9616\n",
      "loss: 0.49388440549373624, train acc: 0.9614\n",
      "loss: 0.5303666025400162, train acc: 0.9598\n",
      "loss: 0.4674081265926361, train acc: 0.962\n",
      "loss: 0.49647608697414397, train acc: 0.9634\n",
      "loss: 0.5088079541921615, train acc: 0.9631\n",
      "loss: 0.5119851708412171, train acc: 0.9613\n",
      "epoch: 68, loss: 0.40166065096855164, train acc: 0.9613, test acc: 0.9145\n",
      "loss: 0.5306462049484253, train acc: 0.9602\n",
      "loss: 0.5180857181549072, train acc: 0.9606\n",
      "loss: 0.5069451659917832, train acc: 0.9617\n",
      "loss: 0.49986524879932404, train acc: 0.9617\n",
      "loss: 0.4844789892435074, train acc: 0.9616\n",
      "loss: 0.4912780076265335, train acc: 0.9643\n",
      "loss: 0.5172958463430405, train acc: 0.9631\n",
      "loss: 0.49476641714572905, train acc: 0.9629\n",
      "epoch: 69, loss: 0.29202914237976074, train acc: 0.9629, test acc: 0.9164\n",
      "loss: 0.42319032549858093, train acc: 0.9629\n",
      "loss: 0.4943079799413681, train acc: 0.9634\n",
      "loss: 0.45662095546722414, train acc: 0.9628\n",
      "loss: 0.529642429947853, train acc: 0.9638\n",
      "loss: 0.48967978954315183, train acc: 0.9597\n",
      "loss: 0.4838020235300064, train acc: 0.963\n",
      "loss: 0.4936708003282547, train acc: 0.9628\n",
      "loss: 0.4936730593442917, train acc: 0.9625\n",
      "epoch: 70, loss: 0.29380083084106445, train acc: 0.9625, test acc: 0.9156\n",
      "loss: 0.42969784140586853, train acc: 0.9622\n",
      "loss: 0.45901871025562285, train acc: 0.9585\n",
      "loss: 0.47370336055755613, train acc: 0.9622\n",
      "loss: 0.5169555157423019, train acc: 0.9615\n",
      "loss: 0.49363948702812194, train acc: 0.964\n",
      "loss: 0.49622690081596377, train acc: 0.9636\n",
      "loss: 0.5573370367288589, train acc: 0.9622\n",
      "loss: 0.4885857433080673, train acc: 0.9617\n",
      "epoch: 71, loss: 0.18305930495262146, train acc: 0.9617, test acc: 0.9141\n",
      "loss: 0.48755350708961487, train acc: 0.9615\n",
      "loss: 0.4971507370471954, train acc: 0.9619\n",
      "loss: 0.47664699852466585, train acc: 0.962\n",
      "loss: 0.5097784757614136, train acc: 0.9625\n",
      "loss: 0.4965053141117096, train acc: 0.964\n",
      "loss: 0.4899629235267639, train acc: 0.9649\n",
      "loss: 0.49450123906135557, train acc: 0.9637\n",
      "loss: 0.4934706211090088, train acc: 0.9639\n",
      "epoch: 72, loss: 0.30930376052856445, train acc: 0.9639, test acc: 0.9177\n",
      "loss: 0.4422220289707184, train acc: 0.964\n",
      "loss: 0.48500698804855347, train acc: 0.9639\n",
      "loss: 0.5009052515029907, train acc: 0.9636\n",
      "loss: 0.5135458052158356, train acc: 0.9592\n",
      "loss: 0.5256642401218414, train acc: 0.9647\n",
      "loss: 0.5051785737276078, train acc: 0.9645\n",
      "loss: 0.49838368892669677, train acc: 0.9628\n",
      "loss: 0.46094624400138856, train acc: 0.9617\n",
      "epoch: 73, loss: 0.2692806124687195, train acc: 0.9617, test acc: 0.9136\n",
      "loss: 0.38691240549087524, train acc: 0.9598\n",
      "loss: 0.496677964925766, train acc: 0.9637\n",
      "loss: 0.5029295206069946, train acc: 0.9623\n",
      "loss: 0.5447867006063462, train acc: 0.9628\n",
      "loss: 0.483164781332016, train acc: 0.963\n",
      "loss: 0.4881226927042007, train acc: 0.9664\n",
      "loss: 0.49305959343910216, train acc: 0.9653\n",
      "loss: 0.4906169354915619, train acc: 0.9636\n",
      "epoch: 74, loss: 0.2975122332572937, train acc: 0.9636, test acc: 0.9153\n",
      "loss: 0.4748692214488983, train acc: 0.9628\n",
      "loss: 0.5112182021141052, train acc: 0.9586\n",
      "loss: 0.5023227751255035, train acc: 0.9638\n",
      "loss: 0.537932276725769, train acc: 0.9652\n",
      "loss: 0.48248760104179383, train acc: 0.9648\n",
      "loss: 0.47746728658676146, train acc: 0.9665\n",
      "loss: 0.5160292267799378, train acc: 0.9649\n",
      "loss: 0.4885073363780975, train acc: 0.9634\n",
      "epoch: 75, loss: 0.2805703282356262, train acc: 0.9634, test acc: 0.9137\n",
      "loss: 0.47238361835479736, train acc: 0.9657\n",
      "loss: 0.4867393016815186, train acc: 0.9638\n",
      "loss: 0.5013213753700256, train acc: 0.9642\n",
      "loss: 0.5064063102006913, train acc: 0.965\n",
      "loss: 0.4746856719255447, train acc: 0.9641\n",
      "loss: 0.46929554343223573, train acc: 0.9656\n",
      "loss: 0.4880611479282379, train acc: 0.9654\n",
      "loss: 0.4814025789499283, train acc: 0.9634\n",
      "epoch: 76, loss: 0.27758675813674927, train acc: 0.9634, test acc: 0.9155\n",
      "loss: 0.4955393373966217, train acc: 0.9648\n",
      "loss: 0.493320831656456, train acc: 0.9649\n",
      "loss: 0.49950543940067293, train acc: 0.9636\n",
      "loss: 0.5236373364925384, train acc: 0.963\n",
      "loss: 0.49480751156806946, train acc: 0.9634\n",
      "loss: 0.4640870332717896, train acc: 0.9644\n",
      "loss: 0.5257966756820679, train acc: 0.9639\n",
      "loss: 0.4648058295249939, train acc: 0.9641\n",
      "epoch: 77, loss: 0.251827210187912, train acc: 0.9641, test acc: 0.9145\n",
      "loss: 0.44092223048210144, train acc: 0.965\n",
      "loss: 0.4864746451377869, train acc: 0.9637\n",
      "loss: 0.45686165392398836, train acc: 0.964\n",
      "loss: 0.5435340762138366, train acc: 0.9649\n",
      "loss: 0.5150252103805542, train acc: 0.9628\n",
      "loss: 0.4530506044626236, train acc: 0.965\n",
      "loss: 0.4861002951860428, train acc: 0.9664\n",
      "loss: 0.48423297703266144, train acc: 0.9644\n",
      "epoch: 78, loss: 0.2200799286365509, train acc: 0.9644, test acc: 0.9155\n",
      "loss: 0.414078027009964, train acc: 0.9647\n",
      "loss: 0.4770282208919525, train acc: 0.9623\n",
      "loss: 0.4827749848365784, train acc: 0.9644\n",
      "loss: 0.4928335964679718, train acc: 0.9635\n",
      "loss: 0.48545235097408296, train acc: 0.9649\n",
      "loss: 0.481176495552063, train acc: 0.9662\n",
      "loss: 0.46020918488502505, train acc: 0.965\n",
      "loss: 0.4766331374645233, train acc: 0.966\n",
      "epoch: 79, loss: 0.33896681666374207, train acc: 0.966, test acc: 0.912\n",
      "#####training and testing end with K:20, P:0.5######\n",
      "#####training and testing start with K:20, P:1######\n",
      "loss: 2.3058040142059326, train acc: 0.172\n",
      "loss: 2.0853769779205322, train acc: 0.4097\n",
      "loss: 1.678828740119934, train acc: 0.5956\n",
      "loss: 1.2567140698432921, train acc: 0.6903\n",
      "loss: 1.0046428859233856, train acc: 0.8014\n",
      "loss: 0.846989107131958, train acc: 0.845\n",
      "loss: 0.6749651730060577, train acc: 0.8631\n",
      "loss: 0.5995949387550354, train acc: 0.8718\n",
      "epoch: 0, loss: 0.521802544593811, train acc: 0.8718, test acc: 0.8766\n",
      "loss: 0.5658747553825378, train acc: 0.8809\n",
      "loss: 0.5032749086618423, train acc: 0.879\n",
      "loss: 0.47147858142852783, train acc: 0.8901\n",
      "loss: 0.40332486629486086, train acc: 0.893\n",
      "loss: 0.41563436985015867, train acc: 0.8971\n",
      "loss: 0.41615926325321195, train acc: 0.9005\n",
      "loss: 0.3717873692512512, train acc: 0.9052\n",
      "loss: 0.3773601770401001, train acc: 0.9042\n",
      "epoch: 1, loss: 0.29369521141052246, train acc: 0.9042, test acc: 0.8985\n",
      "loss: 0.42318233847618103, train acc: 0.9094\n",
      "loss: 0.3725805550813675, train acc: 0.9102\n",
      "loss: 0.3487752079963684, train acc: 0.9114\n",
      "loss: 0.301624259352684, train acc: 0.9141\n",
      "loss: 0.3294679388403893, train acc: 0.9143\n",
      "loss: 0.335232213139534, train acc: 0.9156\n",
      "loss: 0.3037749484181404, train acc: 0.9199\n",
      "loss: 0.3107532262802124, train acc: 0.9191\n",
      "epoch: 2, loss: 0.21115271747112274, train acc: 0.9191, test acc: 0.9084\n",
      "loss: 0.3527866303920746, train acc: 0.9216\n",
      "loss: 0.3238078624010086, train acc: 0.9204\n",
      "loss: 0.3005313009023666, train acc: 0.9235\n",
      "loss: 0.2603946775197983, train acc: 0.9233\n",
      "loss: 0.28781028389930724, train acc: 0.9253\n",
      "loss: 0.291828028857708, train acc: 0.9249\n",
      "loss: 0.2672905385494232, train acc: 0.929\n",
      "loss: 0.27199277430772784, train acc: 0.9279\n",
      "epoch: 3, loss: 0.1671934574842453, train acc: 0.9279, test acc: 0.9128\n",
      "loss: 0.3034854233264923, train acc: 0.9305\n",
      "loss: 0.293473818898201, train acc: 0.9297\n",
      "loss: 0.2712361767888069, train acc: 0.9312\n",
      "loss: 0.2361119046807289, train acc: 0.9297\n",
      "loss: 0.2604353979229927, train acc: 0.9326\n",
      "loss: 0.262317356467247, train acc: 0.9328\n",
      "loss: 0.24303876459598542, train acc: 0.9356\n",
      "loss: 0.24484909772872926, train acc: 0.9335\n",
      "epoch: 4, loss: 0.13532277941703796, train acc: 0.9335, test acc: 0.9147\n",
      "loss: 0.26577121019363403, train acc: 0.937\n",
      "loss: 0.2712900176644325, train acc: 0.9362\n",
      "loss: 0.24934736490249634, train acc: 0.9369\n",
      "loss: 0.21900905221700667, train acc: 0.9358\n",
      "loss: 0.24054350927472115, train acc: 0.9368\n",
      "loss: 0.23946276903152466, train acc: 0.9382\n",
      "loss: 0.2250230148434639, train acc: 0.9415\n",
      "loss: 0.22451884895563126, train acc: 0.9395\n",
      "epoch: 5, loss: 0.11420811712741852, train acc: 0.9395, test acc: 0.9173\n",
      "loss: 0.23428349196910858, train acc: 0.9413\n",
      "loss: 0.2530961990356445, train acc: 0.9419\n",
      "loss: 0.23206664323806764, train acc: 0.9403\n",
      "loss: 0.20502318888902665, train acc: 0.9405\n",
      "loss: 0.2247477538883686, train acc: 0.9406\n",
      "loss: 0.2215949535369873, train acc: 0.9428\n",
      "loss: 0.21067546606063842, train acc: 0.9452\n",
      "loss: 0.208243278414011, train acc: 0.9439\n",
      "epoch: 6, loss: 0.0964854285120964, train acc: 0.9439, test acc: 0.9193\n",
      "loss: 0.21178588271141052, train acc: 0.945\n",
      "loss: 0.2377177208662033, train acc: 0.9463\n",
      "loss: 0.21677531003952027, train acc: 0.945\n",
      "loss: 0.1931063637137413, train acc: 0.9448\n",
      "loss: 0.21111605912446976, train acc: 0.9437\n",
      "loss: 0.2068176969885826, train acc: 0.9458\n",
      "loss: 0.19818797931075097, train acc: 0.9486\n",
      "loss: 0.1935918726027012, train acc: 0.9479\n",
      "epoch: 7, loss: 0.08382254838943481, train acc: 0.9479, test acc: 0.9217\n",
      "loss: 0.19665393233299255, train acc: 0.9481\n",
      "loss: 0.22459301352500916, train acc: 0.9492\n",
      "loss: 0.2043728671967983, train acc: 0.9492\n",
      "loss: 0.18226393014192582, train acc: 0.9482\n",
      "loss: 0.20012838020920753, train acc: 0.9466\n",
      "loss: 0.19405065774917601, train acc: 0.9494\n",
      "loss: 0.1875302441418171, train acc: 0.9519\n",
      "loss: 0.1808206021785736, train acc: 0.9508\n",
      "epoch: 8, loss: 0.07244982570409775, train acc: 0.9508, test acc: 0.9213\n",
      "loss: 0.18472865223884583, train acc: 0.9504\n",
      "loss: 0.21291058659553527, train acc: 0.9511\n",
      "loss: 0.19350648298859596, train acc: 0.9516\n",
      "loss: 0.17195600718259813, train acc: 0.9514\n",
      "loss: 0.18961421474814416, train acc: 0.9496\n",
      "loss: 0.18289476186037062, train acc: 0.9512\n",
      "loss: 0.17764982879161834, train acc: 0.9537\n",
      "loss: 0.16962781995534898, train acc: 0.9546\n",
      "epoch: 9, loss: 0.0652741938829422, train acc: 0.9546, test acc: 0.9227\n",
      "loss: 0.17254319787025452, train acc: 0.9539\n",
      "loss: 0.20216819047927856, train acc: 0.9544\n",
      "loss: 0.18365728482604027, train acc: 0.9546\n",
      "loss: 0.16229831650853158, train acc: 0.9547\n",
      "loss: 0.1804015301167965, train acc: 0.952\n",
      "loss: 0.17241354733705522, train acc: 0.9526\n",
      "loss: 0.16914708837866782, train acc: 0.9566\n",
      "loss: 0.16018686667084694, train acc: 0.9572\n",
      "epoch: 10, loss: 0.058756023645401, train acc: 0.9572, test acc: 0.9235\n",
      "loss: 0.16458088159561157, train acc: 0.9556\n",
      "loss: 0.19231224209070205, train acc: 0.9565\n",
      "loss: 0.17476150542497634, train acc: 0.9572\n",
      "loss: 0.1537501633167267, train acc: 0.957\n",
      "loss: 0.171777206659317, train acc: 0.9541\n",
      "loss: 0.16356223598122596, train acc: 0.9541\n",
      "loss: 0.16125070303678513, train acc: 0.9576\n",
      "loss: 0.15107853561639786, train acc: 0.9597\n",
      "epoch: 11, loss: 0.05358405038714409, train acc: 0.9597, test acc: 0.9238\n",
      "loss: 0.15389573574066162, train acc: 0.9575\n",
      "loss: 0.18293944597244263, train acc: 0.9583\n",
      "loss: 0.16650554314255714, train acc: 0.9592\n",
      "loss: 0.14505492821335791, train acc: 0.9591\n",
      "loss: 0.16318989917635918, train acc: 0.9564\n",
      "loss: 0.15489766970276833, train acc: 0.957\n",
      "loss: 0.15393925979733467, train acc: 0.9596\n",
      "loss: 0.1439208470284939, train acc: 0.9612\n",
      "epoch: 12, loss: 0.0499592125415802, train acc: 0.9612, test acc: 0.9243\n",
      "loss: 0.1448361575603485, train acc: 0.9597\n",
      "loss: 0.173911751806736, train acc: 0.9601\n",
      "loss: 0.15901186764240266, train acc: 0.9603\n",
      "loss: 0.13754579946398734, train acc: 0.9613\n",
      "loss: 0.15552114322781563, train acc: 0.9589\n",
      "loss: 0.1471230037510395, train acc: 0.9606\n",
      "loss: 0.14630492106080056, train acc: 0.9618\n",
      "loss: 0.135978764295578, train acc: 0.9634\n",
      "epoch: 13, loss: 0.0454583540558815, train acc: 0.9634, test acc: 0.9254\n",
      "loss: 0.13877668976783752, train acc: 0.9614\n",
      "loss: 0.16574638932943345, train acc: 0.9619\n",
      "loss: 0.1517927974462509, train acc: 0.9622\n",
      "loss: 0.13053256794810295, train acc: 0.9634\n",
      "loss: 0.1478709891438484, train acc: 0.9609\n",
      "loss: 0.1397459849715233, train acc: 0.9632\n",
      "loss: 0.13993266895413398, train acc: 0.9639\n",
      "loss: 0.1292415589094162, train acc: 0.9648\n",
      "epoch: 14, loss: 0.0419304296374321, train acc: 0.9648, test acc: 0.9261\n",
      "loss: 0.1316148042678833, train acc: 0.9637\n",
      "loss: 0.15747115910053253, train acc: 0.9644\n",
      "loss: 0.1450969971716404, train acc: 0.964\n",
      "loss: 0.1234571911394596, train acc: 0.9645\n",
      "loss: 0.14028904736042022, train acc: 0.9629\n",
      "loss: 0.13282503858208655, train acc: 0.9653\n",
      "loss: 0.1339674711227417, train acc: 0.9657\n",
      "loss: 0.1229706160724163, train acc: 0.9672\n",
      "epoch: 15, loss: 0.03881844878196716, train acc: 0.9672, test acc: 0.9262\n",
      "loss: 0.12436990439891815, train acc: 0.9653\n",
      "loss: 0.14996166080236434, train acc: 0.9656\n",
      "loss: 0.13892027214169503, train acc: 0.9658\n",
      "loss: 0.11709189713001251, train acc: 0.9669\n",
      "loss: 0.13322935812175274, train acc: 0.965\n",
      "loss: 0.12652801647782325, train acc: 0.9668\n",
      "loss: 0.1280548945069313, train acc: 0.9668\n",
      "loss: 0.11699609011411667, train acc: 0.9688\n",
      "epoch: 16, loss: 0.03600141778588295, train acc: 0.9688, test acc: 0.9272\n",
      "loss: 0.11773907393217087, train acc: 0.9668\n",
      "loss: 0.14243016988039017, train acc: 0.9671\n",
      "loss: 0.13309207409620286, train acc: 0.967\n",
      "loss: 0.11112903282046319, train acc: 0.9681\n",
      "loss: 0.12745836675167083, train acc: 0.9667\n",
      "loss: 0.12027032598853112, train acc: 0.9686\n",
      "loss: 0.12280582487583161, train acc: 0.968\n",
      "loss: 0.11150949485599995, train acc: 0.971\n",
      "epoch: 17, loss: 0.03341929614543915, train acc: 0.971, test acc: 0.9278\n",
      "loss: 0.11348400264978409, train acc: 0.9683\n",
      "loss: 0.13674863204360008, train acc: 0.9689\n",
      "loss: 0.12722898945212363, train acc: 0.9684\n",
      "loss: 0.10596364215016366, train acc: 0.97\n",
      "loss: 0.12139427736401558, train acc: 0.9694\n",
      "loss: 0.11479469537734985, train acc: 0.9698\n",
      "loss: 0.11777859702706336, train acc: 0.969\n",
      "loss: 0.10640540532767773, train acc: 0.9722\n",
      "epoch: 18, loss: 0.031427714973688126, train acc: 0.9722, test acc: 0.928\n",
      "loss: 0.1074676439166069, train acc: 0.9693\n",
      "loss: 0.12933234721422196, train acc: 0.9703\n",
      "loss: 0.12202822044491768, train acc: 0.9706\n",
      "loss: 0.10107693076133728, train acc: 0.9718\n",
      "loss: 0.11549923606216908, train acc: 0.9712\n",
      "loss: 0.1098415918648243, train acc: 0.9715\n",
      "loss: 0.11302862018346786, train acc: 0.9707\n",
      "loss: 0.10164952427148818, train acc: 0.9738\n",
      "epoch: 19, loss: 0.028950003907084465, train acc: 0.9738, test acc: 0.9285\n",
      "loss: 0.1042637899518013, train acc: 0.9707\n",
      "loss: 0.12391369789838791, train acc: 0.9714\n",
      "loss: 0.11639116555452347, train acc: 0.9718\n",
      "loss: 0.09655786231160164, train acc: 0.9734\n",
      "loss: 0.11056983098387718, train acc: 0.9726\n",
      "loss: 0.10498800128698349, train acc: 0.9727\n",
      "loss: 0.10817676931619644, train acc: 0.9731\n",
      "loss: 0.09760728999972343, train acc: 0.9754\n",
      "epoch: 20, loss: 0.027110133320093155, train acc: 0.9754, test acc: 0.9283\n",
      "loss: 0.09838105738162994, train acc: 0.9722\n",
      "loss: 0.11728735342621803, train acc: 0.9721\n",
      "loss: 0.11163000538945198, train acc: 0.9739\n",
      "loss: 0.09263092428445815, train acc: 0.9754\n",
      "loss: 0.10561023242771625, train acc: 0.9746\n",
      "loss: 0.10034682676196098, train acc: 0.9744\n",
      "loss: 0.10378581285476685, train acc: 0.9745\n",
      "loss: 0.0928170457482338, train acc: 0.9767\n",
      "epoch: 21, loss: 0.02469300478696823, train acc: 0.9767, test acc: 0.9287\n",
      "loss: 0.09569340944290161, train acc: 0.9741\n",
      "loss: 0.11274372413754463, train acc: 0.9726\n",
      "loss: 0.10660197734832763, train acc: 0.9745\n",
      "loss: 0.08900561928749084, train acc: 0.9769\n",
      "loss: 0.10101047530770302, train acc: 0.976\n",
      "loss: 0.09588603302836418, train acc: 0.9752\n",
      "loss: 0.0994469691067934, train acc: 0.9749\n",
      "loss: 0.08939415402710438, train acc: 0.9777\n",
      "epoch: 22, loss: 0.02287139557301998, train acc: 0.9777, test acc: 0.9291\n",
      "loss: 0.09125819802284241, train acc: 0.9763\n",
      "loss: 0.10766417905688286, train acc: 0.973\n",
      "loss: 0.10298195034265518, train acc: 0.9755\n",
      "loss: 0.08537181541323662, train acc: 0.9787\n",
      "loss: 0.09626452922821045, train acc: 0.9782\n",
      "loss: 0.09176887311041355, train acc: 0.9757\n",
      "loss: 0.09546152092516422, train acc: 0.9763\n",
      "loss: 0.08542534150183201, train acc: 0.9792\n",
      "epoch: 23, loss: 0.021177401766180992, train acc: 0.9792, test acc: 0.9292\n",
      "loss: 0.08774513006210327, train acc: 0.9776\n",
      "loss: 0.1027404397726059, train acc: 0.9735\n",
      "loss: 0.09863264411687851, train acc: 0.9769\n",
      "loss: 0.08143512085080147, train acc: 0.9801\n",
      "loss: 0.09299122542142868, train acc: 0.9795\n",
      "loss: 0.08776877000927925, train acc: 0.9768\n",
      "loss: 0.09166905283927917, train acc: 0.9778\n",
      "loss: 0.08210648372769355, train acc: 0.9805\n",
      "epoch: 24, loss: 0.02011926658451557, train acc: 0.9805, test acc: 0.9295\n",
      "loss: 0.08371572196483612, train acc: 0.9792\n",
      "loss: 0.09795452803373336, train acc: 0.9745\n",
      "loss: 0.09435160607099533, train acc: 0.9781\n",
      "loss: 0.07870756350457668, train acc: 0.9816\n",
      "loss: 0.08845169246196746, train acc: 0.9805\n",
      "loss: 0.0841710913926363, train acc: 0.9778\n",
      "loss: 0.08814939595758915, train acc: 0.9788\n",
      "loss: 0.07863747961819172, train acc: 0.9813\n",
      "epoch: 25, loss: 0.018441414460539818, train acc: 0.9813, test acc: 0.9292\n",
      "loss: 0.0822429358959198, train acc: 0.9801\n",
      "loss: 0.09377981647849083, train acc: 0.9754\n",
      "loss: 0.09033144563436508, train acc: 0.9799\n",
      "loss: 0.07486796379089355, train acc: 0.9822\n",
      "loss: 0.08508494272828102, train acc: 0.9818\n",
      "loss: 0.08049138002097607, train acc: 0.9797\n",
      "loss: 0.08422958068549632, train acc: 0.9805\n",
      "loss: 0.07543691843748093, train acc: 0.9823\n",
      "epoch: 26, loss: 0.017432527616620064, train acc: 0.9823, test acc: 0.9302\n",
      "loss: 0.08063261955976486, train acc: 0.9814\n",
      "loss: 0.08974528759717941, train acc: 0.9762\n",
      "loss: 0.08623799979686737, train acc: 0.9808\n",
      "loss: 0.07190699130296707, train acc: 0.9834\n",
      "loss: 0.08118855562061071, train acc: 0.9828\n",
      "loss: 0.07648991048336029, train acc: 0.9809\n",
      "loss: 0.08082526735961437, train acc: 0.9812\n",
      "loss: 0.07281850166618824, train acc: 0.9835\n",
      "epoch: 27, loss: 0.016170121729373932, train acc: 0.9835, test acc: 0.9301\n",
      "loss: 0.07678641378879547, train acc: 0.9828\n",
      "loss: 0.08514216467738152, train acc: 0.9777\n",
      "loss: 0.08285260871052742, train acc: 0.9812\n",
      "loss: 0.06936791092157364, train acc: 0.9847\n",
      "loss: 0.07780019659548998, train acc: 0.9841\n",
      "loss: 0.07299428582191467, train acc: 0.9813\n",
      "loss: 0.07708788104355335, train acc: 0.9828\n",
      "loss: 0.0695330023765564, train acc: 0.9842\n",
      "epoch: 28, loss: 0.015503502450883389, train acc: 0.9842, test acc: 0.9308\n",
      "loss: 0.07466109842061996, train acc: 0.9833\n",
      "loss: 0.0808002945035696, train acc: 0.9793\n",
      "loss: 0.0782455138862133, train acc: 0.9826\n",
      "loss: 0.06633561253547668, train acc: 0.9852\n",
      "loss: 0.07373929414898157, train acc: 0.9851\n",
      "loss: 0.06923041753470897, train acc: 0.982\n",
      "loss: 0.07375237494707107, train acc: 0.9841\n",
      "loss: 0.06667201183736324, train acc: 0.9857\n",
      "epoch: 29, loss: 0.014142158441245556, train acc: 0.9857, test acc: 0.9311\n",
      "loss: 0.0720246210694313, train acc: 0.9845\n",
      "loss: 0.07757315188646316, train acc: 0.9797\n",
      "loss: 0.07546799294650555, train acc: 0.9835\n",
      "loss: 0.06412065178155898, train acc: 0.9867\n",
      "loss: 0.07107294853776694, train acc: 0.9859\n",
      "loss: 0.0663905318826437, train acc: 0.9832\n",
      "loss: 0.07045003511011601, train acc: 0.9849\n",
      "loss: 0.06414222232997417, train acc: 0.9862\n",
      "epoch: 30, loss: 0.014258158393204212, train acc: 0.9862, test acc: 0.9311\n",
      "loss: 0.06971928477287292, train acc: 0.9853\n",
      "loss: 0.07391899898648262, train acc: 0.981\n",
      "loss: 0.0709109827876091, train acc: 0.9848\n",
      "loss: 0.06116461530327797, train acc: 0.9871\n",
      "loss: 0.06748861763626338, train acc: 0.9864\n",
      "loss: 0.06278524249792099, train acc: 0.9841\n",
      "loss: 0.06690889820456505, train acc: 0.9865\n",
      "loss: 0.06082749143242836, train acc: 0.987\n",
      "epoch: 31, loss: 0.012727190740406513, train acc: 0.987, test acc: 0.9307\n",
      "loss: 0.0676468163728714, train acc: 0.9868\n",
      "loss: 0.07115751169621945, train acc: 0.9815\n",
      "loss: 0.06782508008182049, train acc: 0.9858\n",
      "loss: 0.05874957963824272, train acc: 0.9878\n",
      "loss: 0.06425387989729643, train acc: 0.9871\n",
      "loss: 0.0596726231276989, train acc: 0.9846\n",
      "loss: 0.06405326537787914, train acc: 0.987\n",
      "loss: 0.058640431985259055, train acc: 0.9881\n",
      "epoch: 32, loss: 0.012368344701826572, train acc: 0.9881, test acc: 0.9311\n",
      "loss: 0.06465142220258713, train acc: 0.9875\n",
      "loss: 0.06720192730426788, train acc: 0.9825\n",
      "loss: 0.06429924219846725, train acc: 0.9871\n",
      "loss: 0.056443242356181145, train acc: 0.9878\n",
      "loss: 0.06127949766814709, train acc: 0.9879\n",
      "loss: 0.05722416378557682, train acc: 0.9854\n",
      "loss: 0.06124372333288193, train acc: 0.9878\n",
      "loss: 0.05547041818499565, train acc: 0.9883\n",
      "epoch: 33, loss: 0.011196883395314217, train acc: 0.9883, test acc: 0.9307\n",
      "loss: 0.06213961914181709, train acc: 0.9889\n",
      "loss: 0.06517426483333111, train acc: 0.9824\n",
      "loss: 0.06094497144222259, train acc: 0.9875\n",
      "loss: 0.05406558588147163, train acc: 0.9891\n",
      "loss: 0.05806478653103113, train acc: 0.9896\n",
      "loss: 0.05417377725243568, train acc: 0.9852\n",
      "loss: 0.058569247648119924, train acc: 0.9889\n",
      "loss: 0.05369830727577209, train acc: 0.99\n",
      "epoch: 34, loss: 0.011531933210790157, train acc: 0.99, test acc: 0.9319\n",
      "loss: 0.05810144171118736, train acc: 0.9889\n",
      "loss: 0.06130715161561966, train acc: 0.9835\n",
      "loss: 0.057964212447404864, train acc: 0.9886\n",
      "loss: 0.05172444507479668, train acc: 0.9892\n",
      "loss: 0.05553673077374697, train acc: 0.99\n",
      "loss: 0.05153643004596233, train acc: 0.9864\n",
      "loss: 0.05579851493239403, train acc: 0.9893\n",
      "loss: 0.05045856907963753, train acc: 0.9902\n",
      "epoch: 35, loss: 0.010046598501503468, train acc: 0.9902, test acc: 0.9308\n",
      "loss: 0.05791586637496948, train acc: 0.9901\n",
      "loss: 0.05935042016208172, train acc: 0.9838\n",
      "loss: 0.0548117458820343, train acc: 0.9893\n",
      "loss: 0.04925715290009976, train acc: 0.9902\n",
      "loss: 0.052535240352153775, train acc: 0.9911\n",
      "loss: 0.04888219684362412, train acc: 0.9863\n",
      "loss: 0.05312918871641159, train acc: 0.99\n",
      "loss: 0.048643091134727, train acc: 0.9912\n",
      "epoch: 36, loss: 0.00981127843260765, train acc: 0.9912, test acc: 0.9312\n",
      "loss: 0.05420663580298424, train acc: 0.9905\n",
      "loss: 0.05599446780979633, train acc: 0.9841\n",
      "loss: 0.05219157263636589, train acc: 0.9903\n",
      "loss: 0.047129756212234496, train acc: 0.9906\n",
      "loss: 0.05016879104077816, train acc: 0.9917\n",
      "loss: 0.04651455376297235, train acc: 0.9874\n",
      "loss: 0.05082905925810337, train acc: 0.9905\n",
      "loss: 0.04593218248337507, train acc: 0.9917\n",
      "epoch: 37, loss: 0.009183844551444054, train acc: 0.9917, test acc: 0.9311\n",
      "loss: 0.05120164155960083, train acc: 0.9912\n",
      "loss: 0.05341493412852287, train acc: 0.9855\n",
      "loss: 0.049267285689711574, train acc: 0.9909\n",
      "loss: 0.044842662662267684, train acc: 0.9914\n",
      "loss: 0.04799535032361746, train acc: 0.9925\n",
      "loss: 0.0443358501419425, train acc: 0.9874\n",
      "loss: 0.0489479573443532, train acc: 0.9913\n",
      "loss: 0.044310709647834304, train acc: 0.9927\n",
      "epoch: 38, loss: 0.009031237103044987, train acc: 0.9927, test acc: 0.9317\n",
      "loss: 0.04901685565710068, train acc: 0.9912\n",
      "loss: 0.050405490398406985, train acc: 0.9863\n",
      "loss: 0.0465290192514658, train acc: 0.9916\n",
      "loss: 0.042713756859302524, train acc: 0.9922\n",
      "loss: 0.04571190103888512, train acc: 0.9928\n",
      "loss: 0.04127354770898819, train acc: 0.9893\n",
      "loss: 0.0460218496620655, train acc: 0.9923\n",
      "loss: 0.0414989348500967, train acc: 0.9932\n",
      "epoch: 39, loss: 0.008052892051637173, train acc: 0.9932, test acc: 0.932\n",
      "loss: 0.04737098887562752, train acc: 0.9918\n",
      "loss: 0.048292431980371475, train acc: 0.9864\n",
      "loss: 0.04440102316439152, train acc: 0.9924\n",
      "loss: 0.04068643543869257, train acc: 0.9929\n",
      "loss: 0.04334018817171455, train acc: 0.9938\n",
      "loss: 0.0397623473778367, train acc: 0.9891\n",
      "loss: 0.04427161347121, train acc: 0.9925\n",
      "loss: 0.04011130761355162, train acc: 0.9934\n",
      "epoch: 40, loss: 0.008165791630744934, train acc: 0.9934, test acc: 0.9318\n",
      "loss: 0.04439990967512131, train acc: 0.992\n",
      "loss: 0.04577043503522873, train acc: 0.9877\n",
      "loss: 0.04164123609662056, train acc: 0.9936\n",
      "loss: 0.03853141609579325, train acc: 0.9934\n",
      "loss: 0.04164709709584713, train acc: 0.9939\n",
      "loss: 0.037334483303129676, train acc: 0.9899\n",
      "loss: 0.04218158572912216, train acc: 0.993\n",
      "loss: 0.03812841940671206, train acc: 0.9941\n",
      "epoch: 41, loss: 0.007783721666783094, train acc: 0.9941, test acc: 0.9317\n",
      "loss: 0.04166416823863983, train acc: 0.9928\n",
      "loss: 0.04289863128215075, train acc: 0.9885\n",
      "loss: 0.039116062596440314, train acc: 0.9945\n",
      "loss: 0.036879867501556876, train acc: 0.9936\n",
      "loss: 0.03931294484063983, train acc: 0.9941\n",
      "loss: 0.03545661177486181, train acc: 0.9898\n",
      "loss: 0.04024438317865133, train acc: 0.9936\n",
      "loss: 0.036160805635154246, train acc: 0.9942\n",
      "epoch: 42, loss: 0.007269607856869698, train acc: 0.9942, test acc: 0.9316\n",
      "loss: 0.04034103453159332, train acc: 0.9927\n",
      "loss: 0.04137450233101845, train acc: 0.9888\n",
      "loss: 0.03784627374261618, train acc: 0.995\n",
      "loss: 0.03489538338035345, train acc: 0.9937\n",
      "loss: 0.037852308433502915, train acc: 0.9945\n",
      "loss: 0.033872305788099764, train acc: 0.9907\n",
      "loss: 0.038300282694399355, train acc: 0.9937\n",
      "loss: 0.03455412667244673, train acc: 0.9947\n",
      "epoch: 43, loss: 0.0070074377581477165, train acc: 0.9947, test acc: 0.9312\n",
      "loss: 0.03791259229183197, train acc: 0.9932\n",
      "loss: 0.03878401592373848, train acc: 0.9897\n",
      "loss: 0.035353500954806806, train acc: 0.9955\n",
      "loss: 0.03316116258502007, train acc: 0.9942\n",
      "loss: 0.03545609721913934, train acc: 0.9947\n",
      "loss: 0.03161532338708639, train acc: 0.9907\n",
      "loss: 0.036493961699306965, train acc: 0.9944\n",
      "loss: 0.03292140122503042, train acc: 0.9943\n",
      "epoch: 44, loss: 0.0063544404692947865, train acc: 0.9943, test acc: 0.9313\n",
      "loss: 0.03633115440607071, train acc: 0.9936\n",
      "loss: 0.037429486587643626, train acc: 0.9895\n",
      "loss: 0.03354542758315802, train acc: 0.996\n",
      "loss: 0.031391883827745916, train acc: 0.9942\n",
      "loss: 0.033860277477651836, train acc: 0.995\n",
      "loss: 0.030022748932242393, train acc: 0.9915\n",
      "loss: 0.03455580528825521, train acc: 0.9946\n",
      "loss: 0.03100200481712818, train acc: 0.995\n",
      "epoch: 45, loss: 0.006179576739668846, train acc: 0.995, test acc: 0.9313\n",
      "loss: 0.03435308858752251, train acc: 0.9939\n",
      "loss: 0.03455162476748228, train acc: 0.9911\n",
      "loss: 0.03188650906085968, train acc: 0.9964\n",
      "loss: 0.029783491604030134, train acc: 0.9945\n",
      "loss: 0.03203788083046675, train acc: 0.9952\n",
      "loss: 0.028710272535681723, train acc: 0.9922\n",
      "loss: 0.03322071637958288, train acc: 0.9951\n",
      "loss: 0.029891524091362955, train acc: 0.9951\n",
      "epoch: 46, loss: 0.00593910226598382, train acc: 0.9951, test acc: 0.9319\n",
      "loss: 0.03143475577235222, train acc: 0.9937\n",
      "loss: 0.033088503032922746, train acc: 0.9918\n",
      "loss: 0.02973513714969158, train acc: 0.997\n",
      "loss: 0.02831452004611492, train acc: 0.9944\n",
      "loss: 0.030267967935651542, train acc: 0.9961\n",
      "loss: 0.026727833040058612, train acc: 0.9927\n",
      "loss: 0.03144271969795227, train acc: 0.9956\n",
      "loss: 0.028319539874792098, train acc: 0.9955\n",
      "epoch: 47, loss: 0.005595595110207796, train acc: 0.9955, test acc: 0.9318\n",
      "loss: 0.03172168880701065, train acc: 0.9941\n",
      "loss: 0.031353107653558254, train acc: 0.9919\n",
      "loss: 0.028831829130649567, train acc: 0.9974\n",
      "loss: 0.02690446060150862, train acc: 0.9945\n",
      "loss: 0.028638239670544862, train acc: 0.9961\n",
      "loss: 0.025662220641970636, train acc: 0.9933\n",
      "loss: 0.029982591420412062, train acc: 0.9961\n",
      "loss: 0.02696768082678318, train acc: 0.9961\n",
      "epoch: 48, loss: 0.0051374295726418495, train acc: 0.9961, test acc: 0.9318\n",
      "loss: 0.027672255411744118, train acc: 0.9944\n",
      "loss: 0.0294778598472476, train acc: 0.993\n",
      "loss: 0.027256493270397187, train acc: 0.9975\n",
      "loss: 0.025586366839706897, train acc: 0.9944\n",
      "loss: 0.02733960943296552, train acc: 0.9967\n",
      "loss: 0.024851604737341403, train acc: 0.9931\n",
      "loss: 0.028981531038880348, train acc: 0.9964\n",
      "loss: 0.025860140193253757, train acc: 0.9961\n",
      "epoch: 49, loss: 0.004848063923418522, train acc: 0.9961, test acc: 0.9315\n",
      "loss: 0.025961311534047127, train acc: 0.9944\n",
      "loss: 0.02786830868571997, train acc: 0.9928\n",
      "loss: 0.025633052084594966, train acc: 0.9977\n",
      "loss: 0.024253638461232185, train acc: 0.995\n",
      "loss: 0.025473027024418116, train acc: 0.9964\n",
      "loss: 0.022964898403733967, train acc: 0.9936\n",
      "loss: 0.027392392791807652, train acc: 0.9966\n",
      "loss: 0.024393736943602563, train acc: 0.9962\n",
      "epoch: 50, loss: 0.004347531590610743, train acc: 0.9962, test acc: 0.9315\n",
      "loss: 0.02450176700949669, train acc: 0.9951\n",
      "loss: 0.026326821744441987, train acc: 0.9939\n",
      "loss: 0.024599875323474407, train acc: 0.9981\n",
      "loss: 0.022836824879050255, train acc: 0.9951\n",
      "loss: 0.024130980484187603, train acc: 0.9965\n",
      "loss: 0.022044525295495988, train acc: 0.9941\n",
      "loss: 0.02616653498262167, train acc: 0.997\n",
      "loss: 0.022956629283726215, train acc: 0.9966\n",
      "epoch: 51, loss: 0.004236159380525351, train acc: 0.9966, test acc: 0.9308\n",
      "loss: 0.023371480405330658, train acc: 0.9955\n",
      "loss: 0.02492747437208891, train acc: 0.9939\n",
      "loss: 0.023382936231791975, train acc: 0.9983\n",
      "loss: 0.02170511968433857, train acc: 0.9947\n",
      "loss: 0.022859278321266174, train acc: 0.9975\n",
      "loss: 0.02121409373357892, train acc: 0.9938\n",
      "loss: 0.024890925176441668, train acc: 0.9972\n",
      "loss: 0.02197116082534194, train acc: 0.9965\n",
      "epoch: 52, loss: 0.0038678699638694525, train acc: 0.9965, test acc: 0.9312\n",
      "loss: 0.02207748033106327, train acc: 0.9955\n",
      "loss: 0.023236137442290784, train acc: 0.9944\n",
      "loss: 0.022261917777359485, train acc: 0.9985\n",
      "loss: 0.020678425021469594, train acc: 0.9959\n",
      "loss: 0.021540463250130416, train acc: 0.9974\n",
      "loss: 0.019972712453454732, train acc: 0.9948\n",
      "loss: 0.023223461117595434, train acc: 0.997\n",
      "loss: 0.02058914601802826, train acc: 0.9968\n",
      "epoch: 53, loss: 0.0037036261055618525, train acc: 0.9968, test acc: 0.9313\n",
      "loss: 0.02074645459651947, train acc: 0.9961\n",
      "loss: 0.022368056792765857, train acc: 0.9942\n",
      "loss: 0.021045228000730276, train acc: 0.9986\n",
      "loss: 0.019573026243597268, train acc: 0.996\n",
      "loss: 0.020656884461641312, train acc: 0.9978\n",
      "loss: 0.019263307470828293, train acc: 0.9943\n",
      "loss: 0.022376963309943675, train acc: 0.9975\n",
      "loss: 0.019938809052109718, train acc: 0.9972\n",
      "epoch: 54, loss: 0.0034945665393024683, train acc: 0.9972, test acc: 0.93\n",
      "loss: 0.018916623666882515, train acc: 0.9962\n",
      "loss: 0.020736677665263416, train acc: 0.9949\n",
      "loss: 0.02026097346097231, train acc: 0.9987\n",
      "loss: 0.018713655602186918, train acc: 0.9965\n",
      "loss: 0.019245042465627193, train acc: 0.9978\n",
      "loss: 0.017993753030896186, train acc: 0.9951\n",
      "loss: 0.021076661814004184, train acc: 0.9976\n",
      "loss: 0.018702233023941518, train acc: 0.9973\n",
      "epoch: 55, loss: 0.0032845167443156242, train acc: 0.9973, test acc: 0.93\n",
      "loss: 0.01758471690118313, train acc: 0.9961\n",
      "loss: 0.01975620873272419, train acc: 0.9951\n",
      "loss: 0.01902112681418657, train acc: 0.9986\n",
      "loss: 0.017746540531516076, train acc: 0.9968\n",
      "loss: 0.01850302703678608, train acc: 0.9978\n",
      "loss: 0.017404749616980554, train acc: 0.9952\n",
      "loss: 0.019944926351308824, train acc: 0.9977\n",
      "loss: 0.017685484327375888, train acc: 0.9977\n",
      "epoch: 56, loss: 0.0030666079837828875, train acc: 0.9977, test acc: 0.9302\n",
      "loss: 0.01660129986703396, train acc: 0.9966\n",
      "loss: 0.018691164813935755, train acc: 0.9955\n",
      "loss: 0.017991012986749412, train acc: 0.9988\n",
      "loss: 0.016647317353636025, train acc: 0.997\n",
      "loss: 0.0174541431479156, train acc: 0.9977\n",
      "loss: 0.01670437827706337, train acc: 0.9953\n",
      "loss: 0.01888022366911173, train acc: 0.998\n",
      "loss: 0.017013961914926767, train acc: 0.9974\n",
      "epoch: 57, loss: 0.0028403964824974537, train acc: 0.9974, test acc: 0.9298\n",
      "loss: 0.015895986929535866, train acc: 0.9967\n",
      "loss: 0.017358218505978584, train acc: 0.996\n",
      "loss: 0.01700582858175039, train acc: 0.9987\n",
      "loss: 0.01599459620192647, train acc: 0.9968\n",
      "loss: 0.01658781887963414, train acc: 0.9979\n",
      "loss: 0.01569616897031665, train acc: 0.9963\n",
      "loss: 0.017680243775248527, train acc: 0.9977\n",
      "loss: 0.01612399462610483, train acc: 0.9979\n",
      "epoch: 58, loss: 0.0026986231096088886, train acc: 0.9979, test acc: 0.9303\n",
      "loss: 0.014898606576025486, train acc: 0.9966\n",
      "loss: 0.016679118480533363, train acc: 0.9958\n",
      "loss: 0.016074421163648368, train acc: 0.9988\n",
      "loss: 0.015224590245634318, train acc: 0.9974\n",
      "loss: 0.015732809575274585, train acc: 0.9979\n",
      "loss: 0.015251976158469915, train acc: 0.9961\n",
      "loss: 0.016596005763858557, train acc: 0.9972\n",
      "loss: 0.015360555890947581, train acc: 0.9978\n",
      "epoch: 59, loss: 0.0025684600695967674, train acc: 0.9978, test acc: 0.9304\n",
      "loss: 0.013786187395453453, train acc: 0.9968\n",
      "loss: 0.015827220864593983, train acc: 0.9961\n",
      "loss: 0.015599665045738221, train acc: 0.9988\n",
      "loss: 0.014488058257848024, train acc: 0.9975\n",
      "loss: 0.015029140515252948, train acc: 0.9976\n",
      "loss: 0.014484808966517448, train acc: 0.9962\n",
      "loss: 0.015886666905134916, train acc: 0.9973\n",
      "loss: 0.014790572505444289, train acc: 0.9977\n",
      "epoch: 60, loss: 0.002300713211297989, train acc: 0.9977, test acc: 0.9296\n",
      "loss: 0.012627660296857357, train acc: 0.9971\n",
      "loss: 0.014666879363358021, train acc: 0.9961\n",
      "loss: 0.014391099754720926, train acc: 0.9988\n",
      "loss: 0.013750448077917098, train acc: 0.9978\n",
      "loss: 0.014054443687200546, train acc: 0.9976\n",
      "loss: 0.014019836112856865, train acc: 0.9965\n",
      "loss: 0.014843036606907844, train acc: 0.9978\n",
      "loss: 0.014007799234241248, train acc: 0.9979\n",
      "epoch: 61, loss: 0.0021788510493934155, train acc: 0.9979, test acc: 0.9294\n",
      "loss: 0.012115351855754852, train acc: 0.9974\n",
      "loss: 0.014167993143200875, train acc: 0.9963\n",
      "loss: 0.013799147075042128, train acc: 0.9986\n",
      "loss: 0.013232940062880515, train acc: 0.9982\n",
      "loss: 0.013613918190822005, train acc: 0.9974\n",
      "loss: 0.013453657180070877, train acc: 0.9971\n",
      "loss: 0.014148459117859603, train acc: 0.9971\n",
      "loss: 0.013614293932914735, train acc: 0.9977\n",
      "epoch: 62, loss: 0.0020984362345188856, train acc: 0.9977, test acc: 0.9298\n",
      "loss: 0.011365463957190514, train acc: 0.997\n",
      "loss: 0.013070452120155096, train acc: 0.996\n",
      "loss: 0.01285685896873474, train acc: 0.9988\n",
      "loss: 0.012524091359227895, train acc: 0.9984\n",
      "loss: 0.01282454626634717, train acc: 0.9973\n",
      "loss: 0.012731353472918271, train acc: 0.9969\n",
      "loss: 0.013238483294844627, train acc: 0.9977\n",
      "loss: 0.013046648818999529, train acc: 0.9975\n",
      "epoch: 63, loss: 0.0019106005784124136, train acc: 0.9975, test acc: 0.9298\n",
      "loss: 0.010960172861814499, train acc: 0.9971\n",
      "loss: 0.012574641639366746, train acc: 0.9965\n",
      "loss: 0.01238371809013188, train acc: 0.9986\n",
      "loss: 0.011915389634668826, train acc: 0.9986\n",
      "loss: 0.01210079612210393, train acc: 0.9973\n",
      "loss: 0.012087673880159855, train acc: 0.9975\n",
      "loss: 0.01272844960913062, train acc: 0.9975\n",
      "loss: 0.012429051427170634, train acc: 0.9976\n",
      "epoch: 64, loss: 0.0017760624177753925, train acc: 0.9976, test acc: 0.9294\n",
      "loss: 0.0103105204179883, train acc: 0.9969\n",
      "loss: 0.011856163060292602, train acc: 0.9959\n",
      "loss: 0.011679726000875235, train acc: 0.9986\n",
      "loss: 0.011470864992588758, train acc: 0.9989\n",
      "loss: 0.011743375379592181, train acc: 0.9967\n",
      "loss: 0.011955949291586877, train acc: 0.9972\n",
      "loss: 0.012038296833634377, train acc: 0.9974\n",
      "loss: 0.0118926374707371, train acc: 0.9976\n",
      "epoch: 65, loss: 0.0016503597144037485, train acc: 0.9976, test acc: 0.9292\n",
      "loss: 0.009665745310485363, train acc: 0.9974\n",
      "loss: 0.011303464975208044, train acc: 0.996\n",
      "loss: 0.011055026995018124, train acc: 0.9982\n",
      "loss: 0.011093608848750592, train acc: 0.9991\n",
      "loss: 0.011215551989153028, train acc: 0.9971\n",
      "loss: 0.011284034652635454, train acc: 0.9977\n",
      "loss: 0.011274209432303905, train acc: 0.9977\n",
      "loss: 0.011255909455940127, train acc: 0.9973\n",
      "epoch: 66, loss: 0.001509817666374147, train acc: 0.9973, test acc: 0.9291\n",
      "loss: 0.00942622497677803, train acc: 0.9971\n",
      "loss: 0.010747342417016626, train acc: 0.995\n",
      "loss: 0.010433221748098732, train acc: 0.9987\n",
      "loss: 0.010348929138854147, train acc: 0.9991\n",
      "loss: 0.010577248595654964, train acc: 0.9971\n",
      "loss: 0.011137533467262983, train acc: 0.9981\n",
      "loss: 0.010797129711136222, train acc: 0.998\n",
      "loss: 0.010693076951429247, train acc: 0.9973\n",
      "epoch: 67, loss: 0.001431059092283249, train acc: 0.9973, test acc: 0.929\n",
      "loss: 0.009376263245940208, train acc: 0.9974\n",
      "loss: 0.010280776303261518, train acc: 0.9949\n",
      "loss: 0.009967700811102986, train acc: 0.9986\n",
      "loss: 0.009855819214135409, train acc: 0.9991\n",
      "loss: 0.009938469529151917, train acc: 0.9968\n",
      "loss: 0.010392023250460625, train acc: 0.9984\n",
      "loss: 0.010222399234771728, train acc: 0.9981\n",
      "loss: 0.010232152976095676, train acc: 0.9973\n",
      "epoch: 68, loss: 0.0013454059371724725, train acc: 0.9973, test acc: 0.9286\n",
      "loss: 0.009029088541865349, train acc: 0.9973\n",
      "loss: 0.009873760351911186, train acc: 0.9943\n",
      "loss: 0.009478239761665463, train acc: 0.9986\n",
      "loss: 0.009596816543489695, train acc: 0.9989\n",
      "loss: 0.009781272662803531, train acc: 0.9972\n",
      "loss: 0.00999021683819592, train acc: 0.9985\n",
      "loss: 0.00972352959215641, train acc: 0.9982\n",
      "loss: 0.009661799343302847, train acc: 0.9973\n",
      "epoch: 69, loss: 0.0012541388859972358, train acc: 0.9973, test acc: 0.9291\n",
      "loss: 0.00929278414696455, train acc: 0.998\n",
      "loss: 0.009295068634673952, train acc: 0.9937\n",
      "loss: 0.008957615308463573, train acc: 0.9987\n",
      "loss: 0.008972036093473435, train acc: 0.9989\n",
      "loss: 0.009208340710029007, train acc: 0.9967\n",
      "loss: 0.00988755407743156, train acc: 0.9984\n",
      "loss: 0.009305454371497036, train acc: 0.9979\n",
      "loss: 0.00929668010212481, train acc: 0.9972\n",
      "epoch: 70, loss: 0.0011707606026902795, train acc: 0.9972, test acc: 0.929\n",
      "loss: 0.009447009302675724, train acc: 0.9982\n",
      "loss: 0.009177599055692553, train acc: 0.9936\n",
      "loss: 0.00868975194171071, train acc: 0.9986\n",
      "loss: 0.008706202730536461, train acc: 0.9991\n",
      "loss: 0.00884367381222546, train acc: 0.997\n",
      "loss: 0.009229515725746752, train acc: 0.9986\n",
      "loss: 0.008825326431542635, train acc: 0.9978\n",
      "loss: 0.008804850233718752, train acc: 0.9973\n",
      "epoch: 71, loss: 0.0011198726715520024, train acc: 0.9973, test acc: 0.9292\n",
      "loss: 0.008949654176831245, train acc: 0.9984\n",
      "loss: 0.008710480900481344, train acc: 0.9937\n",
      "loss: 0.008356541767716408, train acc: 0.9987\n",
      "loss: 0.008237899793311954, train acc: 0.9989\n",
      "loss: 0.008579513756558299, train acc: 0.997\n",
      "loss: 0.008750193612650036, train acc: 0.9987\n",
      "loss: 0.008458296908065677, train acc: 0.9975\n",
      "loss: 0.008496330538764595, train acc: 0.9971\n",
      "epoch: 72, loss: 0.0009985803626477718, train acc: 0.9971, test acc: 0.9296\n",
      "loss: 0.009259269572794437, train acc: 0.9986\n",
      "loss: 0.008434828277677298, train acc: 0.9932\n",
      "loss: 0.008094480261206627, train acc: 0.9987\n",
      "loss: 0.007737210486084223, train acc: 0.9988\n",
      "loss: 0.008058951189741492, train acc: 0.9972\n",
      "loss: 0.008436995884403586, train acc: 0.9987\n",
      "loss: 0.008070387877523899, train acc: 0.9974\n",
      "loss: 0.008189328713342547, train acc: 0.997\n",
      "epoch: 73, loss: 0.0009513723198324442, train acc: 0.997, test acc: 0.9298\n",
      "loss: 0.009546521119773388, train acc: 0.9988\n",
      "loss: 0.008153486717492341, train acc: 0.9931\n",
      "loss: 0.007827902957797051, train acc: 0.9987\n",
      "loss: 0.007562143774703145, train acc: 0.9985\n",
      "loss: 0.0077107935445383195, train acc: 0.9977\n",
      "loss: 0.0078690217807889, train acc: 0.999\n",
      "loss: 0.007997838407754898, train acc: 0.9972\n",
      "loss: 0.007801056699827313, train acc: 0.9972\n",
      "epoch: 74, loss: 0.0008800141513347626, train acc: 0.9972, test acc: 0.9299\n",
      "loss: 0.009204483591020107, train acc: 0.9991\n",
      "loss: 0.007779354928061366, train acc: 0.9932\n",
      "loss: 0.007556485757231712, train acc: 0.999\n",
      "loss: 0.007099855039268732, train acc: 0.9985\n",
      "loss: 0.007367936871014535, train acc: 0.9978\n",
      "loss: 0.0075854573398828505, train acc: 0.9988\n",
      "loss: 0.007354422938078642, train acc: 0.9971\n",
      "loss: 0.007786724157631398, train acc: 0.9973\n",
      "epoch: 75, loss: 0.0008451890898868442, train acc: 0.9973, test acc: 0.9299\n",
      "loss: 0.009351314045488834, train acc: 0.999\n",
      "loss: 0.007791127078235149, train acc: 0.9924\n",
      "loss: 0.0074137756135314705, train acc: 0.9989\n",
      "loss: 0.006960826739668846, train acc: 0.9983\n",
      "loss: 0.007307231519371271, train acc: 0.998\n",
      "loss: 0.007117548258975148, train acc: 0.9986\n",
      "loss: 0.007021126756444574, train acc: 0.9972\n",
      "loss: 0.007359808590263128, train acc: 0.9972\n",
      "epoch: 76, loss: 0.0008253449341282248, train acc: 0.9972, test acc: 0.9307\n",
      "loss: 0.009071137756109238, train acc: 0.9993\n",
      "loss: 0.007226199749857188, train acc: 0.9929\n",
      "loss: 0.007097143959254026, train acc: 0.999\n",
      "loss: 0.00652036196552217, train acc: 0.9979\n",
      "loss: 0.007019790168851614, train acc: 0.9981\n",
      "loss: 0.0068889800924807785, train acc: 0.9987\n",
      "loss: 0.0068313113646581766, train acc: 0.9973\n",
      "loss: 0.006852494669146836, train acc: 0.9975\n",
      "epoch: 77, loss: 0.0007771945092827082, train acc: 0.9975, test acc: 0.9299\n",
      "loss: 0.009350267238914967, train acc: 0.9993\n",
      "loss: 0.007098815543577075, train acc: 0.9916\n",
      "loss: 0.006875090580433607, train acc: 0.9991\n",
      "loss: 0.006150635285302996, train acc: 0.9978\n",
      "loss: 0.006694706971757114, train acc: 0.9985\n",
      "loss: 0.006458537420257926, train acc: 0.9988\n",
      "loss: 0.006903162854723632, train acc: 0.9964\n",
      "loss: 0.007252978137694299, train acc: 0.9971\n",
      "epoch: 78, loss: 0.0007302755839191377, train acc: 0.9971, test acc: 0.9298\n",
      "loss: 0.008417999371886253, train acc: 0.999\n",
      "loss: 0.0066619341028854254, train acc: 0.9926\n",
      "loss: 0.006683236081153154, train acc: 0.999\n",
      "loss: 0.005921533284708858, train acc: 0.9974\n",
      "loss: 0.006525637581944466, train acc: 0.9981\n",
      "loss: 0.006254781037569046, train acc: 0.9986\n",
      "loss: 0.006258095940575003, train acc: 0.9971\n",
      "loss: 0.00655799712985754, train acc: 0.9976\n",
      "epoch: 79, loss: 0.0006993935094214976, train acc: 0.9976, test acc: 0.9298\n",
      "#####training and testing end with K:20, P:1######\n",
      "#####training and testing start with K:40, P:0.1######\n",
      "loss: 2.408967971801758, train acc: 0.1875\n",
      "loss: 2.032744514942169, train acc: 0.5604\n",
      "loss: 1.3574711799621582, train acc: 0.7676\n",
      "loss: 0.9477107763290405, train acc: 0.824\n",
      "loss: 0.745790708065033, train acc: 0.8363\n",
      "loss: 0.5999935209751129, train acc: 0.8738\n",
      "loss: 0.49718782901763914, train acc: 0.8861\n",
      "loss: 0.5167902857065201, train acc: 0.8892\n",
      "epoch: 0, loss: 0.16082575917243958, train acc: 0.8892, test acc: 0.8886\n",
      "loss: 0.5040299296379089, train acc: 0.8945\n",
      "loss: 0.46723769009113314, train acc: 0.8926\n",
      "loss: 0.40777885019779203, train acc: 0.9\n",
      "loss: 0.42081778347492216, train acc: 0.9064\n",
      "loss: 0.4091911822557449, train acc: 0.9071\n",
      "loss: 0.3682985305786133, train acc: 0.9111\n",
      "loss: 0.3352543547749519, train acc: 0.9169\n",
      "loss: 0.36132498979568484, train acc: 0.9129\n",
      "epoch: 1, loss: 0.08354867994785309, train acc: 0.9129, test acc: 0.905\n",
      "loss: 0.3718179166316986, train acc: 0.9146\n",
      "loss: 0.34641458094120026, train acc: 0.9166\n",
      "loss: 0.3332982867956161, train acc: 0.9226\n",
      "loss: 0.34189789593219755, train acc: 0.9248\n",
      "loss: 0.34394501745700834, train acc: 0.9234\n",
      "loss: 0.30651999562978743, train acc: 0.9245\n",
      "loss: 0.2761464431881905, train acc: 0.9291\n",
      "loss: 0.29990292638540267, train acc: 0.9269\n",
      "epoch: 2, loss: 0.04168818145990372, train acc: 0.9269, test acc: 0.9145\n",
      "loss: 0.31831637024879456, train acc: 0.9294\n",
      "loss: 0.3119866088032722, train acc: 0.9293\n",
      "loss: 0.2929986909031868, train acc: 0.9345\n",
      "loss: 0.31151566803455355, train acc: 0.934\n",
      "loss: 0.2919372648000717, train acc: 0.9298\n",
      "loss: 0.2751335591077805, train acc: 0.9358\n",
      "loss: 0.2316472128033638, train acc: 0.9379\n",
      "loss: 0.2660575181245804, train acc: 0.9364\n",
      "epoch: 3, loss: 0.021873714402318, train acc: 0.9364, test acc: 0.9191\n",
      "loss: 0.347987562417984, train acc: 0.9386\n",
      "loss: 0.2782714679837227, train acc: 0.9397\n",
      "loss: 0.25263328328728674, train acc: 0.9403\n",
      "loss: 0.2700546994805336, train acc: 0.9426\n",
      "loss: 0.263369183242321, train acc: 0.9401\n",
      "loss: 0.23869034945964812, train acc: 0.9435\n",
      "loss: 0.20300928354263306, train acc: 0.9448\n",
      "loss: 0.22534299939870833, train acc: 0.9429\n",
      "epoch: 4, loss: 0.021340832114219666, train acc: 0.9429, test acc: 0.9257\n",
      "loss: 0.23873960971832275, train acc: 0.9452\n",
      "loss: 0.24909375309944154, train acc: 0.9472\n",
      "loss: 0.23863258585333824, train acc: 0.948\n",
      "loss: 0.23869653642177582, train acc: 0.9486\n",
      "loss: 0.24170679599046707, train acc: 0.949\n",
      "loss: 0.21733473837375641, train acc: 0.9512\n",
      "loss: 0.18422511368989944, train acc: 0.9511\n",
      "loss: 0.202408017963171, train acc: 0.9499\n",
      "epoch: 5, loss: 0.028297211974859238, train acc: 0.9499, test acc: 0.9285\n",
      "loss: 0.24298545718193054, train acc: 0.9503\n",
      "loss: 0.22688619196414947, train acc: 0.9523\n",
      "loss: 0.21560014709830283, train acc: 0.9549\n",
      "loss: 0.21053546965122222, train acc: 0.9535\n",
      "loss: 0.22809529304504395, train acc: 0.9527\n",
      "loss: 0.20643440037965774, train acc: 0.956\n",
      "loss: 0.1756860874593258, train acc: 0.9569\n",
      "loss: 0.18822259455919266, train acc: 0.9566\n",
      "epoch: 6, loss: 0.012303871102631092, train acc: 0.9566, test acc: 0.9324\n",
      "loss: 0.20751358568668365, train acc: 0.9581\n",
      "loss: 0.20586791783571243, train acc: 0.9581\n",
      "loss: 0.1911894641816616, train acc: 0.9596\n",
      "loss: 0.19825857728719712, train acc: 0.9606\n",
      "loss: 0.204097718000412, train acc: 0.9588\n",
      "loss: 0.1858220539987087, train acc: 0.9589\n",
      "loss: 0.14645753279328347, train acc: 0.9606\n",
      "loss: 0.16616027653217316, train acc: 0.9589\n",
      "epoch: 7, loss: 0.01826692372560501, train acc: 0.9589, test acc: 0.9347\n",
      "loss: 0.19627466797828674, train acc: 0.9607\n",
      "loss: 0.1932620845735073, train acc: 0.9618\n",
      "loss: 0.17977774143218994, train acc: 0.9641\n",
      "loss: 0.19845352023839952, train acc: 0.9655\n",
      "loss: 0.20334134846925736, train acc: 0.9626\n",
      "loss: 0.16839062198996543, train acc: 0.9618\n",
      "loss: 0.1347195588052273, train acc: 0.9652\n",
      "loss: 0.15387884378433228, train acc: 0.963\n",
      "epoch: 8, loss: 0.01506548747420311, train acc: 0.963, test acc: 0.9365\n",
      "loss: 0.19368356466293335, train acc: 0.9662\n",
      "loss: 0.17761698737740517, train acc: 0.9643\n",
      "loss: 0.1639673538506031, train acc: 0.9673\n",
      "loss: 0.17338743507862092, train acc: 0.9659\n",
      "loss: 0.18349337726831436, train acc: 0.9663\n",
      "loss: 0.1721998631954193, train acc: 0.9647\n",
      "loss: 0.129424137622118, train acc: 0.968\n",
      "loss: 0.1547462113201618, train acc: 0.9668\n",
      "epoch: 9, loss: 0.02436031773686409, train acc: 0.9668, test acc: 0.9388\n",
      "loss: 0.1651618927717209, train acc: 0.9684\n",
      "loss: 0.16094259396195412, train acc: 0.9667\n",
      "loss: 0.1729302816092968, train acc: 0.9688\n",
      "loss: 0.16822405382990838, train acc: 0.9707\n",
      "loss: 0.16676413118839264, train acc: 0.9716\n",
      "loss: 0.14616921693086624, train acc: 0.9698\n",
      "loss: 0.12360324189066887, train acc: 0.9707\n",
      "loss: 0.13721710816025734, train acc: 0.9696\n",
      "epoch: 10, loss: 0.006884416565299034, train acc: 0.9696, test acc: 0.941\n",
      "loss: 0.21923798322677612, train acc: 0.9715\n",
      "loss: 0.16058841943740845, train acc: 0.9699\n",
      "loss: 0.1536552496254444, train acc: 0.973\n",
      "loss: 0.1626851126551628, train acc: 0.9729\n",
      "loss: 0.16114085763692856, train acc: 0.9728\n",
      "loss: 0.13232142776250838, train acc: 0.972\n",
      "loss: 0.12073382921516895, train acc: 0.9725\n",
      "loss: 0.13487054109573365, train acc: 0.9707\n",
      "epoch: 11, loss: 0.005133485421538353, train acc: 0.9707, test acc: 0.9407\n",
      "loss: 0.13587847352027893, train acc: 0.9753\n",
      "loss: 0.13282317221164702, train acc: 0.973\n",
      "loss: 0.14662424400448798, train acc: 0.9748\n",
      "loss: 0.13174809738993645, train acc: 0.9764\n",
      "loss: 0.1455663576722145, train acc: 0.9745\n",
      "loss: 0.13070226535201074, train acc: 0.9728\n",
      "loss: 0.1066828653216362, train acc: 0.9744\n",
      "loss: 0.12460984252393245, train acc: 0.9746\n",
      "epoch: 12, loss: 0.005690163467079401, train acc: 0.9746, test acc: 0.9392\n",
      "loss: 0.20112870633602142, train acc: 0.9759\n",
      "loss: 0.13462281599640846, train acc: 0.9761\n",
      "loss: 0.12662194333970547, train acc: 0.9764\n",
      "loss: 0.13789672330021857, train acc: 0.9776\n",
      "loss: 0.14305337741971016, train acc: 0.9775\n",
      "loss: 0.1216997291892767, train acc: 0.9756\n",
      "loss: 0.11675712540745735, train acc: 0.9761\n",
      "loss: 0.11715213656425476, train acc: 0.9752\n",
      "epoch: 13, loss: 0.012081289663910866, train acc: 0.9752, test acc: 0.9393\n",
      "loss: 0.11520286649465561, train acc: 0.9792\n",
      "loss: 0.11846872121095657, train acc: 0.9778\n",
      "loss: 0.12865862250328064, train acc: 0.9793\n",
      "loss: 0.1298917129635811, train acc: 0.9795\n",
      "loss: 0.13077289536595343, train acc: 0.9787\n",
      "loss: 0.1082024984061718, train acc: 0.9769\n",
      "loss: 0.10602353699505329, train acc: 0.9771\n",
      "loss: 0.11653567850589752, train acc: 0.9767\n",
      "epoch: 14, loss: 0.008837643079459667, train acc: 0.9767, test acc: 0.9409\n",
      "loss: 0.11311200261116028, train acc: 0.981\n",
      "loss: 0.11941892132163048, train acc: 0.9782\n",
      "loss: 0.12163795344531536, train acc: 0.9808\n",
      "loss: 0.1160014308989048, train acc: 0.9811\n",
      "loss: 0.11917500048875809, train acc: 0.9805\n",
      "loss: 0.1028103195130825, train acc: 0.9804\n",
      "loss: 0.09230109080672264, train acc: 0.9804\n",
      "loss: 0.09780525714159012, train acc: 0.9814\n",
      "epoch: 15, loss: 0.017569271847605705, train acc: 0.9814, test acc: 0.9393\n",
      "loss: 0.1421349048614502, train acc: 0.9833\n",
      "loss: 0.10575555190443993, train acc: 0.9814\n",
      "loss: 0.11771737188100814, train acc: 0.9829\n",
      "loss: 0.11509599089622498, train acc: 0.9824\n",
      "loss: 0.12024651989340782, train acc: 0.982\n",
      "loss: 0.09683114215731621, train acc: 0.9817\n",
      "loss: 0.08202807046473026, train acc: 0.9814\n",
      "loss: 0.10137650221586228, train acc: 0.9821\n",
      "epoch: 16, loss: 0.004423806443810463, train acc: 0.9821, test acc: 0.942\n",
      "loss: 0.10730452835559845, train acc: 0.9832\n",
      "loss: 0.09911178536713124, train acc: 0.983\n",
      "loss: 0.11260259076952935, train acc: 0.9839\n",
      "loss: 0.11221482157707215, train acc: 0.9842\n",
      "loss: 0.10955478772521018, train acc: 0.983\n",
      "loss: 0.080748575553298, train acc: 0.9839\n",
      "loss: 0.07899844236671924, train acc: 0.9834\n",
      "loss: 0.09532078094780445, train acc: 0.9841\n",
      "epoch: 17, loss: 0.005908485502004623, train acc: 0.9841, test acc: 0.941\n",
      "loss: 0.1216481477022171, train acc: 0.9862\n",
      "loss: 0.09634322263300418, train acc: 0.9858\n",
      "loss: 0.10921112187206745, train acc: 0.9868\n",
      "loss: 0.10223230794072151, train acc: 0.9861\n",
      "loss: 0.10707173570990562, train acc: 0.9859\n",
      "loss: 0.08755112662911416, train acc: 0.9846\n",
      "loss: 0.07646811958402396, train acc: 0.9835\n",
      "loss: 0.09416945576667786, train acc: 0.9833\n",
      "epoch: 18, loss: 0.028763841837644577, train acc: 0.9833, test acc: 0.9397\n",
      "loss: 0.12152229249477386, train acc: 0.9867\n",
      "loss: 0.09140579625964165, train acc: 0.9866\n",
      "loss: 0.09347366448491812, train acc: 0.9877\n",
      "loss: 0.11121305227279663, train acc: 0.9863\n",
      "loss: 0.10400701090693473, train acc: 0.9872\n",
      "loss: 0.07800698950886727, train acc: 0.9864\n",
      "loss: 0.07840742208063603, train acc: 0.9851\n",
      "loss: 0.081822151504457, train acc: 0.9868\n",
      "epoch: 19, loss: 0.009008594788610935, train acc: 0.9868, test acc: 0.9422\n",
      "loss: 0.07908879965543747, train acc: 0.9877\n",
      "loss: 0.08308172300457954, train acc: 0.9886\n",
      "loss: 0.09068489912897348, train acc: 0.9883\n",
      "loss: 0.0856219507753849, train acc: 0.9878\n",
      "loss: 0.09868053048849106, train acc: 0.9869\n",
      "loss: 0.07960083223879337, train acc: 0.9852\n",
      "loss: 0.06901755146682262, train acc: 0.9869\n",
      "loss: 0.08280599862337112, train acc: 0.986\n",
      "epoch: 20, loss: 0.00354420836083591, train acc: 0.986, test acc: 0.942\n",
      "loss: 0.13956770300865173, train acc: 0.9896\n",
      "loss: 0.08766721449792385, train acc: 0.9896\n",
      "loss: 0.09424670934677123, train acc: 0.9892\n",
      "loss: 0.09065053351223469, train acc: 0.9897\n",
      "loss: 0.09184958152472973, train acc: 0.9894\n",
      "loss: 0.08116466328501701, train acc: 0.9884\n",
      "loss: 0.06590274088084698, train acc: 0.9892\n",
      "loss: 0.08284778781235218, train acc: 0.9895\n",
      "epoch: 21, loss: 0.0035952781327068806, train acc: 0.9895, test acc: 0.9427\n",
      "loss: 0.09598982334136963, train acc: 0.9908\n",
      "loss: 0.08007592782378196, train acc: 0.9899\n",
      "loss: 0.07875148244202138, train acc: 0.9904\n",
      "loss: 0.07847156673669815, train acc: 0.9893\n",
      "loss: 0.0870888989418745, train acc: 0.9896\n",
      "loss: 0.07363969087600708, train acc: 0.9894\n",
      "loss: 0.06716833785176277, train acc: 0.9892\n",
      "loss: 0.06913425866514444, train acc: 0.9898\n",
      "epoch: 22, loss: 0.00294855865649879, train acc: 0.9898, test acc: 0.9428\n",
      "loss: 0.08917858451604843, train acc: 0.9906\n",
      "loss: 0.06509341821074485, train acc: 0.9904\n",
      "loss: 0.07994755115360022, train acc: 0.9918\n",
      "loss: 0.08427321165800095, train acc: 0.9903\n",
      "loss: 0.08937573917210102, train acc: 0.9901\n",
      "loss: 0.06989878714084626, train acc: 0.9895\n",
      "loss: 0.06069320701062679, train acc: 0.9901\n",
      "loss: 0.07469755597412586, train acc: 0.991\n",
      "epoch: 23, loss: 0.004105831496417522, train acc: 0.991, test acc: 0.9422\n",
      "loss: 0.08832569420337677, train acc: 0.9913\n",
      "loss: 0.07428178600966931, train acc: 0.9917\n",
      "loss: 0.07634022012352944, train acc: 0.9925\n",
      "loss: 0.07264943458139897, train acc: 0.9916\n",
      "loss: 0.07461272552609444, train acc: 0.9912\n",
      "loss: 0.07201369255781173, train acc: 0.9894\n",
      "loss: 0.06308794356882572, train acc: 0.9906\n",
      "loss: 0.07013327553868294, train acc: 0.9916\n",
      "epoch: 24, loss: 0.009171422570943832, train acc: 0.9916, test acc: 0.9424\n",
      "loss: 0.07805252820253372, train acc: 0.9914\n",
      "loss: 0.06633528508245945, train acc: 0.9918\n",
      "loss: 0.07028535939753056, train acc: 0.9928\n",
      "loss: 0.07388082928955556, train acc: 0.9933\n",
      "loss: 0.07582024149596692, train acc: 0.9905\n",
      "loss: 0.05532323159277439, train acc: 0.9922\n",
      "loss: 0.06229163557291031, train acc: 0.991\n",
      "loss: 0.06408282788470387, train acc: 0.9912\n",
      "epoch: 25, loss: 0.0023089363239705563, train acc: 0.9912, test acc: 0.9444\n",
      "loss: 0.0723961740732193, train acc: 0.9925\n",
      "loss: 0.0791931502521038, train acc: 0.9929\n",
      "loss: 0.06485067941248417, train acc: 0.992\n",
      "loss: 0.07167816869914531, train acc: 0.9925\n",
      "loss: 0.06882170010358095, train acc: 0.9922\n",
      "loss: 0.056066401302814484, train acc: 0.9913\n",
      "loss: 0.049653641134500506, train acc: 0.9913\n",
      "loss: 0.06259390674531459, train acc: 0.9924\n",
      "epoch: 26, loss: 0.003849520580843091, train acc: 0.9924, test acc: 0.9455\n",
      "loss: 0.08703765273094177, train acc: 0.9928\n",
      "loss: 0.06822310276329517, train acc: 0.9932\n",
      "loss: 0.06242331732064486, train acc: 0.9945\n",
      "loss: 0.057865652814507486, train acc: 0.994\n",
      "loss: 0.0601769357919693, train acc: 0.9919\n",
      "loss: 0.06383648626506329, train acc: 0.9908\n",
      "loss: 0.051253970339894296, train acc: 0.9914\n",
      "loss: 0.06772467941045761, train acc: 0.9904\n",
      "epoch: 27, loss: 0.006285097450017929, train acc: 0.9904, test acc: 0.9452\n",
      "loss: 0.09107670933008194, train acc: 0.9921\n",
      "loss: 0.07163087613880634, train acc: 0.9939\n",
      "loss: 0.06042945347726345, train acc: 0.9947\n",
      "loss: 0.06253909394145012, train acc: 0.9942\n",
      "loss: 0.06371632404625416, train acc: 0.9948\n",
      "loss: 0.04811287336051464, train acc: 0.9941\n",
      "loss: 0.05578137133270502, train acc: 0.9922\n",
      "loss: 0.05875583589076996, train acc: 0.9941\n",
      "epoch: 28, loss: 0.0026161447167396545, train acc: 0.9941, test acc: 0.9424\n",
      "loss: 0.06354758888483047, train acc: 0.9947\n",
      "loss: 0.05737463161349297, train acc: 0.9956\n",
      "loss: 0.061495143733918664, train acc: 0.9941\n",
      "loss: 0.05918736793100834, train acc: 0.9957\n",
      "loss: 0.058930874615907666, train acc: 0.9959\n",
      "loss: 0.04690250772982836, train acc: 0.9937\n",
      "loss: 0.05503324344754219, train acc: 0.9934\n",
      "loss: 0.04588319119066, train acc: 0.9943\n",
      "epoch: 29, loss: 0.005583729594945908, train acc: 0.9943, test acc: 0.9432\n",
      "loss: 0.05568723380565643, train acc: 0.9953\n",
      "loss: 0.0520688546821475, train acc: 0.996\n",
      "loss: 0.057347301952540876, train acc: 0.9955\n",
      "loss: 0.06493400819599629, train acc: 0.9953\n",
      "loss: 0.06020866893231869, train acc: 0.9951\n",
      "loss: 0.05936794243752956, train acc: 0.9951\n",
      "loss: 0.04242513831704855, train acc: 0.9938\n",
      "loss: 0.05180495008826256, train acc: 0.9938\n",
      "epoch: 30, loss: 0.007933635264635086, train acc: 0.9938, test acc: 0.9438\n",
      "loss: 0.05144571140408516, train acc: 0.9949\n",
      "loss: 0.04625238701701164, train acc: 0.9965\n",
      "loss: 0.05725055951625109, train acc: 0.9963\n",
      "loss: 0.06068870685994625, train acc: 0.9959\n",
      "loss: 0.06216217204928398, train acc: 0.9963\n",
      "loss: 0.05773310903459787, train acc: 0.996\n",
      "loss: 0.0354880066588521, train acc: 0.9946\n",
      "loss: 0.05248791668564081, train acc: 0.9941\n",
      "epoch: 31, loss: 0.00094141426961869, train acc: 0.9941, test acc: 0.9407\n",
      "loss: 0.08896984159946442, train acc: 0.9931\n",
      "loss: 0.06604104042053223, train acc: 0.996\n",
      "loss: 0.0536793852224946, train acc: 0.9951\n",
      "loss: 0.0469063401222229, train acc: 0.996\n",
      "loss: 0.05330803226679563, train acc: 0.9964\n",
      "loss: 0.05397242847830057, train acc: 0.9934\n",
      "loss: 0.048091143183410165, train acc: 0.994\n",
      "loss: 0.046760013233870266, train acc: 0.9953\n",
      "epoch: 32, loss: 0.0011495980434119701, train acc: 0.9953, test acc: 0.9452\n",
      "loss: 0.051097024232149124, train acc: 0.9961\n",
      "loss: 0.04863063134253025, train acc: 0.9964\n",
      "loss: 0.05048764366656542, train acc: 0.9963\n",
      "loss: 0.04567028731107712, train acc: 0.9968\n",
      "loss: 0.05422262419015169, train acc: 0.9963\n",
      "loss: 0.04264901950955391, train acc: 0.9959\n",
      "loss: 0.036344364006072286, train acc: 0.9931\n",
      "loss: 0.050848837569355966, train acc: 0.9957\n",
      "epoch: 33, loss: 0.005221545696258545, train acc: 0.9957, test acc: 0.9424\n",
      "loss: 0.03753673657774925, train acc: 0.9948\n",
      "loss: 0.04282418396323919, train acc: 0.997\n",
      "loss: 0.04873106479644775, train acc: 0.9957\n",
      "loss: 0.05835811570286751, train acc: 0.9976\n",
      "loss: 0.05126318708062172, train acc: 0.9965\n",
      "loss: 0.03541546631604433, train acc: 0.9957\n",
      "loss: 0.04522312749177217, train acc: 0.9951\n",
      "loss: 0.051414777897298335, train acc: 0.995\n",
      "epoch: 34, loss: 0.006513436324894428, train acc: 0.995, test acc: 0.9416\n",
      "loss: 0.09083662182092667, train acc: 0.995\n",
      "loss: 0.05152041632682085, train acc: 0.9961\n",
      "loss: 0.050667808018624784, train acc: 0.9964\n",
      "loss: 0.049649255350232124, train acc: 0.9962\n",
      "loss: 0.051512112468481065, train acc: 0.9973\n",
      "loss: 0.037503081001341344, train acc: 0.9964\n",
      "loss: 0.03809757772833109, train acc: 0.9959\n",
      "loss: 0.05581243541091681, train acc: 0.9946\n",
      "epoch: 35, loss: 0.0012650153366848826, train acc: 0.9946, test acc: 0.9413\n",
      "loss: 0.06089261919260025, train acc: 0.9932\n",
      "loss: 0.05146483946591616, train acc: 0.9962\n",
      "loss: 0.05007779523730278, train acc: 0.9972\n",
      "loss: 0.04583919607102871, train acc: 0.9968\n",
      "loss: 0.04806504100561142, train acc: 0.9974\n",
      "loss: 0.04361579436808825, train acc: 0.9955\n",
      "loss: 0.04768000431358814, train acc: 0.9956\n",
      "loss: 0.041259951703250405, train acc: 0.9955\n",
      "epoch: 36, loss: 0.0015444322489202023, train acc: 0.9955, test acc: 0.9419\n",
      "loss: 0.041964177042245865, train acc: 0.9958\n",
      "loss: 0.038740208558738234, train acc: 0.9966\n",
      "loss: 0.04274389035999775, train acc: 0.9976\n",
      "loss: 0.047290750592947, train acc: 0.9973\n",
      "loss: 0.04119163043797016, train acc: 0.9979\n",
      "loss: 0.049016949348151685, train acc: 0.9979\n",
      "loss: 0.02689552204683423, train acc: 0.9979\n",
      "loss: 0.043431575410068035, train acc: 0.9982\n",
      "epoch: 37, loss: 0.0014226767234504223, train acc: 0.9982, test acc: 0.9428\n",
      "loss: 0.04693293198943138, train acc: 0.997\n",
      "loss: 0.03907116064801812, train acc: 0.9978\n",
      "loss: 0.038410793710500005, train acc: 0.9969\n",
      "loss: 0.05211007446050644, train acc: 0.9965\n",
      "loss: 0.0528447600081563, train acc: 0.9973\n",
      "loss: 0.037498734146356585, train acc: 0.9967\n",
      "loss: 0.05218199416995049, train acc: 0.9956\n",
      "loss: 0.03505571903660894, train acc: 0.9973\n",
      "epoch: 38, loss: 0.0007456494495272636, train acc: 0.9973, test acc: 0.9405\n",
      "loss: 0.095429927110672, train acc: 0.9957\n",
      "loss: 0.047311175335198644, train acc: 0.9987\n",
      "loss: 0.03778177471831441, train acc: 0.9977\n",
      "loss: 0.04133209530264139, train acc: 0.9977\n",
      "loss: 0.04236004324629903, train acc: 0.9986\n",
      "loss: 0.04743241276592016, train acc: 0.9985\n",
      "loss: 0.037689380906522275, train acc: 0.9976\n",
      "loss: 0.03241769699379802, train acc: 0.9978\n",
      "epoch: 39, loss: 0.005280736833810806, train acc: 0.9978, test acc: 0.9444\n",
      "loss: 0.06092488765716553, train acc: 0.9971\n",
      "loss: 0.04265212304890156, train acc: 0.9987\n",
      "loss: 0.041466247942298654, train acc: 0.9977\n",
      "loss: 0.04731114376336336, train acc: 0.9974\n",
      "loss: 0.04817363806068897, train acc: 0.9974\n",
      "loss: 0.033082051388919356, train acc: 0.9974\n",
      "loss: 0.033696424961090085, train acc: 0.9976\n",
      "loss: 0.043378017656505105, train acc: 0.9983\n",
      "epoch: 40, loss: 0.0015090182423591614, train acc: 0.9983, test acc: 0.9434\n",
      "loss: 0.07523203641176224, train acc: 0.9973\n",
      "loss: 0.039685928635299204, train acc: 0.9989\n",
      "loss: 0.04978410126641393, train acc: 0.9988\n",
      "loss: 0.035076823085546494, train acc: 0.9986\n",
      "loss: 0.04233948886394501, train acc: 0.9981\n",
      "loss: 0.03891801154240966, train acc: 0.9985\n",
      "loss: 0.026466494798660277, train acc: 0.998\n",
      "loss: 0.030178162269294262, train acc: 0.9979\n",
      "epoch: 41, loss: 0.0005190408555790782, train acc: 0.9979, test acc: 0.9457\n",
      "loss: 0.02420303039252758, train acc: 0.9965\n",
      "loss: 0.03645297223702073, train acc: 0.998\n",
      "loss: 0.03573806239292025, train acc: 0.9983\n",
      "loss: 0.0382949547842145, train acc: 0.9969\n",
      "loss: 0.04261159710586071, train acc: 0.9981\n",
      "loss: 0.038568041287362576, train acc: 0.9975\n",
      "loss: 0.02709653154015541, train acc: 0.9984\n",
      "loss: 0.03893650379031897, train acc: 0.9979\n",
      "epoch: 42, loss: 0.004255105275660753, train acc: 0.9979, test acc: 0.9446\n",
      "loss: 0.06259129196405411, train acc: 0.9984\n",
      "loss: 0.04099353514611721, train acc: 0.999\n",
      "loss: 0.042146770376712085, train acc: 0.9985\n",
      "loss: 0.035747328959405425, train acc: 0.9985\n",
      "loss: 0.03734738379716873, train acc: 0.9984\n",
      "loss: 0.04205132201313973, train acc: 0.9976\n",
      "loss: 0.028890711162239312, train acc: 0.9972\n",
      "loss: 0.029223332833498716, train acc: 0.9983\n",
      "epoch: 43, loss: 0.0018898943671956658, train acc: 0.9983, test acc: 0.9462\n",
      "loss: 0.05116803199052811, train acc: 0.9974\n",
      "loss: 0.03778480961918831, train acc: 0.9989\n",
      "loss: 0.036296783946454525, train acc: 0.9986\n",
      "loss: 0.03757517263293266, train acc: 0.9991\n",
      "loss: 0.04218031074851751, train acc: 0.9991\n",
      "loss: 0.035122970957309006, train acc: 0.999\n",
      "loss: 0.034144391492009166, train acc: 0.9991\n",
      "loss: 0.03692373903468251, train acc: 0.9976\n",
      "epoch: 44, loss: 0.000958255841396749, train acc: 0.9976, test acc: 0.9461\n",
      "loss: 0.046966180205345154, train acc: 0.9986\n",
      "loss: 0.04012935627251864, train acc: 0.9989\n",
      "loss: 0.036778148636221884, train acc: 0.999\n",
      "loss: 0.03669055225327611, train acc: 0.9989\n",
      "loss: 0.04105516970157623, train acc: 0.9991\n",
      "loss: 0.031312637683004144, train acc: 0.9979\n",
      "loss: 0.040656812489032745, train acc: 0.9992\n",
      "loss: 0.038408329151570796, train acc: 0.9983\n",
      "epoch: 45, loss: 0.0004360396706033498, train acc: 0.9983, test acc: 0.9454\n",
      "loss: 0.018947629258036613, train acc: 0.9979\n",
      "loss: 0.0324446608312428, train acc: 0.9975\n",
      "loss: 0.034057650901377204, train acc: 0.9986\n",
      "loss: 0.04610343500971794, train acc: 0.9984\n",
      "loss: 0.03377908496186137, train acc: 0.9984\n",
      "loss: 0.031333482917398216, train acc: 0.9986\n",
      "loss: 0.03753062430769205, train acc: 0.999\n",
      "loss: 0.03375755753368139, train acc: 0.9982\n",
      "epoch: 46, loss: 0.00024199482868425548, train acc: 0.9982, test acc: 0.9444\n",
      "loss: 0.053608641028404236, train acc: 0.9986\n",
      "loss: 0.03359701898880303, train acc: 0.9991\n",
      "loss: 0.03479684023186565, train acc: 0.9986\n",
      "loss: 0.03504509786143899, train acc: 0.997\n",
      "loss: 0.03652726896107197, train acc: 0.9984\n",
      "loss: 0.04445814462378621, train acc: 0.9964\n",
      "loss: 0.036241940781474115, train acc: 0.9982\n",
      "loss: 0.031479624845087525, train acc: 0.9982\n",
      "epoch: 47, loss: 0.0006896218983456492, train acc: 0.9982, test acc: 0.9451\n",
      "loss: 0.046680666506290436, train acc: 0.9972\n",
      "loss: 0.03096329001709819, train acc: 0.9979\n",
      "loss: 0.03634770791977644, train acc: 0.9985\n",
      "loss: 0.03760616574436426, train acc: 0.9982\n",
      "loss: 0.03747331015765667, train acc: 0.9986\n",
      "loss: 0.03513374719768762, train acc: 0.9984\n",
      "loss: 0.02944477740675211, train acc: 0.9986\n",
      "loss: 0.025948833487927914, train acc: 0.9987\n",
      "epoch: 48, loss: 0.0005338192568160594, train acc: 0.9987, test acc: 0.9455\n",
      "loss: 0.021891433745622635, train acc: 0.9985\n",
      "loss: 0.032268475322052834, train acc: 0.9991\n",
      "loss: 0.03140808269381523, train acc: 0.9995\n",
      "loss: 0.03606350654736161, train acc: 0.9985\n",
      "loss: 0.025463713612407447, train acc: 0.9985\n",
      "loss: 0.03616603156551719, train acc: 0.9985\n",
      "loss: 0.024797461181879043, train acc: 0.9987\n",
      "loss: 0.03507513888180256, train acc: 0.9982\n",
      "epoch: 49, loss: 0.0009236703626811504, train acc: 0.9982, test acc: 0.9462\n",
      "loss: 0.04359440505504608, train acc: 0.9982\n",
      "loss: 0.03199953748844564, train acc: 0.9995\n",
      "loss: 0.04024538788944483, train acc: 0.9991\n",
      "loss: 0.030438440944999458, train acc: 0.9986\n",
      "loss: 0.03892923789098859, train acc: 0.9986\n",
      "loss: 0.02850488000549376, train acc: 0.998\n",
      "loss: 0.02964069815352559, train acc: 0.9988\n",
      "loss: 0.035701084230095145, train acc: 0.9985\n",
      "epoch: 50, loss: 0.003103321185335517, train acc: 0.9985, test acc: 0.9449\n",
      "loss: 0.025869818404316902, train acc: 0.9989\n",
      "loss: 0.03649894231930375, train acc: 0.9992\n",
      "loss: 0.033999298512935636, train acc: 0.9993\n",
      "loss: 0.034245298430323604, train acc: 0.9989\n",
      "loss: 0.037278528418391944, train acc: 0.9992\n",
      "loss: 0.027229273319244386, train acc: 0.9983\n",
      "loss: 0.026294574420899153, train acc: 0.9989\n",
      "loss: 0.025471921311691403, train acc: 0.9989\n",
      "epoch: 51, loss: 0.00216895854100585, train acc: 0.9989, test acc: 0.9458\n",
      "loss: 0.022951792925596237, train acc: 0.9984\n",
      "loss: 0.021990620903670786, train acc: 0.999\n",
      "loss: 0.0270264882594347, train acc: 0.9992\n",
      "loss: 0.031196254026144742, train acc: 0.9994\n",
      "loss: 0.02867542253807187, train acc: 0.9993\n",
      "loss: 0.043443241808563474, train acc: 0.9986\n",
      "loss: 0.019315364444628356, train acc: 0.9992\n",
      "loss: 0.03163860403001308, train acc: 0.9997\n",
      "epoch: 52, loss: 0.00045997780398465693, train acc: 0.9997, test acc: 0.9455\n",
      "loss: 0.03286797180771828, train acc: 0.9989\n",
      "loss: 0.026218175981193782, train acc: 0.9991\n",
      "loss: 0.024162209127098322, train acc: 0.9993\n",
      "loss: 0.02549794102087617, train acc: 0.9992\n",
      "loss: 0.03783033592626452, train acc: 0.9988\n",
      "loss: 0.03397340811789036, train acc: 0.9992\n",
      "loss: 0.022601218707859517, train acc: 0.9992\n",
      "loss: 0.025087866373360156, train acc: 0.9987\n",
      "epoch: 53, loss: 0.0004528085410129279, train acc: 0.9987, test acc: 0.9447\n",
      "loss: 0.08291211724281311, train acc: 0.9982\n",
      "loss: 0.03964691730216145, train acc: 0.9978\n",
      "loss: 0.02927127298898995, train acc: 0.9992\n",
      "loss: 0.029964016191661356, train acc: 0.9989\n",
      "loss: 0.029927743971347807, train acc: 0.9993\n",
      "loss: 0.028725145012140275, train acc: 0.9985\n",
      "loss: 0.03520290795713663, train acc: 0.9993\n",
      "loss: 0.02197873704135418, train acc: 0.9989\n",
      "epoch: 54, loss: 0.0008637051796540618, train acc: 0.9989, test acc: 0.9471\n",
      "loss: 0.04081735387444496, train acc: 0.999\n",
      "loss: 0.026108232513070106, train acc: 0.9999\n",
      "loss: 0.03308881521224975, train acc: 0.9995\n",
      "loss: 0.029283461160957813, train acc: 0.9988\n",
      "loss: 0.03609793623909354, train acc: 0.9988\n",
      "loss: 0.0298683435190469, train acc: 0.9987\n",
      "loss: 0.026647270377725362, train acc: 0.9988\n",
      "loss: 0.03022180199623108, train acc: 0.9991\n",
      "epoch: 55, loss: 0.001933625084348023, train acc: 0.9991, test acc: 0.9463\n",
      "loss: 0.04011623561382294, train acc: 0.9991\n",
      "loss: 0.030399239249527456, train acc: 0.9994\n",
      "loss: 0.025917849596589805, train acc: 0.9993\n",
      "loss: 0.035179286077618596, train acc: 0.9996\n",
      "loss: 0.03298534583300352, train acc: 0.9996\n",
      "loss: 0.03260661954991519, train acc: 0.9988\n",
      "loss: 0.02486303299665451, train acc: 0.9986\n",
      "loss: 0.03078396087512374, train acc: 0.9982\n",
      "epoch: 56, loss: 0.00031270351610146463, train acc: 0.9982, test acc: 0.9431\n",
      "loss: 0.04848679527640343, train acc: 0.9986\n",
      "loss: 0.031740073347464204, train acc: 0.999\n",
      "loss: 0.02891885391436517, train acc: 0.9994\n",
      "loss: 0.02550694067031145, train acc: 0.9995\n",
      "loss: 0.025281759351491927, train acc: 0.9994\n",
      "loss: 0.023428920237347484, train acc: 0.9994\n",
      "loss: 0.021623841812834145, train acc: 0.9997\n",
      "loss: 0.02417628960683942, train acc: 0.9993\n",
      "epoch: 57, loss: 0.007685350254178047, train acc: 0.9993, test acc: 0.9464\n",
      "loss: 0.018207620829343796, train acc: 0.9988\n",
      "loss: 0.023744662571698428, train acc: 0.999\n",
      "loss: 0.03872959297150373, train acc: 0.9996\n",
      "loss: 0.027023922372609376, train acc: 0.9989\n",
      "loss: 0.02979094786569476, train acc: 0.9995\n",
      "loss: 0.018725056247785688, train acc: 0.9995\n",
      "loss: 0.03266031290404499, train acc: 0.9989\n",
      "loss: 0.025707793515175582, train acc: 0.9989\n",
      "epoch: 58, loss: 0.0003801280399784446, train acc: 0.9989, test acc: 0.9443\n",
      "loss: 0.013864285312592983, train acc: 0.9978\n",
      "loss: 0.031241574510931968, train acc: 0.9972\n",
      "loss: 0.030198709480464457, train acc: 0.9986\n",
      "loss: 0.03188972435891628, train acc: 0.999\n",
      "loss: 0.02137684067711234, train acc: 0.9998\n",
      "loss: 0.03629942396655679, train acc: 0.9997\n",
      "loss: 0.022803890984505415, train acc: 0.9995\n",
      "loss: 0.025100524071604013, train acc: 0.9992\n",
      "epoch: 59, loss: 0.001945273601450026, train acc: 0.9992, test acc: 0.9458\n",
      "loss: 0.008297872729599476, train acc: 0.9995\n",
      "loss: 0.021581958420574664, train acc: 0.9994\n",
      "loss: 0.02495561707764864, train acc: 0.9996\n",
      "loss: 0.03183158114552498, train acc: 0.9989\n",
      "loss: 0.041865330655127764, train acc: 0.9996\n",
      "loss: 0.02918738927692175, train acc: 0.9988\n",
      "loss: 0.026242788415402174, train acc: 0.9989\n",
      "loss: 0.03372722705826163, train acc: 0.9996\n",
      "epoch: 60, loss: 0.0008313580183312297, train acc: 0.9996, test acc: 0.9461\n",
      "loss: 0.04156152904033661, train acc: 0.9989\n",
      "loss: 0.026984347216784953, train acc: 0.9987\n",
      "loss: 0.027436381950974465, train acc: 0.9994\n",
      "loss: 0.026611643750220536, train acc: 0.9998\n",
      "loss: 0.0242782408837229, train acc: 0.9996\n",
      "loss: 0.030025909608229994, train acc: 0.9992\n",
      "loss: 0.029876580461859703, train acc: 0.9994\n",
      "loss: 0.023133238404989244, train acc: 0.9998\n",
      "epoch: 61, loss: 0.0009991145925596356, train acc: 0.9998, test acc: 0.9473\n",
      "loss: 0.03387151286005974, train acc: 0.9997\n",
      "loss: 0.023715915530920027, train acc: 0.9995\n",
      "loss: 0.02358191313687712, train acc: 0.9996\n",
      "loss: 0.022019720263779165, train acc: 0.9994\n",
      "loss: 0.028295434918254612, train acc: 0.9997\n",
      "loss: 0.016048305109143256, train acc: 0.9997\n",
      "loss: 0.016993025317788124, train acc: 0.9993\n",
      "loss: 0.021939813066273926, train acc: 0.9997\n",
      "epoch: 62, loss: 0.001862585311755538, train acc: 0.9997, test acc: 0.9468\n",
      "loss: 0.013401567935943604, train acc: 0.9994\n",
      "loss: 0.030135455587878824, train acc: 0.9993\n",
      "loss: 0.023709285212680696, train acc: 0.9995\n",
      "loss: 0.02417744235135615, train acc: 0.9997\n",
      "loss: 0.027134671714156865, train acc: 0.9996\n",
      "loss: 0.02677323929965496, train acc: 0.9992\n",
      "loss: 0.022935559134930374, train acc: 0.9995\n",
      "loss: 0.022537770215421915, train acc: 0.9998\n",
      "epoch: 63, loss: 0.00029616765095852315, train acc: 0.9998, test acc: 0.948\n",
      "loss: 0.035088784992694855, train acc: 0.9996\n",
      "loss: 0.017965139448642732, train acc: 0.9996\n",
      "loss: 0.020023902412503958, train acc: 0.9998\n",
      "loss: 0.027312528667971493, train acc: 0.9997\n",
      "loss: 0.03200022466480732, train acc: 0.9999\n",
      "loss: 0.02437588796019554, train acc: 0.9997\n",
      "loss: 0.02185247614979744, train acc: 0.9994\n",
      "loss: 0.02941454853862524, train acc: 0.9993\n",
      "epoch: 64, loss: 0.0009285369887948036, train acc: 0.9993, test acc: 0.9465\n",
      "loss: 0.01568671315908432, train acc: 0.9984\n",
      "loss: 0.02042172271758318, train acc: 0.9994\n",
      "loss: 0.026819816697388886, train acc: 0.9993\n",
      "loss: 0.021927295066416264, train acc: 0.9996\n",
      "loss: 0.03089040983468294, train acc: 0.9999\n",
      "loss: 0.021395260374993084, train acc: 0.9987\n",
      "loss: 0.021137118292972445, train acc: 0.9993\n",
      "loss: 0.025448010629042984, train acc: 0.9997\n",
      "epoch: 65, loss: 0.0010059773921966553, train acc: 0.9997, test acc: 0.9455\n",
      "loss: 0.01259908638894558, train acc: 0.999\n",
      "loss: 0.023227777145802975, train acc: 0.9993\n",
      "loss: 0.027100572641938925, train acc: 0.9998\n",
      "loss: 0.019083866057917474, train acc: 0.9992\n",
      "loss: 0.032555519789457324, train acc: 0.9997\n",
      "loss: 0.014048357540741564, train acc: 0.9996\n",
      "loss: 0.02261904878541827, train acc: 0.9997\n",
      "loss: 0.018770273425616324, train acc: 0.9997\n",
      "epoch: 66, loss: 0.00014101763372309506, train acc: 0.9997, test acc: 0.9462\n",
      "loss: 0.0390075258910656, train acc: 0.9991\n",
      "loss: 0.024064706824719905, train acc: 0.9996\n",
      "loss: 0.02494545462541282, train acc: 0.9997\n",
      "loss: 0.019446791615337133, train acc: 0.9997\n",
      "loss: 0.02268264926970005, train acc: 0.9998\n",
      "loss: 0.023546338081359863, train acc: 0.9997\n",
      "loss: 0.013538807164877653, train acc: 0.9994\n",
      "loss: 0.022918347525410353, train acc: 0.9997\n",
      "epoch: 67, loss: 0.0004266797623131424, train acc: 0.9997, test acc: 0.9482\n",
      "loss: 0.0051586865447461605, train acc: 0.9998\n",
      "loss: 0.022030939860269428, train acc: 0.9991\n",
      "loss: 0.023340940522029997, train acc: 0.9999\n",
      "loss: 0.021795250289142132, train acc: 0.9998\n",
      "loss: 0.02261041384190321, train acc: 0.9999\n",
      "loss: 0.024506746139377356, train acc: 1.0\n",
      "loss: 0.019829039741307498, train acc: 0.9997\n",
      "loss: 0.013917666207998992, train acc: 0.9998\n",
      "epoch: 68, loss: 7.944018580019474e-05, train acc: 0.9998, test acc: 0.9475\n",
      "loss: 0.031832367181777954, train acc: 0.9999\n",
      "loss: 0.02328604320064187, train acc: 0.9998\n",
      "loss: 0.018964043562300505, train acc: 0.9996\n",
      "loss: 0.021678279992192982, train acc: 0.9994\n",
      "loss: 0.03098618471994996, train acc: 0.9999\n",
      "loss: 0.015472880471497774, train acc: 0.9998\n",
      "loss: 0.021627193968743085, train acc: 0.9996\n",
      "loss: 0.024960493296384813, train acc: 0.9998\n",
      "epoch: 69, loss: 0.0004565063863992691, train acc: 0.9998, test acc: 0.9469\n",
      "loss: 0.03589097410440445, train acc: 0.9994\n",
      "loss: 0.026158356107771397, train acc: 0.9999\n",
      "loss: 0.018951281299814583, train acc: 0.9996\n",
      "loss: 0.021288981568068265, train acc: 0.9998\n",
      "loss: 0.031850965786725284, train acc: 0.9996\n",
      "loss: 0.02279932564124465, train acc: 0.9992\n",
      "loss: 0.018235709238797426, train acc: 0.9998\n",
      "loss: 0.02483620666898787, train acc: 1.0\n",
      "epoch: 70, loss: 0.0014714314602315426, train acc: 1.0, test acc: 0.9486\n",
      "loss: 0.007221904583275318, train acc: 0.9999\n",
      "loss: 0.021809676242992283, train acc: 0.9999\n",
      "loss: 0.018986512697301804, train acc: 1.0\n",
      "loss: 0.017177262343466282, train acc: 0.9999\n",
      "loss: 0.01689604134298861, train acc: 0.9999\n",
      "loss: 0.016511348122730852, train acc: 0.9998\n",
      "loss: 0.022213217429816724, train acc: 0.9992\n",
      "loss: 0.0193417698610574, train acc: 0.9994\n",
      "epoch: 71, loss: 0.00019016610167454928, train acc: 0.9994, test acc: 0.9475\n",
      "loss: 0.06397344917058945, train acc: 0.9999\n",
      "loss: 0.03371248478069901, train acc: 0.9997\n",
      "loss: 0.02632163055241108, train acc: 0.9998\n",
      "loss: 0.01770799371879548, train acc: 0.9997\n",
      "loss: 0.026751908427104353, train acc: 0.9998\n",
      "loss: 0.023694221302866934, train acc: 0.9998\n",
      "loss: 0.021234045410528778, train acc: 0.9997\n",
      "loss: 0.02648755004629493, train acc: 0.9994\n",
      "epoch: 72, loss: 0.0002594385587144643, train acc: 0.9994, test acc: 0.9469\n",
      "loss: 0.014790509827435017, train acc: 0.9997\n",
      "loss: 0.017197107989341022, train acc: 0.9999\n",
      "loss: 0.019789120368659496, train acc: 0.9997\n",
      "loss: 0.026184513606131075, train acc: 0.9997\n",
      "loss: 0.02607345310971141, train acc: 0.9999\n",
      "loss: 0.020194133650511502, train acc: 0.9999\n",
      "loss: 0.02303081941790879, train acc: 0.9994\n",
      "loss: 0.015195890236645937, train acc: 0.9991\n",
      "epoch: 73, loss: 0.0006659111240878701, train acc: 0.9991, test acc: 0.9453\n",
      "loss: 0.025505634024739265, train acc: 0.9998\n",
      "loss: 0.015512551157735288, train acc: 0.9998\n",
      "loss: 0.024172871198970824, train acc: 0.9999\n",
      "loss: 0.02775790523737669, train acc: 0.9999\n",
      "loss: 0.014773313188925385, train acc: 0.9999\n",
      "loss: 0.018963267840445042, train acc: 0.9999\n",
      "loss: 0.013992470921948552, train acc: 0.9999\n",
      "loss: 0.019507509912364183, train acc: 0.9998\n",
      "epoch: 74, loss: 0.00018079236906487495, train acc: 0.9998, test acc: 0.9465\n",
      "loss: 0.007999714463949203, train acc: 0.9993\n",
      "loss: 0.01182338991202414, train acc: 0.9997\n",
      "loss: 0.030434405989944936, train acc: 0.9998\n",
      "loss: 0.019166401075199246, train acc: 0.9996\n",
      "loss: 0.017344081425108016, train acc: 0.9999\n",
      "loss: 0.0162727409042418, train acc: 0.9995\n",
      "loss: 0.017235547956079244, train acc: 0.9995\n",
      "loss: 0.01808170145377517, train acc: 0.9992\n",
      "epoch: 75, loss: 0.00014118643593974411, train acc: 0.9992, test acc: 0.9478\n",
      "loss: 0.01872621849179268, train acc: 0.9996\n",
      "loss: 0.025477275857701896, train acc: 0.9997\n",
      "loss: 0.021279145404696463, train acc: 0.9997\n",
      "loss: 0.026199561450630425, train acc: 0.9996\n",
      "loss: 0.017903532553464174, train acc: 0.9999\n",
      "loss: 0.01200494933873415, train acc: 0.9999\n",
      "loss: 0.015508889872580766, train acc: 1.0\n",
      "loss: 0.016465956112369895, train acc: 1.0\n",
      "epoch: 76, loss: 0.00017428107094019651, train acc: 1.0, test acc: 0.946\n",
      "loss: 0.010082127526402473, train acc: 0.9996\n",
      "loss: 0.01611509514041245, train acc: 0.9997\n",
      "loss: 0.019465928431600332, train acc: 1.0\n",
      "loss: 0.02007637652568519, train acc: 0.9997\n",
      "loss: 0.026295145321637393, train acc: 0.9997\n",
      "loss: 0.01637512263841927, train acc: 0.9998\n",
      "loss: 0.01932457285001874, train acc: 1.0\n",
      "loss: 0.015923067927360535, train acc: 0.9999\n",
      "epoch: 77, loss: 0.0001787183718988672, train acc: 0.9999, test acc: 0.9468\n",
      "loss: 0.029058653861284256, train acc: 0.9995\n",
      "loss: 0.02472349554300308, train acc: 0.9996\n",
      "loss: 0.019788153050467373, train acc: 0.9999\n",
      "loss: 0.020316438097506762, train acc: 0.9996\n",
      "loss: 0.02374538080766797, train acc: 0.9997\n",
      "loss: 0.017423228919506074, train acc: 0.9999\n",
      "loss: 0.01893646754324436, train acc: 0.9995\n",
      "loss: 0.010879925452172756, train acc: 0.9989\n",
      "epoch: 78, loss: 0.008875612169504166, train acc: 0.9989, test acc: 0.9481\n",
      "loss: 0.009260389022529125, train acc: 0.9998\n",
      "loss: 0.01650044715497643, train acc: 0.9999\n",
      "loss: 0.019915739307180047, train acc: 0.9998\n",
      "loss: 0.01827458725310862, train acc: 0.9999\n",
      "loss: 0.021963262790814043, train acc: 0.9988\n",
      "loss: 0.023334528878331186, train acc: 0.9999\n",
      "loss: 0.028138576122000814, train acc: 1.0\n",
      "loss: 0.019878922263160348, train acc: 0.9998\n",
      "epoch: 79, loss: 0.0001311945088673383, train acc: 0.9998, test acc: 0.9469\n",
      "#####training and testing end with K:40, P:0.1######\n",
      "#####training and testing start with K:40, P:0.5######\n",
      "loss: 2.383359909057617, train acc: 0.1181\n",
      "loss: 2.077615666389465, train acc: 0.6471\n",
      "loss: 1.5898331642150878, train acc: 0.756\n",
      "loss: 1.3134921073913575, train acc: 0.809\n",
      "loss: 1.1167145252227784, train acc: 0.8289\n",
      "loss: 0.9929653286933899, train acc: 0.8497\n",
      "loss: 0.8872075617313385, train acc: 0.8661\n",
      "loss: 0.8101949334144593, train acc: 0.8736\n",
      "epoch: 0, loss: 0.7194739580154419, train acc: 0.8736, test acc: 0.8746\n",
      "loss: 0.7833929657936096, train acc: 0.877\n",
      "loss: 0.7512055575847626, train acc: 0.8819\n",
      "loss: 0.7156838417053223, train acc: 0.8855\n",
      "loss: 0.726458078622818, train acc: 0.8946\n",
      "loss: 0.6782057285308838, train acc: 0.8955\n",
      "loss: 0.6292022049427033, train acc: 0.8975\n",
      "loss: 0.6279331028461457, train acc: 0.9034\n",
      "loss: 0.616812264919281, train acc: 0.9044\n",
      "epoch: 1, loss: 0.5528891682624817, train acc: 0.9044, test acc: 0.8977\n",
      "loss: 0.6989949345588684, train acc: 0.9044\n",
      "loss: 0.6095324605703354, train acc: 0.9069\n",
      "loss: 0.5894636422395706, train acc: 0.9118\n",
      "loss: 0.5752532184123993, train acc: 0.9112\n",
      "loss: 0.5321209698915481, train acc: 0.9103\n",
      "loss: 0.5670602440834045, train acc: 0.9133\n",
      "loss: 0.5305691212415695, train acc: 0.9157\n",
      "loss: 0.5026959538459778, train acc: 0.9132\n",
      "epoch: 2, loss: 0.20345492660999298, train acc: 0.9132, test acc: 0.9028\n",
      "loss: 0.6588417887687683, train acc: 0.9112\n",
      "loss: 0.5753307551145553, train acc: 0.9169\n",
      "loss: 0.5602565437555314, train acc: 0.9217\n",
      "loss: 0.5278189450502395, train acc: 0.9219\n",
      "loss: 0.504052397608757, train acc: 0.9225\n",
      "loss: 0.49463661909103396, train acc: 0.9221\n",
      "loss: 0.5001822352409363, train acc: 0.9231\n",
      "loss: 0.4809498727321625, train acc: 0.9243\n",
      "epoch: 3, loss: 0.2120593637228012, train acc: 0.9243, test acc: 0.9103\n",
      "loss: 0.5166293978691101, train acc: 0.9221\n",
      "loss: 0.4730212062597275, train acc: 0.9255\n",
      "loss: 0.5222718864679337, train acc: 0.929\n",
      "loss: 0.4907186388969421, train acc: 0.9278\n",
      "loss: 0.4356163710355759, train acc: 0.9278\n",
      "loss: 0.47500002980232237, train acc: 0.9258\n",
      "loss: 0.4840531826019287, train acc: 0.9318\n",
      "loss: 0.4728660315275192, train acc: 0.9305\n",
      "epoch: 4, loss: 0.12559449672698975, train acc: 0.9305, test acc: 0.9159\n",
      "loss: 0.4354158043861389, train acc: 0.9308\n",
      "loss: 0.45900127589702605, train acc: 0.9319\n",
      "loss: 0.48429262936115264, train acc: 0.9352\n",
      "loss: 0.44270655512809753, train acc: 0.9356\n",
      "loss: 0.4402641445398331, train acc: 0.9347\n",
      "loss: 0.42180063426494596, train acc: 0.9338\n",
      "loss: 0.4561216413974762, train acc: 0.9358\n",
      "loss: 0.44806225001811983, train acc: 0.9341\n",
      "epoch: 5, loss: 0.16459408402442932, train acc: 0.9341, test acc: 0.9166\n",
      "loss: 0.5384331345558167, train acc: 0.934\n",
      "loss: 0.4582572907209396, train acc: 0.9351\n",
      "loss: 0.44738522469997405, train acc: 0.9383\n",
      "loss: 0.43316373527050017, train acc: 0.9371\n",
      "loss: 0.3739636421203613, train acc: 0.9377\n",
      "loss: 0.43464094698429107, train acc: 0.9387\n",
      "loss: 0.4261747509241104, train acc: 0.9403\n",
      "loss: 0.4035645961761475, train acc: 0.9407\n",
      "epoch: 6, loss: 0.22151318192481995, train acc: 0.9407, test acc: 0.9201\n",
      "loss: 0.44631093740463257, train acc: 0.9383\n",
      "loss: 0.42174536883831026, train acc: 0.9398\n",
      "loss: 0.444256004691124, train acc: 0.941\n",
      "loss: 0.4106799364089966, train acc: 0.9419\n",
      "loss: 0.37646963596343996, train acc: 0.9414\n",
      "loss: 0.39981076419353484, train acc: 0.9418\n",
      "loss: 0.3774012953042984, train acc: 0.9425\n",
      "loss: 0.4279689520597458, train acc: 0.9425\n",
      "epoch: 7, loss: 0.15994670987129211, train acc: 0.9425, test acc: 0.9238\n",
      "loss: 0.33143186569213867, train acc: 0.9409\n",
      "loss: 0.3774092376232147, train acc: 0.9413\n",
      "loss: 0.3884020894765854, train acc: 0.9428\n",
      "loss: 0.38410591185092924, train acc: 0.9448\n",
      "loss: 0.38782696425914764, train acc: 0.945\n",
      "loss: 0.3875002145767212, train acc: 0.9458\n",
      "loss: 0.41148272156715393, train acc: 0.9458\n",
      "loss: 0.36633814573287965, train acc: 0.9463\n",
      "epoch: 8, loss: 0.12956076860427856, train acc: 0.9463, test acc: 0.9246\n",
      "loss: 0.409565269947052, train acc: 0.9448\n",
      "loss: 0.397852024435997, train acc: 0.9443\n",
      "loss: 0.38447884023189544, train acc: 0.9466\n",
      "loss: 0.38620905578136444, train acc: 0.9467\n",
      "loss: 0.3500624090433121, train acc: 0.9461\n",
      "loss: 0.3676176846027374, train acc: 0.9479\n",
      "loss: 0.3745300233364105, train acc: 0.9474\n",
      "loss: 0.36040655374526975, train acc: 0.9493\n",
      "epoch: 9, loss: 0.05313345789909363, train acc: 0.9493, test acc: 0.9269\n",
      "loss: 0.4223591983318329, train acc: 0.9484\n",
      "loss: 0.399522203207016, train acc: 0.9481\n",
      "loss: 0.39960393905639646, train acc: 0.9504\n",
      "loss: 0.36486613750457764, train acc: 0.9514\n",
      "loss: 0.3623984009027481, train acc: 0.949\n",
      "loss: 0.37140108197927474, train acc: 0.9491\n",
      "loss: 0.3660397231578827, train acc: 0.9506\n",
      "loss: 0.34725734293460847, train acc: 0.9489\n",
      "epoch: 10, loss: 0.10302184522151947, train acc: 0.9489, test acc: 0.926\n",
      "loss: 0.35394373536109924, train acc: 0.9505\n",
      "loss: 0.3713470697402954, train acc: 0.9494\n",
      "loss: 0.3819092929363251, train acc: 0.9521\n",
      "loss: 0.3420909374952316, train acc: 0.9525\n",
      "loss: 0.33834172785282135, train acc: 0.9532\n",
      "loss: 0.3556596711277962, train acc: 0.9534\n",
      "loss: 0.33683246821165086, train acc: 0.953\n",
      "loss: 0.33305752873420713, train acc: 0.9534\n",
      "epoch: 11, loss: 0.06715287268161774, train acc: 0.9534, test acc: 0.927\n",
      "loss: 0.39124050736427307, train acc: 0.9519\n",
      "loss: 0.3765923514962196, train acc: 0.9512\n",
      "loss: 0.349663682281971, train acc: 0.9533\n",
      "loss: 0.3259612381458282, train acc: 0.9551\n",
      "loss: 0.3219237834215164, train acc: 0.9538\n",
      "loss: 0.34432020634412763, train acc: 0.9535\n",
      "loss: 0.3435025930404663, train acc: 0.9556\n",
      "loss: 0.36112062335014344, train acc: 0.9548\n",
      "epoch: 12, loss: 0.18507088720798492, train acc: 0.9548, test acc: 0.9289\n",
      "loss: 0.42603379487991333, train acc: 0.9549\n",
      "loss: 0.36398259103298186, train acc: 0.9532\n",
      "loss: 0.3684785634279251, train acc: 0.9549\n",
      "loss: 0.3327510505914688, train acc: 0.9558\n",
      "loss: 0.3453033611178398, train acc: 0.956\n",
      "loss: 0.3402496114373207, train acc: 0.957\n",
      "loss: 0.34586709141731264, train acc: 0.9575\n",
      "loss: 0.3500076740980148, train acc: 0.9566\n",
      "epoch: 13, loss: 0.11746164411306381, train acc: 0.9566, test acc: 0.9302\n",
      "loss: 0.2917065918445587, train acc: 0.955\n",
      "loss: 0.34221833050251005, train acc: 0.9549\n",
      "loss: 0.35724270045757295, train acc: 0.9574\n",
      "loss: 0.29859407991170883, train acc: 0.9566\n",
      "loss: 0.3151480436325073, train acc: 0.9568\n",
      "loss: 0.337029741704464, train acc: 0.9586\n",
      "loss: 0.3362541526556015, train acc: 0.9585\n",
      "loss: 0.33987913727760316, train acc: 0.9592\n",
      "epoch: 14, loss: 0.1415296345949173, train acc: 0.9592, test acc: 0.9306\n",
      "loss: 0.3504177927970886, train acc: 0.9579\n",
      "loss: 0.3351574033498764, train acc: 0.9562\n",
      "loss: 0.3672838777303696, train acc: 0.9596\n",
      "loss: 0.3138363346457481, train acc: 0.9595\n",
      "loss: 0.32222563922405245, train acc: 0.9591\n",
      "loss: 0.33210196197032926, train acc: 0.959\n",
      "loss: 0.3181325286626816, train acc: 0.9585\n",
      "loss: 0.30561020225286484, train acc: 0.9583\n",
      "epoch: 15, loss: 0.06746557354927063, train acc: 0.9583, test acc: 0.9311\n",
      "loss: 0.2893828749656677, train acc: 0.9584\n",
      "loss: 0.31768970638513566, train acc: 0.9594\n",
      "loss: 0.37303837537765505, train acc: 0.9605\n",
      "loss: 0.29685438275337217, train acc: 0.9597\n",
      "loss: 0.29956549108028413, train acc: 0.9594\n",
      "loss: 0.3234093010425568, train acc: 0.9594\n",
      "loss: 0.33381391763687135, train acc: 0.9607\n",
      "loss: 0.33311436921358106, train acc: 0.9619\n",
      "epoch: 16, loss: 0.06760506331920624, train acc: 0.9619, test acc: 0.9302\n",
      "loss: 0.3161807954311371, train acc: 0.9612\n",
      "loss: 0.3227609947323799, train acc: 0.9613\n",
      "loss: 0.3505580067634583, train acc: 0.9611\n",
      "loss: 0.28366631716489793, train acc: 0.9621\n",
      "loss: 0.28612169474363325, train acc: 0.9611\n",
      "loss: 0.35044443011283877, train acc: 0.9632\n",
      "loss: 0.323459255695343, train acc: 0.9639\n",
      "loss: 0.3229604497551918, train acc: 0.9644\n",
      "epoch: 17, loss: 0.09706895053386688, train acc: 0.9644, test acc: 0.9296\n",
      "loss: 0.32332223653793335, train acc: 0.962\n",
      "loss: 0.3165989801287651, train acc: 0.9631\n",
      "loss: 0.32501217126846316, train acc: 0.9635\n",
      "loss: 0.3065740317106247, train acc: 0.9618\n",
      "loss: 0.29241582453250886, train acc: 0.9634\n",
      "loss: 0.29844450801610944, train acc: 0.9629\n",
      "loss: 0.3138687938451767, train acc: 0.9643\n",
      "loss: 0.33586556762456893, train acc: 0.9625\n",
      "epoch: 18, loss: 0.07199092954397202, train acc: 0.9625, test acc: 0.9298\n",
      "loss: 0.30614128708839417, train acc: 0.964\n",
      "loss: 0.3230151116847992, train acc: 0.9633\n",
      "loss: 0.3508024841547012, train acc: 0.9626\n",
      "loss: 0.29534264504909513, train acc: 0.964\n",
      "loss: 0.2692110285162926, train acc: 0.9646\n",
      "loss: 0.29373060166835785, train acc: 0.964\n",
      "loss: 0.30241652578115463, train acc: 0.9644\n",
      "loss: 0.2932595148682594, train acc: 0.9642\n",
      "epoch: 19, loss: 0.06162121519446373, train acc: 0.9642, test acc: 0.9321\n",
      "loss: 0.3263622522354126, train acc: 0.965\n",
      "loss: 0.2972005128860474, train acc: 0.9651\n",
      "loss: 0.3210557073354721, train acc: 0.9654\n",
      "loss: 0.2902467861771584, train acc: 0.9651\n",
      "loss: 0.31311832666397094, train acc: 0.9657\n",
      "loss: 0.29308445304632186, train acc: 0.9645\n",
      "loss: 0.29803225845098497, train acc: 0.9658\n",
      "loss: 0.30428274124860766, train acc: 0.9645\n",
      "epoch: 20, loss: 0.1566583216190338, train acc: 0.9645, test acc: 0.9323\n",
      "loss: 0.34197109937667847, train acc: 0.9665\n",
      "loss: 0.3318594962358475, train acc: 0.9649\n",
      "loss: 0.3532760083675385, train acc: 0.9667\n",
      "loss: 0.28618958592414856, train acc: 0.9662\n",
      "loss: 0.28279000967741014, train acc: 0.9638\n",
      "loss: 0.2955090284347534, train acc: 0.9663\n",
      "loss: 0.2866939127445221, train acc: 0.9673\n",
      "loss: 0.3199217602610588, train acc: 0.968\n",
      "epoch: 21, loss: 0.011802625842392445, train acc: 0.968, test acc: 0.9335\n",
      "loss: 0.24928084015846252, train acc: 0.9649\n",
      "loss: 0.3218702062964439, train acc: 0.9656\n",
      "loss: 0.3306052327156067, train acc: 0.9665\n",
      "loss: 0.27940845042467116, train acc: 0.968\n",
      "loss: 0.2868496000766754, train acc: 0.9675\n",
      "loss: 0.29985710978507996, train acc: 0.9678\n",
      "loss: 0.29849167317152026, train acc: 0.9673\n",
      "loss: 0.3175620689988136, train acc: 0.9675\n",
      "epoch: 22, loss: 0.034797314554452896, train acc: 0.9675, test acc: 0.9338\n",
      "loss: 0.24684758484363556, train acc: 0.9678\n",
      "loss: 0.31879921406507494, train acc: 0.9673\n",
      "loss: 0.29676041603088377, train acc: 0.9676\n",
      "loss: 0.258983214199543, train acc: 0.9667\n",
      "loss: 0.26451733857393267, train acc: 0.9671\n",
      "loss: 0.2957763820886612, train acc: 0.9681\n",
      "loss: 0.28551184087991716, train acc: 0.9683\n",
      "loss: 0.29536160081624985, train acc: 0.968\n",
      "epoch: 23, loss: 0.0753571167588234, train acc: 0.968, test acc: 0.9321\n",
      "loss: 0.2581368684768677, train acc: 0.9682\n",
      "loss: 0.3183018758893013, train acc: 0.9685\n",
      "loss: 0.3276044502854347, train acc: 0.9698\n",
      "loss: 0.2766862899065018, train acc: 0.9693\n",
      "loss: 0.27064399272203443, train acc: 0.9699\n",
      "loss: 0.28672595471143725, train acc: 0.9685\n",
      "loss: 0.2867945596575737, train acc: 0.9686\n",
      "loss: 0.2926590725779533, train acc: 0.9686\n",
      "epoch: 24, loss: 0.04586790129542351, train acc: 0.9686, test acc: 0.9334\n",
      "loss: 0.28788185119628906, train acc: 0.9697\n",
      "loss: 0.2814119100570679, train acc: 0.9696\n",
      "loss: 0.30198312997817994, train acc: 0.9694\n",
      "loss: 0.2707661911845207, train acc: 0.9687\n",
      "loss: 0.2564369037747383, train acc: 0.9698\n",
      "loss: 0.26307253539562225, train acc: 0.9702\n",
      "loss: 0.27120283544063567, train acc: 0.9705\n",
      "loss: 0.2882080003619194, train acc: 0.9683\n",
      "epoch: 25, loss: 0.14305144548416138, train acc: 0.9683, test acc: 0.9347\n",
      "loss: 0.29654961824417114, train acc: 0.9687\n",
      "loss: 0.30243139564990995, train acc: 0.97\n",
      "loss: 0.29535613059997556, train acc: 0.9706\n",
      "loss: 0.27179865539073944, train acc: 0.9709\n",
      "loss: 0.2335488885641098, train acc: 0.9703\n",
      "loss: 0.25457710921764376, train acc: 0.969\n",
      "loss: 0.2739967003464699, train acc: 0.9699\n",
      "loss: 0.2679953545331955, train acc: 0.9713\n",
      "epoch: 26, loss: 0.04272323101758957, train acc: 0.9713, test acc: 0.9343\n",
      "loss: 0.2939034402370453, train acc: 0.9707\n",
      "loss: 0.32258942276239394, train acc: 0.9726\n",
      "loss: 0.2885640934109688, train acc: 0.9718\n",
      "loss: 0.2604726776480675, train acc: 0.9701\n",
      "loss: 0.24768081903457642, train acc: 0.9702\n",
      "loss: 0.2647411435842514, train acc: 0.9701\n",
      "loss: 0.30467106103897096, train acc: 0.9724\n",
      "loss: 0.3077260136604309, train acc: 0.9718\n",
      "epoch: 27, loss: 0.027888942509889603, train acc: 0.9718, test acc: 0.9344\n",
      "loss: 0.24149465560913086, train acc: 0.9713\n",
      "loss: 0.27143220156431197, train acc: 0.9719\n",
      "loss: 0.29038142412900925, train acc: 0.9729\n",
      "loss: 0.2637105882167816, train acc: 0.9722\n",
      "loss: 0.2617820993065834, train acc: 0.9726\n",
      "loss: 0.242532579600811, train acc: 0.9707\n",
      "loss: 0.2752251088619232, train acc: 0.9729\n",
      "loss: 0.25873201340436935, train acc: 0.9728\n",
      "epoch: 28, loss: 0.021062519401311874, train acc: 0.9728, test acc: 0.9328\n",
      "loss: 0.29353782534599304, train acc: 0.9717\n",
      "loss: 0.28549022078514097, train acc: 0.9728\n",
      "loss: 0.3070572644472122, train acc: 0.9727\n",
      "loss: 0.2671671539545059, train acc: 0.975\n",
      "loss: 0.25311953872442244, train acc: 0.9743\n",
      "loss: 0.26194223910570147, train acc: 0.9714\n",
      "loss: 0.25767289400100707, train acc: 0.9713\n",
      "loss: 0.29699967205524447, train acc: 0.9726\n",
      "epoch: 29, loss: 0.06321641057729721, train acc: 0.9726, test acc: 0.9348\n",
      "loss: 0.24101178348064423, train acc: 0.9715\n",
      "loss: 0.2845589995384216, train acc: 0.9726\n",
      "loss: 0.28909274786710737, train acc: 0.9719\n",
      "loss: 0.2869721636176109, train acc: 0.9725\n",
      "loss: 0.24745845198631286, train acc: 0.9727\n",
      "loss: 0.25652652233839035, train acc: 0.9741\n",
      "loss: 0.2625310942530632, train acc: 0.973\n",
      "loss: 0.278066648542881, train acc: 0.9717\n",
      "epoch: 30, loss: 0.07531015574932098, train acc: 0.9717, test acc: 0.9333\n",
      "loss: 0.26820552349090576, train acc: 0.9732\n",
      "loss: 0.2693299472332001, train acc: 0.974\n",
      "loss: 0.2635603353381157, train acc: 0.9746\n",
      "loss: 0.2604016959667206, train acc: 0.975\n",
      "loss: 0.2577594667673111, train acc: 0.9753\n",
      "loss: 0.2604216322302818, train acc: 0.974\n",
      "loss: 0.2523308888077736, train acc: 0.9757\n",
      "loss: 0.2606593415141106, train acc: 0.9761\n",
      "epoch: 31, loss: 0.026592377573251724, train acc: 0.9761, test acc: 0.9341\n",
      "loss: 0.2857438623905182, train acc: 0.973\n",
      "loss: 0.28226725310087203, train acc: 0.9738\n",
      "loss: 0.28367785066366197, train acc: 0.9735\n",
      "loss: 0.2790018618106842, train acc: 0.9753\n",
      "loss: 0.25619650185108184, train acc: 0.9753\n",
      "loss: 0.2543006896972656, train acc: 0.9749\n",
      "loss: 0.26574511379003524, train acc: 0.9747\n",
      "loss: 0.25747646689414977, train acc: 0.9749\n",
      "epoch: 32, loss: 0.11119063943624496, train acc: 0.9749, test acc: 0.9352\n",
      "loss: 0.1723875105381012, train acc: 0.976\n",
      "loss: 0.2523899763822556, train acc: 0.9759\n",
      "loss: 0.27024263292551043, train acc: 0.9763\n",
      "loss: 0.26377745270729064, train acc: 0.9748\n",
      "loss: 0.24065354317426682, train acc: 0.9749\n",
      "loss: 0.2422124221920967, train acc: 0.9759\n",
      "loss: 0.26312715113162993, train acc: 0.975\n",
      "loss: 0.25243767350912094, train acc: 0.9753\n",
      "epoch: 33, loss: 0.08287075906991959, train acc: 0.9753, test acc: 0.9356\n",
      "loss: 0.24968385696411133, train acc: 0.9751\n",
      "loss: 0.2734235778450966, train acc: 0.9751\n",
      "loss: 0.2515398845076561, train acc: 0.9763\n",
      "loss: 0.2356415256857872, train acc: 0.977\n",
      "loss: 0.22233298420906067, train acc: 0.9759\n",
      "loss: 0.2516110748052597, train acc: 0.9759\n",
      "loss: 0.2192368671298027, train acc: 0.9758\n",
      "loss: 0.25572853833436965, train acc: 0.9754\n",
      "epoch: 34, loss: 0.04143102839589119, train acc: 0.9754, test acc: 0.936\n",
      "loss: 0.3003854751586914, train acc: 0.9754\n",
      "loss: 0.2737574905157089, train acc: 0.9749\n",
      "loss: 0.2938508316874504, train acc: 0.9765\n",
      "loss: 0.26068277657032013, train acc: 0.9762\n",
      "loss: 0.2263068825006485, train acc: 0.9763\n",
      "loss: 0.24607741683721543, train acc: 0.978\n",
      "loss: 0.2500268429517746, train acc: 0.9771\n",
      "loss: 0.25787206143140795, train acc: 0.976\n",
      "epoch: 35, loss: 0.060962893068790436, train acc: 0.976, test acc: 0.9359\n",
      "loss: 0.32320287823677063, train acc: 0.9759\n",
      "loss: 0.26397146880626676, train acc: 0.9752\n",
      "loss: 0.26765273660421374, train acc: 0.9776\n",
      "loss: 0.24445738643407822, train acc: 0.9763\n",
      "loss: 0.2226710796356201, train acc: 0.9765\n",
      "loss: 0.23640718162059784, train acc: 0.9765\n",
      "loss: 0.2635466992855072, train acc: 0.9776\n",
      "loss: 0.24392472505569457, train acc: 0.9763\n",
      "epoch: 36, loss: 0.0841343030333519, train acc: 0.9763, test acc: 0.9354\n",
      "loss: 0.2566552758216858, train acc: 0.9773\n",
      "loss: 0.2614485412836075, train acc: 0.9762\n",
      "loss: 0.27185703366994857, train acc: 0.978\n",
      "loss: 0.21788668185472487, train acc: 0.9776\n",
      "loss: 0.23185033947229386, train acc: 0.9764\n",
      "loss: 0.2290768340229988, train acc: 0.9764\n",
      "loss: 0.2337241381406784, train acc: 0.9776\n",
      "loss: 0.23542336672544478, train acc: 0.9778\n",
      "epoch: 37, loss: 0.01844240352511406, train acc: 0.9778, test acc: 0.9337\n",
      "loss: 0.19344653189182281, train acc: 0.978\n",
      "loss: 0.23609625846147536, train acc: 0.9789\n",
      "loss: 0.23597992360591888, train acc: 0.9767\n",
      "loss: 0.21473115906119347, train acc: 0.9766\n",
      "loss: 0.23074859976768494, train acc: 0.9779\n",
      "loss: 0.25272387862205503, train acc: 0.9789\n",
      "loss: 0.25670768320560455, train acc: 0.9795\n",
      "loss: 0.2490059345960617, train acc: 0.9794\n",
      "epoch: 38, loss: 0.01832553744316101, train acc: 0.9794, test acc: 0.9353\n",
      "loss: 0.26206639409065247, train acc: 0.9782\n",
      "loss: 0.2590972542762756, train acc: 0.9784\n",
      "loss: 0.2695577383041382, train acc: 0.9798\n",
      "loss: 0.2343294605612755, train acc: 0.9786\n",
      "loss: 0.20901900976896287, train acc: 0.9786\n",
      "loss: 0.2565113142132759, train acc: 0.9788\n",
      "loss: 0.23661359995603562, train acc: 0.9793\n",
      "loss: 0.23637597560882567, train acc: 0.979\n",
      "epoch: 39, loss: 0.08092735707759857, train acc: 0.979, test acc: 0.9351\n",
      "loss: 0.20829558372497559, train acc: 0.9785\n",
      "loss: 0.23377192690968512, train acc: 0.9797\n",
      "loss: 0.24244932383298873, train acc: 0.9787\n",
      "loss: 0.20279572010040284, train acc: 0.98\n",
      "loss: 0.2431531623005867, train acc: 0.9798\n",
      "loss: 0.2391175776720047, train acc: 0.9796\n",
      "loss: 0.2362749196588993, train acc: 0.9794\n",
      "loss: 0.2632903575897217, train acc: 0.9776\n",
      "epoch: 40, loss: 0.0760028138756752, train acc: 0.9776, test acc: 0.9344\n",
      "loss: 0.23249197006225586, train acc: 0.9801\n",
      "loss: 0.23346305787563323, train acc: 0.9803\n",
      "loss: 0.2731822654604912, train acc: 0.9798\n",
      "loss: 0.24313350915908813, train acc: 0.9807\n",
      "loss: 0.2321161925792694, train acc: 0.9805\n",
      "loss: 0.2276644378900528, train acc: 0.978\n",
      "loss: 0.2444541335105896, train acc: 0.9795\n",
      "loss: 0.2480673760175705, train acc: 0.9786\n",
      "epoch: 41, loss: 0.06550769507884979, train acc: 0.9786, test acc: 0.9362\n",
      "loss: 0.23488092422485352, train acc: 0.9794\n",
      "loss: 0.2406891331076622, train acc: 0.9794\n",
      "loss: 0.28496127128601073, train acc: 0.9808\n",
      "loss: 0.21744468063116074, train acc: 0.9799\n",
      "loss: 0.23472826927900314, train acc: 0.9788\n",
      "loss: 0.23552791625261307, train acc: 0.9795\n",
      "loss: 0.23961507827043532, train acc: 0.9802\n",
      "loss: 0.2507772997021675, train acc: 0.9807\n",
      "epoch: 42, loss: 0.14120148122310638, train acc: 0.9807, test acc: 0.9345\n",
      "loss: 0.2940133810043335, train acc: 0.9816\n",
      "loss: 0.2494509108364582, train acc: 0.98\n",
      "loss: 0.256078864634037, train acc: 0.9804\n",
      "loss: 0.2230236753821373, train acc: 0.9814\n",
      "loss: 0.22270139753818513, train acc: 0.9795\n",
      "loss: 0.23324266374111174, train acc: 0.98\n",
      "loss: 0.23138104528188705, train acc: 0.9797\n",
      "loss: 0.24010208249092102, train acc: 0.9791\n",
      "epoch: 43, loss: 0.11850038915872574, train acc: 0.9791, test acc: 0.9357\n",
      "loss: 0.1848727911710739, train acc: 0.98\n",
      "loss: 0.2545750945806503, train acc: 0.9808\n",
      "loss: 0.24672012627124787, train acc: 0.9816\n",
      "loss: 0.22367472648620607, train acc: 0.981\n",
      "loss: 0.23975406736135482, train acc: 0.9804\n",
      "loss: 0.24070804789662362, train acc: 0.9805\n",
      "loss: 0.22153014242649077, train acc: 0.9825\n",
      "loss: 0.20708814784884452, train acc: 0.9816\n",
      "epoch: 44, loss: 0.05285030975937843, train acc: 0.9816, test acc: 0.935\n",
      "loss: 0.17718420922756195, train acc: 0.9811\n",
      "loss: 0.27474352419376374, train acc: 0.9814\n",
      "loss: 0.2700841262936592, train acc: 0.9821\n",
      "loss: 0.2104932814836502, train acc: 0.9825\n",
      "loss: 0.24142650812864302, train acc: 0.9817\n",
      "loss: 0.22382886037230493, train acc: 0.9807\n",
      "loss: 0.2565377250313759, train acc: 0.9819\n",
      "loss: 0.2181192710995674, train acc: 0.9821\n",
      "epoch: 45, loss: 0.10738392919301987, train acc: 0.9821, test acc: 0.935\n",
      "loss: 0.24010537564754486, train acc: 0.9807\n",
      "loss: 0.24908233284950257, train acc: 0.9814\n",
      "loss: 0.2513015136122704, train acc: 0.981\n",
      "loss: 0.21579720079898834, train acc: 0.9834\n",
      "loss: 0.21491941213607788, train acc: 0.9829\n",
      "loss: 0.23556484133005143, train acc: 0.9829\n",
      "loss: 0.23514740020036698, train acc: 0.9822\n",
      "loss: 0.22530213594436646, train acc: 0.9822\n",
      "epoch: 46, loss: 0.015642531216144562, train acc: 0.9822, test acc: 0.9361\n",
      "loss: 0.3137090504169464, train acc: 0.9827\n",
      "loss: 0.2700068220496178, train acc: 0.9822\n",
      "loss: 0.2740447670221329, train acc: 0.9828\n",
      "loss: 0.20829874649643898, train acc: 0.9825\n",
      "loss: 0.20216110348701477, train acc: 0.9819\n",
      "loss: 0.22943204790353774, train acc: 0.9814\n",
      "loss: 0.21866933703422547, train acc: 0.9818\n",
      "loss: 0.22055604308843613, train acc: 0.9811\n",
      "epoch: 47, loss: 0.04207657277584076, train acc: 0.9811, test acc: 0.9352\n",
      "loss: 0.309027761220932, train acc: 0.9834\n",
      "loss: 0.2408788815140724, train acc: 0.983\n",
      "loss: 0.253915011882782, train acc: 0.9845\n",
      "loss: 0.19349349588155745, train acc: 0.983\n",
      "loss: 0.22445778399705887, train acc: 0.9818\n",
      "loss: 0.21641996055841445, train acc: 0.9812\n",
      "loss: 0.23852041214704514, train acc: 0.9799\n",
      "loss: 0.23436882495880126, train acc: 0.9812\n",
      "epoch: 48, loss: 0.030035540461540222, train acc: 0.9812, test acc: 0.935\n",
      "loss: 0.1540369838476181, train acc: 0.9815\n",
      "loss: 0.23433157354593276, train acc: 0.9832\n",
      "loss: 0.2518404260277748, train acc: 0.9814\n",
      "loss: 0.24062363058328629, train acc: 0.9832\n",
      "loss: 0.21275836750864982, train acc: 0.9834\n",
      "loss: 0.1921032428741455, train acc: 0.9823\n",
      "loss: 0.22486016601324083, train acc: 0.9836\n",
      "loss: 0.22077808082103728, train acc: 0.9828\n",
      "epoch: 49, loss: 0.11512394994497299, train acc: 0.9828, test acc: 0.9351\n",
      "loss: 0.23409926891326904, train acc: 0.9832\n",
      "loss: 0.25865614265203474, train acc: 0.9833\n",
      "loss: 0.24406319260597228, train acc: 0.9819\n",
      "loss: 0.22543169409036637, train acc: 0.9816\n",
      "loss: 0.2387925535440445, train acc: 0.9823\n",
      "loss: 0.21651156097650529, train acc: 0.982\n",
      "loss: 0.20139001458883285, train acc: 0.9825\n",
      "loss: 0.23467894196510314, train acc: 0.9831\n",
      "epoch: 50, loss: 0.00935938861221075, train acc: 0.9831, test acc: 0.9359\n",
      "loss: 0.19635967910289764, train acc: 0.9842\n",
      "loss: 0.21820023655891418, train acc: 0.984\n",
      "loss: 0.22161660939455033, train acc: 0.984\n",
      "loss: 0.19791163206100465, train acc: 0.9855\n",
      "loss: 0.19365208894014357, train acc: 0.9845\n",
      "loss: 0.19116825610399246, train acc: 0.9841\n",
      "loss: 0.19808813109993934, train acc: 0.9846\n",
      "loss: 0.22197429984807968, train acc: 0.9832\n",
      "epoch: 51, loss: 0.10098432749509811, train acc: 0.9832, test acc: 0.9379\n",
      "loss: 0.16451513767242432, train acc: 0.9833\n",
      "loss: 0.23313243985176085, train acc: 0.9846\n",
      "loss: 0.24077093750238418, train acc: 0.9837\n",
      "loss: 0.22959562838077546, train acc: 0.9837\n",
      "loss: 0.20988734886050225, train acc: 0.9849\n",
      "loss: 0.22762993425130845, train acc: 0.9848\n",
      "loss: 0.20853666067123414, train acc: 0.9831\n",
      "loss: 0.22483940571546554, train acc: 0.9847\n",
      "epoch: 52, loss: 0.0411221906542778, train acc: 0.9847, test acc: 0.9348\n",
      "loss: 0.1785140335559845, train acc: 0.9837\n",
      "loss: 0.2000971630215645, train acc: 0.9832\n",
      "loss: 0.2221147157251835, train acc: 0.9825\n",
      "loss: 0.2074947863817215, train acc: 0.9843\n",
      "loss: 0.2557423397898674, train acc: 0.9824\n",
      "loss: 0.20953599065542222, train acc: 0.9837\n",
      "loss: 0.24229303300380706, train acc: 0.9852\n",
      "loss: 0.22868681252002715, train acc: 0.9849\n",
      "epoch: 53, loss: 0.015003839507699013, train acc: 0.9849, test acc: 0.9372\n",
      "loss: 0.23378053307533264, train acc: 0.9839\n",
      "loss: 0.24024638682603836, train acc: 0.9845\n",
      "loss: 0.21986297219991685, train acc: 0.9846\n",
      "loss: 0.20971437394618989, train acc: 0.9847\n",
      "loss: 0.21111952364444733, train acc: 0.9836\n",
      "loss: 0.19988904595375062, train acc: 0.9837\n",
      "loss: 0.2176976680755615, train acc: 0.9846\n",
      "loss: 0.24002912193536757, train acc: 0.984\n",
      "epoch: 54, loss: 0.1159738078713417, train acc: 0.984, test acc: 0.9366\n",
      "loss: 0.20557449758052826, train acc: 0.9848\n",
      "loss: 0.23526435643434523, train acc: 0.9854\n",
      "loss: 0.22938930243253708, train acc: 0.9839\n",
      "loss: 0.1976375088095665, train acc: 0.985\n",
      "loss: 0.19486593306064606, train acc: 0.9846\n",
      "loss: 0.22093029022216798, train acc: 0.9839\n",
      "loss: 0.19523675739765167, train acc: 0.9842\n",
      "loss: 0.22296858578920364, train acc: 0.9847\n",
      "epoch: 55, loss: 0.11374977231025696, train acc: 0.9847, test acc: 0.9349\n",
      "loss: 0.2615799307823181, train acc: 0.9834\n",
      "loss: 0.2190941423177719, train acc: 0.9842\n",
      "loss: 0.23372385054826736, train acc: 0.9855\n",
      "loss: 0.2217753767967224, train acc: 0.9853\n",
      "loss: 0.19723496288061143, train acc: 0.9849\n",
      "loss: 0.20778216868638993, train acc: 0.9843\n",
      "loss: 0.2215527430176735, train acc: 0.9844\n",
      "loss: 0.22558165192604065, train acc: 0.9867\n",
      "epoch: 56, loss: 0.04905702918767929, train acc: 0.9867, test acc: 0.9361\n",
      "loss: 0.18334612250328064, train acc: 0.986\n",
      "loss: 0.2172669515013695, train acc: 0.9867\n",
      "loss: 0.220677188038826, train acc: 0.9854\n",
      "loss: 0.21213022768497466, train acc: 0.9851\n",
      "loss: 0.19861167073249816, train acc: 0.9856\n",
      "loss: 0.21000846922397615, train acc: 0.9846\n",
      "loss: 0.20981547385454177, train acc: 0.9852\n",
      "loss: 0.21258150711655616, train acc: 0.9859\n",
      "epoch: 57, loss: 0.013774641789495945, train acc: 0.9859, test acc: 0.9352\n",
      "loss: 0.24826841056346893, train acc: 0.9854\n",
      "loss: 0.24067264944314956, train acc: 0.9874\n",
      "loss: 0.22133079469203948, train acc: 0.9859\n",
      "loss: 0.21506707966327668, train acc: 0.9847\n",
      "loss: 0.21672145277261734, train acc: 0.9844\n",
      "loss: 0.20561875998973847, train acc: 0.9852\n",
      "loss: 0.21972901672124862, train acc: 0.9858\n",
      "loss: 0.20720166191458703, train acc: 0.9866\n",
      "epoch: 58, loss: 0.00170812604483217, train acc: 0.9866, test acc: 0.9366\n",
      "loss: 0.2056291103363037, train acc: 0.9855\n",
      "loss: 0.22642272859811782, train acc: 0.9861\n",
      "loss: 0.22471214085817337, train acc: 0.9859\n",
      "loss: 0.2195589631795883, train acc: 0.9863\n",
      "loss: 0.19404695481061934, train acc: 0.9856\n",
      "loss: 0.1975381463766098, train acc: 0.9853\n",
      "loss: 0.1992644563317299, train acc: 0.9849\n",
      "loss: 0.24613034650683402, train acc: 0.9866\n",
      "epoch: 59, loss: 0.02016700617969036, train acc: 0.9866, test acc: 0.9351\n",
      "loss: 0.2901657223701477, train acc: 0.9868\n",
      "loss: 0.2531189098954201, train acc: 0.9871\n",
      "loss: 0.2156279817223549, train acc: 0.9869\n",
      "loss: 0.2081390216946602, train acc: 0.9865\n",
      "loss: 0.19046951755881308, train acc: 0.9859\n",
      "loss: 0.20344157963991166, train acc: 0.9864\n",
      "loss: 0.20027663856744765, train acc: 0.9869\n",
      "loss: 0.2169499382376671, train acc: 0.9862\n",
      "epoch: 60, loss: 0.057318881154060364, train acc: 0.9862, test acc: 0.9354\n",
      "loss: 0.2037976235151291, train acc: 0.9867\n",
      "loss: 0.23511454313993455, train acc: 0.987\n",
      "loss: 0.21209000200033187, train acc: 0.9871\n",
      "loss: 0.21682001501321793, train acc: 0.9868\n",
      "loss: 0.19910530000925064, train acc: 0.9865\n",
      "loss: 0.18444468826055527, train acc: 0.9859\n",
      "loss: 0.21830825358629227, train acc: 0.9874\n",
      "loss: 0.2028404265642166, train acc: 0.9865\n",
      "epoch: 61, loss: 0.026801668107509613, train acc: 0.9865, test acc: 0.9344\n",
      "loss: 0.24423369765281677, train acc: 0.9854\n",
      "loss: 0.20898771062493324, train acc: 0.987\n",
      "loss: 0.22030310481786727, train acc: 0.9888\n",
      "loss: 0.2015259973704815, train acc: 0.9881\n",
      "loss: 0.192340437322855, train acc: 0.987\n",
      "loss: 0.20369552820920944, train acc: 0.9874\n",
      "loss: 0.21616577357053757, train acc: 0.9867\n",
      "loss: 0.206025168299675, train acc: 0.9862\n",
      "epoch: 62, loss: 0.08116883039474487, train acc: 0.9862, test acc: 0.9376\n",
      "loss: 0.10010185837745667, train acc: 0.9884\n",
      "loss: 0.20968846529722213, train acc: 0.9867\n",
      "loss: 0.19918684363365174, train acc: 0.9876\n",
      "loss: 0.20277086347341539, train acc: 0.9881\n",
      "loss: 0.2025124177336693, train acc: 0.9875\n",
      "loss: 0.19262082427740096, train acc: 0.987\n",
      "loss: 0.21196970716118813, train acc: 0.9866\n",
      "loss: 0.21658323258161544, train acc: 0.9871\n",
      "epoch: 63, loss: 0.04104321449995041, train acc: 0.9871, test acc: 0.9363\n",
      "loss: 0.18941569328308105, train acc: 0.9876\n",
      "loss: 0.2177584171295166, train acc: 0.9875\n",
      "loss: 0.2300562247633934, train acc: 0.9885\n",
      "loss: 0.21343011111021043, train acc: 0.9878\n",
      "loss: 0.20613150000572206, train acc: 0.9867\n",
      "loss: 0.19934756606817244, train acc: 0.9862\n",
      "loss: 0.18458694741129875, train acc: 0.9877\n",
      "loss: 0.21087841242551802, train acc: 0.9881\n",
      "epoch: 64, loss: 0.14196711778640747, train acc: 0.9881, test acc: 0.9364\n",
      "loss: 0.17954175174236298, train acc: 0.9873\n",
      "loss: 0.2105950817465782, train acc: 0.9879\n",
      "loss: 0.23810819089412688, train acc: 0.9869\n",
      "loss: 0.19964933395385742, train acc: 0.9881\n",
      "loss: 0.19206273108720778, train acc: 0.9869\n",
      "loss: 0.18205471485853195, train acc: 0.9856\n",
      "loss: 0.19166696742177008, train acc: 0.9876\n",
      "loss: 0.204813052713871, train acc: 0.9877\n",
      "epoch: 65, loss: 0.01842113770544529, train acc: 0.9877, test acc: 0.9337\n",
      "loss: 0.19389046728610992, train acc: 0.9887\n",
      "loss: 0.20153741240501405, train acc: 0.9887\n",
      "loss: 0.2127021759748459, train acc: 0.9879\n",
      "loss: 0.20175697803497314, train acc: 0.9882\n",
      "loss: 0.186433906853199, train acc: 0.9877\n",
      "loss: 0.20650800913572312, train acc: 0.9874\n",
      "loss: 0.2171180635690689, train acc: 0.9879\n",
      "loss: 0.20625977516174315, train acc: 0.9886\n",
      "epoch: 66, loss: 0.05275791883468628, train acc: 0.9886, test acc: 0.934\n",
      "loss: 0.17448395490646362, train acc: 0.9874\n",
      "loss: 0.19268512055277826, train acc: 0.9882\n",
      "loss: 0.24587552696466447, train acc: 0.9883\n",
      "loss: 0.1982210397720337, train acc: 0.9886\n",
      "loss: 0.21599806249141693, train acc: 0.9879\n",
      "loss: 0.19831473231315613, train acc: 0.9879\n",
      "loss: 0.18556392043828965, train acc: 0.9882\n",
      "loss: 0.1983214072883129, train acc: 0.9879\n",
      "epoch: 67, loss: 0.17170630395412445, train acc: 0.9879, test acc: 0.9346\n",
      "loss: 0.1787908971309662, train acc: 0.9877\n",
      "loss: 0.21659909710288047, train acc: 0.9878\n",
      "loss: 0.21029694825410844, train acc: 0.9877\n",
      "loss: 0.1646533153951168, train acc: 0.9885\n",
      "loss: 0.18684990480542182, train acc: 0.988\n",
      "loss: 0.1848386511206627, train acc: 0.9876\n",
      "loss: 0.20640380084514617, train acc: 0.9874\n",
      "loss: 0.19737437665462493, train acc: 0.9877\n",
      "epoch: 68, loss: 0.024732287973165512, train acc: 0.9877, test acc: 0.9346\n",
      "loss: 0.17155303061008453, train acc: 0.9888\n",
      "loss: 0.22898680865764617, train acc: 0.9896\n",
      "loss: 0.23188370317220688, train acc: 0.9885\n",
      "loss: 0.1703915722668171, train acc: 0.9881\n",
      "loss: 0.19068199545145034, train acc: 0.9875\n",
      "loss: 0.18657087683677673, train acc: 0.9871\n",
      "loss: 0.20092715322971344, train acc: 0.9876\n",
      "loss: 0.20293571501970292, train acc: 0.9878\n",
      "epoch: 69, loss: 0.026606183499097824, train acc: 0.9878, test acc: 0.9353\n",
      "loss: 0.22761191427707672, train acc: 0.9879\n",
      "loss: 0.21775914430618287, train acc: 0.9875\n",
      "loss: 0.21905582547187805, train acc: 0.9875\n",
      "loss: 0.20009863153100013, train acc: 0.9885\n",
      "loss: 0.16995744556188583, train acc: 0.9881\n",
      "loss: 0.1826210528612137, train acc: 0.9887\n",
      "loss: 0.19133040681481361, train acc: 0.9881\n",
      "loss: 0.20588269084692, train acc: 0.9886\n",
      "epoch: 70, loss: 0.27005717158317566, train acc: 0.9886, test acc: 0.9342\n",
      "loss: 0.35407936573028564, train acc: 0.9887\n",
      "loss: 0.20827034413814544, train acc: 0.9879\n",
      "loss: 0.21742640882730485, train acc: 0.9894\n",
      "loss: 0.1900550663471222, train acc: 0.9871\n",
      "loss: 0.17320945635437965, train acc: 0.9877\n",
      "loss: 0.17094772681593895, train acc: 0.9885\n",
      "loss: 0.20569700449705125, train acc: 0.988\n",
      "loss: 0.1903049886226654, train acc: 0.9888\n",
      "epoch: 71, loss: 0.0493822917342186, train acc: 0.9888, test acc: 0.9361\n",
      "loss: 0.20364277064800262, train acc: 0.9886\n",
      "loss: 0.21971775740385055, train acc: 0.9887\n",
      "loss: 0.20435558408498763, train acc: 0.9892\n",
      "loss: 0.19752907007932663, train acc: 0.9894\n",
      "loss: 0.20201568603515624, train acc: 0.9889\n",
      "loss: 0.1974439337849617, train acc: 0.9873\n",
      "loss: 0.21190800070762633, train acc: 0.9876\n",
      "loss: 0.18513109982013704, train acc: 0.9891\n",
      "epoch: 72, loss: 0.21972663700580597, train acc: 0.9891, test acc: 0.9376\n",
      "loss: 0.19534635543823242, train acc: 0.9894\n",
      "loss: 0.22823262512683867, train acc: 0.9884\n",
      "loss: 0.20495517551898956, train acc: 0.9885\n",
      "loss: 0.1982771098613739, train acc: 0.9888\n",
      "loss: 0.20473172068595885, train acc: 0.9893\n",
      "loss: 0.17456694096326827, train acc: 0.9895\n",
      "loss: 0.1799321234226227, train acc: 0.9892\n",
      "loss: 0.18645837679505348, train acc: 0.988\n",
      "epoch: 73, loss: 0.03644777834415436, train acc: 0.988, test acc: 0.9375\n",
      "loss: 0.2097243070602417, train acc: 0.9889\n",
      "loss: 0.2113242954015732, train acc: 0.9904\n",
      "loss: 0.19462902173399926, train acc: 0.9896\n",
      "loss: 0.19530139714479447, train acc: 0.9901\n",
      "loss: 0.17993780970573425, train acc: 0.9907\n",
      "loss: 0.17198804691433905, train acc: 0.9904\n",
      "loss: 0.2007070943713188, train acc: 0.9901\n",
      "loss: 0.19893015921115875, train acc: 0.9912\n",
      "epoch: 74, loss: 0.051307313144207, train acc: 0.9912, test acc: 0.9375\n",
      "loss: 0.2064361423254013, train acc: 0.9903\n",
      "loss: 0.21044121980667113, train acc: 0.9902\n",
      "loss: 0.20452075600624084, train acc: 0.9892\n",
      "loss: 0.19716951102018357, train acc: 0.9894\n",
      "loss: 0.19780737161636353, train acc: 0.9894\n",
      "loss: 0.19013074338436126, train acc: 0.9881\n",
      "loss: 0.20963957533240318, train acc: 0.9902\n",
      "loss: 0.18084044009447098, train acc: 0.9897\n",
      "epoch: 75, loss: 0.19254015386104584, train acc: 0.9897, test acc: 0.935\n",
      "loss: 0.18949468433856964, train acc: 0.9896\n",
      "loss: 0.18698968477547168, train acc: 0.9906\n",
      "loss: 0.1932111531496048, train acc: 0.9908\n",
      "loss: 0.19253591746091842, train acc: 0.9901\n",
      "loss: 0.17813494428992271, train acc: 0.9895\n",
      "loss: 0.18934966176748275, train acc: 0.9884\n",
      "loss: 0.17995765954256057, train acc: 0.9906\n",
      "loss: 0.19279745668172837, train acc: 0.9904\n",
      "epoch: 76, loss: 0.03173544630408287, train acc: 0.9904, test acc: 0.9364\n",
      "loss: 0.1976844072341919, train acc: 0.9906\n",
      "loss: 0.20867718309164046, train acc: 0.9902\n",
      "loss: 0.19293684214353563, train acc: 0.9905\n",
      "loss: 0.1990973800420761, train acc: 0.991\n",
      "loss: 0.1675717607140541, train acc: 0.9895\n",
      "loss: 0.16351442858576776, train acc: 0.9893\n",
      "loss: 0.18894515931606293, train acc: 0.9889\n",
      "loss: 0.20943615287542344, train acc: 0.9902\n",
      "epoch: 77, loss: 0.04646679013967514, train acc: 0.9902, test acc: 0.9354\n",
      "loss: 0.18416382372379303, train acc: 0.9901\n",
      "loss: 0.2213383898139, train acc: 0.9914\n",
      "loss: 0.20051421076059342, train acc: 0.99\n",
      "loss: 0.19697792828083038, train acc: 0.9908\n",
      "loss: 0.2073965013027191, train acc: 0.9908\n",
      "loss: 0.1508932016789913, train acc: 0.9893\n",
      "loss: 0.18966748267412187, train acc: 0.9901\n",
      "loss: 0.19203753024339676, train acc: 0.9889\n",
      "epoch: 78, loss: 0.10862487554550171, train acc: 0.9889, test acc: 0.9346\n",
      "loss: 0.1986696422100067, train acc: 0.9882\n",
      "loss: 0.20250592082738877, train acc: 0.9886\n",
      "loss: 0.2010406419634819, train acc: 0.9905\n",
      "loss: 0.18280992209911345, train acc: 0.9899\n",
      "loss: 0.1931489422917366, train acc: 0.9883\n",
      "loss: 0.21001897528767585, train acc: 0.99\n",
      "loss: 0.17802427262067794, train acc: 0.9909\n",
      "loss: 0.19901555627584458, train acc: 0.9907\n",
      "epoch: 79, loss: 0.10186175256967545, train acc: 0.9907, test acc: 0.9355\n",
      "#####training and testing end with K:40, P:0.5######\n",
      "#####training and testing start with K:40, P:1######\n",
      "loss: 2.353388547897339, train acc: 0.1496\n",
      "loss: 2.0018481016159058, train acc: 0.6454\n",
      "loss: 1.3403282523155213, train acc: 0.7684\n",
      "loss: 0.8827857315540314, train acc: 0.8236\n",
      "loss: 0.7039830684661865, train acc: 0.8424\n",
      "loss: 0.5622603625059128, train acc: 0.8678\n",
      "loss: 0.502828586101532, train acc: 0.8793\n",
      "loss: 0.4384232431650162, train acc: 0.8847\n",
      "epoch: 0, loss: 0.234298974275589, train acc: 0.8847, test acc: 0.8817\n",
      "loss: 0.4163229465484619, train acc: 0.8892\n",
      "loss: 0.3874798595905304, train acc: 0.893\n",
      "loss: 0.412889763712883, train acc: 0.8979\n",
      "loss: 0.34228775948286055, train acc: 0.9055\n",
      "loss: 0.3809836685657501, train acc: 0.9052\n",
      "loss: 0.3507187247276306, train acc: 0.9108\n",
      "loss: 0.3445891857147217, train acc: 0.9126\n",
      "loss: 0.3160876274108887, train acc: 0.9143\n",
      "epoch: 1, loss: 0.0921582281589508, train acc: 0.9143, test acc: 0.8982\n",
      "loss: 0.3094133734703064, train acc: 0.9117\n",
      "loss: 0.2896293818950653, train acc: 0.9121\n",
      "loss: 0.3270727053284645, train acc: 0.9138\n",
      "loss: 0.270005838572979, train acc: 0.92\n",
      "loss: 0.3184799954295158, train acc: 0.9195\n",
      "loss: 0.29573092609643936, train acc: 0.9258\n",
      "loss: 0.29100075364112854, train acc: 0.9262\n",
      "loss: 0.26716290414333344, train acc: 0.927\n",
      "epoch: 2, loss: 0.05516031011939049, train acc: 0.927, test acc: 0.9085\n",
      "loss: 0.25725826621055603, train acc: 0.9235\n",
      "loss: 0.24358054846525193, train acc: 0.9256\n",
      "loss: 0.2799697071313858, train acc: 0.9234\n",
      "loss: 0.2310235545039177, train acc: 0.9281\n",
      "loss: 0.27901138067245485, train acc: 0.9307\n",
      "loss: 0.26161137223243713, train acc: 0.9357\n",
      "loss: 0.2565296530723572, train acc: 0.9356\n",
      "loss: 0.23610446155071257, train acc: 0.9356\n",
      "epoch: 3, loss: 0.03975312411785126, train acc: 0.9356, test acc: 0.915\n",
      "loss: 0.22467023134231567, train acc: 0.9329\n",
      "loss: 0.21258691251277922, train acc: 0.9328\n",
      "loss: 0.24651651531457902, train acc: 0.932\n",
      "loss: 0.20253236517310141, train acc: 0.9368\n",
      "loss: 0.2500434786081314, train acc: 0.9388\n",
      "loss: 0.23667572885751725, train acc: 0.9405\n",
      "loss: 0.23007158041000367, train acc: 0.9431\n",
      "loss: 0.21311938017606735, train acc: 0.9417\n",
      "epoch: 4, loss: 0.031785670667886734, train acc: 0.9417, test acc: 0.9188\n",
      "loss: 0.20081013441085815, train acc: 0.9396\n",
      "loss: 0.18936306238174438, train acc: 0.9376\n",
      "loss: 0.2205251008272171, train acc: 0.9374\n",
      "loss: 0.18111720010638238, train acc: 0.9426\n",
      "loss: 0.22779831290245056, train acc: 0.9442\n",
      "loss: 0.21636298298835754, train acc: 0.9456\n",
      "loss: 0.2082229569554329, train acc: 0.9482\n",
      "loss: 0.19449156820774077, train acc: 0.9466\n",
      "epoch: 5, loss: 0.024603350088000298, train acc: 0.9466, test acc: 0.9226\n",
      "loss: 0.18226158618927002, train acc: 0.947\n",
      "loss: 0.1702088251709938, train acc: 0.9431\n",
      "loss: 0.19839713871479034, train acc: 0.9422\n",
      "loss: 0.16281656473875045, train acc: 0.9481\n",
      "loss: 0.20953713357448578, train acc: 0.95\n",
      "loss: 0.19962638467550278, train acc: 0.9507\n",
      "loss: 0.1887723609805107, train acc: 0.9522\n",
      "loss: 0.1780656859278679, train acc: 0.9518\n",
      "epoch: 6, loss: 0.021891625598073006, train acc: 0.9518, test acc: 0.9242\n",
      "loss: 0.16569066047668457, train acc: 0.9514\n",
      "loss: 0.1536972403526306, train acc: 0.948\n",
      "loss: 0.17985257133841515, train acc: 0.9476\n",
      "loss: 0.14772821515798568, train acc: 0.9522\n",
      "loss: 0.19292647242546082, train acc: 0.9558\n",
      "loss: 0.18556758463382722, train acc: 0.9549\n",
      "loss: 0.1728309139609337, train acc: 0.9565\n",
      "loss: 0.16193845495581627, train acc: 0.9563\n",
      "epoch: 7, loss: 0.019013168290257454, train acc: 0.9563, test acc: 0.9267\n",
      "loss: 0.1467975378036499, train acc: 0.957\n",
      "loss: 0.13913174867630004, train acc: 0.9529\n",
      "loss: 0.16357439160346984, train acc: 0.9518\n",
      "loss: 0.13461302891373633, train acc: 0.955\n",
      "loss: 0.1780028223991394, train acc: 0.9597\n",
      "loss: 0.17217948287725449, train acc: 0.9591\n",
      "loss: 0.15908624678850175, train acc: 0.9605\n",
      "loss: 0.14807096645236015, train acc: 0.9599\n",
      "epoch: 8, loss: 0.016751354560256004, train acc: 0.9599, test acc: 0.9288\n",
      "loss: 0.13484029471874237, train acc: 0.9611\n",
      "loss: 0.1265524446964264, train acc: 0.9575\n",
      "loss: 0.14810004830360413, train acc: 0.9562\n",
      "loss: 0.12288053408265114, train acc: 0.9602\n",
      "loss: 0.1644616737961769, train acc: 0.9646\n",
      "loss: 0.15978218168020247, train acc: 0.9631\n",
      "loss: 0.14661887660622597, train acc: 0.9645\n",
      "loss: 0.13453544080257415, train acc: 0.9652\n",
      "epoch: 9, loss: 0.014757050201296806, train acc: 0.9652, test acc: 0.9305\n",
      "loss: 0.12284477055072784, train acc: 0.9641\n",
      "loss: 0.1162582851946354, train acc: 0.9612\n",
      "loss: 0.13407138586044312, train acc: 0.96\n",
      "loss: 0.11115965135395527, train acc: 0.9652\n",
      "loss: 0.15058876276016236, train acc: 0.9678\n",
      "loss: 0.1484414182603359, train acc: 0.9664\n",
      "loss: 0.1340255133807659, train acc: 0.968\n",
      "loss: 0.1233248807489872, train acc: 0.9686\n",
      "epoch: 10, loss: 0.013213247992098331, train acc: 0.9686, test acc: 0.9311\n",
      "loss: 0.1116359606385231, train acc: 0.9676\n",
      "loss: 0.10691450759768487, train acc: 0.9647\n",
      "loss: 0.12172392681241036, train acc: 0.9636\n",
      "loss: 0.10097499750554562, train acc: 0.9685\n",
      "loss: 0.13794804289937018, train acc: 0.9711\n",
      "loss: 0.13728002309799195, train acc: 0.9687\n",
      "loss: 0.12377045303583145, train acc: 0.9701\n",
      "loss: 0.11395544782280922, train acc: 0.9717\n",
      "epoch: 11, loss: 0.012543215416371822, train acc: 0.9717, test acc: 0.9336\n",
      "loss: 0.10152748972177505, train acc: 0.971\n",
      "loss: 0.09973047226667404, train acc: 0.9673\n",
      "loss: 0.11012648940086364, train acc: 0.9671\n",
      "loss: 0.0917817872017622, train acc: 0.9721\n",
      "loss: 0.12630748748779297, train acc: 0.974\n",
      "loss: 0.12755430340766907, train acc: 0.9716\n",
      "loss: 0.11425846591591834, train acc: 0.9721\n",
      "loss: 0.10407270267605781, train acc: 0.9742\n",
      "epoch: 12, loss: 0.01150265522301197, train acc: 0.9742, test acc: 0.9342\n",
      "loss: 0.09285169839859009, train acc: 0.9737\n",
      "loss: 0.0920392271131277, train acc: 0.9704\n",
      "loss: 0.09992369227111339, train acc: 0.9695\n",
      "loss: 0.08452262841165066, train acc: 0.9747\n",
      "loss: 0.11581018939614296, train acc: 0.9762\n",
      "loss: 0.11847927048802376, train acc: 0.9741\n",
      "loss: 0.10593039840459824, train acc: 0.9731\n",
      "loss: 0.096049864590168, train acc: 0.9767\n",
      "epoch: 13, loss: 0.010299550369381905, train acc: 0.9767, test acc: 0.9341\n",
      "loss: 0.08468242734670639, train acc: 0.9769\n",
      "loss: 0.0852920364588499, train acc: 0.9729\n",
      "loss: 0.09029516465961933, train acc: 0.9729\n",
      "loss: 0.07765301316976547, train acc: 0.9763\n",
      "loss: 0.10662435293197632, train acc: 0.9789\n",
      "loss: 0.1096785880625248, train acc: 0.9763\n",
      "loss: 0.09796407148241996, train acc: 0.9762\n",
      "loss: 0.08989670984447003, train acc: 0.9798\n",
      "epoch: 14, loss: 0.0097142793238163, train acc: 0.9798, test acc: 0.9355\n",
      "loss: 0.07965364307165146, train acc: 0.9786\n",
      "loss: 0.0801801223307848, train acc: 0.9751\n",
      "loss: 0.08286440409719945, train acc: 0.9759\n",
      "loss: 0.07199395652860403, train acc: 0.9793\n",
      "loss: 0.09755875393748284, train acc: 0.9811\n",
      "loss: 0.10196947678923607, train acc: 0.9769\n",
      "loss: 0.09121583923697471, train acc: 0.9777\n",
      "loss: 0.08406066969037056, train acc: 0.9812\n",
      "epoch: 15, loss: 0.009289034642279148, train acc: 0.9812, test acc: 0.9356\n",
      "loss: 0.07239799946546555, train acc: 0.9798\n",
      "loss: 0.07452644184231758, train acc: 0.9763\n",
      "loss: 0.07554144524037838, train acc: 0.9788\n",
      "loss: 0.06663580182939768, train acc: 0.9815\n",
      "loss: 0.08994855210185052, train acc: 0.9828\n",
      "loss: 0.09545316323637962, train acc: 0.9783\n",
      "loss: 0.08530540391802788, train acc: 0.9794\n",
      "loss: 0.07783278711140156, train acc: 0.9828\n",
      "epoch: 16, loss: 0.008340389467775822, train acc: 0.9828, test acc: 0.9365\n",
      "loss: 0.06821955740451813, train acc: 0.981\n",
      "loss: 0.07035618498921395, train acc: 0.9783\n",
      "loss: 0.06942987591028213, train acc: 0.9807\n",
      "loss: 0.06205041781067848, train acc: 0.9843\n",
      "loss: 0.08174534253776074, train acc: 0.9844\n",
      "loss: 0.08892672993242741, train acc: 0.9794\n",
      "loss: 0.07991191335022449, train acc: 0.9805\n",
      "loss: 0.07271477729082107, train acc: 0.9842\n",
      "epoch: 17, loss: 0.00774350855499506, train acc: 0.9842, test acc: 0.9369\n",
      "loss: 0.06290845572948456, train acc: 0.9819\n",
      "loss: 0.06502062417566776, train acc: 0.98\n",
      "loss: 0.06251159571111202, train acc: 0.9826\n",
      "loss: 0.057112560421228406, train acc: 0.9858\n",
      "loss: 0.07532076574862004, train acc: 0.9855\n",
      "loss: 0.08264187537133694, train acc: 0.9803\n",
      "loss: 0.0751664698123932, train acc: 0.9822\n",
      "loss: 0.06801782436668873, train acc: 0.9859\n",
      "epoch: 18, loss: 0.006921257358044386, train acc: 0.9859, test acc: 0.9373\n",
      "loss: 0.05971520394086838, train acc: 0.9835\n",
      "loss: 0.062075529247522354, train acc: 0.9813\n",
      "loss: 0.05711865127086639, train acc: 0.9844\n",
      "loss: 0.05341216828674078, train acc: 0.988\n",
      "loss: 0.0689275585114956, train acc: 0.9872\n",
      "loss: 0.07680516093969345, train acc: 0.9819\n",
      "loss: 0.07050743326544762, train acc: 0.9842\n",
      "loss: 0.0630379531532526, train acc: 0.9876\n",
      "epoch: 19, loss: 0.006628118455410004, train acc: 0.9876, test acc: 0.9371\n",
      "loss: 0.05503716692328453, train acc: 0.9853\n",
      "loss: 0.05685395412147045, train acc: 0.9849\n",
      "loss: 0.05175542719662189, train acc: 0.9864\n",
      "loss: 0.049090097472071645, train acc: 0.9888\n",
      "loss: 0.0632496252655983, train acc: 0.9884\n",
      "loss: 0.07181287556886673, train acc: 0.9834\n",
      "loss: 0.06603452786803246, train acc: 0.985\n",
      "loss: 0.05835328698158264, train acc: 0.9891\n",
      "epoch: 20, loss: 0.00582301989197731, train acc: 0.9891, test acc: 0.9367\n",
      "loss: 0.05094347149133682, train acc: 0.9862\n",
      "loss: 0.053451618365943435, train acc: 0.9864\n",
      "loss: 0.04813547618687153, train acc: 0.9879\n",
      "loss: 0.045820971950888634, train acc: 0.9895\n",
      "loss: 0.05812027938663959, train acc: 0.989\n",
      "loss: 0.06690048202872276, train acc: 0.9849\n",
      "loss: 0.061771908402442934, train acc: 0.9863\n",
      "loss: 0.05314552001655102, train acc: 0.9904\n",
      "epoch: 21, loss: 0.005132859572768211, train acc: 0.9904, test acc: 0.9366\n",
      "loss: 0.04726913198828697, train acc: 0.9874\n",
      "loss: 0.049218519032001494, train acc: 0.988\n",
      "loss: 0.04398594163358212, train acc: 0.9894\n",
      "loss: 0.04163490012288094, train acc: 0.9904\n",
      "loss: 0.053355957940220834, train acc: 0.9901\n",
      "loss: 0.06220784857869148, train acc: 0.9865\n",
      "loss: 0.05832682214677334, train acc: 0.988\n",
      "loss: 0.04907134082168341, train acc: 0.9912\n",
      "epoch: 22, loss: 0.004768759477883577, train acc: 0.9912, test acc: 0.9374\n",
      "loss: 0.043992575258016586, train acc: 0.989\n",
      "loss: 0.04625456538051367, train acc: 0.9888\n",
      "loss: 0.040175558626651765, train acc: 0.9905\n",
      "loss: 0.03786379955708981, train acc: 0.9915\n",
      "loss: 0.04878097735345364, train acc: 0.9908\n",
      "loss: 0.0578223779797554, train acc: 0.9872\n",
      "loss: 0.05389651246368885, train acc: 0.9889\n",
      "loss: 0.044778498075902465, train acc: 0.9914\n",
      "epoch: 23, loss: 0.004002826754003763, train acc: 0.9914, test acc: 0.9388\n",
      "loss: 0.04091278463602066, train acc: 0.9902\n",
      "loss: 0.04264425113797188, train acc: 0.9904\n",
      "loss: 0.036982282251119616, train acc: 0.9907\n",
      "loss: 0.03445977354422212, train acc: 0.9924\n",
      "loss: 0.04476826712489128, train acc: 0.9926\n",
      "loss: 0.053686282597482204, train acc: 0.9886\n",
      "loss: 0.04980031326413155, train acc: 0.9908\n",
      "loss: 0.04080753792077303, train acc: 0.9922\n",
      "epoch: 24, loss: 0.00374318053945899, train acc: 0.9922, test acc: 0.9388\n",
      "loss: 0.03708425164222717, train acc: 0.9916\n",
      "loss: 0.039595649018883704, train acc: 0.9913\n",
      "loss: 0.033801129274070266, train acc: 0.9917\n",
      "loss: 0.03123841416090727, train acc: 0.9934\n",
      "loss: 0.0409489668905735, train acc: 0.994\n",
      "loss: 0.04958488438278437, train acc: 0.9896\n",
      "loss: 0.046141771227121355, train acc: 0.9915\n",
      "loss: 0.0377024257555604, train acc: 0.9923\n",
      "epoch: 25, loss: 0.0032574834767729044, train acc: 0.9923, test acc: 0.939\n",
      "loss: 0.03524141013622284, train acc: 0.9922\n",
      "loss: 0.03720341846346855, train acc: 0.9928\n",
      "loss: 0.03106140960007906, train acc: 0.9927\n",
      "loss: 0.02873146627098322, train acc: 0.994\n",
      "loss: 0.037451062351465225, train acc: 0.9944\n",
      "loss: 0.04602880980819464, train acc: 0.9918\n",
      "loss: 0.04261926766484976, train acc: 0.992\n",
      "loss: 0.03422392662614584, train acc: 0.9932\n",
      "epoch: 26, loss: 0.003123031696304679, train acc: 0.9932, test acc: 0.9393\n",
      "loss: 0.032471612095832825, train acc: 0.9922\n",
      "loss: 0.03432724922895432, train acc: 0.9937\n",
      "loss: 0.028672867454588413, train acc: 0.9937\n",
      "loss: 0.02597238142043352, train acc: 0.9945\n",
      "loss: 0.03460593223571777, train acc: 0.9956\n",
      "loss: 0.042677664570510386, train acc: 0.9924\n",
      "loss: 0.04002312533557415, train acc: 0.993\n",
      "loss: 0.030601897835731508, train acc: 0.9935\n",
      "epoch: 27, loss: 0.0027496444527059793, train acc: 0.9935, test acc: 0.9396\n",
      "loss: 0.03125328570604324, train acc: 0.9931\n",
      "loss: 0.03219879493117332, train acc: 0.9946\n",
      "loss: 0.0260913772508502, train acc: 0.9941\n",
      "loss: 0.023857356142252685, train acc: 0.9951\n",
      "loss: 0.032030009850859645, train acc: 0.9958\n",
      "loss: 0.03983757607638836, train acc: 0.9942\n",
      "loss: 0.036690961383283135, train acc: 0.993\n",
      "loss: 0.028167229052633046, train acc: 0.994\n",
      "epoch: 28, loss: 0.0025279454421252012, train acc: 0.994, test acc: 0.9398\n",
      "loss: 0.02728758007287979, train acc: 0.994\n",
      "loss: 0.029573552310466766, train acc: 0.9951\n",
      "loss: 0.023886298295110464, train acc: 0.9948\n",
      "loss: 0.02187782358378172, train acc: 0.9956\n",
      "loss: 0.029245472699403762, train acc: 0.9963\n",
      "loss: 0.03694714941084385, train acc: 0.9947\n",
      "loss: 0.03328645322471857, train acc: 0.9938\n",
      "loss: 0.025075181014835836, train acc: 0.9943\n",
      "epoch: 29, loss: 0.002371247624978423, train acc: 0.9943, test acc: 0.9395\n",
      "loss: 0.026541439816355705, train acc: 0.9942\n",
      "loss: 0.027667661383748054, train acc: 0.996\n",
      "loss: 0.02210241463035345, train acc: 0.9948\n",
      "loss: 0.01982802152633667, train acc: 0.9962\n",
      "loss: 0.026853123772889377, train acc: 0.9967\n",
      "loss: 0.034321865811944005, train acc: 0.9959\n",
      "loss: 0.03109402544796467, train acc: 0.9941\n",
      "loss: 0.022970136906951665, train acc: 0.9946\n",
      "epoch: 30, loss: 0.0020496018696576357, train acc: 0.9946, test acc: 0.9397\n",
      "loss: 0.023191452026367188, train acc: 0.9941\n",
      "loss: 0.025230348482728003, train acc: 0.9967\n",
      "loss: 0.019876629766076803, train acc: 0.9954\n",
      "loss: 0.01828677551820874, train acc: 0.9966\n",
      "loss: 0.02486828323453665, train acc: 0.9967\n",
      "loss: 0.03168386407196522, train acc: 0.9963\n",
      "loss: 0.028057390451431276, train acc: 0.9943\n",
      "loss: 0.021354755479842426, train acc: 0.9952\n",
      "epoch: 31, loss: 0.0019153449684381485, train acc: 0.9952, test acc: 0.9393\n",
      "loss: 0.022221172228455544, train acc: 0.9946\n",
      "loss: 0.023372579365968704, train acc: 0.9966\n",
      "loss: 0.018495716620236635, train acc: 0.9961\n",
      "loss: 0.01710366946645081, train acc: 0.9976\n",
      "loss: 0.02258197097107768, train acc: 0.9973\n",
      "loss: 0.029865468479692937, train acc: 0.9969\n",
      "loss: 0.025201478227972984, train acc: 0.9947\n",
      "loss: 0.019606932159513236, train acc: 0.9956\n",
      "epoch: 32, loss: 0.0017623220337554812, train acc: 0.9956, test acc: 0.9391\n",
      "loss: 0.020996611565351486, train acc: 0.9951\n",
      "loss: 0.021377999614924193, train acc: 0.997\n",
      "loss: 0.01688190959393978, train acc: 0.9966\n",
      "loss: 0.015796018950641155, train acc: 0.9975\n",
      "loss: 0.021154651325196026, train acc: 0.9976\n",
      "loss: 0.028340017702430487, train acc: 0.9975\n",
      "loss: 0.022850155737251042, train acc: 0.995\n",
      "loss: 0.018399437703192235, train acc: 0.9961\n",
      "epoch: 33, loss: 0.0014331757556647062, train acc: 0.9961, test acc: 0.9387\n",
      "loss: 0.019446702674031258, train acc: 0.9957\n",
      "loss: 0.01990478588268161, train acc: 0.9969\n",
      "loss: 0.015768566448241473, train acc: 0.9969\n",
      "loss: 0.014637625589966775, train acc: 0.9978\n",
      "loss: 0.019256672449409963, train acc: 0.9977\n",
      "loss: 0.026771094463765622, train acc: 0.9979\n",
      "loss: 0.020647450163960458, train acc: 0.9955\n",
      "loss: 0.017091957246884705, train acc: 0.9967\n",
      "epoch: 34, loss: 0.0013655168004333973, train acc: 0.9967, test acc: 0.9393\n",
      "loss: 0.018261775374412537, train acc: 0.9957\n",
      "loss: 0.018462280835956336, train acc: 0.9969\n",
      "loss: 0.01474361978471279, train acc: 0.9974\n",
      "loss: 0.013639707677066326, train acc: 0.9979\n",
      "loss: 0.017996687162667512, train acc: 0.9982\n",
      "loss: 0.02496177824214101, train acc: 0.9983\n",
      "loss: 0.018795061577111482, train acc: 0.9954\n",
      "loss: 0.015923513658344747, train acc: 0.997\n",
      "epoch: 35, loss: 0.0011863700347021222, train acc: 0.997, test acc: 0.9398\n",
      "loss: 0.016876306384801865, train acc: 0.996\n",
      "loss: 0.017216737382113934, train acc: 0.9971\n",
      "loss: 0.01370205096900463, train acc: 0.9978\n",
      "loss: 0.012550207413733005, train acc: 0.9981\n",
      "loss: 0.016593992430716752, train acc: 0.9983\n",
      "loss: 0.02352750739082694, train acc: 0.9985\n",
      "loss: 0.017131033539772033, train acc: 0.9958\n",
      "loss: 0.014641183335334063, train acc: 0.9972\n",
      "epoch: 36, loss: 0.0010087448172271252, train acc: 0.9972, test acc: 0.9396\n",
      "loss: 0.015173734165728092, train acc: 0.9967\n",
      "loss: 0.01587529983371496, train acc: 0.997\n",
      "loss: 0.012890929821878672, train acc: 0.998\n",
      "loss: 0.012062483793124556, train acc: 0.9982\n",
      "loss: 0.015664557088166477, train acc: 0.9986\n",
      "loss: 0.022021165769547223, train acc: 0.9986\n",
      "loss: 0.015384321194142103, train acc: 0.9961\n",
      "loss: 0.01387506159953773, train acc: 0.9972\n",
      "epoch: 37, loss: 0.0008416136261075735, train acc: 0.9972, test acc: 0.9399\n",
      "loss: 0.014361245557665825, train acc: 0.9969\n",
      "loss: 0.014497870672494173, train acc: 0.997\n",
      "loss: 0.0121641774661839, train acc: 0.9985\n",
      "loss: 0.011157662235200405, train acc: 0.9982\n",
      "loss: 0.014457774814218283, train acc: 0.9987\n",
      "loss: 0.02059467528015375, train acc: 0.9986\n",
      "loss: 0.014353276789188385, train acc: 0.9961\n",
      "loss: 0.012586873071268202, train acc: 0.9975\n",
      "epoch: 38, loss: 0.0007541447994299233, train acc: 0.9975, test acc: 0.9401\n",
      "loss: 0.012964346446096897, train acc: 0.9972\n",
      "loss: 0.013439627643674613, train acc: 0.9972\n",
      "loss: 0.011568648368120193, train acc: 0.9989\n",
      "loss: 0.01025301031768322, train acc: 0.9988\n",
      "loss: 0.013396361749619246, train acc: 0.9988\n",
      "loss: 0.019457567669451236, train acc: 0.9989\n",
      "loss: 0.013285834807902575, train acc: 0.9966\n",
      "loss: 0.011864096345379948, train acc: 0.9975\n",
      "epoch: 39, loss: 0.0006579412147402763, train acc: 0.9975, test acc: 0.94\n",
      "loss: 0.01215333677828312, train acc: 0.9972\n",
      "loss: 0.012269688863307237, train acc: 0.9973\n",
      "loss: 0.010724226478487253, train acc: 0.9989\n",
      "loss: 0.009541941015049816, train acc: 0.9988\n",
      "loss: 0.012561311107128858, train acc: 0.9993\n",
      "loss: 0.017898173350840808, train acc: 0.9989\n",
      "loss: 0.012236976437270642, train acc: 0.9971\n",
      "loss: 0.010747488494962454, train acc: 0.9981\n",
      "epoch: 40, loss: 0.000608301954343915, train acc: 0.9981, test acc: 0.9408\n",
      "loss: 0.010577589273452759, train acc: 0.9975\n",
      "loss: 0.011062453873455525, train acc: 0.997\n",
      "loss: 0.01030243798159063, train acc: 0.9992\n",
      "loss: 0.008711588615551592, train acc: 0.9988\n",
      "loss: 0.011708688829094172, train acc: 0.9993\n",
      "loss: 0.016859308211132885, train acc: 0.9988\n",
      "loss: 0.011170143401250244, train acc: 0.9976\n",
      "loss: 0.010352352540940047, train acc: 0.998\n",
      "epoch: 41, loss: 0.0005337392212823033, train acc: 0.998, test acc: 0.9407\n",
      "loss: 0.009692905470728874, train acc: 0.9978\n",
      "loss: 0.010354172484949232, train acc: 0.997\n",
      "loss: 0.009657805599272252, train acc: 0.9991\n",
      "loss: 0.008088226104155183, train acc: 0.9989\n",
      "loss: 0.010756403068080544, train acc: 0.9994\n",
      "loss: 0.015705409133806823, train acc: 0.9985\n",
      "loss: 0.010576975904405117, train acc: 0.9977\n",
      "loss: 0.00936834008898586, train acc: 0.9977\n",
      "epoch: 42, loss: 0.00047622196143493056, train acc: 0.9977, test acc: 0.9412\n",
      "loss: 0.009265323169529438, train acc: 0.9978\n",
      "loss: 0.009736584965139627, train acc: 0.9963\n",
      "loss: 0.009058693377301096, train acc: 0.9993\n",
      "loss: 0.007436414225958287, train acc: 0.9988\n",
      "loss: 0.010155486455187202, train acc: 0.9994\n",
      "loss: 0.014558674860745668, train acc: 0.9988\n",
      "loss: 0.009653723519295453, train acc: 0.9979\n",
      "loss: 0.00853226096369326, train acc: 0.9978\n",
      "epoch: 43, loss: 0.000393605645513162, train acc: 0.9978, test acc: 0.9419\n",
      "loss: 0.00813199207186699, train acc: 0.9984\n",
      "loss: 0.009022772032767534, train acc: 0.9961\n",
      "loss: 0.008797656558454037, train acc: 0.9993\n",
      "loss: 0.007052820269018411, train acc: 0.9987\n",
      "loss: 0.00940132448449731, train acc: 0.9993\n",
      "loss: 0.01370091293938458, train acc: 0.9988\n",
      "loss: 0.008960785204544663, train acc: 0.9983\n",
      "loss: 0.008032654062844813, train acc: 0.9978\n",
      "epoch: 44, loss: 0.00037996715400367975, train acc: 0.9978, test acc: 0.9423\n",
      "loss: 0.007721684407442808, train acc: 0.9988\n",
      "loss: 0.008120949706062674, train acc: 0.9963\n",
      "loss: 0.0083578459918499, train acc: 0.9992\n",
      "loss: 0.006420773221179843, train acc: 0.9988\n",
      "loss: 0.008812590083107353, train acc: 0.9994\n",
      "loss: 0.012648315355181693, train acc: 0.9988\n",
      "loss: 0.008391782687976957, train acc: 0.9984\n",
      "loss: 0.007219147030264139, train acc: 0.9979\n",
      "epoch: 45, loss: 0.00033401374821551144, train acc: 0.9979, test acc: 0.9423\n",
      "loss: 0.007537649013102055, train acc: 0.9989\n",
      "loss: 0.007724954281002283, train acc: 0.9959\n",
      "loss: 0.007784456806257367, train acc: 0.9991\n",
      "loss: 0.006061152764596045, train acc: 0.9988\n",
      "loss: 0.008036597212776542, train acc: 0.9994\n",
      "loss: 0.011595303658396006, train acc: 0.9988\n",
      "loss: 0.00794211020693183, train acc: 0.9984\n",
      "loss: 0.006819880404509604, train acc: 0.9978\n",
      "epoch: 46, loss: 0.00029725319473072886, train acc: 0.9978, test acc: 0.9424\n",
      "loss: 0.006843627896159887, train acc: 0.9989\n",
      "loss: 0.0071387134958058596, train acc: 0.9963\n",
      "loss: 0.007331518363207579, train acc: 0.9993\n",
      "loss: 0.005577270407229662, train acc: 0.9988\n",
      "loss: 0.007416221918538213, train acc: 0.9996\n",
      "loss: 0.010490716667845845, train acc: 0.999\n",
      "loss: 0.007211275072768331, train acc: 0.9988\n",
      "loss: 0.006201984151266516, train acc: 0.9978\n",
      "epoch: 47, loss: 0.0002772538864519447, train acc: 0.9978, test acc: 0.9421\n",
      "loss: 0.006238413508981466, train acc: 0.9992\n",
      "loss: 0.006633086409419775, train acc: 0.9964\n",
      "loss: 0.007045504217967391, train acc: 0.9992\n",
      "loss: 0.005294494680128992, train acc: 0.9989\n",
      "loss: 0.006988778430968523, train acc: 0.9997\n",
      "loss: 0.009703507833182811, train acc: 0.9989\n",
      "loss: 0.006930292118340731, train acc: 0.999\n",
      "loss: 0.00597619095351547, train acc: 0.9979\n",
      "epoch: 48, loss: 0.0002625116321723908, train acc: 0.9979, test acc: 0.9422\n",
      "loss: 0.006250863429158926, train acc: 0.9993\n",
      "loss: 0.0063222925644367935, train acc: 0.9968\n",
      "loss: 0.006521316501311958, train acc: 0.9994\n",
      "loss: 0.00495529934996739, train acc: 0.9993\n",
      "loss: 0.00639852536842227, train acc: 0.9998\n",
      "loss: 0.008725033653900028, train acc: 0.9989\n",
      "loss: 0.006514589209109545, train acc: 0.9989\n",
      "loss: 0.005381079390645027, train acc: 0.9979\n",
      "epoch: 49, loss: 0.00023705785861238837, train acc: 0.9979, test acc: 0.9421\n",
      "loss: 0.006109328009188175, train acc: 0.9995\n",
      "loss: 0.0058504670392721895, train acc: 0.9967\n",
      "loss: 0.006239607860334217, train acc: 0.9992\n",
      "loss: 0.004648050083778799, train acc: 0.9993\n",
      "loss: 0.005894718156196177, train acc: 0.9998\n",
      "loss: 0.00791079862974584, train acc: 0.999\n",
      "loss: 0.006031918991357088, train acc: 0.999\n",
      "loss: 0.004936826671473682, train acc: 0.9982\n",
      "epoch: 50, loss: 0.0002131584769813344, train acc: 0.9982, test acc: 0.9422\n",
      "loss: 0.005842844024300575, train acc: 0.9995\n",
      "loss: 0.005525623355060816, train acc: 0.9963\n",
      "loss: 0.005814520409330726, train acc: 0.9995\n",
      "loss: 0.0043353457353077825, train acc: 0.9994\n",
      "loss: 0.0055205735843628645, train acc: 0.9999\n",
      "loss: 0.007178562507033348, train acc: 0.9991\n",
      "loss: 0.005920079187490046, train acc: 0.9991\n",
      "loss: 0.004882241296581924, train acc: 0.998\n",
      "epoch: 51, loss: 0.00019652009359560907, train acc: 0.998, test acc: 0.9424\n",
      "loss: 0.005410329904407263, train acc: 0.9995\n",
      "loss: 0.0053534280275925996, train acc: 0.997\n",
      "loss: 0.005386902135796845, train acc: 0.9992\n",
      "loss: 0.004082563682459295, train acc: 0.9995\n",
      "loss: 0.005224683345295489, train acc: 0.9998\n",
      "loss: 0.006482638185843825, train acc: 0.9991\n",
      "loss: 0.005456315143965185, train acc: 0.9992\n",
      "loss: 0.004305748641490937, train acc: 0.9983\n",
      "epoch: 52, loss: 0.00017744372598826885, train acc: 0.9983, test acc: 0.9424\n",
      "loss: 0.0054749781265854836, train acc: 0.9998\n",
      "loss: 0.004994887509383261, train acc: 0.9965\n",
      "loss: 0.005232710158452392, train acc: 0.9991\n",
      "loss: 0.003884867113083601, train acc: 0.9996\n",
      "loss: 0.004791352688334883, train acc: 0.9997\n",
      "loss: 0.0059718889184296135, train acc: 0.9992\n",
      "loss: 0.005217138212174177, train acc: 0.9992\n",
      "loss: 0.004192936257459224, train acc: 0.9983\n",
      "epoch: 53, loss: 0.00016071111895143986, train acc: 0.9983, test acc: 0.9419\n",
      "loss: 0.0049016037955880165, train acc: 0.9998\n",
      "loss: 0.004735641018487513, train acc: 0.9966\n",
      "loss: 0.004909419408068061, train acc: 0.9993\n",
      "loss: 0.0035852855537086724, train acc: 0.9996\n",
      "loss: 0.004587057186290622, train acc: 0.9997\n",
      "loss: 0.005450429068878293, train acc: 0.9993\n",
      "loss: 0.00489716911688447, train acc: 0.9995\n",
      "loss: 0.003834650316275656, train acc: 0.9983\n",
      "epoch: 54, loss: 0.0001554289337946102, train acc: 0.9983, test acc: 0.9422\n",
      "loss: 0.005142198409885168, train acc: 0.9998\n",
      "loss: 0.004454801441170275, train acc: 0.9968\n",
      "loss: 0.004650839627720416, train acc: 0.9992\n",
      "loss: 0.0035281215677969158, train acc: 0.9996\n",
      "loss: 0.004267028113827109, train acc: 0.9998\n",
      "loss: 0.0050252459710463885, train acc: 0.9995\n",
      "loss: 0.00459031811915338, train acc: 0.9995\n",
      "loss: 0.003556147892959416, train acc: 0.9984\n",
      "epoch: 55, loss: 0.00013252547068987042, train acc: 0.9984, test acc: 0.9422\n",
      "loss: 0.005112319253385067, train acc: 0.9998\n",
      "loss: 0.004147550766356289, train acc: 0.9971\n",
      "loss: 0.004358599404804408, train acc: 0.9991\n",
      "loss: 0.00323747101938352, train acc: 0.9998\n",
      "loss: 0.004044058243744075, train acc: 0.9997\n",
      "loss: 0.004755686572752893, train acc: 0.9996\n",
      "loss: 0.004275274020619691, train acc: 0.9995\n",
      "loss: 0.0033246935112401844, train acc: 0.9985\n",
      "epoch: 56, loss: 0.0001208558096550405, train acc: 0.9985, test acc: 0.942\n",
      "loss: 0.0049935877323150635, train acc: 0.9997\n",
      "loss: 0.004013511305674911, train acc: 0.9972\n",
      "loss: 0.004189666383899749, train acc: 0.9991\n",
      "loss: 0.003019569837488234, train acc: 0.9998\n",
      "loss: 0.00386313833296299, train acc: 0.9998\n",
      "loss: 0.00439099867362529, train acc: 0.9997\n",
      "loss: 0.004138599196448922, train acc: 0.9997\n",
      "loss: 0.0032081711338832974, train acc: 0.9986\n",
      "epoch: 57, loss: 0.00011867086868733168, train acc: 0.9986, test acc: 0.9418\n",
      "loss: 0.0047113727778196335, train acc: 0.9997\n",
      "loss: 0.003721640142612159, train acc: 0.9979\n",
      "loss: 0.0038440085714682936, train acc: 0.9988\n",
      "loss: 0.0029964536952320486, train acc: 0.9999\n",
      "loss: 0.0036810389254242184, train acc: 0.9997\n",
      "loss: 0.00412037291098386, train acc: 0.9999\n",
      "loss: 0.0037166471825912594, train acc: 0.9999\n",
      "loss: 0.002997297886759043, train acc: 0.9988\n",
      "epoch: 58, loss: 0.00010629389726091176, train acc: 0.9988, test acc: 0.9419\n",
      "loss: 0.004506119526922703, train acc: 0.9997\n",
      "loss: 0.0034644138533622025, train acc: 0.998\n",
      "loss: 0.003551765577867627, train acc: 0.9989\n",
      "loss: 0.0028785895672626795, train acc: 0.9999\n",
      "loss: 0.0034832432866096497, train acc: 0.9997\n",
      "loss: 0.0039487080182880165, train acc: 0.9999\n",
      "loss: 0.003596885618753731, train acc: 0.9999\n",
      "loss: 0.002804540074430406, train acc: 0.9989\n",
      "epoch: 59, loss: 9.769151802174747e-05, train acc: 0.9989, test acc: 0.9419\n",
      "loss: 0.004418487194925547, train acc: 0.9997\n",
      "loss: 0.003356704325415194, train acc: 0.9985\n",
      "loss: 0.0033089712727814914, train acc: 0.9989\n",
      "loss: 0.0026953666587360204, train acc: 0.9999\n",
      "loss: 0.003238909924402833, train acc: 0.9997\n",
      "loss: 0.003562675788998604, train acc: 0.9999\n",
      "loss: 0.0034116039285436274, train acc: 1.0\n",
      "loss: 0.0026449095923453568, train acc: 0.9991\n",
      "epoch: 60, loss: 8.290239929920062e-05, train acc: 0.9991, test acc: 0.9414\n",
      "loss: 0.004218374378979206, train acc: 0.9997\n",
      "loss: 0.003116497863084078, train acc: 0.9986\n",
      "loss: 0.003044763789512217, train acc: 0.9988\n",
      "loss: 0.0026246784720569847, train acc: 1.0\n",
      "loss: 0.0030786215560510755, train acc: 0.9998\n",
      "loss: 0.0034077252959832547, train acc: 1.0\n",
      "loss: 0.003134453413076699, train acc: 1.0\n",
      "loss: 0.0025636270409449937, train acc: 0.9993\n",
      "epoch: 61, loss: 7.945332617964596e-05, train acc: 0.9993, test acc: 0.9419\n",
      "loss: 0.004005160182714462, train acc: 0.9997\n",
      "loss: 0.002939628332387656, train acc: 0.9988\n",
      "loss: 0.0028143436298705637, train acc: 0.9989\n",
      "loss: 0.0025441094825509936, train acc: 1.0\n",
      "loss: 0.002872522105462849, train acc: 0.9998\n",
      "loss: 0.0031671072589233517, train acc: 1.0\n",
      "loss: 0.0029989224625751377, train acc: 1.0\n",
      "loss: 0.0023561452748253942, train acc: 0.9995\n",
      "epoch: 62, loss: 7.024095975793898e-05, train acc: 0.9995, test acc: 0.9419\n",
      "loss: 0.00372696528211236, train acc: 0.9998\n",
      "loss: 0.0027485624188557265, train acc: 0.9997\n",
      "loss: 0.0025568990386091173, train acc: 0.9988\n",
      "loss: 0.0024254947260487826, train acc: 0.9999\n",
      "loss: 0.0027544432668946683, train acc: 0.9998\n",
      "loss: 0.00297785063739866, train acc: 1.0\n",
      "loss: 0.0028346174396574495, train acc: 1.0\n",
      "loss: 0.002297885250300169, train acc: 0.9998\n",
      "epoch: 63, loss: 6.344029679894447e-05, train acc: 0.9998, test acc: 0.9419\n",
      "loss: 0.003609366249293089, train acc: 0.9999\n",
      "loss: 0.00256250211969018, train acc: 0.9997\n",
      "loss: 0.0023191167390905322, train acc: 0.9991\n",
      "loss: 0.0022972353908699006, train acc: 1.0\n",
      "loss: 0.0025542708113789558, train acc: 0.9999\n",
      "loss: 0.002805425331462175, train acc: 1.0\n",
      "loss: 0.002635159052442759, train acc: 1.0\n",
      "loss: 0.002166923962067813, train acc: 0.9999\n",
      "epoch: 64, loss: 5.678896923200227e-05, train acc: 0.9999, test acc: 0.9412\n",
      "loss: 0.0032353121787309647, train acc: 0.9999\n",
      "loss: 0.0023618471925146876, train acc: 0.9999\n",
      "loss: 0.002160932507831603, train acc: 0.9992\n",
      "loss: 0.002146505139535293, train acc: 1.0\n",
      "loss: 0.002409008238464594, train acc: 0.9999\n",
      "loss: 0.002670985250733793, train acc: 1.0\n",
      "loss: 0.002519267739262432, train acc: 1.0\n",
      "loss: 0.002057901246007532, train acc: 0.9999\n",
      "epoch: 65, loss: 4.9608581321081147e-05, train acc: 0.9999, test acc: 0.9411\n",
      "loss: 0.0030733817256987095, train acc: 0.9999\n",
      "loss: 0.002215711260214448, train acc: 0.9999\n",
      "loss: 0.001996631035581231, train acc: 0.9992\n",
      "loss: 0.00198786758701317, train acc: 0.9999\n",
      "loss: 0.002287021966185421, train acc: 0.9999\n",
      "loss: 0.0024827617919072507, train acc: 1.0\n",
      "loss: 0.002316890191286802, train acc: 1.0\n",
      "loss: 0.0019562653615139425, train acc: 0.9999\n",
      "epoch: 66, loss: 4.382828046800569e-05, train acc: 0.9999, test acc: 0.9416\n",
      "loss: 0.0027782542165368795, train acc: 0.9999\n",
      "loss: 0.0020725825685076415, train acc: 0.9999\n",
      "loss: 0.0018691944191232325, train acc: 0.9995\n",
      "loss: 0.0018325195997022092, train acc: 1.0\n",
      "loss: 0.00215527598047629, train acc: 0.9999\n",
      "loss: 0.002347182494122535, train acc: 1.0\n",
      "loss: 0.0021940048318356276, train acc: 1.0\n",
      "loss: 0.0018307230551727117, train acc: 0.9999\n",
      "epoch: 67, loss: 3.993224527221173e-05, train acc: 0.9999, test acc: 0.9414\n",
      "loss: 0.0025392973329871893, train acc: 0.9998\n",
      "loss: 0.0019367795903235674, train acc: 0.9999\n",
      "loss: 0.001745721569750458, train acc: 0.9997\n",
      "loss: 0.0017068547836970537, train acc: 1.0\n",
      "loss: 0.002035251003690064, train acc: 0.9999\n",
      "loss: 0.002193855098448694, train acc: 0.9999\n",
      "loss: 0.0019844565889798106, train acc: 1.0\n",
      "loss: 0.0017395355156622828, train acc: 0.9999\n",
      "epoch: 68, loss: 3.8457128539448604e-05, train acc: 0.9999, test acc: 0.9412\n",
      "loss: 0.002355827484279871, train acc: 0.9999\n",
      "loss: 0.001839080103673041, train acc: 1.0\n",
      "loss: 0.001621280494146049, train acc: 0.9997\n",
      "loss: 0.0015859112318139523, train acc: 1.0\n",
      "loss: 0.0019365531275980174, train acc: 0.9999\n",
      "loss: 0.002047586115077138, train acc: 0.9999\n",
      "loss: 0.0018148534931242467, train acc: 1.0\n",
      "loss: 0.0016487722517922522, train acc: 0.9999\n",
      "epoch: 69, loss: 3.514211130095646e-05, train acc: 0.9999, test acc: 0.9416\n",
      "loss: 0.0021734023466706276, train acc: 0.9999\n",
      "loss: 0.0017148235579952597, train acc: 1.0\n",
      "loss: 0.0015268404618836938, train acc: 0.9997\n",
      "loss: 0.0014787650026846677, train acc: 1.0\n",
      "loss: 0.0018133090808987618, train acc: 0.9999\n",
      "loss: 0.0019039169419556856, train acc: 0.9999\n",
      "loss: 0.0016765444655902685, train acc: 1.0\n",
      "loss: 0.0015445880009792746, train acc: 0.9999\n",
      "epoch: 70, loss: 3.38160534738563e-05, train acc: 0.9999, test acc: 0.9419\n",
      "loss: 0.002036890247836709, train acc: 0.9999\n",
      "loss: 0.001628634298685938, train acc: 1.0\n",
      "loss: 0.0014292630250565708, train acc: 0.9997\n",
      "loss: 0.00139078464708291, train acc: 1.0\n",
      "loss: 0.0017235311795957386, train acc: 0.9999\n",
      "loss: 0.00175822121091187, train acc: 0.9999\n",
      "loss: 0.0015401010401546954, train acc: 1.0\n",
      "loss: 0.0014374040067195893, train acc: 0.9999\n",
      "epoch: 71, loss: 3.236327393096872e-05, train acc: 0.9999, test acc: 0.942\n",
      "loss: 0.0018644590163603425, train acc: 0.9999\n",
      "loss: 0.0015211758436635137, train acc: 1.0\n",
      "loss: 0.0013440183829516172, train acc: 0.9998\n",
      "loss: 0.0012981914449483156, train acc: 1.0\n",
      "loss: 0.0016397493076510728, train acc: 0.9999\n",
      "loss: 0.001643058517947793, train acc: 1.0\n",
      "loss: 0.0014356322819367052, train acc: 1.0\n",
      "loss: 0.0013398263254202902, train acc: 1.0\n",
      "epoch: 72, loss: 3.175973688485101e-05, train acc: 1.0, test acc: 0.9423\n",
      "loss: 0.001729428069666028, train acc: 0.9999\n",
      "loss: 0.0014243367710150777, train acc: 1.0\n",
      "loss: 0.0012646730872802436, train acc: 0.9997\n",
      "loss: 0.001214160257950425, train acc: 1.0\n",
      "loss: 0.001572285743895918, train acc: 1.0\n",
      "loss: 0.0015506810625083745, train acc: 1.0\n",
      "loss: 0.0013574435375630855, train acc: 1.0\n",
      "loss: 0.0012588789337314664, train acc: 1.0\n",
      "epoch: 73, loss: 3.0381535907508805e-05, train acc: 1.0, test acc: 0.9426\n",
      "loss: 0.0015845419839024544, train acc: 1.0\n",
      "loss: 0.0013369849999435245, train acc: 1.0\n",
      "loss: 0.0011875645839609205, train acc: 0.9997\n",
      "loss: 0.0011373075656592845, train acc: 1.0\n",
      "loss: 0.001478058286011219, train acc: 1.0\n",
      "loss: 0.0014549124345649034, train acc: 1.0\n",
      "loss: 0.0012702836073003709, train acc: 1.0\n",
      "loss: 0.0011797699204180389, train acc: 1.0\n",
      "epoch: 74, loss: 2.9778078896924853e-05, train acc: 1.0, test acc: 0.9425\n",
      "loss: 0.0014985580928623676, train acc: 1.0\n",
      "loss: 0.0012653620797209442, train acc: 1.0\n",
      "loss: 0.0011283868516329676, train acc: 0.9999\n",
      "loss: 0.0010709492780733854, train acc: 1.0\n",
      "loss: 0.0014108793460763992, train acc: 1.0\n",
      "loss: 0.0013633619353640824, train acc: 1.0\n",
      "loss: 0.001204241521190852, train acc: 1.0\n",
      "loss: 0.001101651682984084, train acc: 1.0\n",
      "epoch: 75, loss: 2.8109408958698623e-05, train acc: 1.0, test acc: 0.9431\n",
      "loss: 0.0014249602099880576, train acc: 1.0\n",
      "loss: 0.00118286813958548, train acc: 1.0\n",
      "loss: 0.001064709312049672, train acc: 1.0\n",
      "loss: 0.0009973808832000942, train acc: 1.0\n",
      "loss: 0.001334402512293309, train acc: 1.0\n",
      "loss: 0.0012870506383478642, train acc: 1.0\n",
      "loss: 0.0011399993498343974, train acc: 1.0\n",
      "loss: 0.001045955898007378, train acc: 1.0\n",
      "epoch: 76, loss: 2.6239500584779307e-05, train acc: 1.0, test acc: 0.9429\n",
      "loss: 0.00134440662804991, train acc: 1.0\n",
      "loss: 0.0011444222007412463, train acc: 1.0\n",
      "loss: 0.0010033486236352474, train acc: 0.9999\n",
      "loss: 0.0009371368709253147, train acc: 1.0\n",
      "loss: 0.0012605193420313298, train acc: 1.0\n",
      "loss: 0.0012035941006615758, train acc: 1.0\n",
      "loss: 0.0010819260380230844, train acc: 1.0\n",
      "loss: 0.0009900277364067733, train acc: 1.0\n",
      "epoch: 77, loss: 2.5002798793138936e-05, train acc: 1.0, test acc: 0.943\n",
      "loss: 0.0012660599313676357, train acc: 1.0\n",
      "loss: 0.0010684342472814024, train acc: 1.0\n",
      "loss: 0.0009551737457513809, train acc: 0.9999\n",
      "loss: 0.0008937777922255919, train acc: 1.0\n",
      "loss: 0.0012034544604830443, train acc: 1.0\n",
      "loss: 0.001143588830018416, train acc: 1.0\n",
      "loss: 0.0010293009225279092, train acc: 1.0\n",
      "loss: 0.000921536615351215, train acc: 1.0\n",
      "epoch: 78, loss: 2.4913384550018236e-05, train acc: 1.0, test acc: 0.9428\n",
      "loss: 0.001184206223115325, train acc: 1.0\n",
      "loss: 0.001010231621330604, train acc: 1.0\n",
      "loss: 0.0008998152217827738, train acc: 0.9999\n",
      "loss: 0.0008350992808118463, train acc: 1.0\n",
      "loss: 0.0011419748130720108, train acc: 1.0\n",
      "loss: 0.0010827876161783935, train acc: 1.0\n",
      "loss: 0.0009807398892007768, train acc: 1.0\n",
      "loss: 0.000880922720534727, train acc: 1.0\n",
      "epoch: 79, loss: 2.33638274949044e-05, train acc: 1.0, test acc: 0.943\n",
      "#####training and testing end with K:40, P:1######\n"
     ]
    }
   ],
   "source": [
    "param_dropout_grid(128, [1, 5, 10, 20, 40], [0.1, 0.5, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T05:26:31.643038639Z",
     "start_time": "2023-06-06T05:23:40.231851125Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## No dropout regularization and effect of $k$"
   ],
   "metadata": {
    "collapsed": false,
    "id": "4-ZydV1EyrPx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training accuracy for each $k$ and $p$"
   ],
   "metadata": {
    "collapsed": false,
    "id": "jORwMDQ0yrPx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test accuracy for each $k$ and $p$"
   ],
   "metadata": {
    "collapsed": false,
    "id": "k6RS_JbFyrPx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 3 - Adding Noise to Labels"
   ],
   "metadata": {
    "collapsed": false,
    "id": "tbyQX1ZZyrPy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "SAVED_PARAMS_PATH = os.path.join('.', 'saved_params_with_noise')\n",
    "if not os.path.exists(SAVED_PARAMS_PATH):\n",
    "    os.mkdir(SAVED_PARAMS_PATH)\n",
    "    # logging config\n",
    "log_file_path = os.path.join('./', 'mnist_with_dropout_and_noise__with_noise.log')\n",
    "logging.basicConfig(filename=log_file_path, encoding='utf-8', level=logging.DEBUG, force=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T05:26:57.757286156Z",
     "start_time": "2023-06-06T05:26:57.713785427Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####training and testing start with K:1, P:0.1######\n",
      "loss: 2.503192186355591, train acc: 0.1\n",
      "loss: 2.4422786235809326, train acc: 0.1\n",
      "loss: 2.4630873680114744, train acc: 0.1\n",
      "loss: 2.4626322269439695, train acc: 0.1\n",
      "loss: 2.4083083152770994, train acc: 0.1\n",
      "loss: 2.425239086151123, train acc: 0.1\n",
      "loss: 2.3850432634353638, train acc: 0.1\n",
      "loss: 2.3874953985214233, train acc: 0.1\n",
      "epoch: 0, loss: 2.4133033752441406, train acc: 0.1, test acc: 0.0982\n",
      "loss: 2.3798000812530518, train acc: 0.1\n",
      "loss: 2.313624119758606, train acc: 0.1\n",
      "loss: 2.336075520515442, train acc: 0.1\n",
      "loss: 2.3362366914749146, train acc: 0.1\n",
      "loss: 2.293461036682129, train acc: 0.1\n",
      "loss: 2.3319353818893434, train acc: 0.1\n",
      "loss: 2.3005876779556274, train acc: 0.1\n",
      "loss: 2.297777462005615, train acc: 0.1\n",
      "epoch: 1, loss: 2.387000560760498, train acc: 0.1, test acc: 0.0982\n",
      "loss: 2.3072304725646973, train acc: 0.1\n",
      "loss: 2.247397708892822, train acc: 0.1\n",
      "loss: 2.271711802482605, train acc: 0.1\n",
      "loss: 2.2748739242553713, train acc: 0.1\n",
      "loss: 2.2410805225372314, train acc: 0.1\n",
      "loss: 2.2660104990005494, train acc: 0.1\n",
      "loss: 2.2384157180786133, train acc: 0.1\n",
      "loss: 2.2488781929016115, train acc: 0.1\n",
      "epoch: 2, loss: 2.268738031387329, train acc: 0.1, test acc: 0.0982\n",
      "loss: 2.269066095352173, train acc: 0.1\n",
      "loss: 2.2019042491912844, train acc: 0.1\n",
      "loss: 2.2169434309005736, train acc: 0.1\n",
      "loss: 2.2119434118270873, train acc: 0.1\n",
      "loss: 2.188733196258545, train acc: 0.1\n",
      "loss: 2.2150347232818604, train acc: 0.1057\n",
      "loss: 2.1879778623580934, train acc: 0.1097\n",
      "loss: 2.1954285383224486, train acc: 0.1056\n",
      "epoch: 3, loss: 2.251008987426758, train acc: 0.1056, test acc: 0.2003\n",
      "loss: 2.1986520290374756, train acc: 0.2028\n",
      "loss: 2.1387577056884766, train acc: 0.2031\n",
      "loss: 2.159885573387146, train acc: 0.2048\n",
      "loss: 2.154208016395569, train acc: 0.1993\n",
      "loss: 2.1341665267944334, train acc: 0.1992\n",
      "loss: 2.1480276823043822, train acc: 0.1971\n",
      "loss: 2.137574052810669, train acc: 0.1986\n",
      "loss: 2.1434223651885986, train acc: 0.1944\n",
      "epoch: 4, loss: 2.0889711380004883, train acc: 0.1944, test acc: 0.196\n",
      "loss: 2.1536402702331543, train acc: 0.1977\n",
      "loss: 2.087895941734314, train acc: 0.1966\n",
      "loss: 2.097685718536377, train acc: 0.1983\n",
      "loss: 2.095421814918518, train acc: 0.1992\n",
      "loss: 2.0913064956665037, train acc: 0.1965\n",
      "loss: 2.0766082286834715, train acc: 0.1918\n",
      "loss: 2.0874653458595276, train acc: 0.1952\n",
      "loss: 2.0964718580245973, train acc: 0.1939\n",
      "epoch: 5, loss: 2.1042990684509277, train acc: 0.1939, test acc: 0.1896\n",
      "loss: 2.0741004943847656, train acc: 0.1941\n",
      "loss: 2.038576579093933, train acc: 0.1933\n",
      "loss: 2.0507798194885254, train acc: 0.195\n",
      "loss: 2.0578404903411864, train acc: 0.1958\n",
      "loss: 2.0381099343299867, train acc: 0.194\n",
      "loss: 2.042787051200867, train acc: 0.1916\n",
      "loss: 2.0480969905853272, train acc: 0.1954\n",
      "loss: 2.0516612768173217, train acc: 0.1944\n",
      "epoch: 6, loss: 2.0567469596862793, train acc: 0.1944, test acc: 0.1873\n",
      "loss: 2.046773910522461, train acc: 0.1941\n",
      "loss: 2.032215988636017, train acc: 0.1965\n",
      "loss: 2.0240463972091676, train acc: 0.1982\n",
      "loss: 2.023472082614899, train acc: 0.1975\n",
      "loss: 2.0248131155967712, train acc: 0.1973\n",
      "loss: 2.0051656007766723, train acc: 0.1944\n",
      "loss: 2.028718054294586, train acc: 0.198\n",
      "loss: 2.0355335474014282, train acc: 0.1986\n",
      "epoch: 7, loss: 2.0029776096343994, train acc: 0.1986, test acc: 0.1904\n",
      "loss: 2.055711507797241, train acc: 0.1962\n",
      "loss: 1.9986672282218934, train acc: 0.1963\n",
      "loss: 2.0071627378463743, train acc: 0.1998\n",
      "loss: 2.006278944015503, train acc: 0.2008\n",
      "loss: 2.006318998336792, train acc: 0.1996\n",
      "loss: 1.985303556919098, train acc: 0.1974\n",
      "loss: 2.008831536769867, train acc: 0.1984\n",
      "loss: 2.00060750246048, train acc: 0.1988\n",
      "epoch: 8, loss: 1.9104092121124268, train acc: 0.1988, test acc: 0.1902\n",
      "loss: 2.023411273956299, train acc: 0.196\n",
      "loss: 1.9910077214241029, train acc: 0.1998\n",
      "loss: 2.0008793234825135, train acc: 0.2011\n",
      "loss: 1.9837095975875854, train acc: 0.2011\n",
      "loss: 1.9928192615509033, train acc: 0.2009\n",
      "loss: 1.968549382686615, train acc: 0.1992\n",
      "loss: 1.9947848916053772, train acc: 0.201\n",
      "loss: 2.0097980380058287, train acc: 0.2004\n",
      "epoch: 9, loss: 2.0074045658111572, train acc: 0.2004, test acc: 0.1931\n",
      "loss: 1.99693763256073, train acc: 0.2006\n",
      "loss: 1.9630083203315736, train acc: 0.2022\n",
      "loss: 1.9840230822563172, train acc: 0.2033\n",
      "loss: 1.9789024472236634, train acc: 0.2042\n",
      "loss: 1.9831138491630553, train acc: 0.2019\n",
      "loss: 1.967349898815155, train acc: 0.2008\n",
      "loss: 1.9757246375083923, train acc: 0.2017\n",
      "loss: 1.9737778067588807, train acc: 0.2021\n",
      "epoch: 10, loss: 1.979877233505249, train acc: 0.2021, test acc: 0.1942\n",
      "loss: 2.011869430541992, train acc: 0.2029\n",
      "loss: 1.956613004207611, train acc: 0.2035\n",
      "loss: 1.9532840490341186, train acc: 0.2038\n",
      "loss: 1.9788042426109314, train acc: 0.204\n",
      "loss: 1.9719011545181275, train acc: 0.2032\n",
      "loss: 1.9460034012794494, train acc: 0.2024\n",
      "loss: 1.9675463080406188, train acc: 0.2012\n",
      "loss: 1.971605896949768, train acc: 0.2028\n",
      "epoch: 11, loss: 1.9810115098953247, train acc: 0.2028, test acc: 0.1939\n",
      "loss: 2.02360463142395, train acc: 0.2011\n",
      "loss: 1.9522762298583984, train acc: 0.2024\n",
      "loss: 1.9559832096099854, train acc: 0.204\n",
      "loss: 1.9392840147018433, train acc: 0.2041\n",
      "loss: 1.9542251586914063, train acc: 0.2033\n",
      "loss: 1.940024185180664, train acc: 0.2011\n",
      "loss: 1.9559380292892456, train acc: 0.2031\n",
      "loss: 1.9426771998405457, train acc: 0.204\n",
      "epoch: 12, loss: 1.9729771614074707, train acc: 0.204, test acc: 0.1939\n",
      "loss: 1.9911060333251953, train acc: 0.2024\n",
      "loss: 1.9441231608390808, train acc: 0.2044\n",
      "loss: 1.9400097131729126, train acc: 0.2051\n",
      "loss: 1.9371914982795715, train acc: 0.2085\n",
      "loss: 1.9385406017303466, train acc: 0.2046\n",
      "loss: 1.9410804748535155, train acc: 0.2022\n",
      "loss: 1.949332344532013, train acc: 0.2033\n",
      "loss: 1.9536494135856628, train acc: 0.2047\n",
      "epoch: 13, loss: 1.8281309604644775, train acc: 0.2047, test acc: 0.1946\n",
      "loss: 1.9555127620697021, train acc: 0.2038\n",
      "loss: 1.925273871421814, train acc: 0.2046\n",
      "loss: 1.9168877243995666, train acc: 0.2065\n",
      "loss: 1.9319459199905396, train acc: 0.208\n",
      "loss: 1.925653839111328, train acc: 0.2067\n",
      "loss: 1.9135501265525818, train acc: 0.2031\n",
      "loss: 1.9321014642715455, train acc: 0.2041\n",
      "loss: 1.94715873003006, train acc: 0.2062\n",
      "epoch: 14, loss: 1.8176885843276978, train acc: 0.2062, test acc: 0.1962\n",
      "loss: 1.9388337135314941, train acc: 0.2049\n",
      "loss: 1.9223037838935852, train acc: 0.2068\n",
      "loss: 1.9271622180938721, train acc: 0.2083\n",
      "loss: 1.9156407594680787, train acc: 0.208\n",
      "loss: 1.9207654237747191, train acc: 0.2063\n",
      "loss: 1.92616947889328, train acc: 0.2036\n",
      "loss: 1.9150928020477296, train acc: 0.2058\n",
      "loss: 1.9336608409881593, train acc: 0.2072\n",
      "epoch: 15, loss: 1.883466124534607, train acc: 0.2072, test acc: 0.1959\n",
      "loss: 1.9388622045516968, train acc: 0.2055\n",
      "loss: 1.9264091849327087, train acc: 0.2078\n",
      "loss: 1.9165951251983642, train acc: 0.209\n",
      "loss: 1.8878369450569152, train acc: 0.208\n",
      "loss: 1.920579969882965, train acc: 0.2071\n",
      "loss: 1.9078755974769592, train acc: 0.2053\n",
      "loss: 1.9288126587867738, train acc: 0.2064\n",
      "loss: 1.9118101358413697, train acc: 0.2088\n",
      "epoch: 16, loss: 1.932259440422058, train acc: 0.2088, test acc: 0.1979\n",
      "loss: 1.976674199104309, train acc: 0.2064\n",
      "loss: 1.9103102445602418, train acc: 0.2079\n",
      "loss: 1.9064263224601745, train acc: 0.2124\n",
      "loss: 1.8874506115913392, train acc: 0.2131\n",
      "loss: 1.9091583013534545, train acc: 0.2101\n",
      "loss: 1.8928021311759948, train acc: 0.2069\n",
      "loss: 1.907564389705658, train acc: 0.2083\n",
      "loss: 1.9142773032188416, train acc: 0.211\n",
      "epoch: 17, loss: 1.7854501008987427, train acc: 0.211, test acc: 0.1995\n",
      "loss: 1.938615322113037, train acc: 0.2073\n",
      "loss: 1.8950445890426635, train acc: 0.2073\n",
      "loss: 1.890528953075409, train acc: 0.2118\n",
      "loss: 1.8990372657775878, train acc: 0.2154\n",
      "loss: 1.9120804905891418, train acc: 0.2093\n",
      "loss: 1.8936382174491881, train acc: 0.2062\n",
      "loss: 1.9061898350715638, train acc: 0.2103\n",
      "loss: 1.8954293727874756, train acc: 0.2098\n",
      "epoch: 18, loss: 2.004694938659668, train acc: 0.2098, test acc: 0.1991\n",
      "loss: 1.9130572080612183, train acc: 0.2075\n",
      "loss: 1.8954284071922303, train acc: 0.2111\n",
      "loss: 1.8707166194915772, train acc: 0.2136\n",
      "loss: 1.8892726182937623, train acc: 0.2147\n",
      "loss: 1.8963382601737977, train acc: 0.212\n",
      "loss: 1.894823157787323, train acc: 0.2105\n",
      "loss: 1.901536774635315, train acc: 0.2104\n",
      "loss: 1.8981629490852356, train acc: 0.2194\n",
      "epoch: 19, loss: 1.9153380393981934, train acc: 0.2194, test acc: 0.2015\n",
      "loss: 1.9501161575317383, train acc: 0.2115\n",
      "loss: 1.895313286781311, train acc: 0.2148\n",
      "loss: 1.8915498852729797, train acc: 0.2177\n",
      "loss: 1.8848230481147765, train acc: 0.2233\n",
      "loss: 1.9004572868347167, train acc: 0.2117\n",
      "loss: 1.8628073453903198, train acc: 0.2081\n",
      "loss: 1.9100001811981202, train acc: 0.2131\n",
      "loss: 1.8907012701034547, train acc: 0.219\n",
      "epoch: 20, loss: 1.8481184244155884, train acc: 0.219, test acc: 0.2012\n",
      "loss: 1.9012768268585205, train acc: 0.2115\n",
      "loss: 1.8899882674217223, train acc: 0.2111\n",
      "loss: 1.8823704719543457, train acc: 0.2174\n",
      "loss: 1.8732489585876464, train acc: 0.2207\n",
      "loss: 1.893261444568634, train acc: 0.2176\n",
      "loss: 1.889837872982025, train acc: 0.2121\n",
      "loss: 1.8811569809913635, train acc: 0.2159\n",
      "loss: 1.878661608695984, train acc: 0.2224\n",
      "epoch: 21, loss: 1.7543996572494507, train acc: 0.2224, test acc: 0.2027\n",
      "loss: 1.9111301898956299, train acc: 0.2155\n",
      "loss: 1.874057698249817, train acc: 0.2194\n",
      "loss: 1.8789027094841004, train acc: 0.2219\n",
      "loss: 1.8903869032859801, train acc: 0.2218\n",
      "loss: 1.880046260356903, train acc: 0.2207\n",
      "loss: 1.8788583040237428, train acc: 0.213\n",
      "loss: 1.8836965799331664, train acc: 0.2201\n",
      "loss: 1.8990251898765564, train acc: 0.2259\n",
      "epoch: 22, loss: 1.9086463451385498, train acc: 0.2259, test acc: 0.2063\n",
      "loss: 1.9296905994415283, train acc: 0.2196\n",
      "loss: 1.8678597688674927, train acc: 0.2201\n",
      "loss: 1.8616854667663574, train acc: 0.2258\n",
      "loss: 1.8798896074295044, train acc: 0.2276\n",
      "loss: 1.8745213985443114, train acc: 0.222\n",
      "loss: 1.8527094006538392, train acc: 0.2139\n",
      "loss: 1.8849663972854613, train acc: 0.2233\n",
      "loss: 1.8652995586395265, train acc: 0.2242\n",
      "epoch: 23, loss: 1.739061713218689, train acc: 0.2242, test acc: 0.2053\n",
      "loss: 1.9223114252090454, train acc: 0.2193\n",
      "loss: 1.8726062417030334, train acc: 0.2234\n",
      "loss: 1.8883708596229554, train acc: 0.2264\n",
      "loss: 1.8580891370773316, train acc: 0.2301\n",
      "loss: 1.8661317706108094, train acc: 0.2231\n",
      "loss: 1.8450990080833436, train acc: 0.2123\n",
      "loss: 1.8905874490737915, train acc: 0.2281\n",
      "loss: 1.8721625208854675, train acc: 0.2267\n",
      "epoch: 24, loss: 1.729008674621582, train acc: 0.2267, test acc: 0.21\n",
      "loss: 1.9200464487075806, train acc: 0.2233\n",
      "loss: 1.873765766620636, train acc: 0.2252\n",
      "loss: 1.8896605730056764, train acc: 0.234\n",
      "loss: 1.859879696369171, train acc: 0.2332\n",
      "loss: 1.8770564198493958, train acc: 0.2286\n",
      "loss: 1.864020013809204, train acc: 0.2181\n",
      "loss: 1.861955714225769, train acc: 0.2282\n",
      "loss: 1.8563837885856629, train acc: 0.2325\n",
      "epoch: 25, loss: 1.8786753416061401, train acc: 0.2325, test acc: 0.2098\n",
      "loss: 1.8927110433578491, train acc: 0.2224\n",
      "loss: 1.8497790932655334, train acc: 0.226\n",
      "loss: 1.86392502784729, train acc: 0.23\n",
      "loss: 1.8584593772888183, train acc: 0.2342\n",
      "loss: 1.8824609756469726, train acc: 0.23\n",
      "loss: 1.8411860704421996, train acc: 0.2189\n",
      "loss: 1.8592136263847352, train acc: 0.2286\n",
      "loss: 1.8738504648208618, train acc: 0.2358\n",
      "epoch: 26, loss: 2.213313102722168, train acc: 0.2358, test acc: 0.2153\n",
      "loss: 1.8912439346313477, train acc: 0.229\n",
      "loss: 1.891201913356781, train acc: 0.2357\n",
      "loss: 1.8376599550247192, train acc: 0.236\n",
      "loss: 1.853820788860321, train acc: 0.2417\n",
      "loss: 1.8823167443275453, train acc: 0.2323\n",
      "loss: 1.8657674312591552, train acc: 0.2208\n",
      "loss: 1.8789946556091308, train acc: 0.2377\n",
      "loss: 1.8876790165901185, train acc: 0.2346\n",
      "epoch: 27, loss: 1.7100112438201904, train acc: 0.2346, test acc: 0.2169\n",
      "loss: 1.8944001197814941, train acc: 0.232\n",
      "loss: 1.8736955046653747, train acc: 0.2346\n",
      "loss: 1.8647725462913514, train acc: 0.2369\n",
      "loss: 1.8494940519332885, train acc: 0.243\n",
      "loss: 1.8704802989959717, train acc: 0.2378\n",
      "loss: 1.8391807436943055, train acc: 0.2278\n",
      "loss: 1.8751473903656006, train acc: 0.2348\n",
      "loss: 1.8651587247848511, train acc: 0.2377\n",
      "epoch: 28, loss: 1.86187744140625, train acc: 0.2377, test acc: 0.223\n",
      "loss: 1.8687862157821655, train acc: 0.2366\n",
      "loss: 1.8692989826202393, train acc: 0.2368\n",
      "loss: 1.8637184739112853, train acc: 0.2461\n",
      "loss: 1.8411891222000123, train acc: 0.2482\n",
      "loss: 1.8620071172714234, train acc: 0.2393\n",
      "loss: 1.8427828431129456, train acc: 0.2321\n",
      "loss: 1.866576910018921, train acc: 0.2402\n",
      "loss: 1.837112843990326, train acc: 0.2414\n",
      "epoch: 29, loss: 1.6986374855041504, train acc: 0.2414, test acc: 0.2229\n",
      "loss: 1.902074933052063, train acc: 0.2362\n",
      "loss: 1.861031186580658, train acc: 0.2401\n",
      "loss: 1.8660871863365174, train acc: 0.2482\n",
      "loss: 1.8491054773330688, train acc: 0.2516\n",
      "loss: 1.8816588163375854, train acc: 0.2454\n",
      "loss: 1.846821415424347, train acc: 0.2327\n",
      "loss: 1.8563212633132935, train acc: 0.2433\n",
      "loss: 1.8631163716316224, train acc: 0.2406\n",
      "epoch: 30, loss: 1.8556262254714966, train acc: 0.2406, test acc: 0.2276\n",
      "loss: 1.8661330938339233, train acc: 0.2409\n",
      "loss: 1.8587725400924682, train acc: 0.2435\n",
      "loss: 1.8846057057380676, train acc: 0.2515\n",
      "loss: 1.836814546585083, train acc: 0.2528\n",
      "loss: 1.8632792830467224, train acc: 0.2466\n",
      "loss: 1.829032564163208, train acc: 0.2361\n",
      "loss: 1.8685579538345336, train acc: 0.2464\n",
      "loss: 1.859517514705658, train acc: 0.2491\n",
      "epoch: 31, loss: 1.7976211309432983, train acc: 0.2491, test acc: 0.2278\n",
      "loss: 1.8552610874176025, train acc: 0.2408\n",
      "loss: 1.854848325252533, train acc: 0.2493\n",
      "loss: 1.841872751712799, train acc: 0.2534\n",
      "loss: 1.8484847664833068, train acc: 0.2541\n",
      "loss: 1.8809090733528138, train acc: 0.2509\n",
      "loss: 1.8313225150108337, train acc: 0.2344\n",
      "loss: 1.8660786867141723, train acc: 0.2501\n",
      "loss: 1.8484061241149903, train acc: 0.2474\n",
      "epoch: 32, loss: 1.7910757064819336, train acc: 0.2474, test acc: 0.2263\n",
      "loss: 1.9411007165908813, train acc: 0.2384\n",
      "loss: 1.8567418575286865, train acc: 0.2421\n",
      "loss: 1.8537849187850952, train acc: 0.2564\n",
      "loss: 1.8367430806159972, train acc: 0.254\n",
      "loss: 1.8533875465393066, train acc: 0.2527\n",
      "loss: 1.812191665172577, train acc: 0.2391\n",
      "loss: 1.8629539012908936, train acc: 0.2516\n",
      "loss: 1.8478458762168883, train acc: 0.2517\n",
      "epoch: 33, loss: 1.8911412954330444, train acc: 0.2517, test acc: 0.2311\n",
      "loss: 1.9554411172866821, train acc: 0.2462\n",
      "loss: 1.8493811249732972, train acc: 0.2535\n",
      "loss: 1.8603305697441102, train acc: 0.2605\n",
      "loss: 1.8205684542655944, train acc: 0.2551\n",
      "loss: 1.8523686051368713, train acc: 0.2548\n",
      "loss: 1.8234901309013367, train acc: 0.2403\n",
      "loss: 1.8642428040504455, train acc: 0.2527\n",
      "loss: 1.8632364869117737, train acc: 0.2585\n",
      "epoch: 34, loss: 1.672135353088379, train acc: 0.2585, test acc: 0.2317\n",
      "loss: 1.9388012886047363, train acc: 0.2479\n",
      "loss: 1.8605305314064027, train acc: 0.2596\n",
      "loss: 1.8670628786087036, train acc: 0.2614\n",
      "loss: 1.8222663164138795, train acc: 0.2605\n",
      "loss: 1.8484532475471496, train acc: 0.2555\n",
      "loss: 1.8478523731231689, train acc: 0.2501\n",
      "loss: 1.8638040661811828, train acc: 0.2551\n",
      "loss: 1.8405715227127075, train acc: 0.257\n",
      "epoch: 35, loss: 1.667372703552246, train acc: 0.257, test acc: 0.238\n",
      "loss: 1.8965555429458618, train acc: 0.2517\n",
      "loss: 1.8279685497283935, train acc: 0.2552\n",
      "loss: 1.848062264919281, train acc: 0.2631\n",
      "loss: 1.8291808843612671, train acc: 0.2632\n",
      "loss: 1.8624902606010436, train acc: 0.2575\n",
      "loss: 1.8415435671806335, train acc: 0.2485\n",
      "loss: 1.8673137307167054, train acc: 0.2607\n",
      "loss: 1.8405672192573548, train acc: 0.259\n",
      "epoch: 36, loss: 1.663386344909668, train acc: 0.259, test acc: 0.2435\n",
      "loss: 1.8865362405776978, train acc: 0.2556\n",
      "loss: 1.837723481655121, train acc: 0.2593\n",
      "loss: 1.8454905509948731, train acc: 0.2636\n",
      "loss: 1.8224502682685852, train acc: 0.2653\n",
      "loss: 1.8486887097358704, train acc: 0.2572\n",
      "loss: 1.8228147506713868, train acc: 0.2558\n",
      "loss: 1.848051655292511, train acc: 0.2619\n",
      "loss: 1.8603630781173706, train acc: 0.2607\n",
      "epoch: 37, loss: 1.6593270301818848, train acc: 0.2607, test acc: 0.2436\n",
      "loss: 1.9752720594406128, train acc: 0.2566\n",
      "loss: 1.8377636313438415, train acc: 0.2593\n",
      "loss: 1.842618477344513, train acc: 0.2646\n",
      "loss: 1.82863130569458, train acc: 0.2669\n",
      "loss: 1.8506911039352416, train acc: 0.2641\n",
      "loss: 1.8317842841148377, train acc: 0.2511\n",
      "loss: 1.8737884044647217, train acc: 0.2632\n",
      "loss: 1.8381858825683595, train acc: 0.2644\n",
      "epoch: 38, loss: 1.825788974761963, train acc: 0.2644, test acc: 0.2428\n",
      "loss: 1.855594277381897, train acc: 0.257\n",
      "loss: 1.853189754486084, train acc: 0.2637\n",
      "loss: 1.8078193068504333, train acc: 0.2659\n",
      "loss: 1.8214576840400696, train acc: 0.2651\n",
      "loss: 1.8229439735412598, train acc: 0.2652\n",
      "loss: 1.7998316884040833, train acc: 0.2579\n",
      "loss: 1.830594801902771, train acc: 0.2646\n",
      "loss: 1.8519000649452209, train acc: 0.2628\n",
      "epoch: 39, loss: 1.8858095407485962, train acc: 0.2628, test acc: 0.2464\n",
      "loss: 1.921108365058899, train acc: 0.2611\n",
      "loss: 1.846503531932831, train acc: 0.2641\n",
      "loss: 1.843740177154541, train acc: 0.2704\n",
      "loss: 1.8087944865226746, train acc: 0.2724\n",
      "loss: 1.8542974591255188, train acc: 0.2649\n",
      "loss: 1.8013194680213929, train acc: 0.2583\n",
      "loss: 1.8422351360321045, train acc: 0.2706\n",
      "loss: 1.8537444353103638, train acc: 0.2685\n",
      "epoch: 40, loss: 1.8865859508514404, train acc: 0.2685, test acc: 0.2588\n",
      "loss: 1.9545960426330566, train acc: 0.2715\n",
      "loss: 1.8506415247917176, train acc: 0.2701\n",
      "loss: 1.8569358706474304, train acc: 0.2753\n",
      "loss: 1.8281297922134399, train acc: 0.2747\n",
      "loss: 1.8188947558403015, train acc: 0.2657\n",
      "loss: 1.8325669050216675, train acc: 0.2604\n",
      "loss: 1.820248055458069, train acc: 0.2687\n",
      "loss: 1.8258788466453553, train acc: 0.267\n",
      "epoch: 41, loss: 1.9368715286254883, train acc: 0.267, test acc: 0.2559\n",
      "loss: 1.8625082969665527, train acc: 0.2702\n",
      "loss: 1.8584257364273071, train acc: 0.2665\n",
      "loss: 1.8304149746894836, train acc: 0.2785\n",
      "loss: 1.8204492688179017, train acc: 0.2786\n",
      "loss: 1.8379556775093078, train acc: 0.2694\n",
      "loss: 1.8291589498519898, train acc: 0.2593\n",
      "loss: 1.8387726902961732, train acc: 0.271\n",
      "loss: 1.8385278224945067, train acc: 0.2703\n",
      "epoch: 42, loss: 1.8911309242248535, train acc: 0.2703, test acc: 0.2577\n",
      "loss: 1.852229118347168, train acc: 0.2651\n",
      "loss: 1.8314357161521913, train acc: 0.2704\n",
      "loss: 1.851967215538025, train acc: 0.2792\n",
      "loss: 1.8218407034873962, train acc: 0.2779\n",
      "loss: 1.8360262513160706, train acc: 0.272\n",
      "loss: 1.8053058981895447, train acc: 0.2685\n",
      "loss: 1.8370546221733093, train acc: 0.2774\n",
      "loss: 1.8087201237678527, train acc: 0.2714\n",
      "epoch: 43, loss: 1.8123315572738647, train acc: 0.2714, test acc: 0.2756\n",
      "loss: 1.8825527429580688, train acc: 0.2667\n",
      "loss: 1.8166258335113525, train acc: 0.27\n",
      "loss: 1.813812518119812, train acc: 0.2782\n",
      "loss: 1.8141354322433472, train acc: 0.2802\n",
      "loss: 1.8606237888336181, train acc: 0.2781\n",
      "loss: 1.814895236492157, train acc: 0.2653\n",
      "loss: 1.8403598546981812, train acc: 0.2754\n",
      "loss: 1.8210662841796874, train acc: 0.2702\n",
      "epoch: 44, loss: 1.6344492435455322, train acc: 0.2702, test acc: 0.2801\n",
      "loss: 1.8876574039459229, train acc: 0.2701\n",
      "loss: 1.8197725534439086, train acc: 0.2687\n",
      "loss: 1.838100504875183, train acc: 0.2827\n",
      "loss: 1.8231548905372619, train acc: 0.2815\n",
      "loss: 1.8329676032066344, train acc: 0.2776\n",
      "loss: 1.8171889185905457, train acc: 0.2683\n",
      "loss: 1.8457868218421936, train acc: 0.2816\n",
      "loss: 1.8407610297203063, train acc: 0.2785\n",
      "epoch: 45, loss: 1.885128140449524, train acc: 0.2785, test acc: 0.2841\n",
      "loss: 1.8261346817016602, train acc: 0.277\n",
      "loss: 1.8446999311447143, train acc: 0.2824\n",
      "loss: 1.8229051947593689, train acc: 0.2846\n",
      "loss: 1.8380151510238647, train acc: 0.2854\n",
      "loss: 1.837368905544281, train acc: 0.2802\n",
      "loss: 1.8285191059112549, train acc: 0.2697\n",
      "loss: 1.8211103320121764, train acc: 0.2803\n",
      "loss: 1.8486844778060914, train acc: 0.2815\n",
      "epoch: 46, loss: 1.7580116987228394, train acc: 0.2815, test acc: 0.2807\n",
      "loss: 1.8462648391723633, train acc: 0.2728\n",
      "loss: 1.8224014163017273, train acc: 0.2825\n",
      "loss: 1.8143872380256654, train acc: 0.2839\n",
      "loss: 1.8363034009933472, train acc: 0.2834\n",
      "loss: 1.841370689868927, train acc: 0.2791\n",
      "loss: 1.825670313835144, train acc: 0.2713\n",
      "loss: 1.8409943103790283, train acc: 0.282\n",
      "loss: 1.8607703924179078, train acc: 0.2818\n",
      "epoch: 47, loss: 1.6271560192108154, train acc: 0.2818, test acc: 0.2827\n",
      "loss: 1.8395599126815796, train acc: 0.2734\n",
      "loss: 1.8186564087867736, train acc: 0.2819\n",
      "loss: 1.836070191860199, train acc: 0.2885\n",
      "loss: 1.8102782249450684, train acc: 0.2832\n",
      "loss: 1.8294332861900329, train acc: 0.2852\n",
      "loss: 1.7927340030670167, train acc: 0.2761\n",
      "loss: 1.8452165722846985, train acc: 0.2865\n",
      "loss: 1.832605767250061, train acc: 0.2867\n",
      "epoch: 48, loss: 1.9299603700637817, train acc: 0.2867, test acc: 0.2829\n",
      "loss: 1.8925269842147827, train acc: 0.2761\n",
      "loss: 1.83118816614151, train acc: 0.2827\n",
      "loss: 1.8294968962669373, train acc: 0.2851\n",
      "loss: 1.8034232020378114, train acc: 0.2842\n",
      "loss: 1.8300400853157044, train acc: 0.281\n",
      "loss: 1.8173556804656983, train acc: 0.274\n",
      "loss: 1.829892671108246, train acc: 0.2864\n",
      "loss: 1.8319645404815674, train acc: 0.2818\n",
      "epoch: 49, loss: 1.6228801012039185, train acc: 0.2818, test acc: 0.2839\n",
      "loss: 1.8281118869781494, train acc: 0.278\n",
      "loss: 1.8188979983329774, train acc: 0.2841\n",
      "loss: 1.8196146130561828, train acc: 0.2883\n",
      "loss: 1.8131357550621032, train acc: 0.2876\n",
      "loss: 1.832560384273529, train acc: 0.2823\n",
      "loss: 1.7968384981155396, train acc: 0.278\n",
      "loss: 1.8554213404655457, train acc: 0.2888\n",
      "loss: 1.8265435576438904, train acc: 0.2862\n",
      "epoch: 50, loss: 1.88626229763031, train acc: 0.2862, test acc: 0.2864\n",
      "loss: 1.8626189231872559, train acc: 0.283\n",
      "loss: 1.828626525402069, train acc: 0.2828\n",
      "loss: 1.8256132006645203, train acc: 0.2888\n",
      "loss: 1.8058528542518615, train acc: 0.288\n",
      "loss: 1.8266610383987427, train acc: 0.2841\n",
      "loss: 1.7972252726554871, train acc: 0.282\n",
      "loss: 1.8469912528991699, train acc: 0.2854\n",
      "loss: 1.859713649749756, train acc: 0.2889\n",
      "epoch: 51, loss: 1.8178610801696777, train acc: 0.2889, test acc: 0.2836\n",
      "loss: 1.8240315914154053, train acc: 0.2769\n",
      "loss: 1.806369137763977, train acc: 0.2854\n",
      "loss: 1.830080223083496, train acc: 0.2884\n",
      "loss: 1.8042163729667664, train acc: 0.2869\n",
      "loss: 1.8344686508178711, train acc: 0.2889\n",
      "loss: 1.850013267993927, train acc: 0.2811\n",
      "loss: 1.8519261002540588, train acc: 0.2884\n",
      "loss: 1.8385697841644286, train acc: 0.2858\n",
      "epoch: 52, loss: 1.6161274909973145, train acc: 0.2858, test acc: 0.2876\n",
      "loss: 1.8953421115875244, train acc: 0.2847\n",
      "loss: 1.8322077631950378, train acc: 0.2853\n",
      "loss: 1.8154690146446228, train acc: 0.2918\n",
      "loss: 1.8180413842201233, train acc: 0.2904\n",
      "loss: 1.8371598362922668, train acc: 0.2889\n",
      "loss: 1.8344357132911682, train acc: 0.2797\n",
      "loss: 1.8491978168487548, train acc: 0.2866\n",
      "loss: 1.8355143427848817, train acc: 0.2883\n",
      "epoch: 53, loss: 1.636048436164856, train acc: 0.2883, test acc: 0.2857\n",
      "loss: 1.9296106100082397, train acc: 0.2824\n",
      "loss: 1.8183428645133972, train acc: 0.2853\n",
      "loss: 1.8209511280059814, train acc: 0.2902\n",
      "loss: 1.8019979596138, train acc: 0.2889\n",
      "loss: 1.8290994882583618, train acc: 0.287\n",
      "loss: 1.8116797924041748, train acc: 0.2811\n",
      "loss: 1.8070904970169068, train acc: 0.2867\n",
      "loss: 1.8553544878959656, train acc: 0.289\n",
      "epoch: 54, loss: 1.8897900581359863, train acc: 0.289, test acc: 0.2854\n",
      "loss: 1.8553476333618164, train acc: 0.2837\n",
      "loss: 1.8044758558273315, train acc: 0.2903\n",
      "loss: 1.8292699813842774, train acc: 0.2931\n",
      "loss: 1.8052175998687745, train acc: 0.2903\n",
      "loss: 1.8111178398132324, train acc: 0.2884\n",
      "loss: 1.8180541038513183, train acc: 0.285\n",
      "loss: 1.846926474571228, train acc: 0.2883\n",
      "loss: 1.8203862667083741, train acc: 0.2912\n",
      "epoch: 55, loss: 1.6093885898590088, train acc: 0.2912, test acc: 0.289\n",
      "loss: 1.8687140941619873, train acc: 0.2866\n",
      "loss: 1.823943293094635, train acc: 0.2912\n",
      "loss: 1.8292322397232055, train acc: 0.2939\n",
      "loss: 1.8006389021873475, train acc: 0.2923\n",
      "loss: 1.8535985708236695, train acc: 0.2896\n",
      "loss: 1.8178337454795837, train acc: 0.2839\n",
      "loss: 1.8545889377593994, train acc: 0.2897\n",
      "loss: 1.821940016746521, train acc: 0.2899\n",
      "epoch: 56, loss: 1.887234091758728, train acc: 0.2899, test acc: 0.2764\n",
      "loss: 1.863723635673523, train acc: 0.2875\n",
      "loss: 1.8218805313110351, train acc: 0.2878\n",
      "loss: 1.8423036813735962, train acc: 0.294\n",
      "loss: 1.8210506677627563, train acc: 0.2931\n",
      "loss: 1.844015371799469, train acc: 0.2875\n",
      "loss: 1.8076613664627075, train acc: 0.2842\n",
      "loss: 1.852670383453369, train acc: 0.2882\n",
      "loss: 1.8142226099967957, train acc: 0.2892\n",
      "epoch: 57, loss: 1.6294491291046143, train acc: 0.2892, test acc: 0.275\n",
      "loss: 1.8590511083602905, train acc: 0.2926\n",
      "loss: 1.833245038986206, train acc: 0.2898\n",
      "loss: 1.841928744316101, train acc: 0.2943\n",
      "loss: 1.8539032220840455, train acc: 0.2922\n",
      "loss: 1.8498372197151185, train acc: 0.292\n",
      "loss: 1.8007280707359314, train acc: 0.2887\n",
      "loss: 1.8243893027305602, train acc: 0.2905\n",
      "loss: 1.8300909042358398, train acc: 0.2929\n",
      "epoch: 58, loss: 1.605342149734497, train acc: 0.2929, test acc: 0.2737\n",
      "loss: 1.902464747428894, train acc: 0.2927\n",
      "loss: 1.8222198247909547, train acc: 0.2924\n",
      "loss: 1.8241666793823241, train acc: 0.2932\n",
      "loss: 1.7901823759078979, train acc: 0.2925\n",
      "loss: 1.8268762230873108, train acc: 0.2912\n",
      "loss: 1.809319019317627, train acc: 0.2872\n",
      "loss: 1.8107322216033936, train acc: 0.2903\n",
      "loss: 1.813010025024414, train acc: 0.2911\n",
      "epoch: 59, loss: 1.7781574726104736, train acc: 0.2911, test acc: 0.2758\n",
      "loss: 1.8334838151931763, train acc: 0.2935\n",
      "loss: 1.7904409050941468, train acc: 0.2887\n",
      "loss: 1.8268230080604553, train acc: 0.2927\n",
      "loss: 1.807598054409027, train acc: 0.2935\n",
      "loss: 1.8158192873001098, train acc: 0.2899\n",
      "loss: 1.804426670074463, train acc: 0.2891\n",
      "loss: 1.8561524868011474, train acc: 0.2916\n",
      "loss: 1.8346889734268188, train acc: 0.2912\n",
      "epoch: 60, loss: 1.5993067026138306, train acc: 0.2912, test acc: 0.2823\n",
      "loss: 1.8517547845840454, train acc: 0.2993\n",
      "loss: 1.8110541701316833, train acc: 0.2929\n",
      "loss: 1.8182431936264039, train acc: 0.2988\n",
      "loss: 1.8049990773200988, train acc: 0.299\n",
      "loss: 1.8366986632347106, train acc: 0.2927\n",
      "loss: 1.8195328950881957, train acc: 0.2919\n",
      "loss: 1.8206811785697936, train acc: 0.2931\n",
      "loss: 1.8209518790245056, train acc: 0.2925\n",
      "epoch: 61, loss: 1.599043846130371, train acc: 0.2925, test acc: 0.2797\n",
      "loss: 1.8922245502471924, train acc: 0.2975\n",
      "loss: 1.8417993783950806, train acc: 0.2949\n",
      "loss: 1.8206858515739441, train acc: 0.2979\n",
      "loss: 1.798967146873474, train acc: 0.2981\n",
      "loss: 1.8320818781852721, train acc: 0.2928\n",
      "loss: 1.780413019657135, train acc: 0.2899\n",
      "loss: 1.8151369094848633, train acc: 0.2933\n",
      "loss: 1.8137726306915283, train acc: 0.2932\n",
      "epoch: 62, loss: 1.597348690032959, train acc: 0.2932, test acc: 0.2785\n",
      "loss: 1.88023042678833, train acc: 0.2985\n",
      "loss: 1.8173292398452758, train acc: 0.2947\n",
      "loss: 1.8430347919464112, train acc: 0.2987\n",
      "loss: 1.7746024966239928, train acc: 0.2978\n",
      "loss: 1.7878511905670167, train acc: 0.2975\n",
      "loss: 1.8021527051925659, train acc: 0.292\n",
      "loss: 1.822806966304779, train acc: 0.293\n",
      "loss: 1.8163523077964783, train acc: 0.2964\n",
      "epoch: 63, loss: 1.596983551979065, train acc: 0.2964, test acc: 0.2771\n",
      "loss: 1.920983076095581, train acc: 0.2973\n",
      "loss: 1.8095141887664794, train acc: 0.2922\n",
      "loss: 1.8373021841049195, train acc: 0.299\n",
      "loss: 1.790783703327179, train acc: 0.2979\n",
      "loss: 1.837156391143799, train acc: 0.2964\n",
      "loss: 1.7731409311294555, train acc: 0.2924\n",
      "loss: 1.8262605547904969, train acc: 0.2952\n",
      "loss: 1.8158775448799134, train acc: 0.3007\n",
      "epoch: 64, loss: 1.59255051612854, train acc: 0.3007, test acc: 0.2811\n",
      "loss: 1.8300482034683228, train acc: 0.3001\n",
      "loss: 1.836757528781891, train acc: 0.3023\n",
      "loss: 1.8127795577049255, train acc: 0.3011\n",
      "loss: 1.797801637649536, train acc: 0.2979\n",
      "loss: 1.825597333908081, train acc: 0.2961\n",
      "loss: 1.7915155410766601, train acc: 0.292\n",
      "loss: 1.8248486518859863, train acc: 0.2933\n",
      "loss: 1.8143502354621888, train acc: 0.2978\n",
      "epoch: 65, loss: 1.593629002571106, train acc: 0.2978, test acc: 0.2796\n",
      "loss: 1.8488422632217407, train acc: 0.2978\n",
      "loss: 1.801903212070465, train acc: 0.3004\n",
      "loss: 1.7979580640792847, train acc: 0.3007\n",
      "loss: 1.8107792377471923, train acc: 0.2999\n",
      "loss: 1.8326373815536499, train acc: 0.2979\n",
      "loss: 1.7893901348114014, train acc: 0.2918\n",
      "loss: 1.8128173351287842, train acc: 0.2935\n",
      "loss: 1.8197612047195435, train acc: 0.2974\n",
      "epoch: 66, loss: 1.9034998416900635, train acc: 0.2974, test acc: 0.2816\n",
      "loss: 1.8525454998016357, train acc: 0.299\n",
      "loss: 1.8007932424545288, train acc: 0.2999\n",
      "loss: 1.8611236929893493, train acc: 0.2995\n",
      "loss: 1.8010997772216797, train acc: 0.2987\n",
      "loss: 1.8458758354187013, train acc: 0.2976\n",
      "loss: 1.788672721385956, train acc: 0.2926\n",
      "loss: 1.8232334017753602, train acc: 0.2944\n",
      "loss: 1.8060282111167907, train acc: 0.2979\n",
      "epoch: 67, loss: 1.9050939083099365, train acc: 0.2979, test acc: 0.2831\n",
      "loss: 1.8497459888458252, train acc: 0.3006\n",
      "loss: 1.8360516667366027, train acc: 0.3003\n",
      "loss: 1.8227339506149292, train acc: 0.301\n",
      "loss: 1.7922975659370421, train acc: 0.2994\n",
      "loss: 1.8127801418304443, train acc: 0.297\n",
      "loss: 1.8003626465797424, train acc: 0.2967\n",
      "loss: 1.8024387240409852, train acc: 0.2986\n",
      "loss: 1.8248642563819886, train acc: 0.2988\n",
      "epoch: 68, loss: 1.9086743593215942, train acc: 0.2988, test acc: 0.2795\n",
      "loss: 1.8044705390930176, train acc: 0.299\n",
      "loss: 1.7963532567024232, train acc: 0.3005\n",
      "loss: 1.8145260453224181, train acc: 0.3021\n",
      "loss: 1.8136047840118408, train acc: 0.3003\n",
      "loss: 1.8005761742591857, train acc: 0.2985\n",
      "loss: 1.7956957936286926, train acc: 0.2967\n",
      "loss: 1.846995437145233, train acc: 0.2983\n",
      "loss: 1.831563127040863, train acc: 0.2992\n",
      "epoch: 69, loss: 1.5856523513793945, train acc: 0.2992, test acc: 0.2828\n",
      "loss: 1.8703755140304565, train acc: 0.3012\n",
      "loss: 1.8225709080696106, train acc: 0.3008\n",
      "loss: 1.8301317691802979, train acc: 0.3021\n",
      "loss: 1.8128279805183412, train acc: 0.3008\n",
      "loss: 1.8142762422561645, train acc: 0.299\n",
      "loss: 1.7870641350746155, train acc: 0.2979\n",
      "loss: 1.808770775794983, train acc: 0.299\n",
      "loss: 1.8414014101028442, train acc: 0.3\n",
      "epoch: 70, loss: 1.5849334001541138, train acc: 0.3, test acc: 0.2822\n",
      "loss: 1.811190128326416, train acc: 0.3013\n",
      "loss: 1.7940930247306823, train acc: 0.3025\n",
      "loss: 1.8020875573158264, train acc: 0.3024\n",
      "loss: 1.8023780703544616, train acc: 0.2998\n",
      "loss: 1.8501817226409911, train acc: 0.2997\n",
      "loss: 1.7864031553268434, train acc: 0.2969\n",
      "loss: 1.810216224193573, train acc: 0.2998\n",
      "loss: 1.820955014228821, train acc: 0.3\n",
      "epoch: 71, loss: 1.5844810009002686, train acc: 0.3, test acc: 0.2827\n",
      "loss: 1.8611782789230347, train acc: 0.3021\n",
      "loss: 1.8026393532752991, train acc: 0.3022\n",
      "loss: 1.826828908920288, train acc: 0.3016\n",
      "loss: 1.796249258518219, train acc: 0.3008\n",
      "loss: 1.8136426329612731, train acc: 0.2999\n",
      "loss: 1.8138066530227661, train acc: 0.2984\n",
      "loss: 1.804944598674774, train acc: 0.299\n",
      "loss: 1.826520526409149, train acc: 0.2981\n",
      "epoch: 72, loss: 1.5833117961883545, train acc: 0.2981, test acc: 0.2803\n",
      "loss: 1.8419160842895508, train acc: 0.3015\n",
      "loss: 1.8131513237953185, train acc: 0.3017\n",
      "loss: 1.8487585544586183, train acc: 0.3015\n",
      "loss: 1.7994804382324219, train acc: 0.3006\n",
      "loss: 1.8322344422340393, train acc: 0.2995\n",
      "loss: 1.8326095700263978, train acc: 0.2983\n",
      "loss: 1.7964144825935364, train acc: 0.2979\n",
      "loss: 1.805580985546112, train acc: 0.2989\n",
      "epoch: 73, loss: 1.5822465419769287, train acc: 0.2989, test acc: 0.2816\n",
      "loss: 1.8621128797531128, train acc: 0.3012\n",
      "loss: 1.8003338694572448, train acc: 0.3021\n",
      "loss: 1.8024581670761108, train acc: 0.3015\n",
      "loss: 1.7655997395515441, train acc: 0.3014\n",
      "loss: 1.839420485496521, train acc: 0.2995\n",
      "loss: 1.8099183559417724, train acc: 0.2999\n",
      "loss: 1.8052484750747682, train acc: 0.3007\n",
      "loss: 1.798768651485443, train acc: 0.3013\n",
      "epoch: 74, loss: 1.5822023153305054, train acc: 0.3013, test acc: 0.2831\n",
      "loss: 1.895556926727295, train acc: 0.3018\n",
      "loss: 1.8512750267982483, train acc: 0.3014\n",
      "loss: 1.8077758431434632, train acc: 0.3015\n",
      "loss: 1.8038208127021789, train acc: 0.3008\n",
      "loss: 1.801554763317108, train acc: 0.3001\n",
      "loss: 1.7983897566795348, train acc: 0.2998\n",
      "loss: 1.8232163310050964, train acc: 0.2989\n",
      "loss: 1.8010316729545592, train acc: 0.3008\n",
      "epoch: 75, loss: 2.1045470237731934, train acc: 0.3008, test acc: 0.2815\n",
      "loss: 1.8705216646194458, train acc: 0.3018\n",
      "loss: 1.8150075554847718, train acc: 0.304\n",
      "loss: 1.8000627517700196, train acc: 0.3025\n",
      "loss: 1.780646288394928, train acc: 0.3011\n",
      "loss: 1.8220415234565734, train acc: 0.3006\n",
      "loss: 1.7912413358688355, train acc: 0.2984\n",
      "loss: 1.812695026397705, train acc: 0.2995\n",
      "loss: 1.8241915822029113, train acc: 0.2999\n",
      "epoch: 76, loss: 1.5796902179718018, train acc: 0.2999, test acc: 0.2823\n",
      "loss: 1.8510656356811523, train acc: 0.3011\n",
      "loss: 1.8216254949569701, train acc: 0.3035\n",
      "loss: 1.8340579509735107, train acc: 0.3032\n",
      "loss: 1.767659330368042, train acc: 0.3019\n",
      "loss: 1.827901554107666, train acc: 0.3022\n",
      "loss: 1.8192777514457703, train acc: 0.2988\n",
      "loss: 1.8249891996383667, train acc: 0.3003\n",
      "loss: 1.8406125903129578, train acc: 0.301\n",
      "epoch: 77, loss: 1.5780243873596191, train acc: 0.301, test acc: 0.2815\n",
      "loss: 1.911004900932312, train acc: 0.302\n",
      "loss: 1.7982037901878356, train acc: 0.3037\n",
      "loss: 1.8187227368354797, train acc: 0.3016\n",
      "loss: 1.764195704460144, train acc: 0.3007\n",
      "loss: 1.824766755104065, train acc: 0.3007\n",
      "loss: 1.8045441031455993, train acc: 0.2992\n",
      "loss: 1.8137189269065856, train acc: 0.2998\n",
      "loss: 1.8149100184440612, train acc: 0.2991\n",
      "epoch: 78, loss: 1.6066794395446777, train acc: 0.2991, test acc: 0.281\n",
      "loss: 1.8085193634033203, train acc: 0.3012\n",
      "loss: 1.8287234902381897, train acc: 0.3011\n",
      "loss: 1.81336829662323, train acc: 0.3011\n",
      "loss: 1.7826145768165589, train acc: 0.3011\n",
      "loss: 1.8266574382781982, train acc: 0.3\n",
      "loss: 1.7671815991401671, train acc: 0.2988\n",
      "loss: 1.812060546875, train acc: 0.3008\n",
      "loss: 1.8041044235229493, train acc: 0.3003\n",
      "epoch: 79, loss: 1.9112579822540283, train acc: 0.3003, test acc: 0.2821\n",
      "#####training and testing end with K:1, P:0.1######\n",
      "#####training and testing start with K:1, P:0.5######\n",
      "loss: 2.5010151863098145, train acc: 0.1\n",
      "loss: 2.3712489366531373, train acc: 0.1749\n",
      "loss: 2.363807129859924, train acc: 0.1822\n",
      "loss: 2.3483190059661867, train acc: 0.1852\n",
      "loss: 2.324506974220276, train acc: 0.1833\n",
      "loss: 2.3182995557785033, train acc: 0.188\n",
      "loss: 2.287518358230591, train acc: 0.1894\n",
      "loss: 2.2910162925720217, train acc: 0.1906\n",
      "epoch: 0, loss: 2.1390159130096436, train acc: 0.1906, test acc: 0.1914\n",
      "loss: 2.3682873249053955, train acc: 0.1899\n",
      "loss: 2.2689926624298096, train acc: 0.1903\n",
      "loss: 2.2878534317016603, train acc: 0.1906\n",
      "loss: 2.25069055557251, train acc: 0.1921\n",
      "loss: 2.2725024461746215, train acc: 0.1924\n",
      "loss: 2.282045078277588, train acc: 0.1913\n",
      "loss: 2.2415231227874757, train acc: 0.1925\n",
      "loss: 2.249539041519165, train acc: 0.1922\n",
      "epoch: 1, loss: 1.9738476276397705, train acc: 0.1922, test acc: 0.1931\n",
      "loss: 2.299193859100342, train acc: 0.1912\n",
      "loss: 2.214432406425476, train acc: 0.1924\n",
      "loss: 2.258544087409973, train acc: 0.1925\n",
      "loss: 2.2685808658599855, train acc: 0.194\n",
      "loss: 2.2381200075149534, train acc: 0.1964\n",
      "loss: 2.2706942796707152, train acc: 0.1987\n",
      "loss: 2.2256385564804075, train acc: 0.2023\n",
      "loss: 2.2467709302902223, train acc: 0.2034\n",
      "epoch: 2, loss: 2.1997430324554443, train acc: 0.2034, test acc: 0.21\n",
      "loss: 2.3390145301818848, train acc: 0.2073\n",
      "loss: 2.248260736465454, train acc: 0.21\n",
      "loss: 2.2311520099639894, train acc: 0.2146\n",
      "loss: 2.2231835603713987, train acc: 0.2187\n",
      "loss: 2.230474281311035, train acc: 0.2218\n",
      "loss: 2.2196987867355347, train acc: 0.224\n",
      "loss: 2.210084843635559, train acc: 0.2252\n",
      "loss: 2.21004114151001, train acc: 0.2184\n",
      "epoch: 3, loss: 2.112529993057251, train acc: 0.2184, test acc: 0.2199\n",
      "loss: 2.3465657234191895, train acc: 0.2191\n",
      "loss: 2.2074214220046997, train acc: 0.2244\n",
      "loss: 2.223616862297058, train acc: 0.2285\n",
      "loss: 2.201189708709717, train acc: 0.2343\n",
      "loss: 2.236392450332642, train acc: 0.2353\n",
      "loss: 2.2220701456069945, train acc: 0.2349\n",
      "loss: 2.1888381004333497, train acc: 0.2291\n",
      "loss: 2.1809012651443482, train acc: 0.238\n",
      "epoch: 4, loss: 1.96309494972229, train acc: 0.238, test acc: 0.2364\n",
      "loss: 2.232905387878418, train acc: 0.2419\n",
      "loss: 2.200281023979187, train acc: 0.2468\n",
      "loss: 2.2018818140029905, train acc: 0.2486\n",
      "loss: 2.2083973407745363, train acc: 0.2501\n",
      "loss: 2.2092525482177736, train acc: 0.25\n",
      "loss: 2.1894702196121214, train acc: 0.2502\n",
      "loss: 2.1477835416793822, train acc: 0.245\n",
      "loss: 2.1785977125167846, train acc: 0.2519\n",
      "epoch: 5, loss: 2.124675989151001, train acc: 0.2519, test acc: 0.2417\n",
      "loss: 2.2431888580322266, train acc: 0.2506\n",
      "loss: 2.1691147804260256, train acc: 0.2593\n",
      "loss: 2.187479758262634, train acc: 0.2581\n",
      "loss: 2.1812088966369627, train acc: 0.2584\n",
      "loss: 2.1942569494247435, train acc: 0.2599\n",
      "loss: 2.1694393396377563, train acc: 0.2611\n",
      "loss: 2.139973258972168, train acc: 0.2562\n",
      "loss: 2.1648653030395506, train acc: 0.2586\n",
      "epoch: 6, loss: 2.2155301570892334, train acc: 0.2586, test acc: 0.2497\n",
      "loss: 2.1880736351013184, train acc: 0.2615\n",
      "loss: 2.145594048500061, train acc: 0.2651\n",
      "loss: 2.157792901992798, train acc: 0.2662\n",
      "loss: 2.1483355283737184, train acc: 0.2607\n",
      "loss: 2.1675191640853884, train acc: 0.2612\n",
      "loss: 2.15443868637085, train acc: 0.2557\n",
      "loss: 2.1374302387237547, train acc: 0.2525\n",
      "loss: 2.1773345470428467, train acc: 0.2488\n",
      "epoch: 7, loss: 1.9780011177062988, train acc: 0.2488, test acc: 0.2408\n",
      "loss: 2.198042392730713, train acc: 0.2523\n",
      "loss: 2.133739185333252, train acc: 0.2537\n",
      "loss: 2.176922583580017, train acc: 0.2546\n",
      "loss: 2.1794032573699953, train acc: 0.2488\n",
      "loss: 2.1720399618148805, train acc: 0.2507\n",
      "loss: 2.1625013828277586, train acc: 0.2483\n",
      "loss: 2.159492087364197, train acc: 0.2492\n",
      "loss: 2.146931862831116, train acc: 0.2479\n",
      "epoch: 8, loss: 1.8882455825805664, train acc: 0.2479, test acc: 0.2411\n",
      "loss: 2.1911933422088623, train acc: 0.2541\n",
      "loss: 2.150234079360962, train acc: 0.2552\n",
      "loss: 2.1522001028060913, train acc: 0.2551\n",
      "loss: 2.1404312372207643, train acc: 0.2539\n",
      "loss: 2.1602338790893554, train acc: 0.2541\n",
      "loss: 2.155849027633667, train acc: 0.2521\n",
      "loss: 2.144505262374878, train acc: 0.2542\n",
      "loss: 2.1681896686553954, train acc: 0.2505\n",
      "epoch: 9, loss: 2.0230062007904053, train acc: 0.2505, test acc: 0.242\n",
      "loss: 2.1372201442718506, train acc: 0.2533\n",
      "loss: 2.1402132749557494, train acc: 0.2541\n",
      "loss: 2.147054433822632, train acc: 0.2536\n",
      "loss: 2.127179741859436, train acc: 0.2532\n",
      "loss: 2.1543055057525633, train acc: 0.2532\n",
      "loss: 2.1721593856811525, train acc: 0.2535\n",
      "loss: 2.1340158700942995, train acc: 0.2521\n",
      "loss: 2.1383626222610475, train acc: 0.2499\n",
      "epoch: 10, loss: 1.8795013427734375, train acc: 0.2499, test acc: 0.2405\n",
      "loss: 2.150939702987671, train acc: 0.256\n",
      "loss: 2.156097912788391, train acc: 0.251\n",
      "loss: 2.1103750467300415, train acc: 0.2543\n",
      "loss: 2.1387797236442565, train acc: 0.253\n",
      "loss: 2.1499584197998045, train acc: 0.252\n",
      "loss: 2.142102599143982, train acc: 0.2507\n",
      "loss: 2.1202689170837403, train acc: 0.2515\n",
      "loss: 2.160545539855957, train acc: 0.2515\n",
      "epoch: 11, loss: 2.0252037048339844, train acc: 0.2515, test acc: 0.2391\n",
      "loss: 2.220980167388916, train acc: 0.2542\n",
      "loss: 2.1288989543914796, train acc: 0.2563\n",
      "loss: 2.134743094444275, train acc: 0.2537\n",
      "loss: 2.135733437538147, train acc: 0.2519\n",
      "loss: 2.1386110305786135, train acc: 0.2528\n",
      "loss: 2.1424400091171263, train acc: 0.2522\n",
      "loss: 2.1369787216186524, train acc: 0.2495\n",
      "loss: 2.1318698644638063, train acc: 0.2505\n",
      "epoch: 12, loss: 2.0259249210357666, train acc: 0.2505, test acc: 0.2396\n",
      "loss: 2.174793243408203, train acc: 0.2537\n",
      "loss: 2.130364513397217, train acc: 0.2566\n",
      "loss: 2.1353104591369627, train acc: 0.2554\n",
      "loss: 2.134672689437866, train acc: 0.2535\n",
      "loss: 2.1464003801345823, train acc: 0.2548\n",
      "loss: 2.1429680585861206, train acc: 0.2531\n",
      "loss: 2.1334665298461912, train acc: 0.2523\n",
      "loss: 2.127718114852905, train acc: 0.2535\n",
      "epoch: 13, loss: 2.1002955436706543, train acc: 0.2535, test acc: 0.2404\n",
      "loss: 2.188917875289917, train acc: 0.2534\n",
      "loss: 2.1328622341156005, train acc: 0.2548\n",
      "loss: 2.1168325424194334, train acc: 0.2536\n",
      "loss: 2.1419016122817993, train acc: 0.2529\n",
      "loss: 2.124259901046753, train acc: 0.2538\n",
      "loss: 2.138891339302063, train acc: 0.2546\n",
      "loss: 2.1314417362213134, train acc: 0.2553\n",
      "loss: 2.12090060710907, train acc: 0.2553\n",
      "epoch: 14, loss: 2.1777350902557373, train acc: 0.2553, test acc: 0.2411\n",
      "loss: 2.151055335998535, train acc: 0.2557\n",
      "loss: 2.115670013427734, train acc: 0.2561\n",
      "loss: 2.117404270172119, train acc: 0.2544\n",
      "loss: 2.112405252456665, train acc: 0.2541\n",
      "loss: 2.1417587280273436, train acc: 0.2525\n",
      "loss: 2.150631856918335, train acc: 0.2512\n",
      "loss: 2.120695924758911, train acc: 0.2533\n",
      "loss: 2.12822425365448, train acc: 0.2522\n",
      "epoch: 15, loss: 2.033787965774536, train acc: 0.2522, test acc: 0.2396\n",
      "loss: 2.169193744659424, train acc: 0.2559\n",
      "loss: 2.1413207769393923, train acc: 0.2568\n",
      "loss: 2.14214506149292, train acc: 0.254\n",
      "loss: 2.1267616748809814, train acc: 0.2547\n",
      "loss: 2.143937420845032, train acc: 0.2529\n",
      "loss: 2.129968190193176, train acc: 0.2541\n",
      "loss: 2.1032903671264647, train acc: 0.2516\n",
      "loss: 2.1180434703826903, train acc: 0.2527\n",
      "epoch: 16, loss: 2.1060941219329834, train acc: 0.2527, test acc: 0.2431\n",
      "loss: 2.2313477993011475, train acc: 0.2571\n",
      "loss: 2.099178373813629, train acc: 0.2572\n",
      "loss: 2.138826012611389, train acc: 0.2519\n",
      "loss: 2.1302669763565065, train acc: 0.2493\n",
      "loss: 2.130077934265137, train acc: 0.2498\n",
      "loss: 2.1174436807632446, train acc: 0.2525\n",
      "loss: 2.1052032709121704, train acc: 0.2531\n",
      "loss: 2.1289709329605104, train acc: 0.2546\n",
      "epoch: 17, loss: 2.106804609298706, train acc: 0.2546, test acc: 0.2391\n",
      "loss: 2.1220388412475586, train acc: 0.2581\n",
      "loss: 2.1281838178634644, train acc: 0.2548\n",
      "loss: 2.129686188697815, train acc: 0.2604\n",
      "loss: 2.138171744346619, train acc: 0.2575\n",
      "loss: 2.141701316833496, train acc: 0.2595\n",
      "loss: 2.1507248640060426, train acc: 0.2612\n",
      "loss: 2.126055920124054, train acc: 0.2603\n",
      "loss: 2.1513895750045777, train acc: 0.2548\n",
      "epoch: 18, loss: 2.118083953857422, train acc: 0.2548, test acc: 0.2454\n",
      "loss: 2.121713161468506, train acc: 0.2631\n",
      "loss: 2.109569787979126, train acc: 0.2616\n",
      "loss: 2.1126973390579225, train acc: 0.2633\n",
      "loss: 2.1206100463867186, train acc: 0.2629\n",
      "loss: 2.11772038936615, train acc: 0.2621\n",
      "loss: 2.130090284347534, train acc: 0.2607\n",
      "loss: 2.1016469478607176, train acc: 0.2618\n",
      "loss: 2.113989782333374, train acc: 0.2577\n",
      "epoch: 19, loss: 2.042632818222046, train acc: 0.2577, test acc: 0.2441\n",
      "loss: 2.1785528659820557, train acc: 0.2631\n",
      "loss: 2.101584720611572, train acc: 0.2652\n",
      "loss: 2.125168466567993, train acc: 0.2642\n",
      "loss: 2.1360998153686523, train acc: 0.2659\n",
      "loss: 2.109739303588867, train acc: 0.2614\n",
      "loss: 2.1397864818573, train acc: 0.2609\n",
      "loss: 2.1210124492645264, train acc: 0.2646\n",
      "loss: 2.1223718404769896, train acc: 0.2645\n",
      "epoch: 20, loss: 1.9568270444869995, train acc: 0.2645, test acc: 0.2448\n",
      "loss: 2.1469366550445557, train acc: 0.265\n",
      "loss: 2.124878454208374, train acc: 0.2672\n",
      "loss: 2.1244534969329836, train acc: 0.268\n",
      "loss: 2.1141191840171816, train acc: 0.2657\n",
      "loss: 2.1301556825637817, train acc: 0.2674\n",
      "loss: 2.146292209625244, train acc: 0.2723\n",
      "loss: 2.095361924171448, train acc: 0.2718\n",
      "loss: 2.1194152355194094, train acc: 0.2717\n",
      "epoch: 21, loss: 2.1945033073425293, train acc: 0.2717, test acc: 0.2509\n",
      "loss: 2.0550949573516846, train acc: 0.2726\n",
      "loss: 2.0902171611785887, train acc: 0.2734\n",
      "loss: 2.1295895338058473, train acc: 0.2689\n",
      "loss: 2.1212599515914916, train acc: 0.2724\n",
      "loss: 2.146971249580383, train acc: 0.2728\n",
      "loss: 2.1196756601333617, train acc: 0.2725\n",
      "loss: 2.11536180973053, train acc: 0.2742\n",
      "loss: 2.130765748023987, train acc: 0.2685\n",
      "epoch: 22, loss: 2.1182806491851807, train acc: 0.2685, test acc: 0.2559\n",
      "loss: 2.189871311187744, train acc: 0.2754\n",
      "loss: 2.113635444641113, train acc: 0.273\n",
      "loss: 2.1206086397171022, train acc: 0.27\n",
      "loss: 2.126360464096069, train acc: 0.2703\n",
      "loss: 2.079223322868347, train acc: 0.2766\n",
      "loss: 2.1221778631210326, train acc: 0.2743\n",
      "loss: 2.1460830211639403, train acc: 0.273\n",
      "loss: 2.123796010017395, train acc: 0.2714\n",
      "epoch: 23, loss: 1.9632338285446167, train acc: 0.2714, test acc: 0.2556\n",
      "loss: 2.1324729919433594, train acc: 0.276\n",
      "loss: 2.1183367490768434, train acc: 0.2763\n",
      "loss: 2.1064526796340943, train acc: 0.2769\n",
      "loss: 2.153273320198059, train acc: 0.279\n",
      "loss: 2.1178188562393188, train acc: 0.2779\n",
      "loss: 2.107740306854248, train acc: 0.2768\n",
      "loss: 2.114932656288147, train acc: 0.275\n",
      "loss: 2.1198524713516234, train acc: 0.2769\n",
      "epoch: 24, loss: 2.1254961490631104, train acc: 0.2769, test acc: 0.258\n",
      "loss: 2.111665964126587, train acc: 0.2811\n",
      "loss: 2.103824257850647, train acc: 0.2787\n",
      "loss: 2.1314792156219484, train acc: 0.2793\n",
      "loss: 2.125672793388367, train acc: 0.2811\n",
      "loss: 2.120633268356323, train acc: 0.2812\n",
      "loss: 2.121948170661926, train acc: 0.2822\n",
      "loss: 2.1076508522033692, train acc: 0.2792\n",
      "loss: 2.1225267171859743, train acc: 0.2815\n",
      "epoch: 25, loss: 1.9618862867355347, train acc: 0.2815, test acc: 0.2608\n",
      "loss: 2.302018642425537, train acc: 0.2777\n",
      "loss: 2.1247450590133665, train acc: 0.2838\n",
      "loss: 2.1031132698059083, train acc: 0.2801\n",
      "loss: 2.120001935958862, train acc: 0.2837\n",
      "loss: 2.1350767612457275, train acc: 0.2811\n",
      "loss: 2.1150883197784425, train acc: 0.2803\n",
      "loss: 2.117862343788147, train acc: 0.2855\n",
      "loss: 2.113316059112549, train acc: 0.2801\n",
      "epoch: 26, loss: 2.042285203933716, train acc: 0.2801, test acc: 0.264\n",
      "loss: 2.1106932163238525, train acc: 0.2859\n",
      "loss: 2.109408235549927, train acc: 0.2877\n",
      "loss: 2.123159956932068, train acc: 0.2831\n",
      "loss: 2.099519443511963, train acc: 0.2837\n",
      "loss: 2.1064879655838014, train acc: 0.2793\n",
      "loss: 2.136065363883972, train acc: 0.2868\n",
      "loss: 2.1265334606170656, train acc: 0.2819\n",
      "loss: 2.119187569618225, train acc: 0.2837\n",
      "epoch: 27, loss: 2.0468034744262695, train acc: 0.2837, test acc: 0.2672\n",
      "loss: 2.183960437774658, train acc: 0.2846\n",
      "loss: 2.128778672218323, train acc: 0.2851\n",
      "loss: 2.1038141012191773, train acc: 0.2861\n",
      "loss: 2.106434369087219, train acc: 0.2828\n",
      "loss: 2.1004860401153564, train acc: 0.2818\n",
      "loss: 2.123657155036926, train acc: 0.2817\n",
      "loss: 2.1162484169006346, train acc: 0.2868\n",
      "loss: 2.1214149713516237, train acc: 0.2805\n",
      "epoch: 28, loss: 1.963147759437561, train acc: 0.2805, test acc: 0.2672\n",
      "loss: 2.1940557956695557, train acc: 0.2881\n",
      "loss: 2.1099069595336912, train acc: 0.2868\n",
      "loss: 2.0978534698486326, train acc: 0.2788\n",
      "loss: 2.100585734844208, train acc: 0.2791\n",
      "loss: 2.13209810256958, train acc: 0.2829\n",
      "loss: 2.1282240390777587, train acc: 0.2874\n",
      "loss: 2.0930561065673827, train acc: 0.2889\n",
      "loss: 2.08165180683136, train acc: 0.2892\n",
      "epoch: 29, loss: 2.1985857486724854, train acc: 0.2892, test acc: 0.2693\n",
      "loss: 2.1783909797668457, train acc: 0.2865\n",
      "loss: 2.12745623588562, train acc: 0.286\n",
      "loss: 2.1183958053588867, train acc: 0.2832\n",
      "loss: 2.102191412448883, train acc: 0.2803\n",
      "loss: 2.1303833961486816, train acc: 0.282\n",
      "loss: 2.1166275262832643, train acc: 0.2818\n",
      "loss: 2.109704780578613, train acc: 0.2856\n",
      "loss: 2.112881827354431, train acc: 0.2865\n",
      "epoch: 30, loss: 1.9672682285308838, train acc: 0.2865, test acc: 0.2656\n",
      "loss: 2.1981875896453857, train acc: 0.2857\n",
      "loss: 2.1306631088256838, train acc: 0.2796\n",
      "loss: 2.096305322647095, train acc: 0.2816\n",
      "loss: 2.1073426723480226, train acc: 0.278\n",
      "loss: 2.1005676984786987, train acc: 0.2789\n",
      "loss: 2.1104357957839968, train acc: 0.289\n",
      "loss: 2.1217683792114257, train acc: 0.2808\n",
      "loss: 2.1106529951095583, train acc: 0.2877\n",
      "epoch: 31, loss: 1.9637266397476196, train acc: 0.2877, test acc: 0.2743\n",
      "loss: 2.215848684310913, train acc: 0.2878\n",
      "loss: 2.132308006286621, train acc: 0.284\n",
      "loss: 2.110852766036987, train acc: 0.2863\n",
      "loss: 2.0740827322006226, train acc: 0.2844\n",
      "loss: 2.117606353759766, train acc: 0.2867\n",
      "loss: 2.124040937423706, train acc: 0.2888\n",
      "loss: 2.1181919813156127, train acc: 0.2867\n",
      "loss: 2.112401580810547, train acc: 0.2831\n",
      "epoch: 32, loss: 2.1196441650390625, train acc: 0.2831, test acc: 0.2736\n",
      "loss: 2.104661226272583, train acc: 0.2895\n",
      "loss: 2.12447190284729, train acc: 0.2881\n",
      "loss: 2.0853044629096984, train acc: 0.2848\n",
      "loss: 2.1043789386749268, train acc: 0.2882\n",
      "loss: 2.1158470630645754, train acc: 0.2856\n",
      "loss: 2.1074487447738646, train acc: 0.2847\n",
      "loss: 2.111606168746948, train acc: 0.2855\n",
      "loss: 2.098969602584839, train acc: 0.2826\n",
      "epoch: 33, loss: 2.054532766342163, train acc: 0.2826, test acc: 0.2687\n",
      "loss: 2.136636257171631, train acc: 0.2858\n",
      "loss: 2.114092469215393, train acc: 0.2874\n",
      "loss: 2.132115340232849, train acc: 0.2832\n",
      "loss: 2.0984790086746217, train acc: 0.2845\n",
      "loss: 2.1375685691833497, train acc: 0.284\n",
      "loss: 2.103473401069641, train acc: 0.2883\n",
      "loss: 2.109289455413818, train acc: 0.2833\n",
      "loss: 2.1172980308532714, train acc: 0.2835\n",
      "epoch: 34, loss: 1.972243070602417, train acc: 0.2835, test acc: 0.263\n",
      "loss: 2.131958484649658, train acc: 0.2866\n",
      "loss: 2.1282227754592897, train acc: 0.2831\n",
      "loss: 2.098114013671875, train acc: 0.2847\n",
      "loss: 2.122086763381958, train acc: 0.2801\n",
      "loss: 2.1091753244400024, train acc: 0.2882\n",
      "loss: 2.1058024168014526, train acc: 0.2914\n",
      "loss: 2.121847891807556, train acc: 0.2893\n",
      "loss: 2.1146785020828247, train acc: 0.2887\n",
      "epoch: 35, loss: 2.1143312454223633, train acc: 0.2887, test acc: 0.2619\n",
      "loss: 2.1708874702453613, train acc: 0.2858\n",
      "loss: 2.1145826101303102, train acc: 0.2896\n",
      "loss: 2.0895998001098635, train acc: 0.2889\n",
      "loss: 2.1021730422973635, train acc: 0.2872\n",
      "loss: 2.129819560050964, train acc: 0.2889\n",
      "loss: 2.116755509376526, train acc: 0.2874\n",
      "loss: 2.133974528312683, train acc: 0.2741\n",
      "loss: 2.134609007835388, train acc: 0.286\n",
      "epoch: 36, loss: 2.0397891998291016, train acc: 0.286, test acc: 0.2575\n",
      "loss: 2.1882717609405518, train acc: 0.2806\n",
      "loss: 2.0948931217193603, train acc: 0.2876\n",
      "loss: 2.0888858318328856, train acc: 0.2847\n",
      "loss: 2.1329843759536744, train acc: 0.2873\n",
      "loss: 2.1059843063354493, train acc: 0.2881\n",
      "loss: 2.1148835897445677, train acc: 0.2899\n",
      "loss: 2.1075775146484377, train acc: 0.2875\n",
      "loss: 2.070683538913727, train acc: 0.2858\n",
      "epoch: 37, loss: 2.042534351348877, train acc: 0.2858, test acc: 0.2574\n",
      "loss: 2.1296637058258057, train acc: 0.2839\n",
      "loss: 2.1148730993270872, train acc: 0.2891\n",
      "loss: 2.0977671146392822, train acc: 0.2871\n",
      "loss: 2.1372365951538086, train acc: 0.2872\n",
      "loss: 2.1149597644805906, train acc: 0.2853\n",
      "loss: 2.11427047252655, train acc: 0.2859\n",
      "loss: 2.0855735540390015, train acc: 0.2853\n",
      "loss: 2.1276394367218017, train acc: 0.2837\n",
      "epoch: 38, loss: 2.1089422702789307, train acc: 0.2837, test acc: 0.262\n",
      "loss: 2.1549530029296875, train acc: 0.2855\n",
      "loss: 2.122843933105469, train acc: 0.2892\n",
      "loss: 2.1086224794387816, train acc: 0.2856\n",
      "loss: 2.092721462249756, train acc: 0.2861\n",
      "loss: 2.107301139831543, train acc: 0.2864\n",
      "loss: 2.0843268632888794, train acc: 0.2873\n",
      "loss: 2.0729188919067383, train acc: 0.2872\n",
      "loss: 2.113588261604309, train acc: 0.2807\n",
      "epoch: 39, loss: 2.2087290287017822, train acc: 0.2807, test acc: 0.2649\n",
      "loss: 2.149763822555542, train acc: 0.289\n",
      "loss: 2.124897336959839, train acc: 0.2844\n",
      "loss: 2.1089992761611938, train acc: 0.2864\n",
      "loss: 2.1207210779190064, train acc: 0.2875\n",
      "loss: 2.1080328702926634, train acc: 0.2857\n",
      "loss: 2.1193152189254763, train acc: 0.2876\n",
      "loss: 2.0996458768844604, train acc: 0.2893\n",
      "loss: 2.1446842193603515, train acc: 0.2888\n",
      "epoch: 40, loss: 2.0397934913635254, train acc: 0.2888, test acc: 0.2674\n",
      "loss: 2.235020399093628, train acc: 0.289\n",
      "loss: 2.1225826263427736, train acc: 0.2795\n",
      "loss: 2.095883345603943, train acc: 0.2796\n",
      "loss: 2.117823934555054, train acc: 0.2826\n",
      "loss: 2.0998729467391968, train acc: 0.2868\n",
      "loss: 2.1158599734306334, train acc: 0.2888\n",
      "loss: 2.104867148399353, train acc: 0.2881\n",
      "loss: 2.1166364669799806, train acc: 0.286\n",
      "epoch: 41, loss: 2.18975567817688, train acc: 0.286, test acc: 0.2644\n",
      "loss: 2.0945749282836914, train acc: 0.2872\n",
      "loss: 2.0973016023635864, train acc: 0.2861\n",
      "loss: 2.1028907537460326, train acc: 0.2855\n",
      "loss: 2.089066505432129, train acc: 0.2889\n",
      "loss: 2.11484158039093, train acc: 0.285\n",
      "loss: 2.127280902862549, train acc: 0.2855\n",
      "loss: 2.0756403684616087, train acc: 0.2834\n",
      "loss: 2.100487399101257, train acc: 0.2766\n",
      "epoch: 42, loss: 1.9497390985488892, train acc: 0.2766, test acc: 0.2638\n",
      "loss: 2.133924722671509, train acc: 0.2882\n",
      "loss: 2.1169180631637574, train acc: 0.2853\n",
      "loss: 2.098318099975586, train acc: 0.2868\n",
      "loss: 2.101681709289551, train acc: 0.2864\n",
      "loss: 2.1252372026443482, train acc: 0.2861\n",
      "loss: 2.11018226146698, train acc: 0.2879\n",
      "loss: 2.109049069881439, train acc: 0.2889\n",
      "loss: 2.1155250549316404, train acc: 0.2863\n",
      "epoch: 43, loss: 1.9729623794555664, train acc: 0.2863, test acc: 0.2683\n",
      "loss: 2.127854585647583, train acc: 0.2859\n",
      "loss: 2.1026464223861696, train acc: 0.2887\n",
      "loss: 2.1159929990768434, train acc: 0.2729\n",
      "loss: 2.113087010383606, train acc: 0.2753\n",
      "loss: 2.1004570722579956, train acc: 0.2777\n",
      "loss: 2.116660785675049, train acc: 0.285\n",
      "loss: 2.0940145015716554, train acc: 0.2866\n",
      "loss: 2.1039844036102293, train acc: 0.2886\n",
      "epoch: 44, loss: 2.1182076930999756, train acc: 0.2886, test acc: 0.2704\n",
      "loss: 2.1683008670806885, train acc: 0.293\n",
      "loss: 2.1187880516052244, train acc: 0.2813\n",
      "loss: 2.1198458433151246, train acc: 0.2823\n",
      "loss: 2.096489465236664, train acc: 0.289\n",
      "loss: 2.1127935647964478, train acc: 0.2853\n",
      "loss: 2.100027298927307, train acc: 0.2862\n",
      "loss: 2.125169205665588, train acc: 0.2872\n",
      "loss: 2.0872539043426515, train acc: 0.284\n",
      "epoch: 45, loss: 2.209028482437134, train acc: 0.284, test acc: 0.2651\n",
      "loss: 2.176785945892334, train acc: 0.2897\n",
      "loss: 2.1251309156417846, train acc: 0.2882\n",
      "loss: 2.12472403049469, train acc: 0.2879\n",
      "loss: 2.085301995277405, train acc: 0.2888\n",
      "loss: 2.1011608839035034, train acc: 0.2886\n",
      "loss: 2.091812324523926, train acc: 0.2826\n",
      "loss: 2.0916484355926515, train acc: 0.2866\n",
      "loss: 2.0978134870529175, train acc: 0.2755\n",
      "epoch: 46, loss: 2.18998384475708, train acc: 0.2755, test acc: 0.2597\n",
      "loss: 2.027587890625, train acc: 0.2783\n",
      "loss: 2.1109405279159548, train acc: 0.2867\n",
      "loss: 2.1049163579940795, train acc: 0.2717\n",
      "loss: 2.088746726512909, train acc: 0.2793\n",
      "loss: 2.116925668716431, train acc: 0.2806\n",
      "loss: 2.1187856674194334, train acc: 0.2834\n",
      "loss: 2.0950548887252807, train acc: 0.279\n",
      "loss: 2.0930330991744994, train acc: 0.2773\n",
      "epoch: 47, loss: 2.111006498336792, train acc: 0.2773, test acc: 0.2684\n",
      "loss: 2.144418954849243, train acc: 0.2865\n",
      "loss: 2.1419575214385986, train acc: 0.2789\n",
      "loss: 2.1125313520431517, train acc: 0.2745\n",
      "loss: 2.11677565574646, train acc: 0.2777\n",
      "loss: 2.1127658128738402, train acc: 0.2826\n",
      "loss: 2.1173139095306395, train acc: 0.2858\n",
      "loss: 2.115969514846802, train acc: 0.2855\n",
      "loss: 2.122360849380493, train acc: 0.2771\n",
      "epoch: 48, loss: 2.106412649154663, train acc: 0.2771, test acc: 0.2661\n",
      "loss: 2.105316638946533, train acc: 0.2889\n",
      "loss: 2.1132941246032715, train acc: 0.2858\n",
      "loss: 2.140313744544983, train acc: 0.2828\n",
      "loss: 2.0891244173049928, train acc: 0.2812\n",
      "loss: 2.112405467033386, train acc: 0.2832\n",
      "loss: 2.108580231666565, train acc: 0.2864\n",
      "loss: 2.106134819984436, train acc: 0.2857\n",
      "loss: 2.117628335952759, train acc: 0.2839\n",
      "epoch: 49, loss: 2.2040982246398926, train acc: 0.2839, test acc: 0.2759\n",
      "loss: 2.1359944343566895, train acc: 0.2858\n",
      "loss: 2.14298996925354, train acc: 0.2784\n",
      "loss: 2.1040738224983215, train acc: 0.2785\n",
      "loss: 2.108807182312012, train acc: 0.2775\n",
      "loss: 2.106385517120361, train acc: 0.2797\n",
      "loss: 2.09997980594635, train acc: 0.2856\n",
      "loss: 2.102664256095886, train acc: 0.2857\n",
      "loss: 2.114125895500183, train acc: 0.2765\n",
      "epoch: 50, loss: 1.9582372903823853, train acc: 0.2765, test acc: 0.2696\n",
      "loss: 2.2528464794158936, train acc: 0.2887\n",
      "loss: 2.137007474899292, train acc: 0.2797\n",
      "loss: 2.1038108348846434, train acc: 0.2816\n",
      "loss: 2.1256282329559326, train acc: 0.2831\n",
      "loss: 2.1220289707183837, train acc: 0.2851\n",
      "loss: 2.094040322303772, train acc: 0.2854\n",
      "loss: 2.10169095993042, train acc: 0.2893\n",
      "loss: 2.1230769872665407, train acc: 0.2786\n",
      "epoch: 51, loss: 2.1889567375183105, train acc: 0.2786, test acc: 0.2566\n",
      "loss: 2.128300905227661, train acc: 0.277\n",
      "loss: 2.1190646409988405, train acc: 0.291\n",
      "loss: 2.112581706047058, train acc: 0.2865\n",
      "loss: 2.112005591392517, train acc: 0.2854\n",
      "loss: 2.107127833366394, train acc: 0.2821\n",
      "loss: 2.0907695293426514, train acc: 0.2827\n",
      "loss: 2.09289608001709, train acc: 0.2803\n",
      "loss: 2.0813806533813475, train acc: 0.2797\n",
      "epoch: 52, loss: 2.0347397327423096, train acc: 0.2797, test acc: 0.2732\n",
      "loss: 2.115678071975708, train acc: 0.2866\n",
      "loss: 2.105243718624115, train acc: 0.2821\n",
      "loss: 2.094010257720947, train acc: 0.2843\n",
      "loss: 2.105627012252808, train acc: 0.2884\n",
      "loss: 2.10561888217926, train acc: 0.2855\n",
      "loss: 2.10658221244812, train acc: 0.2796\n",
      "loss: 2.1239348888397216, train acc: 0.2832\n",
      "loss: 2.0966667532920837, train acc: 0.285\n",
      "epoch: 53, loss: 2.2671279907226562, train acc: 0.285, test acc: 0.2537\n",
      "loss: 2.076608896255493, train acc: 0.2674\n",
      "loss: 2.1102477073669434, train acc: 0.2862\n",
      "loss: 2.1194953680038453, train acc: 0.2836\n",
      "loss: 2.1131467819213867, train acc: 0.2875\n",
      "loss: 2.1081193685531616, train acc: 0.2845\n",
      "loss: 2.1324673175811766, train acc: 0.2834\n",
      "loss: 2.112539052963257, train acc: 0.2856\n",
      "loss: 2.112544083595276, train acc: 0.2808\n",
      "epoch: 54, loss: 2.0489237308502197, train acc: 0.2808, test acc: 0.2686\n",
      "loss: 2.2156057357788086, train acc: 0.2889\n",
      "loss: 2.1106160640716554, train acc: 0.2832\n",
      "loss: 2.0952661752700807, train acc: 0.2833\n",
      "loss: 2.095250201225281, train acc: 0.2847\n",
      "loss: 2.102642369270325, train acc: 0.2884\n",
      "loss: 2.097103476524353, train acc: 0.2872\n",
      "loss: 2.138061785697937, train acc: 0.2856\n",
      "loss: 2.115998148918152, train acc: 0.2818\n",
      "epoch: 55, loss: 2.0416905879974365, train acc: 0.2818, test acc: 0.2639\n",
      "loss: 2.1031031608581543, train acc: 0.2823\n",
      "loss: 2.0951579093933104, train acc: 0.2819\n",
      "loss: 2.119697833061218, train acc: 0.2784\n",
      "loss: 2.077601361274719, train acc: 0.2835\n",
      "loss: 2.110317015647888, train acc: 0.2866\n",
      "loss: 2.1106833934783937, train acc: 0.2908\n",
      "loss: 2.111509108543396, train acc: 0.2911\n",
      "loss: 2.1135876178741455, train acc: 0.281\n",
      "epoch: 56, loss: 1.8927862644195557, train acc: 0.281, test acc: 0.2709\n",
      "loss: 2.1722352504730225, train acc: 0.2899\n",
      "loss: 2.1116360664367675, train acc: 0.2859\n",
      "loss: 2.1113246202468874, train acc: 0.2875\n",
      "loss: 2.0932711124420167, train acc: 0.2904\n",
      "loss: 2.103851580619812, train acc: 0.2913\n",
      "loss: 2.098656415939331, train acc: 0.2843\n",
      "loss: 2.129528832435608, train acc: 0.291\n",
      "loss: 2.090507388114929, train acc: 0.2756\n",
      "epoch: 57, loss: 2.125955581665039, train acc: 0.2756, test acc: 0.2698\n",
      "loss: 2.140092134475708, train acc: 0.2873\n",
      "loss: 2.106318950653076, train acc: 0.2752\n",
      "loss: 2.103847861289978, train acc: 0.2787\n",
      "loss: 2.10131950378418, train acc: 0.2799\n",
      "loss: 2.104992628097534, train acc: 0.2812\n",
      "loss: 2.09180588722229, train acc: 0.2837\n",
      "loss: 2.12686288356781, train acc: 0.2798\n",
      "loss: 2.0840733289718627, train acc: 0.285\n",
      "epoch: 58, loss: 2.029237747192383, train acc: 0.285, test acc: 0.2672\n",
      "loss: 2.1238937377929688, train acc: 0.2888\n",
      "loss: 2.130057525634766, train acc: 0.2866\n",
      "loss: 2.1044851779937743, train acc: 0.2796\n",
      "loss: 2.0835150599479677, train acc: 0.28\n",
      "loss: 2.139052081108093, train acc: 0.2787\n",
      "loss: 2.1213556051254274, train acc: 0.2785\n",
      "loss: 2.1111522197723387, train acc: 0.2784\n",
      "loss: 2.1497090816497804, train acc: 0.2777\n",
      "epoch: 59, loss: 2.0434651374816895, train acc: 0.2777, test acc: 0.2615\n",
      "loss: 2.142613649368286, train acc: 0.278\n",
      "loss: 2.1216578006744387, train acc: 0.2736\n",
      "loss: 2.089705967903137, train acc: 0.2768\n",
      "loss: 2.126562309265137, train acc: 0.277\n",
      "loss: 2.1026708841323853, train acc: 0.274\n",
      "loss: 2.1117280244827272, train acc: 0.2812\n",
      "loss: 2.0890848636627197, train acc: 0.2844\n",
      "loss: 2.112612724304199, train acc: 0.2848\n",
      "epoch: 60, loss: 2.266566753387451, train acc: 0.2848, test acc: 0.2732\n",
      "loss: 2.188234329223633, train acc: 0.2925\n",
      "loss: 2.138846826553345, train acc: 0.2814\n",
      "loss: 2.1292891263961793, train acc: 0.2869\n",
      "loss: 2.077630174160004, train acc: 0.2879\n",
      "loss: 2.1064690828323362, train acc: 0.2912\n",
      "loss: 2.1148239374160767, train acc: 0.2781\n",
      "loss: 2.1048410415649412, train acc: 0.2887\n",
      "loss: 2.118595576286316, train acc: 0.287\n",
      "epoch: 61, loss: 2.111781358718872, train acc: 0.287, test acc: 0.2736\n",
      "loss: 2.166233777999878, train acc: 0.2896\n",
      "loss: 2.1339199542999268, train acc: 0.2806\n",
      "loss: 2.1152495622634886, train acc: 0.2875\n",
      "loss: 2.0854890823364256, train acc: 0.2857\n",
      "loss: 2.1206307411193848, train acc: 0.2839\n",
      "loss: 2.1112855195999147, train acc: 0.2893\n",
      "loss: 2.089066410064697, train acc: 0.2866\n",
      "loss: 2.134877014160156, train acc: 0.283\n",
      "epoch: 62, loss: 2.0501856803894043, train acc: 0.283, test acc: 0.2673\n",
      "loss: 2.209440231323242, train acc: 0.2894\n",
      "loss: 2.130781817436218, train acc: 0.2809\n",
      "loss: 2.1185171842575072, train acc: 0.2844\n",
      "loss: 2.0785732507705688, train acc: 0.289\n",
      "loss: 2.09544723033905, train acc: 0.2839\n",
      "loss: 2.1058825254440308, train acc: 0.2802\n",
      "loss: 2.1395586490631104, train acc: 0.2738\n",
      "loss: 2.0936904430389403, train acc: 0.2899\n",
      "epoch: 63, loss: 1.8875727653503418, train acc: 0.2899, test acc: 0.2748\n",
      "loss: 2.177515983581543, train acc: 0.289\n",
      "loss: 2.1311699867248537, train acc: 0.2855\n",
      "loss: 2.110607957839966, train acc: 0.2808\n",
      "loss: 2.138686203956604, train acc: 0.2835\n",
      "loss: 2.105029582977295, train acc: 0.2848\n",
      "loss: 2.072685384750366, train acc: 0.2875\n",
      "loss: 2.0951122760772707, train acc: 0.2905\n",
      "loss: 2.1221396207809446, train acc: 0.2878\n",
      "epoch: 64, loss: 1.8885136842727661, train acc: 0.2878, test acc: 0.2688\n",
      "loss: 2.2559635639190674, train acc: 0.2872\n",
      "loss: 2.12957558631897, train acc: 0.2758\n",
      "loss: 2.1145505905151367, train acc: 0.2881\n",
      "loss: 2.1036413431167604, train acc: 0.2906\n",
      "loss: 2.117384934425354, train acc: 0.2816\n",
      "loss: 2.0943294048309324, train acc: 0.2866\n",
      "loss: 2.121411991119385, train acc: 0.2877\n",
      "loss: 2.1166122913360597, train acc: 0.2794\n",
      "epoch: 65, loss: 1.8929811716079712, train acc: 0.2794, test acc: 0.266\n",
      "loss: 2.0945940017700195, train acc: 0.2839\n",
      "loss: 2.123719048500061, train acc: 0.2821\n",
      "loss: 2.131542372703552, train acc: 0.2815\n",
      "loss: 2.0810999155044554, train acc: 0.2815\n",
      "loss: 2.126625490188599, train acc: 0.2829\n",
      "loss: 2.074798345565796, train acc: 0.2876\n",
      "loss: 2.0936391830444334, train acc: 0.2852\n",
      "loss: 2.1334113836288453, train acc: 0.2852\n",
      "epoch: 66, loss: 2.2655513286590576, train acc: 0.2852, test acc: 0.2682\n",
      "loss: 2.1307759284973145, train acc: 0.288\n",
      "loss: 2.105184817314148, train acc: 0.2856\n",
      "loss: 2.1125036239624024, train acc: 0.2793\n",
      "loss: 2.114278721809387, train acc: 0.2772\n",
      "loss: 2.1064751148223877, train acc: 0.2814\n",
      "loss: 2.1310212135314943, train acc: 0.2853\n",
      "loss: 2.10028977394104, train acc: 0.2869\n",
      "loss: 2.1268346548080443, train acc: 0.2881\n",
      "epoch: 67, loss: 1.8899909257888794, train acc: 0.2881, test acc: 0.27\n",
      "loss: 2.09774112701416, train acc: 0.2917\n",
      "loss: 2.0981265783309935, train acc: 0.2872\n",
      "loss: 2.0911781549453736, train acc: 0.2843\n",
      "loss: 2.1045256853103638, train acc: 0.2814\n",
      "loss: 2.146345067024231, train acc: 0.2862\n",
      "loss: 2.123971700668335, train acc: 0.2864\n",
      "loss: 2.0907561779022217, train acc: 0.2873\n",
      "loss: 2.1182939052581786, train acc: 0.2845\n",
      "epoch: 68, loss: 2.1096761226654053, train acc: 0.2845, test acc: 0.2668\n",
      "loss: 2.122032642364502, train acc: 0.2877\n",
      "loss: 2.115200090408325, train acc: 0.2892\n",
      "loss: 2.0934479594230653, train acc: 0.2851\n",
      "loss: 2.0971813917160036, train acc: 0.2854\n",
      "loss: 2.1193809270858766, train acc: 0.2861\n",
      "loss: 2.1044991254806518, train acc: 0.2897\n",
      "loss: 2.0719285726547243, train acc: 0.2902\n",
      "loss: 2.0985461950302122, train acc: 0.286\n",
      "epoch: 69, loss: 1.9591479301452637, train acc: 0.286, test acc: 0.2621\n",
      "loss: 2.268522262573242, train acc: 0.2836\n",
      "loss: 2.115424370765686, train acc: 0.2872\n",
      "loss: 2.0894421339035034, train acc: 0.2871\n",
      "loss: 2.084775686264038, train acc: 0.289\n",
      "loss: 2.089841532707214, train acc: 0.2881\n",
      "loss: 2.1062161445617678, train acc: 0.2872\n",
      "loss: 2.089911961555481, train acc: 0.2879\n",
      "loss: 2.0918740034103394, train acc: 0.2894\n",
      "epoch: 70, loss: 1.9643101692199707, train acc: 0.2894, test acc: 0.2703\n",
      "loss: 2.167870044708252, train acc: 0.2897\n",
      "loss: 2.09648916721344, train acc: 0.2848\n",
      "loss: 2.0991631746292114, train acc: 0.2809\n",
      "loss: 2.1121843814849854, train acc: 0.2808\n",
      "loss: 2.110990858078003, train acc: 0.2855\n",
      "loss: 2.1247124910354613, train acc: 0.2875\n",
      "loss: 2.12240891456604, train acc: 0.2907\n",
      "loss: 2.1160796642303468, train acc: 0.2916\n",
      "epoch: 71, loss: 2.126849889755249, train acc: 0.2916, test acc: 0.2632\n",
      "loss: 2.138465166091919, train acc: 0.288\n",
      "loss: 2.1064239740371704, train acc: 0.2904\n",
      "loss: 2.121906566619873, train acc: 0.2859\n",
      "loss: 2.078790283203125, train acc: 0.2834\n",
      "loss: 2.1185017108917235, train acc: 0.2905\n",
      "loss: 2.0813289403915407, train acc: 0.2882\n",
      "loss: 2.1053236961364745, train acc: 0.2905\n",
      "loss: 2.1240037441253663, train acc: 0.2873\n",
      "epoch: 72, loss: 2.1283769607543945, train acc: 0.2873, test acc: 0.27\n",
      "loss: 2.195406675338745, train acc: 0.2879\n",
      "loss: 2.1187813520431518, train acc: 0.2649\n",
      "loss: 2.1043411016464235, train acc: 0.2779\n",
      "loss: 2.1118255853652954, train acc: 0.2804\n",
      "loss: 2.1221242427825926, train acc: 0.2828\n",
      "loss: 2.0967995882034303, train acc: 0.289\n",
      "loss: 2.1327919483184816, train acc: 0.2859\n",
      "loss: 2.1326614379882813, train acc: 0.2811\n",
      "epoch: 73, loss: 2.1856186389923096, train acc: 0.2811, test acc: 0.2698\n",
      "loss: 2.091963768005371, train acc: 0.288\n",
      "loss: 2.1284382343292236, train acc: 0.2756\n",
      "loss: 2.087579274177551, train acc: 0.2785\n",
      "loss: 2.095331645011902, train acc: 0.28\n",
      "loss: 2.110126423835754, train acc: 0.2784\n",
      "loss: 2.115755009651184, train acc: 0.2856\n",
      "loss: 2.088284420967102, train acc: 0.2872\n",
      "loss: 2.113416886329651, train acc: 0.2861\n",
      "epoch: 74, loss: 2.120950698852539, train acc: 0.2861, test acc: 0.2595\n",
      "loss: 2.0773186683654785, train acc: 0.2738\n",
      "loss: 2.10839729309082, train acc: 0.2862\n",
      "loss: 2.086571550369263, train acc: 0.283\n",
      "loss: 2.1253757238388062, train acc: 0.2833\n",
      "loss: 2.1123960494995115, train acc: 0.284\n",
      "loss: 2.093711733818054, train acc: 0.2822\n",
      "loss: 2.0981972932815554, train acc: 0.2878\n",
      "loss: 2.1110591173171995, train acc: 0.2812\n",
      "epoch: 75, loss: 2.1868276596069336, train acc: 0.2812, test acc: 0.2637\n",
      "loss: 2.210716485977173, train acc: 0.2833\n",
      "loss: 2.1268949031829836, train acc: 0.2839\n",
      "loss: 2.111425042152405, train acc: 0.2838\n",
      "loss: 2.114965629577637, train acc: 0.2835\n",
      "loss: 2.084563660621643, train acc: 0.2854\n",
      "loss: 2.1175577878952025, train acc: 0.2873\n",
      "loss: 2.1219820737838746, train acc: 0.284\n",
      "loss: 2.127297019958496, train acc: 0.2763\n",
      "epoch: 76, loss: 1.9689841270446777, train acc: 0.2763, test acc: 0.2689\n",
      "loss: 2.089933395385742, train acc: 0.2833\n",
      "loss: 2.0626943707466125, train acc: 0.2797\n",
      "loss: 2.0878769636154173, train acc: 0.2854\n",
      "loss: 2.1219189405441283, train acc: 0.2868\n",
      "loss: 2.1164368391036987, train acc: 0.2834\n",
      "loss: 2.110368800163269, train acc: 0.2858\n",
      "loss: 2.1099286556243895, train acc: 0.2849\n",
      "loss: 2.110472226142883, train acc: 0.2867\n",
      "epoch: 77, loss: 1.9684966802597046, train acc: 0.2867, test acc: 0.2654\n",
      "loss: 2.1737008094787598, train acc: 0.2836\n",
      "loss: 2.116925764083862, train acc: 0.2855\n",
      "loss: 2.123864030838013, train acc: 0.2775\n",
      "loss: 2.0807743668556213, train acc: 0.2751\n",
      "loss: 2.1033517360687255, train acc: 0.279\n",
      "loss: 2.0875982522964476, train acc: 0.2839\n",
      "loss: 2.1158178567886354, train acc: 0.2874\n",
      "loss: 2.1170574188232423, train acc: 0.2748\n",
      "epoch: 78, loss: 2.1220736503601074, train acc: 0.2748, test acc: 0.2717\n",
      "loss: 2.177738666534424, train acc: 0.2883\n",
      "loss: 2.106849956512451, train acc: 0.2734\n",
      "loss: 2.1209844589233398, train acc: 0.2769\n",
      "loss: 2.077242207527161, train acc: 0.2775\n",
      "loss: 2.1255678653717043, train acc: 0.278\n",
      "loss: 2.0888728857040406, train acc: 0.2838\n",
      "loss: 2.1016631841659548, train acc: 0.2881\n",
      "loss: 2.125477504730225, train acc: 0.2823\n",
      "epoch: 79, loss: 2.122457981109619, train acc: 0.2823, test acc: 0.2676\n",
      "#####training and testing end with K:1, P:0.5######\n",
      "#####training and testing start with K:1, P:1######\n",
      "loss: 2.6396260261535645, train acc: 0.1\n",
      "loss: 2.5247353076934815, train acc: 0.1\n",
      "loss: 2.510713982582092, train acc: 0.1\n",
      "loss: 2.505038332939148, train acc: 0.1\n",
      "loss: 2.5106950283050535, train acc: 0.1\n",
      "loss: 2.4883641958236695, train acc: 0.1\n",
      "loss: 2.5127236366271974, train acc: 0.1\n",
      "loss: 2.46410186290741, train acc: 0.1\n",
      "epoch: 0, loss: 2.259812593460083, train acc: 0.1, test acc: 0.098\n",
      "loss: 2.590413808822632, train acc: 0.1\n",
      "loss: 2.4898671865463258, train acc: 0.1\n",
      "loss: 2.4799651861190797, train acc: 0.1\n",
      "loss: 2.4744300365448, train acc: 0.1\n",
      "loss: 2.478782081604004, train acc: 0.1\n",
      "loss: 2.4595525741577147, train acc: 0.1\n",
      "loss: 2.4808979272842406, train acc: 0.1\n",
      "loss: 2.437160325050354, train acc: 0.1\n",
      "epoch: 1, loss: 2.2533762454986572, train acc: 0.1, test acc: 0.098\n",
      "loss: 2.5512197017669678, train acc: 0.1\n",
      "loss: 2.46032235622406, train acc: 0.1\n",
      "loss: 2.4529430866241455, train acc: 0.1\n",
      "loss: 2.447823905944824, train acc: 0.1\n",
      "loss: 2.4509645223617555, train acc: 0.1\n",
      "loss: 2.4344703912734986, train acc: 0.1\n",
      "loss: 2.452987241744995, train acc: 0.1\n",
      "loss: 2.413838005065918, train acc: 0.1\n",
      "epoch: 2, loss: 2.2493698596954346, train acc: 0.1, test acc: 0.098\n",
      "loss: 2.516498327255249, train acc: 0.1\n",
      "loss: 2.4345645189285277, train acc: 0.1\n",
      "loss: 2.429213213920593, train acc: 0.1\n",
      "loss: 2.4246733665466307, train acc: 0.1\n",
      "loss: 2.426747035980225, train acc: 0.1\n",
      "loss: 2.412666606903076, train acc: 0.1\n",
      "loss: 2.4286011695861816, train acc: 0.1\n",
      "loss: 2.39371919631958, train acc: 0.1\n",
      "epoch: 3, loss: 2.2473349571228027, train acc: 0.1, test acc: 0.098\n",
      "loss: 2.485872983932495, train acc: 0.1\n",
      "loss: 2.412210750579834, train acc: 0.1\n",
      "loss: 2.4084452867507933, train acc: 0.1\n",
      "loss: 2.404585576057434, train acc: 0.1\n",
      "loss: 2.4057381391525268, train acc: 0.1\n",
      "loss: 2.393785810470581, train acc: 0.1\n",
      "loss: 2.407401442527771, train acc: 0.1\n",
      "loss: 2.3764577388763426, train acc: 0.1\n",
      "epoch: 4, loss: 2.2468626499176025, train acc: 0.1, test acc: 0.098\n",
      "loss: 2.458984851837158, train acc: 0.1\n",
      "loss: 2.3929229259490965, train acc: 0.1\n",
      "loss: 2.390360403060913, train acc: 0.1\n",
      "loss: 2.3872308254241945, train acc: 0.1\n",
      "loss: 2.3876046895980836, train acc: 0.1\n",
      "loss: 2.377525544166565, train acc: 0.1\n",
      "loss: 2.3890857696533203, train acc: 0.1\n",
      "loss: 2.3617544651031492, train acc: 0.1\n",
      "epoch: 5, loss: 2.247619152069092, train acc: 0.1, test acc: 0.098\n",
      "loss: 2.435490369796753, train acc: 0.1\n",
      "loss: 2.376395606994629, train acc: 0.1\n",
      "loss: 2.3747170209884643, train acc: 0.1\n",
      "loss: 2.3723256826400756, train acc: 0.1\n",
      "loss: 2.3720557928085326, train acc: 0.1\n",
      "loss: 2.3636210680007936, train acc: 0.1\n",
      "loss: 2.3733779430389403, train acc: 0.1\n",
      "loss: 2.3493435382843018, train acc: 0.1\n",
      "epoch: 6, loss: 2.249326467514038, train acc: 0.1, test acc: 0.098\n",
      "loss: 2.415062189102173, train acc: 0.1\n",
      "loss: 2.3623481512069704, train acc: 0.1\n",
      "loss: 2.3612977027893067, train acc: 0.1\n",
      "loss: 2.359617519378662, train acc: 0.1\n",
      "loss: 2.3588300943374634, train acc: 0.1\n",
      "loss: 2.35183265209198, train acc: 0.1\n",
      "loss: 2.3600208520889283, train acc: 0.1\n",
      "loss: 2.3389819145202635, train acc: 0.1\n",
      "epoch: 7, loss: 2.2517521381378174, train acc: 0.1, test acc: 0.098\n",
      "loss: 2.397390127182007, train acc: 0.1\n",
      "loss: 2.3505181312561034, train acc: 0.1\n",
      "loss: 2.3498982191085815, train acc: 0.1\n",
      "loss: 2.3488744497299194, train acc: 0.1\n",
      "loss: 2.347684907913208, train acc: 0.1\n",
      "loss: 2.3419359922409058, train acc: 0.1\n",
      "loss: 2.348769688606262, train acc: 0.1\n",
      "loss: 2.33044011592865, train acc: 0.1\n",
      "epoch: 8, loss: 2.2546920776367188, train acc: 0.1, test acc: 0.098\n",
      "loss: 2.3821823596954346, train acc: 0.1\n",
      "loss: 2.340656280517578, train acc: 0.1\n",
      "loss: 2.3403189897537233, train acc: 0.1\n",
      "loss: 2.3398767471313477, train acc: 0.1\n",
      "loss: 2.338388681411743, train acc: 0.1\n",
      "loss: 2.3337161540985107, train acc: 0.1\n",
      "loss: 2.3393879413604735, train acc: 0.1\n",
      "loss: 2.323496460914612, train acc: 0.1\n",
      "epoch: 9, loss: 2.2579660415649414, train acc: 0.1, test acc: 0.098\n",
      "loss: 2.369161605834961, train acc: 0.1\n",
      "loss: 2.3325235843658447, train acc: 0.1\n",
      "loss: 2.3323612213134766, train acc: 0.1\n",
      "loss: 2.332413339614868, train acc: 0.1\n",
      "loss: 2.3307185411453246, train acc: 0.1\n",
      "loss: 2.3269650459289553, train acc: 0.1\n",
      "loss: 2.3316470623016357, train acc: 0.1\n",
      "loss: 2.3179378747940063, train acc: 0.1\n",
      "epoch: 10, loss: 2.2614171504974365, train acc: 0.1, test acc: 0.098\n",
      "loss: 2.358071804046631, train acc: 0.1\n",
      "loss: 2.3258920907974243, train acc: 0.1\n",
      "loss: 2.325828289985657, train acc: 0.1\n",
      "loss: 2.326283407211304, train acc: 0.1\n",
      "loss: 2.3244606018066407, train acc: 0.1\n",
      "loss: 2.3214836359024047, train acc: 0.1\n",
      "loss: 2.3253283739089965, train acc: 0.1\n",
      "loss: 2.3135613441467284, train acc: 0.1\n",
      "epoch: 11, loss: 2.2649121284484863, train acc: 0.1, test acc: 0.098\n",
      "loss: 2.348670721054077, train acc: 0.1\n",
      "loss: 2.320546579360962, train acc: 0.1\n",
      "loss: 2.320529818534851, train acc: 0.1\n",
      "loss: 2.321297287940979, train acc: 0.1\n",
      "loss: 2.319413447380066, train acc: 0.1\n",
      "loss: 2.3170846700668335, train acc: 0.1\n",
      "loss: 2.3202263593673704, train acc: 0.1\n",
      "loss: 2.310177445411682, train acc: 0.1\n",
      "epoch: 12, loss: 2.268343210220337, train acc: 0.1, test acc: 0.098\n",
      "loss: 2.34074068069458, train acc: 0.1\n",
      "loss: 2.3162885189056395, train acc: 0.1\n",
      "loss: 2.3162842273712156, train acc: 0.1\n",
      "loss: 2.3172802686691285, train acc: 0.1\n",
      "loss: 2.3153902530670165, train acc: 0.1\n",
      "loss: 2.3135961294174194, train acc: 0.1\n",
      "loss: 2.3161513566970826, train acc: 0.1\n",
      "loss: 2.3076125144958497, train acc: 0.1\n",
      "epoch: 13, loss: 2.27162766456604, train acc: 0.1, test acc: 0.098\n",
      "loss: 2.3340795040130615, train acc: 0.1\n",
      "loss: 2.3129369735717775, train acc: 0.1\n",
      "loss: 2.312924122810364, train acc: 0.1\n",
      "loss: 2.314074754714966, train acc: 0.1\n",
      "loss: 2.312221145629883, train acc: 0.1\n",
      "loss: 2.310862421989441, train acc: 0.1\n",
      "loss: 2.3129315853118895, train acc: 0.1\n",
      "loss: 2.3057128190994263, train acc: 0.1\n",
      "epoch: 14, loss: 2.2747063636779785, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.328510284423828, train acc: 0.1\n",
      "loss: 2.3103312253952026, train acc: 0.1\n",
      "loss: 2.3102981805801392, train acc: 0.1\n",
      "loss: 2.3115401029586793, train acc: 0.1\n",
      "loss: 2.309755301475525, train acc: 0.1\n",
      "loss: 2.30874650478363, train acc: 0.1\n",
      "loss: 2.310415768623352, train acc: 0.1\n",
      "loss: 2.3043422222137453, train acc: 0.1\n",
      "epoch: 15, loss: 2.277540683746338, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3238723278045654, train acc: 0.1\n",
      "loss: 2.308330798149109, train acc: 0.1\n",
      "loss: 2.308271646499634, train acc: 0.1\n",
      "loss: 2.3095542907714846, train acc: 0.1\n",
      "loss: 2.3078606128692627, train acc: 0.1\n",
      "loss: 2.3071289539337156, train acc: 0.1\n",
      "loss: 2.3084704637527467, train acc: 0.1\n",
      "loss: 2.3033852338790894, train acc: 0.1\n",
      "epoch: 16, loss: 2.2801098823547363, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.320026397705078, train acc: 0.1\n",
      "loss: 2.306814503669739, train acc: 0.1\n",
      "loss: 2.306728148460388, train acc: 0.1\n",
      "loss: 2.3080118179321287, train acc: 0.1\n",
      "loss: 2.3064231634140016, train acc: 0.1\n",
      "loss: 2.305908274650574, train acc: 0.1\n",
      "loss: 2.3069824457168577, train acc: 0.1\n",
      "loss: 2.3027446269989014, train acc: 0.1\n",
      "epoch: 17, loss: 2.282407283782959, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3168487548828125, train acc: 0.1\n",
      "loss: 2.305680346488953, train acc: 0.1\n",
      "loss: 2.305568075180054, train acc: 0.1\n",
      "loss: 2.306823658943176, train acc: 0.1\n",
      "loss: 2.3053466796875, train acc: 0.1\n",
      "loss: 2.304998683929443, train acc: 0.1\n",
      "loss: 2.3058563232421876, train acc: 0.1\n",
      "loss: 2.302339959144592, train acc: 0.1\n",
      "epoch: 18, loss: 2.284435987472534, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3142333030700684, train acc: 0.1\n",
      "loss: 2.304842948913574, train acc: 0.1\n",
      "loss: 2.304707431793213, train acc: 0.1\n",
      "loss: 2.305915093421936, train acc: 0.1\n",
      "loss: 2.3045515775680543, train acc: 0.1\n",
      "loss: 2.3043297052383425, train acc: 0.1\n",
      "loss: 2.305012035369873, train acc: 0.1\n",
      "loss: 2.3021063804626465, train acc: 0.1\n",
      "epoch: 19, loss: 2.286207437515259, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3120880126953125, train acc: 0.1\n",
      "loss: 2.3042331457138063, train acc: 0.1\n",
      "loss: 2.304077482223511, train acc: 0.1\n",
      "loss: 2.3052247524261475, train acc: 0.1\n",
      "loss: 2.3039719104766845, train acc: 0.1\n",
      "loss: 2.3038438081741335, train acc: 0.1\n",
      "loss: 2.3043853998184205, train acc: 0.1\n",
      "loss: 2.3019932985305784, train acc: 0.1\n",
      "epoch: 20, loss: 2.2877392768859863, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3103344440460205, train acc: 0.1\n",
      "loss: 2.3037946224212646, train acc: 0.1\n",
      "loss: 2.303623008728027, train acc: 0.1\n",
      "loss: 2.3047033071517946, train acc: 0.1\n",
      "loss: 2.3035550355911254, train acc: 0.1\n",
      "loss: 2.303495097160339, train acc: 0.1\n",
      "loss: 2.303924036026001, train acc: 0.1\n",
      "loss: 2.3019611120223997, train acc: 0.1\n",
      "epoch: 21, loss: 2.289051055908203, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3089051246643066, train acc: 0.1\n",
      "loss: 2.3034835338592528, train acc: 0.1\n",
      "loss: 2.303299331665039, train acc: 0.1\n",
      "loss: 2.304310607910156, train acc: 0.1\n",
      "loss: 2.3032592296600343, train acc: 0.1\n",
      "loss: 2.3032480001449587, train acc: 0.1\n",
      "loss: 2.303586983680725, train acc: 0.1\n",
      "loss: 2.3019806146621704, train acc: 0.1\n",
      "epoch: 22, loss: 2.290166139602661, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.307743787765503, train acc: 0.1\n",
      "loss: 2.3032657384872435, train acc: 0.1\n",
      "loss: 2.3030720472335817, train acc: 0.1\n",
      "loss: 2.3040157556533813, train acc: 0.1\n",
      "loss: 2.3030524969100954, train acc: 0.1\n",
      "loss: 2.303074598312378, train acc: 0.1\n",
      "loss: 2.303342413902283, train acc: 0.1\n",
      "loss: 2.3020301818847657, train acc: 0.1\n",
      "epoch: 23, loss: 2.2911062240600586, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3068032264709473, train acc: 0.1\n",
      "loss: 2.303114891052246, train acc: 0.1\n",
      "loss: 2.30291428565979, train acc: 0.1\n",
      "loss: 2.303794240951538, train acc: 0.1\n",
      "loss: 2.302909564971924, train acc: 0.1\n",
      "loss: 2.3029541254043577, train acc: 0.1\n",
      "loss: 2.303165912628174, train acc: 0.1\n",
      "loss: 2.302095055580139, train acc: 0.1\n",
      "epoch: 24, loss: 2.29189395904541, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.306044340133667, train acc: 0.1\n",
      "loss: 2.3030120611190794, train acc: 0.1\n",
      "loss: 2.302806758880615, train acc: 0.1\n",
      "loss: 2.3036278009414675, train acc: 0.1\n",
      "loss: 2.3028123140335084, train acc: 0.1\n",
      "loss: 2.3028707265853883, train acc: 0.1\n",
      "loss: 2.303039026260376, train acc: 0.1\n",
      "loss: 2.3021649599075316, train acc: 0.1\n",
      "epoch: 25, loss: 2.292550563812256, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3054332733154297, train acc: 0.1\n",
      "loss: 2.3029423713684083, train acc: 0.1\n",
      "loss: 2.302734303474426, train acc: 0.1\n",
      "loss: 2.3035021305084227, train acc: 0.1\n",
      "loss: 2.302747130393982, train acc: 0.1\n",
      "loss: 2.3028136014938356, train acc: 0.1\n",
      "loss: 2.3029479026794433, train acc: 0.1\n",
      "loss: 2.302233600616455, train acc: 0.1\n",
      "epoch: 26, loss: 2.2930946350097656, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3049428462982178, train acc: 0.1\n",
      "loss: 2.302895736694336, train acc: 0.1\n",
      "loss: 2.302686262130737, train acc: 0.1\n",
      "loss: 2.3034070253372194, train acc: 0.1\n",
      "loss: 2.3027039527893067, train acc: 0.1\n",
      "loss: 2.3027743816375734, train acc: 0.1\n",
      "loss: 2.302882742881775, train acc: 0.1\n",
      "loss: 2.302297067642212, train acc: 0.1\n",
      "epoch: 27, loss: 2.293543577194214, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.304550886154175, train acc: 0.1\n",
      "loss: 2.302864599227905, train acc: 0.1\n",
      "loss: 2.302654981613159, train acc: 0.1\n",
      "loss: 2.303334903717041, train acc: 0.1\n",
      "loss: 2.302676057815552, train acc: 0.1\n",
      "loss: 2.3027474880218506, train acc: 0.1\n",
      "loss: 2.3028361320495607, train acc: 0.1\n",
      "loss: 2.30235378742218, train acc: 0.1\n",
      "epoch: 28, loss: 2.293912887573242, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3042380809783936, train acc: 0.1\n",
      "loss: 2.302844262123108, train acc: 0.1\n",
      "loss: 2.302634763717651, train acc: 0.1\n",
      "loss: 2.3032795429229735, train acc: 0.1\n",
      "loss: 2.3026579856872558, train acc: 0.1\n",
      "loss: 2.3027289867401124, train acc: 0.1\n",
      "loss: 2.3028027534484865, train acc: 0.1\n",
      "loss: 2.302402639389038, train acc: 0.1\n",
      "epoch: 29, loss: 2.294215679168701, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303989887237549, train acc: 0.1\n",
      "loss: 2.302830767631531, train acc: 0.1\n",
      "loss: 2.30262234210968, train acc: 0.1\n",
      "loss: 2.303236889839172, train acc: 0.1\n",
      "loss: 2.302646803855896, train acc: 0.1\n",
      "loss: 2.3027156352996827, train acc: 0.1\n",
      "loss: 2.30277898311615, train acc: 0.1\n",
      "loss: 2.302443790435791, train acc: 0.1\n",
      "epoch: 30, loss: 2.294463872909546, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3037939071655273, train acc: 0.1\n",
      "loss: 2.3028220415115355, train acc: 0.1\n",
      "loss: 2.3026147842407227, train acc: 0.1\n",
      "loss: 2.3032041072845457, train acc: 0.1\n",
      "loss: 2.302639937400818, train acc: 0.1\n",
      "loss: 2.3027065277099608, train acc: 0.1\n",
      "loss: 2.3027617216110228, train acc: 0.1\n",
      "loss: 2.302478289604187, train acc: 0.1\n",
      "epoch: 31, loss: 2.294665813446045, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3036394119262695, train acc: 0.1\n",
      "loss: 2.302816367149353, train acc: 0.1\n",
      "loss: 2.3026103496551515, train acc: 0.1\n",
      "loss: 2.303178596496582, train acc: 0.1\n",
      "loss: 2.3026358127593993, train acc: 0.1\n",
      "loss: 2.302699851989746, train acc: 0.1\n",
      "loss: 2.3027496337890625, train acc: 0.1\n",
      "loss: 2.302506375312805, train acc: 0.1\n",
      "epoch: 32, loss: 2.294830799102783, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303518533706665, train acc: 0.1\n",
      "loss: 2.3028124570846558, train acc: 0.1\n",
      "loss: 2.302608013153076, train acc: 0.1\n",
      "loss: 2.3031586408615112, train acc: 0.1\n",
      "loss: 2.302633357048035, train acc: 0.1\n",
      "loss: 2.3026949167251587, train acc: 0.1\n",
      "loss: 2.302740526199341, train acc: 0.1\n",
      "loss: 2.302529144287109, train acc: 0.1\n",
      "epoch: 33, loss: 2.2949652671813965, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3034234046936035, train acc: 0.1\n",
      "loss: 2.302809953689575, train acc: 0.1\n",
      "loss: 2.302606964111328, train acc: 0.1\n",
      "loss: 2.303143262863159, train acc: 0.1\n",
      "loss: 2.302632236480713, train acc: 0.1\n",
      "loss: 2.302691125869751, train acc: 0.1\n",
      "loss: 2.302734351158142, train acc: 0.1\n",
      "loss: 2.3025471687316896, train acc: 0.1\n",
      "epoch: 34, loss: 2.295074701309204, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3033499717712402, train acc: 0.1\n",
      "loss: 2.302808332443237, train acc: 0.1\n",
      "loss: 2.3026065826416016, train acc: 0.1\n",
      "loss: 2.3031312227249146, train acc: 0.1\n",
      "loss: 2.3026316165924072, train acc: 0.1\n",
      "loss: 2.3026882886886595, train acc: 0.1\n",
      "loss: 2.3027297258377075, train acc: 0.1\n",
      "loss: 2.302561712265015, train acc: 0.1\n",
      "epoch: 35, loss: 2.295163869857788, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303292751312256, train acc: 0.1\n",
      "loss: 2.3028072357177733, train acc: 0.1\n",
      "loss: 2.3026066541671755, train acc: 0.1\n",
      "loss: 2.303121781349182, train acc: 0.1\n",
      "loss: 2.3026313543319703, train acc: 0.1\n",
      "loss: 2.3026859760284424, train acc: 0.1\n",
      "loss: 2.3027265310287475, train acc: 0.1\n",
      "loss: 2.3025731801986695, train acc: 0.1\n",
      "epoch: 36, loss: 2.295236110687256, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303248882293701, train acc: 0.1\n",
      "loss: 2.302806568145752, train acc: 0.1\n",
      "loss: 2.3026071310043337, train acc: 0.1\n",
      "loss: 2.3031145095825196, train acc: 0.1\n",
      "loss: 2.3026312828063964, train acc: 0.1\n",
      "loss: 2.302684187889099, train acc: 0.1\n",
      "loss: 2.3027242183685304, train acc: 0.1\n",
      "loss: 2.302582025527954, train acc: 0.1\n",
      "epoch: 37, loss: 2.295295000076294, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3032147884368896, train acc: 0.1\n",
      "loss: 2.302805852890015, train acc: 0.1\n",
      "loss: 2.3026075839996336, train acc: 0.1\n",
      "loss: 2.3031086683273316, train acc: 0.1\n",
      "loss: 2.302631449699402, train acc: 0.1\n",
      "loss: 2.302682709693909, train acc: 0.1\n",
      "loss: 2.3027228355407714, train acc: 0.1\n",
      "loss: 2.3025891065597532, train acc: 0.1\n",
      "epoch: 38, loss: 2.2953426837921143, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303189516067505, train acc: 0.1\n",
      "loss: 2.3028055667877196, train acc: 0.1\n",
      "loss: 2.302608108520508, train acc: 0.1\n",
      "loss: 2.3031044483184813, train acc: 0.1\n",
      "loss: 2.302631640434265, train acc: 0.1\n",
      "loss: 2.3026814222335816, train acc: 0.1\n",
      "loss: 2.3027217626571654, train acc: 0.1\n",
      "loss: 2.3025944948196413, train acc: 0.1\n",
      "epoch: 39, loss: 2.295381784439087, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031694889068604, train acc: 0.1\n",
      "loss: 2.302805209159851, train acc: 0.1\n",
      "loss: 2.302608776092529, train acc: 0.1\n",
      "loss: 2.303100919723511, train acc: 0.1\n",
      "loss: 2.3026317358016968, train acc: 0.1\n",
      "loss: 2.3026805400848387, train acc: 0.1\n",
      "loss: 2.3027209281921386, train acc: 0.1\n",
      "loss: 2.302598738670349, train acc: 0.1\n",
      "epoch: 40, loss: 2.2954137325286865, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303154945373535, train acc: 0.1\n",
      "loss: 2.3028049945831297, train acc: 0.1\n",
      "loss: 2.3026092767715456, train acc: 0.1\n",
      "loss: 2.30309853553772, train acc: 0.1\n",
      "loss: 2.3026319265365602, train acc: 0.1\n",
      "loss: 2.3026798009872436, train acc: 0.1\n",
      "loss: 2.3027204275131226, train acc: 0.1\n",
      "loss: 2.3026020526885986, train acc: 0.1\n",
      "epoch: 41, loss: 2.2954399585723877, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031439781188965, train acc: 0.1\n",
      "loss: 2.302804970741272, train acc: 0.1\n",
      "loss: 2.302609753608704, train acc: 0.1\n",
      "loss: 2.3030965328216553, train acc: 0.1\n",
      "loss: 2.3026319265365602, train acc: 0.1\n",
      "loss: 2.3026792287826536, train acc: 0.1\n",
      "loss: 2.3027201652526856, train acc: 0.1\n",
      "loss: 2.3026045322418214, train acc: 0.1\n",
      "epoch: 42, loss: 2.2954604625701904, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303135395050049, train acc: 0.1\n",
      "loss: 2.302804708480835, train acc: 0.1\n",
      "loss: 2.3026102066040037, train acc: 0.1\n",
      "loss: 2.3030950307846068, train acc: 0.1\n",
      "loss: 2.3026320934295654, train acc: 0.1\n",
      "loss: 2.3026785612106324, train acc: 0.1\n",
      "loss: 2.30271999835968, train acc: 0.1\n",
      "loss: 2.30260648727417, train acc: 0.1\n",
      "epoch: 43, loss: 2.2954773902893066, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303129196166992, train acc: 0.1\n",
      "loss: 2.302804636955261, train acc: 0.1\n",
      "loss: 2.3026106119155885, train acc: 0.1\n",
      "loss: 2.3030940532684325, train acc: 0.1\n",
      "loss: 2.302632284164429, train acc: 0.1\n",
      "loss: 2.3026782035827638, train acc: 0.1\n",
      "loss: 2.30271999835968, train acc: 0.1\n",
      "loss: 2.3026079654693605, train acc: 0.1\n",
      "epoch: 44, loss: 2.295491933822632, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.30312442779541, train acc: 0.1\n",
      "loss: 2.302804732322693, train acc: 0.1\n",
      "loss: 2.302611064910889, train acc: 0.1\n",
      "loss: 2.3030934572219848, train acc: 0.1\n",
      "loss: 2.3026324987411497, train acc: 0.1\n",
      "loss: 2.302677798271179, train acc: 0.1\n",
      "loss: 2.3027200222015383, train acc: 0.1\n",
      "loss: 2.3026092290878295, train acc: 0.1\n",
      "epoch: 45, loss: 2.295503616333008, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031210899353027, train acc: 0.1\n",
      "loss: 2.3028046131134032, train acc: 0.1\n",
      "loss: 2.3026113748550414, train acc: 0.1\n",
      "loss: 2.3030927658081053, train acc: 0.1\n",
      "loss: 2.302632522583008, train acc: 0.1\n",
      "loss: 2.3026777029037477, train acc: 0.1\n",
      "loss: 2.3027201652526856, train acc: 0.1\n",
      "loss: 2.3026101112365724, train acc: 0.1\n",
      "epoch: 46, loss: 2.295513153076172, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031187057495117, train acc: 0.1\n",
      "loss: 2.302804470062256, train acc: 0.1\n",
      "loss: 2.302611780166626, train acc: 0.1\n",
      "loss: 2.303092432022095, train acc: 0.1\n",
      "loss: 2.3026326179504393, train acc: 0.1\n",
      "loss: 2.3026773929595947, train acc: 0.1\n",
      "loss: 2.3027201652526856, train acc: 0.1\n",
      "loss: 2.3026105642318724, train acc: 0.1\n",
      "epoch: 47, loss: 2.295520544052124, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303117513656616, train acc: 0.1\n",
      "loss: 2.3028046131134032, train acc: 0.1\n",
      "loss: 2.302612042427063, train acc: 0.1\n",
      "loss: 2.303092432022095, train acc: 0.1\n",
      "loss: 2.3026326179504393, train acc: 0.1\n",
      "loss: 2.3026773929595947, train acc: 0.1\n",
      "loss: 2.302720308303833, train acc: 0.1\n",
      "loss: 2.3026113748550414, train acc: 0.1\n",
      "epoch: 48, loss: 2.295527458190918, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031160831451416, train acc: 0.1\n",
      "loss: 2.302804636955261, train acc: 0.1\n",
      "loss: 2.3026124477386474, train acc: 0.1\n",
      "loss: 2.303092050552368, train acc: 0.1\n",
      "loss: 2.3026326417922975, train acc: 0.1\n",
      "loss: 2.302677059173584, train acc: 0.1\n",
      "loss: 2.302720284461975, train acc: 0.1\n",
      "loss: 2.3026116847991944, train acc: 0.1\n",
      "epoch: 49, loss: 2.295532464981079, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031153678894043, train acc: 0.1\n",
      "loss: 2.302804708480835, train acc: 0.1\n",
      "loss: 2.302612543106079, train acc: 0.1\n",
      "loss: 2.3030921459197997, train acc: 0.1\n",
      "loss: 2.302632737159729, train acc: 0.1\n",
      "loss: 2.3026770114898683, train acc: 0.1\n",
      "loss: 2.3027204036712647, train acc: 0.1\n",
      "loss: 2.302612137794495, train acc: 0.1\n",
      "epoch: 50, loss: 2.295536756515503, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303115129470825, train acc: 0.1\n",
      "loss: 2.302804708480835, train acc: 0.1\n",
      "loss: 2.302612781524658, train acc: 0.1\n",
      "loss: 2.3030922174453736, train acc: 0.1\n",
      "loss: 2.3026328802108766, train acc: 0.1\n",
      "loss: 2.302676868438721, train acc: 0.1\n",
      "loss: 2.302720642089844, train acc: 0.1\n",
      "loss: 2.302612376213074, train acc: 0.1\n",
      "epoch: 51, loss: 2.2955405712127686, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303114891052246, train acc: 0.1\n",
      "loss: 2.302804708480835, train acc: 0.1\n",
      "loss: 2.302613043785095, train acc: 0.1\n",
      "loss: 2.3030921459197997, train acc: 0.1\n",
      "loss: 2.3026328325271606, train acc: 0.1\n",
      "loss: 2.3026768445968626, train acc: 0.1\n",
      "loss: 2.3027206659317017, train acc: 0.1\n",
      "loss: 2.302612638473511, train acc: 0.1\n",
      "epoch: 52, loss: 2.295543670654297, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303114652633667, train acc: 0.1\n",
      "loss: 2.3028047561645506, train acc: 0.1\n",
      "loss: 2.3026132583618164, train acc: 0.1\n",
      "loss: 2.303092288970947, train acc: 0.1\n",
      "loss: 2.3026328086853027, train acc: 0.1\n",
      "loss: 2.3026768922805787, train acc: 0.1\n",
      "loss: 2.3027206897735595, train acc: 0.1\n",
      "loss: 2.302612805366516, train acc: 0.1\n",
      "epoch: 53, loss: 2.295546054840088, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303114652633667, train acc: 0.1\n",
      "loss: 2.30280487537384, train acc: 0.1\n",
      "loss: 2.302613544464111, train acc: 0.1\n",
      "loss: 2.303092432022095, train acc: 0.1\n",
      "loss: 2.302632999420166, train acc: 0.1\n",
      "loss: 2.3026769161224365, train acc: 0.1\n",
      "loss: 2.302720785140991, train acc: 0.1\n",
      "loss: 2.302613115310669, train acc: 0.1\n",
      "epoch: 54, loss: 2.2955482006073, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303114891052246, train acc: 0.1\n",
      "loss: 2.302804970741272, train acc: 0.1\n",
      "loss: 2.3026134967803955, train acc: 0.1\n",
      "loss: 2.3030925989151, train acc: 0.1\n",
      "loss: 2.30263295173645, train acc: 0.1\n",
      "loss: 2.302676773071289, train acc: 0.1\n",
      "loss: 2.3027209281921386, train acc: 0.1\n",
      "loss: 2.302613091468811, train acc: 0.1\n",
      "epoch: 55, loss: 2.2955503463745117, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303115129470825, train acc: 0.1\n",
      "loss: 2.302805018424988, train acc: 0.1\n",
      "loss: 2.302613711357117, train acc: 0.1\n",
      "loss: 2.303092670440674, train acc: 0.1\n",
      "loss: 2.3026329278945923, train acc: 0.1\n",
      "loss: 2.3026767492294313, train acc: 0.1\n",
      "loss: 2.3027209997177125, train acc: 0.1\n",
      "loss: 2.302613306045532, train acc: 0.1\n",
      "epoch: 56, loss: 2.2955515384674072, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303114891052246, train acc: 0.1\n",
      "loss: 2.302805018424988, train acc: 0.1\n",
      "loss: 2.3026138067245485, train acc: 0.1\n",
      "loss: 2.303092861175537, train acc: 0.1\n",
      "loss: 2.302632999420166, train acc: 0.1\n",
      "loss: 2.302676796913147, train acc: 0.1\n",
      "loss: 2.302721118927002, train acc: 0.1\n",
      "loss: 2.302613401412964, train acc: 0.1\n",
      "epoch: 57, loss: 2.2955527305603027, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303115129470825, train acc: 0.1\n",
      "loss: 2.302805042266846, train acc: 0.1\n",
      "loss: 2.3026139736175537, train acc: 0.1\n",
      "loss: 2.3030930280685427, train acc: 0.1\n",
      "loss: 2.3026330947875975, train acc: 0.1\n",
      "loss: 2.302676630020142, train acc: 0.1\n",
      "loss: 2.3027212858200072, train acc: 0.1\n",
      "loss: 2.3026135206222533, train acc: 0.1\n",
      "epoch: 58, loss: 2.2955543994903564, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031158447265625, train acc: 0.1\n",
      "loss: 2.3028051137924193, train acc: 0.1\n",
      "loss: 2.302614212036133, train acc: 0.1\n",
      "loss: 2.3030932903289796, train acc: 0.1\n",
      "loss: 2.302633047103882, train acc: 0.1\n",
      "loss: 2.302676725387573, train acc: 0.1\n",
      "loss: 2.302721309661865, train acc: 0.1\n",
      "loss: 2.3026135206222533, train acc: 0.1\n",
      "epoch: 59, loss: 2.2955551147460938, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031160831451416, train acc: 0.1\n",
      "loss: 2.302805256843567, train acc: 0.1\n",
      "loss: 2.3026142358779906, train acc: 0.1\n",
      "loss: 2.3030932188034057, train acc: 0.1\n",
      "loss: 2.3026330709457397, train acc: 0.1\n",
      "loss: 2.302676773071289, train acc: 0.1\n",
      "loss: 2.3027212381362916, train acc: 0.1\n",
      "loss: 2.302613544464111, train acc: 0.1\n",
      "epoch: 60, loss: 2.2955563068389893, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031160831451416, train acc: 0.1\n",
      "loss: 2.302805209159851, train acc: 0.1\n",
      "loss: 2.3026144027709963, train acc: 0.1\n",
      "loss: 2.3030932426452635, train acc: 0.1\n",
      "loss: 2.302633047103882, train acc: 0.1\n",
      "loss: 2.3026766777038574, train acc: 0.1\n",
      "loss: 2.3027214288711546, train acc: 0.1\n",
      "loss: 2.3026135683059694, train acc: 0.1\n",
      "epoch: 61, loss: 2.2955570220947266, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031163215637207, train acc: 0.1\n",
      "loss: 2.3028051614761353, train acc: 0.1\n",
      "loss: 2.30261435508728, train acc: 0.1\n",
      "loss: 2.3030932664871218, train acc: 0.1\n",
      "loss: 2.3026330709457397, train acc: 0.1\n",
      "loss: 2.302676773071289, train acc: 0.1\n",
      "loss: 2.3027215003967285, train acc: 0.1\n",
      "loss: 2.302613615989685, train acc: 0.1\n",
      "epoch: 62, loss: 2.295557737350464, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031160831451416, train acc: 0.1\n",
      "loss: 2.302805209159851, train acc: 0.1\n",
      "loss: 2.3026145696640015, train acc: 0.1\n",
      "loss: 2.3030935287475587, train acc: 0.1\n",
      "loss: 2.3026331424713136, train acc: 0.1\n",
      "loss: 2.3026767015457152, train acc: 0.1\n",
      "loss: 2.3027215003967285, train acc: 0.1\n",
      "loss: 2.3026137590408324, train acc: 0.1\n",
      "epoch: 63, loss: 2.295558214187622, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031165599823, train acc: 0.1\n",
      "loss: 2.3028051614761353, train acc: 0.1\n",
      "loss: 2.3026145935058593, train acc: 0.1\n",
      "loss: 2.303093671798706, train acc: 0.1\n",
      "loss: 2.302632999420166, train acc: 0.1\n",
      "loss: 2.302676630020142, train acc: 0.1\n",
      "loss: 2.3027215719223024, train acc: 0.1\n",
      "loss: 2.3026137351989746, train acc: 0.1\n",
      "epoch: 64, loss: 2.295558452606201, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303116798400879, train acc: 0.1\n",
      "loss: 2.302805209159851, train acc: 0.1\n",
      "loss: 2.302614688873291, train acc: 0.1\n",
      "loss: 2.3030938148498534, train acc: 0.1\n",
      "loss: 2.3026331424713136, train acc: 0.1\n",
      "loss: 2.302676725387573, train acc: 0.1\n",
      "loss: 2.302721691131592, train acc: 0.1\n",
      "loss: 2.3026138305664063, train acc: 0.1\n",
      "epoch: 65, loss: 2.2955596446990967, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303117036819458, train acc: 0.1\n",
      "loss: 2.302805280685425, train acc: 0.1\n",
      "loss: 2.3026147365570067, train acc: 0.1\n",
      "loss: 2.303093934059143, train acc: 0.1\n",
      "loss: 2.3026330709457397, train acc: 0.1\n",
      "loss: 2.3026766538619996, train acc: 0.1\n",
      "loss: 2.3027217626571654, train acc: 0.1\n",
      "loss: 2.302613878250122, train acc: 0.1\n",
      "epoch: 66, loss: 2.295559883117676, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303117275238037, train acc: 0.1\n",
      "loss: 2.302805209159851, train acc: 0.1\n",
      "loss: 2.30261492729187, train acc: 0.1\n",
      "loss: 2.303093981742859, train acc: 0.1\n",
      "loss: 2.3026331186294557, train acc: 0.1\n",
      "loss: 2.302676773071289, train acc: 0.1\n",
      "loss: 2.302721691131592, train acc: 0.1\n",
      "loss: 2.302613925933838, train acc: 0.1\n",
      "epoch: 67, loss: 2.295560121536255, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303117275238037, train acc: 0.1\n",
      "loss: 2.3028053283691405, train acc: 0.1\n",
      "loss: 2.30261492729187, train acc: 0.1\n",
      "loss: 2.303093981742859, train acc: 0.1\n",
      "loss: 2.302633190155029, train acc: 0.1\n",
      "loss: 2.3026767492294313, train acc: 0.1\n",
      "loss: 2.302721858024597, train acc: 0.1\n",
      "loss: 2.30261390209198, train acc: 0.1\n",
      "epoch: 68, loss: 2.295560359954834, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303117275238037, train acc: 0.1\n",
      "loss: 2.3028053045272827, train acc: 0.1\n",
      "loss: 2.3026148796081545, train acc: 0.1\n",
      "loss: 2.303094005584717, train acc: 0.1\n",
      "loss: 2.3026330947875975, train acc: 0.1\n",
      "loss: 2.302676773071289, train acc: 0.1\n",
      "loss: 2.302721881866455, train acc: 0.1\n",
      "loss: 2.302613949775696, train acc: 0.1\n",
      "epoch: 69, loss: 2.2955610752105713, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303117275238037, train acc: 0.1\n",
      "loss: 2.3028053045272827, train acc: 0.1\n",
      "loss: 2.3026148319244384, train acc: 0.1\n",
      "loss: 2.3030940771102903, train acc: 0.1\n",
      "loss: 2.3026330709457397, train acc: 0.1\n",
      "loss: 2.302676773071289, train acc: 0.1\n",
      "loss: 2.3027218341827393, train acc: 0.1\n",
      "loss: 2.30261390209198, train acc: 0.1\n",
      "epoch: 70, loss: 2.2955613136291504, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303117513656616, train acc: 0.1\n",
      "loss: 2.3028053283691405, train acc: 0.1\n",
      "loss: 2.3026149749755858, train acc: 0.1\n",
      "loss: 2.3030942916870116, train acc: 0.1\n",
      "loss: 2.3026331186294557, train acc: 0.1\n",
      "loss: 2.3026767015457152, train acc: 0.1\n",
      "loss: 2.3027218103408815, train acc: 0.1\n",
      "loss: 2.302613925933838, train acc: 0.1\n",
      "epoch: 71, loss: 2.2955617904663086, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.303117513656616, train acc: 0.1\n",
      "loss: 2.3028054237365723, train acc: 0.1\n",
      "loss: 2.3026149749755858, train acc: 0.1\n",
      "loss: 2.3030943870544434, train acc: 0.1\n",
      "loss: 2.3026331424713136, train acc: 0.1\n",
      "loss: 2.3026767492294313, train acc: 0.1\n",
      "loss: 2.3027217626571654, train acc: 0.1\n",
      "loss: 2.3026140213012694, train acc: 0.1\n",
      "epoch: 72, loss: 2.2955617904663086, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031179904937744, train acc: 0.1\n",
      "loss: 2.3028053760528566, train acc: 0.1\n",
      "loss: 2.302615022659302, train acc: 0.1\n",
      "loss: 2.303094410896301, train acc: 0.1\n",
      "loss: 2.302633190155029, train acc: 0.1\n",
      "loss: 2.302676868438721, train acc: 0.1\n",
      "loss: 2.3027217626571654, train acc: 0.1\n",
      "loss: 2.302614116668701, train acc: 0.1\n",
      "epoch: 73, loss: 2.2955617904663086, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031177520751953, train acc: 0.1\n",
      "loss: 2.3028053283691405, train acc: 0.1\n",
      "loss: 2.302614951133728, train acc: 0.1\n",
      "loss: 2.303094410896301, train acc: 0.1\n",
      "loss: 2.302633261680603, train acc: 0.1\n",
      "loss: 2.302676868438721, train acc: 0.1\n",
      "loss: 2.302721881866455, train acc: 0.1\n",
      "loss: 2.302614140510559, train acc: 0.1\n",
      "epoch: 74, loss: 2.2955617904663086, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031179904937744, train acc: 0.1\n",
      "loss: 2.3028053522109984, train acc: 0.1\n",
      "loss: 2.302615189552307, train acc: 0.1\n",
      "loss: 2.303094434738159, train acc: 0.1\n",
      "loss: 2.3026331663131714, train acc: 0.1\n",
      "loss: 2.302676868438721, train acc: 0.1\n",
      "loss: 2.302721881866455, train acc: 0.1\n",
      "loss: 2.302614164352417, train acc: 0.1\n",
      "epoch: 75, loss: 2.2955620288848877, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031177520751953, train acc: 0.1\n",
      "loss: 2.3028053283691405, train acc: 0.1\n",
      "loss: 2.302615165710449, train acc: 0.1\n",
      "loss: 2.303094482421875, train acc: 0.1\n",
      "loss: 2.3026331663131714, train acc: 0.1\n",
      "loss: 2.3026768445968626, train acc: 0.1\n",
      "loss: 2.302721881866455, train acc: 0.1\n",
      "loss: 2.302614140510559, train acc: 0.1\n",
      "epoch: 76, loss: 2.295562267303467, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031177520751953, train acc: 0.1\n",
      "loss: 2.3028053522109984, train acc: 0.1\n",
      "loss: 2.3026151418685914, train acc: 0.1\n",
      "loss: 2.3030945539474486, train acc: 0.1\n",
      "loss: 2.302633285522461, train acc: 0.1\n",
      "loss: 2.3026768922805787, train acc: 0.1\n",
      "loss: 2.3027218341827393, train acc: 0.1\n",
      "loss: 2.302614164352417, train acc: 0.1\n",
      "epoch: 77, loss: 2.295562267303467, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031182289123535, train acc: 0.1\n",
      "loss: 2.302805495262146, train acc: 0.1\n",
      "loss: 2.3026153564453127, train acc: 0.1\n",
      "loss: 2.3030945539474486, train acc: 0.1\n",
      "loss: 2.302633213996887, train acc: 0.1\n",
      "loss: 2.302676868438721, train acc: 0.1\n",
      "loss: 2.302721858024597, train acc: 0.1\n",
      "loss: 2.302614188194275, train acc: 0.1\n",
      "epoch: 78, loss: 2.295562744140625, train acc: 0.1, test acc: 0.1009\n",
      "loss: 2.3031182289123535, train acc: 0.1\n",
      "loss: 2.30280544757843, train acc: 0.1\n",
      "loss: 2.30261549949646, train acc: 0.1\n",
      "loss: 2.3030946493148803, train acc: 0.1\n",
      "loss: 2.3026332378387453, train acc: 0.1\n",
      "loss: 2.302676820755005, train acc: 0.1\n",
      "loss: 2.302721953392029, train acc: 0.1\n",
      "loss: 2.302614212036133, train acc: 0.1\n",
      "epoch: 79, loss: 2.295562744140625, train acc: 0.1, test acc: 0.1009\n",
      "#####training and testing end with K:1, P:1######\n",
      "#####training and testing start with K:5, P:0.1######\n",
      "loss: 2.343024253845215, train acc: 0.0875\n",
      "loss: 2.297653079032898, train acc: 0.101\n",
      "loss: 2.1492804050445558, train acc: 0.3525\n",
      "loss: 1.924568235874176, train acc: 0.401\n",
      "loss: 1.8726474285125732, train acc: 0.4059\n",
      "loss: 1.7792967438697815, train acc: 0.4028\n",
      "loss: 1.7143115520477294, train acc: 0.4032\n",
      "loss: 1.6777187585830688, train acc: 0.4149\n",
      "epoch: 0, loss: 1.4504358768463135, train acc: 0.4149, test acc: 0.4261\n",
      "loss: 1.477174162864685, train acc: 0.4251\n",
      "loss: 1.6248680233955384, train acc: 0.4451\n",
      "loss: 1.5796215415000916, train acc: 0.4602\n",
      "loss: 1.5160632491111756, train acc: 0.4618\n",
      "loss: 1.5401057004928589, train acc: 0.4664\n",
      "loss: 1.5059008240699767, train acc: 0.4737\n",
      "loss: 1.4940151572227478, train acc: 0.4798\n",
      "loss: 1.4668686151504517, train acc: 0.4825\n",
      "epoch: 1, loss: 1.3353642225265503, train acc: 0.4825, test acc: 0.5002\n",
      "loss: 1.3744961023330688, train acc: 0.5051\n",
      "loss: 1.4546483635902405, train acc: 0.5204\n",
      "loss: 1.422021746635437, train acc: 0.5592\n",
      "loss: 1.3733748078346253, train acc: 0.574\n",
      "loss: 1.3833078622817994, train acc: 0.5876\n",
      "loss: 1.3477819204330443, train acc: 0.6011\n",
      "loss: 1.3395848274230957, train acc: 0.6208\n",
      "loss: 1.3425519466400146, train acc: 0.6446\n",
      "epoch: 2, loss: 1.2969574928283691, train acc: 0.6446, test acc: 0.6446\n",
      "loss: 1.1958775520324707, train acc: 0.6473\n",
      "loss: 1.2453266501426696, train acc: 0.7048\n",
      "loss: 1.2332949042320251, train acc: 0.7195\n",
      "loss: 1.133378380537033, train acc: 0.7235\n",
      "loss: 1.175270187854767, train acc: 0.7389\n",
      "loss: 1.1420010924339294, train acc: 0.7495\n",
      "loss: 1.0961913764476776, train acc: 0.7592\n",
      "loss: 1.0909862995147706, train acc: 0.7584\n",
      "epoch: 3, loss: 0.6611192226409912, train acc: 0.7584, test acc: 0.7598\n",
      "loss: 0.8865468502044678, train acc: 0.7706\n",
      "loss: 1.0582525491714478, train acc: 0.7742\n",
      "loss: 1.0579425394535065, train acc: 0.7718\n",
      "loss: 1.0169344604015351, train acc: 0.7738\n",
      "loss: 1.0113200187683105, train acc: 0.7735\n",
      "loss: 1.0118381798267364, train acc: 0.7833\n",
      "loss: 1.0329883456230164, train acc: 0.7874\n",
      "loss: 1.0162067711353302, train acc: 0.7911\n",
      "epoch: 4, loss: 0.8000929355621338, train acc: 0.7911, test acc: 0.7792\n",
      "loss: 0.8378077745437622, train acc: 0.7961\n",
      "loss: 0.9864930391311646, train acc: 0.7969\n",
      "loss: 0.9946140050888062, train acc: 0.7987\n",
      "loss: 0.975157743692398, train acc: 0.7993\n",
      "loss: 0.9383754074573517, train acc: 0.8\n",
      "loss: 0.9751349925994873, train acc: 0.8072\n",
      "loss: 0.9462773621082305, train acc: 0.8139\n",
      "loss: 0.9307270586490631, train acc: 0.8201\n",
      "epoch: 5, loss: 0.47614043951034546, train acc: 0.8201, test acc: 0.8085\n",
      "loss: 0.7765407562255859, train acc: 0.8237\n",
      "loss: 0.9081965625286103, train acc: 0.8227\n",
      "loss: 0.9107788741588593, train acc: 0.827\n",
      "loss: 0.8777514040470124, train acc: 0.8215\n",
      "loss: 0.8780608355998993, train acc: 0.8294\n",
      "loss: 0.8991690874099731, train acc: 0.8317\n",
      "loss: 0.8901591241359711, train acc: 0.8342\n",
      "loss: 0.8989232420921326, train acc: 0.8373\n",
      "epoch: 6, loss: 0.46354299783706665, train acc: 0.8373, test acc: 0.8275\n",
      "loss: 0.8205997347831726, train acc: 0.8358\n",
      "loss: 0.865552830696106, train acc: 0.8338\n",
      "loss: 0.8607202768325806, train acc: 0.838\n",
      "loss: 0.8406987607479095, train acc: 0.8367\n",
      "loss: 0.8342581331729889, train acc: 0.8414\n",
      "loss: 0.9097152411937713, train acc: 0.8427\n",
      "loss: 0.8888257443904877, train acc: 0.8441\n",
      "loss: 0.8558853387832641, train acc: 0.8457\n",
      "epoch: 7, loss: 0.5474714040756226, train acc: 0.8457, test acc: 0.8335\n",
      "loss: 0.7898839712142944, train acc: 0.8468\n",
      "loss: 0.8511675655841827, train acc: 0.8463\n",
      "loss: 0.8540589272975921, train acc: 0.8469\n",
      "loss: 0.8000119626522064, train acc: 0.8466\n",
      "loss: 0.8277741849422455, train acc: 0.8496\n",
      "loss: 0.8339982151985168, train acc: 0.8524\n",
      "loss: 0.8134713470935822, train acc: 0.8543\n",
      "loss: 0.8104170203208924, train acc: 0.853\n",
      "epoch: 8, loss: 0.42186248302459717, train acc: 0.853, test acc: 0.8389\n",
      "loss: 0.7435646653175354, train acc: 0.8559\n",
      "loss: 0.7906708478927612, train acc: 0.8565\n",
      "loss: 0.8148783326148987, train acc: 0.8553\n",
      "loss: 0.759051525592804, train acc: 0.8577\n",
      "loss: 0.7760306060314178, train acc: 0.8567\n",
      "loss: 0.7799551486968994, train acc: 0.858\n",
      "loss: 0.7887487947940827, train acc: 0.8593\n",
      "loss: 0.7972760617733001, train acc: 0.8592\n",
      "epoch: 9, loss: 0.28871312737464905, train acc: 0.8592, test acc: 0.8435\n",
      "loss: 0.6630651354789734, train acc: 0.8615\n",
      "loss: 0.8119380950927735, train acc: 0.8626\n",
      "loss: 0.805512273311615, train acc: 0.8597\n",
      "loss: 0.8366235256195068, train acc: 0.8626\n",
      "loss: 0.7610039055347443, train acc: 0.8646\n",
      "loss: 0.7876666247844696, train acc: 0.8625\n",
      "loss: 0.797820371389389, train acc: 0.8638\n",
      "loss: 0.778163880109787, train acc: 0.8629\n",
      "epoch: 10, loss: 0.3318791687488556, train acc: 0.8629, test acc: 0.8447\n",
      "loss: 0.729849636554718, train acc: 0.8662\n",
      "loss: 0.7840774059295654, train acc: 0.867\n",
      "loss: 0.7581409573554992, train acc: 0.865\n",
      "loss: 0.7574717462062835, train acc: 0.8671\n",
      "loss: 0.774551796913147, train acc: 0.8683\n",
      "loss: 0.7670955121517181, train acc: 0.8679\n",
      "loss: 0.799096155166626, train acc: 0.8691\n",
      "loss: 0.7867700815200805, train acc: 0.8671\n",
      "epoch: 11, loss: 0.5611969232559204, train acc: 0.8671, test acc: 0.8483\n",
      "loss: 0.628065288066864, train acc: 0.8721\n",
      "loss: 0.7674202442169189, train acc: 0.8705\n",
      "loss: 0.7729076981544495, train acc: 0.8678\n",
      "loss: 0.7710638523101807, train acc: 0.8723\n",
      "loss: 0.7338448226451874, train acc: 0.8699\n",
      "loss: 0.7686430096626282, train acc: 0.8711\n",
      "loss: 0.7863479852676392, train acc: 0.8701\n",
      "loss: 0.7573773324489593, train acc: 0.8714\n",
      "epoch: 12, loss: 0.49748820066452026, train acc: 0.8714, test acc: 0.8517\n",
      "loss: 0.6165947914123535, train acc: 0.8734\n",
      "loss: 0.7439877927303314, train acc: 0.875\n",
      "loss: 0.7504599332809448, train acc: 0.8724\n",
      "loss: 0.7527445673942565, train acc: 0.8745\n",
      "loss: 0.7495344519615174, train acc: 0.8737\n",
      "loss: 0.7794040501117706, train acc: 0.8737\n",
      "loss: 0.7194243848323822, train acc: 0.8701\n",
      "loss: 0.7122735559940339, train acc: 0.8715\n",
      "epoch: 13, loss: 0.39312466979026794, train acc: 0.8715, test acc: 0.8529\n",
      "loss: 0.575864315032959, train acc: 0.8737\n",
      "loss: 0.7498141288757324, train acc: 0.8759\n",
      "loss: 0.7543883800506592, train acc: 0.8733\n",
      "loss: 0.7990894734859466, train acc: 0.875\n",
      "loss: 0.7066708743572235, train acc: 0.8748\n",
      "loss: 0.7548974096775055, train acc: 0.8741\n",
      "loss: 0.7426244199275971, train acc: 0.8741\n",
      "loss: 0.7519475221633911, train acc: 0.8721\n",
      "epoch: 14, loss: 0.5397488474845886, train acc: 0.8721, test acc: 0.8522\n",
      "loss: 0.6306818723678589, train acc: 0.8764\n",
      "loss: 0.7486529886722565, train acc: 0.8768\n",
      "loss: 0.7872128784656525, train acc: 0.8752\n",
      "loss: 0.7274630963802338, train acc: 0.8793\n",
      "loss: 0.7429885327816009, train acc: 0.876\n",
      "loss: 0.7376188188791275, train acc: 0.8771\n",
      "loss: 0.7765444040298461, train acc: 0.8758\n",
      "loss: 0.7526097238063812, train acc: 0.8753\n",
      "epoch: 15, loss: 0.5424360632896423, train acc: 0.8753, test acc: 0.8521\n",
      "loss: 0.6327971816062927, train acc: 0.8772\n",
      "loss: 0.7586696445941925, train acc: 0.8784\n",
      "loss: 0.7547323763370514, train acc: 0.8755\n",
      "loss: 0.7361337244510651, train acc: 0.8779\n",
      "loss: 0.717631071805954, train acc: 0.8767\n",
      "loss: 0.7530356824398041, train acc: 0.8785\n",
      "loss: 0.7575264215469361, train acc: 0.878\n",
      "loss: 0.7220449090003968, train acc: 0.8748\n",
      "epoch: 16, loss: 0.3132171928882599, train acc: 0.8748, test acc: 0.8521\n",
      "loss: 0.6643533706665039, train acc: 0.8765\n",
      "loss: 0.743833351135254, train acc: 0.8794\n",
      "loss: 0.6961586058139801, train acc: 0.8783\n",
      "loss: 0.7193745672702789, train acc: 0.8813\n",
      "loss: 0.7172727704048156, train acc: 0.8786\n",
      "loss: 0.7418943345546722, train acc: 0.8796\n",
      "loss: 0.7390501201152802, train acc: 0.8787\n",
      "loss: 0.7362404525279999, train acc: 0.8793\n",
      "epoch: 17, loss: 0.19525644183158875, train acc: 0.8793, test acc: 0.8549\n",
      "loss: 0.5863998532295227, train acc: 0.88\n",
      "loss: 0.7311265289783477, train acc: 0.8769\n",
      "loss: 0.7245565533638001, train acc: 0.88\n",
      "loss: 0.7372565269470215, train acc: 0.8806\n",
      "loss: 0.6895651757717133, train acc: 0.881\n",
      "loss: 0.7572436392307281, train acc: 0.8821\n",
      "loss: 0.7596757411956787, train acc: 0.8816\n",
      "loss: 0.721271401643753, train acc: 0.8804\n",
      "epoch: 18, loss: 0.7685662508010864, train acc: 0.8804, test acc: 0.855\n",
      "loss: 0.5250595808029175, train acc: 0.8797\n",
      "loss: 0.7257568538188934, train acc: 0.8814\n",
      "loss: 0.7041609048843384, train acc: 0.8835\n",
      "loss: 0.726102215051651, train acc: 0.8814\n",
      "loss: 0.7380767285823822, train acc: 0.8807\n",
      "loss: 0.7076817035675049, train acc: 0.8829\n",
      "loss: 0.6903844535350799, train acc: 0.8823\n",
      "loss: 0.6859982967376709, train acc: 0.8808\n",
      "epoch: 19, loss: 0.49079716205596924, train acc: 0.8808, test acc: 0.8564\n",
      "loss: 0.6629307866096497, train acc: 0.8821\n",
      "loss: 0.6855711877346039, train acc: 0.8813\n",
      "loss: 0.6955292463302613, train acc: 0.8846\n",
      "loss: 0.7100753307342529, train acc: 0.8846\n",
      "loss: 0.6570243954658508, train acc: 0.8843\n",
      "loss: 0.6956654191017151, train acc: 0.884\n",
      "loss: 0.6898435890674591, train acc: 0.8842\n",
      "loss: 0.7447084426879883, train acc: 0.8843\n",
      "epoch: 20, loss: 0.5478253364562988, train acc: 0.8843, test acc: 0.8568\n",
      "loss: 0.5798439383506775, train acc: 0.8854\n",
      "loss: 0.7282839298248291, train acc: 0.8844\n",
      "loss: 0.7358303785324096, train acc: 0.8853\n",
      "loss: 0.7226235508918762, train acc: 0.8852\n",
      "loss: 0.7046934366226196, train acc: 0.8841\n",
      "loss: 0.7270245492458344, train acc: 0.885\n",
      "loss: 0.7225945115089416, train acc: 0.8839\n",
      "loss: 0.6959886670112609, train acc: 0.8854\n",
      "epoch: 21, loss: 0.30064788460731506, train acc: 0.8854, test acc: 0.8576\n",
      "loss: 0.6603657007217407, train acc: 0.883\n",
      "loss: 0.7312935829162598, train acc: 0.8828\n",
      "loss: 0.723578953742981, train acc: 0.884\n",
      "loss: 0.7272072672843933, train acc: 0.8854\n",
      "loss: 0.6719487607479095, train acc: 0.8853\n",
      "loss: 0.717118364572525, train acc: 0.8878\n",
      "loss: 0.7268633544445038, train acc: 0.8874\n",
      "loss: 0.6889159619808197, train acc: 0.8859\n",
      "epoch: 22, loss: 0.26964911818504333, train acc: 0.8859, test acc: 0.8556\n",
      "loss: 0.6245194673538208, train acc: 0.8842\n",
      "loss: 0.7455637931823731, train acc: 0.8845\n",
      "loss: 0.6750481307506562, train acc: 0.8861\n",
      "loss: 0.7129932880401612, train acc: 0.8856\n",
      "loss: 0.6842506468296051, train acc: 0.8866\n",
      "loss: 0.7069810211658478, train acc: 0.8875\n",
      "loss: 0.7395083189010621, train acc: 0.8868\n",
      "loss: 0.6942544460296631, train acc: 0.8847\n",
      "epoch: 23, loss: 0.5983607769012451, train acc: 0.8847, test acc: 0.8583\n",
      "loss: 0.6540847420692444, train acc: 0.8858\n",
      "loss: 0.7246118366718293, train acc: 0.8851\n",
      "loss: 0.7514562904834747, train acc: 0.8863\n",
      "loss: 0.7008664220571518, train acc: 0.8866\n",
      "loss: 0.6570441663265228, train acc: 0.8874\n",
      "loss: 0.6854814827442169, train acc: 0.888\n",
      "loss: 0.7151287913322448, train acc: 0.8869\n",
      "loss: 0.7256597936153412, train acc: 0.8879\n",
      "epoch: 24, loss: 0.4551851749420166, train acc: 0.8879, test acc: 0.858\n",
      "loss: 0.5849028825759888, train acc: 0.8859\n",
      "loss: 0.7110552728176117, train acc: 0.887\n",
      "loss: 0.7213581740856171, train acc: 0.8878\n",
      "loss: 0.6802037000656128, train acc: 0.8878\n",
      "loss: 0.6965992152690887, train acc: 0.888\n",
      "loss: 0.6919507801532745, train acc: 0.8897\n",
      "loss: 0.6743589460849762, train acc: 0.8893\n",
      "loss: 0.6768009781837463, train acc: 0.8897\n",
      "epoch: 25, loss: 0.11271841824054718, train acc: 0.8897, test acc: 0.8588\n",
      "loss: 0.4708509147167206, train acc: 0.8888\n",
      "loss: 0.6801095515489578, train acc: 0.8883\n",
      "loss: 0.686175960302353, train acc: 0.8877\n",
      "loss: 0.6857811510562897, train acc: 0.8881\n",
      "loss: 0.6986999988555909, train acc: 0.8885\n",
      "loss: 0.7349432945251465, train acc: 0.8906\n",
      "loss: 0.695774495601654, train acc: 0.8911\n",
      "loss: 0.6580744385719299, train acc: 0.8902\n",
      "epoch: 26, loss: 0.35637542605400085, train acc: 0.8902, test acc: 0.8581\n",
      "loss: 0.6625468134880066, train acc: 0.8885\n",
      "loss: 0.6779738187789917, train acc: 0.8872\n",
      "loss: 0.705182009935379, train acc: 0.8912\n",
      "loss: 0.6477738589048385, train acc: 0.8897\n",
      "loss: 0.6832948803901673, train acc: 0.8903\n",
      "loss: 0.7098574429750443, train acc: 0.8906\n",
      "loss: 0.7311505556106568, train acc: 0.8915\n",
      "loss: 0.687760055065155, train acc: 0.8897\n",
      "epoch: 27, loss: 0.3673703670501709, train acc: 0.8897, test acc: 0.859\n",
      "loss: 0.5795825123786926, train acc: 0.891\n",
      "loss: 0.6687802672386169, train acc: 0.8897\n",
      "loss: 0.6778297781944275, train acc: 0.8918\n",
      "loss: 0.6518631696701049, train acc: 0.8903\n",
      "loss: 0.6445831805467606, train acc: 0.8902\n",
      "loss: 0.6627154588699341, train acc: 0.8903\n",
      "loss: 0.697070187330246, train acc: 0.8929\n",
      "loss: 0.6649918794631958, train acc: 0.8918\n",
      "epoch: 28, loss: 0.45069220662117004, train acc: 0.8918, test acc: 0.859\n",
      "loss: 0.5813195109367371, train acc: 0.8915\n",
      "loss: 0.6985517978668213, train acc: 0.8917\n",
      "loss: 0.6818741619586944, train acc: 0.8909\n",
      "loss: 0.6748906672000885, train acc: 0.89\n",
      "loss: 0.6525756657123566, train acc: 0.8921\n",
      "loss: 0.6775699347257614, train acc: 0.8921\n",
      "loss: 0.7047274053096771, train acc: 0.8923\n",
      "loss: 0.6559297740459442, train acc: 0.8919\n",
      "epoch: 29, loss: 0.39952465891838074, train acc: 0.8919, test acc: 0.8589\n",
      "loss: 0.47322389483451843, train acc: 0.8912\n",
      "loss: 0.6346124023199081, train acc: 0.8896\n",
      "loss: 0.6560948669910431, train acc: 0.8923\n",
      "loss: 0.6454725861549377, train acc: 0.8912\n",
      "loss: 0.7000562250614166, train acc: 0.8901\n",
      "loss: 0.6463990986347199, train acc: 0.8929\n",
      "loss: 0.7043611705303192, train acc: 0.8932\n",
      "loss: 0.6464625656604767, train acc: 0.8934\n",
      "epoch: 30, loss: 0.323133647441864, train acc: 0.8934, test acc: 0.8591\n",
      "loss: 0.639553964138031, train acc: 0.8905\n",
      "loss: 0.6715002357959747, train acc: 0.8897\n",
      "loss: 0.6744418919086457, train acc: 0.8926\n",
      "loss: 0.6452247858047485, train acc: 0.8917\n",
      "loss: 0.6284596443176269, train acc: 0.888\n",
      "loss: 0.7159821271896363, train acc: 0.8933\n",
      "loss: 0.6302212655544281, train acc: 0.8927\n",
      "loss: 0.6833986341953278, train acc: 0.8924\n",
      "epoch: 31, loss: 0.5059775114059448, train acc: 0.8924, test acc: 0.8601\n",
      "loss: 0.5570220947265625, train acc: 0.8899\n",
      "loss: 0.6299837708473206, train acc: 0.8914\n",
      "loss: 0.6672299802303314, train acc: 0.8934\n",
      "loss: 0.6771795690059662, train acc: 0.8925\n",
      "loss: 0.6482802718877793, train acc: 0.8925\n",
      "loss: 0.6868489861488343, train acc: 0.8954\n",
      "loss: 0.7318069756031036, train acc: 0.894\n",
      "loss: 0.6517118215560913, train acc: 0.8948\n",
      "epoch: 32, loss: 0.3443807065486908, train acc: 0.8948, test acc: 0.8599\n",
      "loss: 0.5437040328979492, train acc: 0.8928\n",
      "loss: 0.6720134675502777, train acc: 0.8927\n",
      "loss: 0.7107882559299469, train acc: 0.8914\n",
      "loss: 0.6792291045188904, train acc: 0.8927\n",
      "loss: 0.6870068669319153, train acc: 0.8897\n",
      "loss: 0.6695770978927612, train acc: 0.8945\n",
      "loss: 0.6964653909206391, train acc: 0.8937\n",
      "loss: 0.6989699959754944, train acc: 0.8922\n",
      "epoch: 33, loss: 0.5240895748138428, train acc: 0.8922, test acc: 0.8588\n",
      "loss: 0.49831995368003845, train acc: 0.8921\n",
      "loss: 0.6764855474233628, train acc: 0.892\n",
      "loss: 0.6385124027729034, train acc: 0.8941\n",
      "loss: 0.6390592664480209, train acc: 0.8935\n",
      "loss: 0.6718719899654388, train acc: 0.8951\n",
      "loss: 0.6579319626092911, train acc: 0.8935\n",
      "loss: 0.684831690788269, train acc: 0.8944\n",
      "loss: 0.6801117837429047, train acc: 0.8957\n",
      "epoch: 34, loss: 0.11875519901514053, train acc: 0.8957, test acc: 0.8588\n",
      "loss: 0.5821149945259094, train acc: 0.8927\n",
      "loss: 0.6961067676544189, train acc: 0.893\n",
      "loss: 0.6462443828582763, train acc: 0.8949\n",
      "loss: 0.6595529198646546, train acc: 0.8927\n",
      "loss: 0.6822274148464202, train acc: 0.8935\n",
      "loss: 0.649607115983963, train acc: 0.8962\n",
      "loss: 0.6492919445037841, train acc: 0.895\n",
      "loss: 0.6304446816444397, train acc: 0.8939\n",
      "epoch: 35, loss: 0.5144233703613281, train acc: 0.8939, test acc: 0.8593\n",
      "loss: 0.5422689318656921, train acc: 0.8918\n",
      "loss: 0.6441871643066406, train acc: 0.8938\n",
      "loss: 0.6466665834188461, train acc: 0.8962\n",
      "loss: 0.6379104435443879, train acc: 0.8941\n",
      "loss: 0.6587802827358246, train acc: 0.8938\n",
      "loss: 0.6481040805578232, train acc: 0.8955\n",
      "loss: 0.6684913694858551, train acc: 0.8969\n",
      "loss: 0.6640804886817933, train acc: 0.8946\n",
      "epoch: 36, loss: 0.5433216094970703, train acc: 0.8946, test acc: 0.8578\n",
      "loss: 0.4800955057144165, train acc: 0.8945\n",
      "loss: 0.6934487700462342, train acc: 0.8933\n",
      "loss: 0.6654564261436462, train acc: 0.8958\n",
      "loss: 0.6302393257617951, train acc: 0.8933\n",
      "loss: 0.6556146919727326, train acc: 0.8952\n",
      "loss: 0.6676902443170547, train acc: 0.8967\n",
      "loss: 0.6696768701076508, train acc: 0.8955\n",
      "loss: 0.6573274612426758, train acc: 0.8949\n",
      "epoch: 37, loss: 0.5184081792831421, train acc: 0.8949, test acc: 0.8575\n",
      "loss: 0.4692571759223938, train acc: 0.8936\n",
      "loss: 0.6344305098056793, train acc: 0.8946\n",
      "loss: 0.6406329363584519, train acc: 0.8961\n",
      "loss: 0.6435033947229385, train acc: 0.895\n",
      "loss: 0.6356646955013275, train acc: 0.8941\n",
      "loss: 0.7140329957008362, train acc: 0.8968\n",
      "loss: 0.6668972849845887, train acc: 0.8967\n",
      "loss: 0.6511597871780396, train acc: 0.8957\n",
      "epoch: 38, loss: 0.5378170609474182, train acc: 0.8957, test acc: 0.8589\n",
      "loss: 0.5418824553489685, train acc: 0.8947\n",
      "loss: 0.6995410263538361, train acc: 0.8942\n",
      "loss: 0.6426842033863067, train acc: 0.896\n",
      "loss: 0.6511054396629333, train acc: 0.8939\n",
      "loss: 0.6270720303058624, train acc: 0.8949\n",
      "loss: 0.6650127738714218, train acc: 0.8968\n",
      "loss: 0.6510190427303314, train acc: 0.8985\n",
      "loss: 0.6550484418869018, train acc: 0.8954\n",
      "epoch: 39, loss: 0.8645880222320557, train acc: 0.8954, test acc: 0.8585\n",
      "loss: 0.4983319640159607, train acc: 0.8952\n",
      "loss: 0.6255736231803894, train acc: 0.8965\n",
      "loss: 0.6640985012054443, train acc: 0.8957\n",
      "loss: 0.6550681173801423, train acc: 0.8973\n",
      "loss: 0.6403790652751923, train acc: 0.897\n",
      "loss: 0.6607142746448517, train acc: 0.8969\n",
      "loss: 0.6655665159225463, train acc: 0.8984\n",
      "loss: 0.6562700510025025, train acc: 0.8967\n",
      "epoch: 40, loss: 0.40630537271499634, train acc: 0.8967, test acc: 0.8569\n",
      "loss: 0.6270785331726074, train acc: 0.8943\n",
      "loss: 0.6651590228080749, train acc: 0.8974\n",
      "loss: 0.644223290681839, train acc: 0.8982\n",
      "loss: 0.6543488800525665, train acc: 0.8958\n",
      "loss: 0.6103262484073639, train acc: 0.8952\n",
      "loss: 0.6271980047225952, train acc: 0.899\n",
      "loss: 0.6663952887058258, train acc: 0.8993\n",
      "loss: 0.6424447923898697, train acc: 0.897\n",
      "epoch: 41, loss: 0.26564261317253113, train acc: 0.897, test acc: 0.8598\n",
      "loss: 0.5828315019607544, train acc: 0.8954\n",
      "loss: 0.6864716112613678, train acc: 0.8973\n",
      "loss: 0.6618168532848359, train acc: 0.8979\n",
      "loss: 0.6687311828136444, train acc: 0.8972\n",
      "loss: 0.6316975891590119, train acc: 0.8962\n",
      "loss: 0.6322796404361725, train acc: 0.899\n",
      "loss: 0.6357849717140198, train acc: 0.8984\n",
      "loss: 0.6509770691394806, train acc: 0.8957\n",
      "epoch: 42, loss: 0.45228397846221924, train acc: 0.8957, test acc: 0.8586\n",
      "loss: 0.5234267115592957, train acc: 0.8952\n",
      "loss: 0.6295997738838196, train acc: 0.8961\n",
      "loss: 0.6359659522771836, train acc: 0.8986\n",
      "loss: 0.6530041486024857, train acc: 0.8963\n",
      "loss: 0.6400377035140992, train acc: 0.8966\n",
      "loss: 0.6509612739086151, train acc: 0.8984\n",
      "loss: 0.6901432335376739, train acc: 0.8978\n",
      "loss: 0.6873497933149337, train acc: 0.896\n",
      "epoch: 43, loss: 0.10903333127498627, train acc: 0.896, test acc: 0.8565\n",
      "loss: 0.5190520286560059, train acc: 0.8956\n",
      "loss: 0.6624857902526855, train acc: 0.8959\n",
      "loss: 0.6560715705156326, train acc: 0.8954\n",
      "loss: 0.6707364559173584, train acc: 0.8959\n",
      "loss: 0.641952383518219, train acc: 0.8956\n",
      "loss: 0.6551240921020508, train acc: 0.8984\n",
      "loss: 0.6767677903175354, train acc: 0.8986\n",
      "loss: 0.6512673139572144, train acc: 0.8944\n",
      "epoch: 44, loss: 0.19719800353050232, train acc: 0.8944, test acc: 0.8577\n",
      "loss: 0.5078805685043335, train acc: 0.8953\n",
      "loss: 0.6833453953266144, train acc: 0.8966\n",
      "loss: 0.6584049224853515, train acc: 0.8964\n",
      "loss: 0.6389225006103516, train acc: 0.8975\n",
      "loss: 0.6176198244094848, train acc: 0.8977\n",
      "loss: 0.6251878082752228, train acc: 0.8996\n",
      "loss: 0.668743497133255, train acc: 0.8981\n",
      "loss: 0.6573333501815796, train acc: 0.8969\n",
      "epoch: 45, loss: 0.6568820476531982, train acc: 0.8969, test acc: 0.8601\n",
      "loss: 0.5079663395881653, train acc: 0.8995\n",
      "loss: 0.6303439050912857, train acc: 0.8965\n",
      "loss: 0.6698674023151397, train acc: 0.8983\n",
      "loss: 0.6378634512424469, train acc: 0.8993\n",
      "loss: 0.6685846507549286, train acc: 0.8989\n",
      "loss: 0.5905851542949676, train acc: 0.9001\n",
      "loss: 0.6470375776290893, train acc: 0.8979\n",
      "loss: 0.6224423468112945, train acc: 0.8974\n",
      "epoch: 46, loss: 0.3491303324699402, train acc: 0.8974, test acc: 0.8589\n",
      "loss: 0.6091103553771973, train acc: 0.8985\n",
      "loss: 0.6327824115753173, train acc: 0.8986\n",
      "loss: 0.6586520969867706, train acc: 0.8989\n",
      "loss: 0.6409479200839996, train acc: 0.8991\n",
      "loss: 0.6127319008111953, train acc: 0.8987\n",
      "loss: 0.6593531727790832, train acc: 0.8978\n",
      "loss: 0.6807903409004211, train acc: 0.8996\n",
      "loss: 0.6502136647701263, train acc: 0.8967\n",
      "epoch: 47, loss: 0.4672451317310333, train acc: 0.8967, test acc: 0.859\n",
      "loss: 0.5165165662765503, train acc: 0.8996\n",
      "loss: 0.6823798418045044, train acc: 0.8983\n",
      "loss: 0.6611678570508956, train acc: 0.8995\n",
      "loss: 0.6472983300685883, train acc: 0.899\n",
      "loss: 0.6442001581192016, train acc: 0.8992\n",
      "loss: 0.6943457156419754, train acc: 0.901\n",
      "loss: 0.642817297577858, train acc: 0.9009\n",
      "loss: 0.6432598412036896, train acc: 0.898\n",
      "epoch: 48, loss: 0.42514660954475403, train acc: 0.898, test acc: 0.8587\n",
      "loss: 0.4386526942253113, train acc: 0.8995\n",
      "loss: 0.6382087230682373, train acc: 0.8987\n",
      "loss: 0.6503450632095337, train acc: 0.9001\n",
      "loss: 0.6502849221229553, train acc: 0.8991\n",
      "loss: 0.5985811889171601, train acc: 0.8991\n",
      "loss: 0.6549796819686889, train acc: 0.9013\n",
      "loss: 0.6816732525825501, train acc: 0.9003\n",
      "loss: 0.6953147172927856, train acc: 0.8986\n",
      "epoch: 49, loss: 0.41049912571907043, train acc: 0.8986, test acc: 0.8595\n",
      "loss: 0.45523062348365784, train acc: 0.8987\n",
      "loss: 0.6115710705518722, train acc: 0.8981\n",
      "loss: 0.6406709492206574, train acc: 0.8992\n",
      "loss: 0.632405012845993, train acc: 0.9002\n",
      "loss: 0.6123742580413818, train acc: 0.8991\n",
      "loss: 0.6783947050571442, train acc: 0.9023\n",
      "loss: 0.655716723203659, train acc: 0.9014\n",
      "loss: 0.6506688356399536, train acc: 0.9002\n",
      "epoch: 50, loss: 0.1935598999261856, train acc: 0.9002, test acc: 0.8577\n",
      "loss: 0.5156758427619934, train acc: 0.898\n",
      "loss: 0.6903065621852875, train acc: 0.8998\n",
      "loss: 0.5891442835330963, train acc: 0.9005\n",
      "loss: 0.6496221780776977, train acc: 0.9005\n",
      "loss: 0.600927111506462, train acc: 0.9007\n",
      "loss: 0.6831756830215454, train acc: 0.9021\n",
      "loss: 0.6286393523216247, train acc: 0.9013\n",
      "loss: 0.6227997213602066, train acc: 0.9014\n",
      "epoch: 51, loss: 0.5047861933708191, train acc: 0.9014, test acc: 0.8592\n",
      "loss: 0.562774658203125, train acc: 0.9014\n",
      "loss: 0.6250082463026047, train acc: 0.9011\n",
      "loss: 0.6064596891403198, train acc: 0.8995\n",
      "loss: 0.6148245096206665, train acc: 0.9011\n",
      "loss: 0.6443954348564148, train acc: 0.8991\n",
      "loss: 0.6329367101192475, train acc: 0.9012\n",
      "loss: 0.6540273070335388, train acc: 0.9013\n",
      "loss: 0.6581971824169159, train acc: 0.8977\n",
      "epoch: 52, loss: 0.1429228037595749, train acc: 0.8977, test acc: 0.8582\n",
      "loss: 0.40571263432502747, train acc: 0.8985\n",
      "loss: 0.6255774348974228, train acc: 0.8992\n",
      "loss: 0.5917504549026489, train acc: 0.9008\n",
      "loss: 0.6505120813846588, train acc: 0.901\n",
      "loss: 0.6051164269447327, train acc: 0.9\n",
      "loss: 0.6332731962203979, train acc: 0.9031\n",
      "loss: 0.6400562226772308, train acc: 0.9013\n",
      "loss: 0.6352131009101868, train acc: 0.9002\n",
      "epoch: 53, loss: 0.4412929117679596, train acc: 0.9002, test acc: 0.859\n",
      "loss: 0.5387080311775208, train acc: 0.8999\n",
      "loss: 0.644541734457016, train acc: 0.9013\n",
      "loss: 0.6524603456258774, train acc: 0.9012\n",
      "loss: 0.6124310255050659, train acc: 0.9017\n",
      "loss: 0.6199124872684478, train acc: 0.901\n",
      "loss: 0.6647178351879119, train acc: 0.902\n",
      "loss: 0.6731030344963074, train acc: 0.9002\n",
      "loss: 0.6257727324962616, train acc: 0.9008\n",
      "epoch: 54, loss: 0.41310155391693115, train acc: 0.9008, test acc: 0.8585\n",
      "loss: 0.5488945841789246, train acc: 0.8985\n",
      "loss: 0.6349446922540665, train acc: 0.8995\n",
      "loss: 0.6731126427650451, train acc: 0.8993\n",
      "loss: 0.6370804816484451, train acc: 0.9006\n",
      "loss: 0.6187904834747314, train acc: 0.8999\n",
      "loss: 0.6666562169790268, train acc: 0.9001\n",
      "loss: 0.6178770214319229, train acc: 0.9025\n",
      "loss: 0.6190268754959106, train acc: 0.9007\n",
      "epoch: 55, loss: 0.3082224428653717, train acc: 0.9007, test acc: 0.8576\n",
      "loss: 0.47598662972450256, train acc: 0.9008\n",
      "loss: 0.6178773522377015, train acc: 0.9029\n",
      "loss: 0.6392695426940918, train acc: 0.902\n",
      "loss: 0.6384228885173797, train acc: 0.9031\n",
      "loss: 0.5881311923265458, train acc: 0.9024\n",
      "loss: 0.5971209168434143, train acc: 0.9041\n",
      "loss: 0.6567592620849609, train acc: 0.904\n",
      "loss: 0.5939604043960571, train acc: 0.9021\n",
      "epoch: 56, loss: 0.5411039590835571, train acc: 0.9021, test acc: 0.8607\n",
      "loss: 0.3919079601764679, train acc: 0.9035\n",
      "loss: 0.5987762063741684, train acc: 0.9042\n",
      "loss: 0.6267294108867645, train acc: 0.9025\n",
      "loss: 0.5833529323339463, train acc: 0.9035\n",
      "loss: 0.58602734208107, train acc: 0.9017\n",
      "loss: 0.6215270102024079, train acc: 0.9062\n",
      "loss: 0.632484233379364, train acc: 0.9041\n",
      "loss: 0.6499592244625092, train acc: 0.9015\n",
      "epoch: 57, loss: 0.10329665243625641, train acc: 0.9015, test acc: 0.8586\n",
      "loss: 0.4343623220920563, train acc: 0.9046\n",
      "loss: 0.6145208567380905, train acc: 0.9025\n",
      "loss: 0.6267817229032516, train acc: 0.9013\n",
      "loss: 0.6486969709396362, train acc: 0.9035\n",
      "loss: 0.565628120303154, train acc: 0.9002\n",
      "loss: 0.6312119781970977, train acc: 0.9039\n",
      "loss: 0.6772743046283722, train acc: 0.9048\n",
      "loss: 0.6325342327356338, train acc: 0.9017\n",
      "epoch: 58, loss: 0.14618578553199768, train acc: 0.9017, test acc: 0.86\n",
      "loss: 0.4993244707584381, train acc: 0.9049\n",
      "loss: 0.6078239977359772, train acc: 0.9017\n",
      "loss: 0.6052914291620255, train acc: 0.9026\n",
      "loss: 0.5909076720476151, train acc: 0.9028\n",
      "loss: 0.6502982288599014, train acc: 0.9037\n",
      "loss: 0.6872755765914917, train acc: 0.9042\n",
      "loss: 0.6108815878629684, train acc: 0.9026\n",
      "loss: 0.6232935965061188, train acc: 0.9017\n",
      "epoch: 59, loss: 0.3877524733543396, train acc: 0.9017, test acc: 0.8573\n",
      "loss: 0.5091689229011536, train acc: 0.9037\n",
      "loss: 0.6290552079677582, train acc: 0.9019\n",
      "loss: 0.6166396558284759, train acc: 0.9035\n",
      "loss: 0.6181116789579392, train acc: 0.9029\n",
      "loss: 0.5733869880437851, train acc: 0.9025\n",
      "loss: 0.617395207285881, train acc: 0.905\n",
      "loss: 0.6383383810520172, train acc: 0.9045\n",
      "loss: 0.63932124376297, train acc: 0.9039\n",
      "epoch: 60, loss: 0.4051418900489807, train acc: 0.9039, test acc: 0.8603\n",
      "loss: 0.5106317400932312, train acc: 0.9043\n",
      "loss: 0.6777271509170533, train acc: 0.9032\n",
      "loss: 0.6148567020893096, train acc: 0.9032\n",
      "loss: 0.6289423078298568, train acc: 0.9036\n",
      "loss: 0.6269655108451844, train acc: 0.9034\n",
      "loss: 0.6261136949062347, train acc: 0.9061\n",
      "loss: 0.6584921956062317, train acc: 0.9058\n",
      "loss: 0.6295088201761245, train acc: 0.9039\n",
      "epoch: 61, loss: 0.1563500612974167, train acc: 0.9039, test acc: 0.8579\n",
      "loss: 0.4810645282268524, train acc: 0.9053\n",
      "loss: 0.6625540882349015, train acc: 0.9033\n",
      "loss: 0.6035099804401398, train acc: 0.9023\n",
      "loss: 0.6295485317707061, train acc: 0.9025\n",
      "loss: 0.5792353838682175, train acc: 0.9033\n",
      "loss: 0.6584022641181946, train acc: 0.9057\n",
      "loss: 0.6598249614238739, train acc: 0.9031\n",
      "loss: 0.597239488363266, train acc: 0.904\n",
      "epoch: 62, loss: 0.265705943107605, train acc: 0.904, test acc: 0.858\n",
      "loss: 0.6046417951583862, train acc: 0.9055\n",
      "loss: 0.6536380171775817, train acc: 0.9048\n",
      "loss: 0.610646054148674, train acc: 0.9036\n",
      "loss: 0.6107791244983674, train acc: 0.9026\n",
      "loss: 0.587309530377388, train acc: 0.9047\n",
      "loss: 0.6377572357654572, train acc: 0.9073\n",
      "loss: 0.6263501077890397, train acc: 0.9064\n",
      "loss: 0.6259579539299012, train acc: 0.9036\n",
      "epoch: 63, loss: 0.4509800672531128, train acc: 0.9036, test acc: 0.8584\n",
      "loss: 0.5255711674690247, train acc: 0.9037\n",
      "loss: 0.6338292479515075, train acc: 0.9042\n",
      "loss: 0.6508769869804383, train acc: 0.9051\n",
      "loss: 0.6326210707426071, train acc: 0.9056\n",
      "loss: 0.5483855754137039, train acc: 0.9028\n",
      "loss: 0.6798170387744904, train acc: 0.9055\n",
      "loss: 0.6084194183349609, train acc: 0.9047\n",
      "loss: 0.5970520704984665, train acc: 0.9023\n",
      "epoch: 64, loss: 0.4127618074417114, train acc: 0.9023, test acc: 0.8594\n",
      "loss: 0.5038955807685852, train acc: 0.9046\n",
      "loss: 0.6235936790704727, train acc: 0.905\n",
      "loss: 0.6141608417034149, train acc: 0.9041\n",
      "loss: 0.5842274844646453, train acc: 0.905\n",
      "loss: 0.6062813639640808, train acc: 0.9048\n",
      "loss: 0.5935570806264877, train acc: 0.9075\n",
      "loss: 0.6691813826560974, train acc: 0.9068\n",
      "loss: 0.594627970457077, train acc: 0.9039\n",
      "epoch: 65, loss: 0.4958144426345825, train acc: 0.9039, test acc: 0.858\n",
      "loss: 0.45935019850730896, train acc: 0.9043\n",
      "loss: 0.625273683667183, train acc: 0.9043\n",
      "loss: 0.6277261078357697, train acc: 0.9028\n",
      "loss: 0.6102926671504975, train acc: 0.904\n",
      "loss: 0.6435581535100937, train acc: 0.9039\n",
      "loss: 0.6252369165420533, train acc: 0.906\n",
      "loss: 0.6521784901618958, train acc: 0.9063\n",
      "loss: 0.5988028943538666, train acc: 0.9038\n",
      "epoch: 66, loss: 0.2622685432434082, train acc: 0.9038, test acc: 0.8572\n",
      "loss: 0.4427829682826996, train acc: 0.9046\n",
      "loss: 0.6389831811189651, train acc: 0.9046\n",
      "loss: 0.588088122010231, train acc: 0.9051\n",
      "loss: 0.6187465906143188, train acc: 0.9054\n",
      "loss: 0.6013646513223648, train acc: 0.9054\n",
      "loss: 0.630517554283142, train acc: 0.9071\n",
      "loss: 0.6077225804328918, train acc: 0.9077\n",
      "loss: 0.6179528057575225, train acc: 0.9057\n",
      "epoch: 67, loss: 0.28154903650283813, train acc: 0.9057, test acc: 0.8589\n",
      "loss: 0.4416460394859314, train acc: 0.9045\n",
      "loss: 0.6683309078216553, train acc: 0.9052\n",
      "loss: 0.6039527982473374, train acc: 0.9051\n",
      "loss: 0.5981070071458816, train acc: 0.9057\n",
      "loss: 0.6042849481105804, train acc: 0.9055\n",
      "loss: 0.6275169759988785, train acc: 0.9065\n",
      "loss: 0.667653226852417, train acc: 0.9061\n",
      "loss: 0.6103810399770737, train acc: 0.9059\n",
      "epoch: 68, loss: 0.2920863628387451, train acc: 0.9059, test acc: 0.8576\n",
      "loss: 0.5250295996665955, train acc: 0.9045\n",
      "loss: 0.6339154064655304, train acc: 0.906\n",
      "loss: 0.6018057405948639, train acc: 0.9067\n",
      "loss: 0.654817807674408, train acc: 0.9062\n",
      "loss: 0.5898346364498138, train acc: 0.907\n",
      "loss: 0.6155489355325698, train acc: 0.9085\n",
      "loss: 0.6142449855804444, train acc: 0.9075\n",
      "loss: 0.6130714386701583, train acc: 0.9056\n",
      "epoch: 69, loss: 0.348445862531662, train acc: 0.9056, test acc: 0.8578\n",
      "loss: 0.4448257386684418, train acc: 0.9067\n",
      "loss: 0.6266273647546768, train acc: 0.9053\n",
      "loss: 0.6300031810998916, train acc: 0.9051\n",
      "loss: 0.6451878845691681, train acc: 0.9066\n",
      "loss: 0.6177460491657257, train acc: 0.907\n",
      "loss: 0.6279728233814239, train acc: 0.9057\n",
      "loss: 0.6653862655162811, train acc: 0.9056\n",
      "loss: 0.6335969686508178, train acc: 0.9023\n",
      "epoch: 70, loss: 0.2893228530883789, train acc: 0.9023, test acc: 0.8574\n",
      "loss: 0.4752058684825897, train acc: 0.9047\n",
      "loss: 0.6202628284692764, train acc: 0.9043\n",
      "loss: 0.6032466143369675, train acc: 0.9046\n",
      "loss: 0.6334915041923523, train acc: 0.9057\n",
      "loss: 0.6012803763151169, train acc: 0.9042\n",
      "loss: 0.5828159511089325, train acc: 0.9074\n",
      "loss: 0.6329960763454437, train acc: 0.9074\n",
      "loss: 0.602601844072342, train acc: 0.9039\n",
      "epoch: 71, loss: 0.5477153062820435, train acc: 0.9039, test acc: 0.8573\n",
      "loss: 0.46215465664863586, train acc: 0.9056\n",
      "loss: 0.59411461353302, train acc: 0.9036\n",
      "loss: 0.6269622266292572, train acc: 0.9049\n",
      "loss: 0.5979345947504043, train acc: 0.9053\n",
      "loss: 0.5711501568555832, train acc: 0.9031\n",
      "loss: 0.6364803701639176, train acc: 0.9059\n",
      "loss: 0.6121991813182831, train acc: 0.9066\n",
      "loss: 0.6088255047798157, train acc: 0.903\n",
      "epoch: 72, loss: 0.4750441014766693, train acc: 0.903, test acc: 0.8536\n",
      "loss: 0.43519484996795654, train acc: 0.9029\n",
      "loss: 0.5741825014352798, train acc: 0.9039\n",
      "loss: 0.623453962802887, train acc: 0.9048\n",
      "loss: 0.6397253811359406, train acc: 0.9066\n",
      "loss: 0.5965695321559906, train acc: 0.9068\n",
      "loss: 0.6476094663143158, train acc: 0.909\n",
      "loss: 0.6051080733537674, train acc: 0.908\n",
      "loss: 0.6342345118522644, train acc: 0.9045\n",
      "epoch: 73, loss: 0.4560520052909851, train acc: 0.9045, test acc: 0.8583\n",
      "loss: 0.515430748462677, train acc: 0.9069\n",
      "loss: 0.5886832177639008, train acc: 0.9047\n",
      "loss: 0.6380062162876129, train acc: 0.9036\n",
      "loss: 0.6017044961452485, train acc: 0.9068\n",
      "loss: 0.6105195581912994, train acc: 0.9057\n",
      "loss: 0.6287088453769684, train acc: 0.9072\n",
      "loss: 0.6445856869220734, train acc: 0.9076\n",
      "loss: 0.5958608657121658, train acc: 0.9041\n",
      "epoch: 74, loss: 0.20155566930770874, train acc: 0.9041, test acc: 0.8569\n",
      "loss: 0.4486265778541565, train acc: 0.9065\n",
      "loss: 0.6339785993099213, train acc: 0.9058\n",
      "loss: 0.6791430234909057, train acc: 0.9047\n",
      "loss: 0.6441291451454163, train acc: 0.9071\n",
      "loss: 0.5845184892416, train acc: 0.905\n",
      "loss: 0.6328248262405396, train acc: 0.9081\n",
      "loss: 0.6573744595050812, train acc: 0.9077\n",
      "loss: 0.6042816698551178, train acc: 0.9047\n",
      "epoch: 75, loss: 0.3855549395084381, train acc: 0.9047, test acc: 0.8588\n",
      "loss: 0.5093618631362915, train acc: 0.9077\n",
      "loss: 0.6411394000053405, train acc: 0.9046\n",
      "loss: 0.6313751697540283, train acc: 0.9041\n",
      "loss: 0.6254698723554611, train acc: 0.9065\n",
      "loss: 0.5535355150699616, train acc: 0.9057\n",
      "loss: 0.6364796787500382, train acc: 0.9091\n",
      "loss: 0.6754292368888855, train acc: 0.9081\n",
      "loss: 0.6420465409755707, train acc: 0.9048\n",
      "epoch: 76, loss: 0.33806416392326355, train acc: 0.9048, test acc: 0.8553\n",
      "loss: 0.5478514432907104, train acc: 0.9061\n",
      "loss: 0.6345957159996033, train acc: 0.905\n",
      "loss: 0.5915580451488495, train acc: 0.9069\n",
      "loss: 0.6462811827659607, train acc: 0.9057\n",
      "loss: 0.587374922633171, train acc: 0.9075\n",
      "loss: 0.5979689568281173, train acc: 0.9086\n",
      "loss: 0.6518187522888184, train acc: 0.9082\n",
      "loss: 0.5899540096521377, train acc: 0.905\n",
      "epoch: 77, loss: 0.3051937520503998, train acc: 0.905, test acc: 0.8573\n",
      "loss: 0.42386743426322937, train acc: 0.9062\n",
      "loss: 0.6279514342546463, train acc: 0.9064\n",
      "loss: 0.607921352982521, train acc: 0.9055\n",
      "loss: 0.5980757862329483, train acc: 0.9054\n",
      "loss: 0.5938532590866089, train acc: 0.9052\n",
      "loss: 0.6257748574018478, train acc: 0.9079\n",
      "loss: 0.6067045599222183, train acc: 0.907\n",
      "loss: 0.5787225037813186, train acc: 0.905\n",
      "epoch: 78, loss: 0.3495074510574341, train acc: 0.905, test acc: 0.8555\n",
      "loss: 0.596860408782959, train acc: 0.9066\n",
      "loss: 0.6396039664745331, train acc: 0.9053\n",
      "loss: 0.6189944624900818, train acc: 0.9039\n",
      "loss: 0.6345367699861526, train acc: 0.9081\n",
      "loss: 0.5765011101961136, train acc: 0.9068\n",
      "loss: 0.6242145776748658, train acc: 0.9091\n",
      "loss: 0.6556488513946533, train acc: 0.9071\n",
      "loss: 0.5738787174224853, train acc: 0.9039\n",
      "epoch: 79, loss: 0.17310626804828644, train acc: 0.9039, test acc: 0.8555\n",
      "#####training and testing end with K:5, P:0.1######\n",
      "#####training and testing start with K:5, P:0.5######\n",
      "loss: 2.418597459793091, train acc: 0.1045\n",
      "loss: 2.3062750101089478, train acc: 0.1\n",
      "loss: 2.255602216720581, train acc: 0.1\n",
      "loss: 2.1492449045181274, train acc: 0.1\n",
      "loss: 2.0868237018585205, train acc: 0.1641\n",
      "loss: 2.062049961090088, train acc: 0.1714\n",
      "loss: 2.0400437355041503, train acc: 0.1761\n",
      "loss: 1.977361023426056, train acc: 0.217\n",
      "epoch: 0, loss: 1.6094251871109009, train acc: 0.217, test acc: 0.2864\n",
      "loss: 1.8868154287338257, train acc: 0.2949\n",
      "loss: 1.9450339913368224, train acc: 0.3235\n",
      "loss: 1.9777987003326416, train acc: 0.3295\n",
      "loss: 1.9730211973190308, train acc: 0.3934\n",
      "loss: 1.9310180068016052, train acc: 0.4134\n",
      "loss: 1.8958694219589234, train acc: 0.4143\n",
      "loss: 1.932673418521881, train acc: 0.4205\n",
      "loss: 1.8736926198005677, train acc: 0.422\n",
      "epoch: 1, loss: 1.5003478527069092, train acc: 0.422, test acc: 0.4469\n",
      "loss: 1.85482919216156, train acc: 0.4442\n",
      "loss: 1.8672340989112854, train acc: 0.4505\n",
      "loss: 1.906982946395874, train acc: 0.4613\n",
      "loss: 1.8817340731620789, train acc: 0.472\n",
      "loss: 1.8585250735282899, train acc: 0.4751\n",
      "loss: 1.8706863164901733, train acc: 0.4823\n",
      "loss: 1.8524514436721802, train acc: 0.4853\n",
      "loss: 1.7971001863479614, train acc: 0.4831\n",
      "epoch: 2, loss: 1.6964685916900635, train acc: 0.4831, test acc: 0.4918\n",
      "loss: 1.7454688549041748, train acc: 0.4931\n",
      "loss: 1.804126000404358, train acc: 0.4969\n",
      "loss: 1.8470767140388489, train acc: 0.5288\n",
      "loss: 1.860184645652771, train acc: 0.5544\n",
      "loss: 1.7955896496772765, train acc: 0.57\n",
      "loss: 1.809491753578186, train acc: 0.5904\n",
      "loss: 1.8138573050498963, train acc: 0.5869\n",
      "loss: 1.7719661593437195, train acc: 0.5893\n",
      "epoch: 3, loss: 1.8225494623184204, train acc: 0.5893, test acc: 0.6031\n",
      "loss: 1.773895025253296, train acc: 0.5973\n",
      "loss: 1.7411991834640503, train acc: 0.6003\n",
      "loss: 1.8186226963996888, train acc: 0.5995\n",
      "loss: 1.834104025363922, train acc: 0.5972\n",
      "loss: 1.7930423736572265, train acc: 0.5956\n",
      "loss: 1.7771336317062378, train acc: 0.6008\n",
      "loss: 1.7936014413833619, train acc: 0.6009\n",
      "loss: 1.7303105235099792, train acc: 0.6029\n",
      "epoch: 4, loss: 1.5626106262207031, train acc: 0.6029, test acc: 0.6087\n",
      "loss: 1.7707098722457886, train acc: 0.6005\n",
      "loss: 1.7355426788330077, train acc: 0.6067\n",
      "loss: 1.794824755191803, train acc: 0.6019\n",
      "loss: 1.7735678553581238, train acc: 0.6083\n",
      "loss: 1.7624985098838806, train acc: 0.6061\n",
      "loss: 1.7742103815078736, train acc: 0.6099\n",
      "loss: 1.744441068172455, train acc: 0.609\n",
      "loss: 1.6987176179885863, train acc: 0.605\n",
      "epoch: 5, loss: 1.6632716655731201, train acc: 0.605, test acc: 0.6131\n",
      "loss: 1.63671875, train acc: 0.6071\n",
      "loss: 1.706280481815338, train acc: 0.6112\n",
      "loss: 1.740749752521515, train acc: 0.6039\n",
      "loss: 1.7471296429634093, train acc: 0.6125\n",
      "loss: 1.7485950350761414, train acc: 0.6063\n",
      "loss: 1.7473055005073548, train acc: 0.6592\n",
      "loss: 1.7252806305885315, train acc: 0.662\n",
      "loss: 1.7355928301811219, train acc: 0.6619\n",
      "epoch: 6, loss: 1.6553359031677246, train acc: 0.6619, test acc: 0.6586\n",
      "loss: 1.6450294256210327, train acc: 0.6626\n",
      "loss: 1.6959092020988464, train acc: 0.6623\n",
      "loss: 1.7269009947776794, train acc: 0.6601\n",
      "loss: 1.7806550502777099, train acc: 0.6603\n",
      "loss: 1.74993896484375, train acc: 0.659\n",
      "loss: 1.7327691078186036, train acc: 0.6621\n",
      "loss: 1.7311593651771546, train acc: 0.6657\n",
      "loss: 1.6962085843086243, train acc: 0.6695\n",
      "epoch: 7, loss: 1.9109808206558228, train acc: 0.6695, test acc: 0.6665\n",
      "loss: 1.5706970691680908, train acc: 0.6695\n",
      "loss: 1.6766205549240112, train acc: 0.6708\n",
      "loss: 1.744146728515625, train acc: 0.673\n",
      "loss: 1.7257242441177367, train acc: 0.67\n",
      "loss: 1.7802944898605346, train acc: 0.6723\n",
      "loss: 1.722832691669464, train acc: 0.67\n",
      "loss: 1.7236170291900634, train acc: 0.6747\n",
      "loss: 1.6697711229324341, train acc: 0.6728\n",
      "epoch: 8, loss: 1.5710234642028809, train acc: 0.6728, test acc: 0.6711\n",
      "loss: 1.75018310546875, train acc: 0.6742\n",
      "loss: 1.7130974054336547, train acc: 0.674\n",
      "loss: 1.6970840215682983, train acc: 0.6745\n",
      "loss: 1.7370539903640747, train acc: 0.6742\n",
      "loss: 1.7143728137016296, train acc: 0.6769\n",
      "loss: 1.703971016407013, train acc: 0.6797\n",
      "loss: 1.675433611869812, train acc: 0.6774\n",
      "loss: 1.6825141668319703, train acc: 0.6764\n",
      "epoch: 9, loss: 1.6344187259674072, train acc: 0.6764, test acc: 0.6688\n",
      "loss: 1.790283441543579, train acc: 0.6757\n",
      "loss: 1.6616001009941102, train acc: 0.6727\n",
      "loss: 1.6868486285209656, train acc: 0.677\n",
      "loss: 1.6803590059280396, train acc: 0.6785\n",
      "loss: 1.6738075017929077, train acc: 0.6825\n",
      "loss: 1.7315802097320556, train acc: 0.6773\n",
      "loss: 1.6507495403289796, train acc: 0.6832\n",
      "loss: 1.6639259696006774, train acc: 0.679\n",
      "epoch: 10, loss: 1.87923264503479, train acc: 0.679, test acc: 0.6723\n",
      "loss: 1.7053446769714355, train acc: 0.6827\n",
      "loss: 1.660779881477356, train acc: 0.6783\n",
      "loss: 1.6986497044563293, train acc: 0.6802\n",
      "loss: 1.7148741841316224, train acc: 0.6787\n",
      "loss: 1.6842443823814393, train acc: 0.6833\n",
      "loss: 1.690738272666931, train acc: 0.6817\n",
      "loss: 1.70918710231781, train acc: 0.6829\n",
      "loss: 1.6922146558761597, train acc: 0.6773\n",
      "epoch: 11, loss: 2.0503063201904297, train acc: 0.6773, test acc: 0.6703\n",
      "loss: 1.6978033781051636, train acc: 0.6819\n",
      "loss: 1.6819862604141236, train acc: 0.6798\n",
      "loss: 1.682033634185791, train acc: 0.6829\n",
      "loss: 1.7114088654518127, train acc: 0.682\n",
      "loss: 1.719629693031311, train acc: 0.685\n",
      "loss: 1.6828978419303895, train acc: 0.6854\n",
      "loss: 1.6635047674179078, train acc: 0.6868\n",
      "loss: 1.671580731868744, train acc: 0.7217\n",
      "epoch: 12, loss: 1.329281210899353, train acc: 0.7217, test acc: 0.7047\n",
      "loss: 1.7601819038391113, train acc: 0.7257\n",
      "loss: 1.6806144833564758, train acc: 0.7197\n",
      "loss: 1.6609806180000306, train acc: 0.7225\n",
      "loss: 1.7412460803985597, train acc: 0.7202\n",
      "loss: 1.6868938684463501, train acc: 0.7256\n",
      "loss: 1.6939446568489074, train acc: 0.7311\n",
      "loss: 1.680193018913269, train acc: 0.7271\n",
      "loss: 1.667731761932373, train acc: 0.7188\n",
      "epoch: 13, loss: 1.2260479927062988, train acc: 0.7188, test acc: 0.7064\n",
      "loss: 1.6543270349502563, train acc: 0.727\n",
      "loss: 1.6310871720314026, train acc: 0.7282\n",
      "loss: 1.7179051280021667, train acc: 0.7239\n",
      "loss: 1.6883066177368165, train acc: 0.7278\n",
      "loss: 1.6736871600151062, train acc: 0.7334\n",
      "loss: 1.6801600813865663, train acc: 0.7296\n",
      "loss: 1.6978925347328186, train acc: 0.7276\n",
      "loss: 1.664262592792511, train acc: 0.728\n",
      "epoch: 14, loss: 1.0323984622955322, train acc: 0.728, test acc: 0.709\n",
      "loss: 1.7097952365875244, train acc: 0.732\n",
      "loss: 1.6548513889312744, train acc: 0.7303\n",
      "loss: 1.6800936698913573, train acc: 0.7294\n",
      "loss: 1.6685040831565856, train acc: 0.7302\n",
      "loss: 1.683681035041809, train acc: 0.7307\n",
      "loss: 1.66775643825531, train acc: 0.736\n",
      "loss: 1.6380550861358643, train acc: 0.732\n",
      "loss: 1.70009126663208, train acc: 0.7304\n",
      "epoch: 15, loss: 1.783357858657837, train acc: 0.7304, test acc: 0.7135\n",
      "loss: 1.7270026206970215, train acc: 0.7366\n",
      "loss: 1.6130119681358337, train acc: 0.7294\n",
      "loss: 1.6499370217323304, train acc: 0.7232\n",
      "loss: 1.6925723195075988, train acc: 0.7292\n",
      "loss: 1.662909710407257, train acc: 0.7387\n",
      "loss: 1.6683337450027467, train acc: 0.7407\n",
      "loss: 1.6446698188781739, train acc: 0.7399\n",
      "loss: 1.6560561895370483, train acc: 0.7361\n",
      "epoch: 16, loss: 1.2378103733062744, train acc: 0.7361, test acc: 0.7173\n",
      "loss: 1.8307470083236694, train acc: 0.7414\n",
      "loss: 1.6624536991119385, train acc: 0.7371\n",
      "loss: 1.6869932532310485, train acc: 0.731\n",
      "loss: 1.7029177784919738, train acc: 0.7379\n",
      "loss: 1.6975538730621338, train acc: 0.7426\n",
      "loss: 1.7043166041374207, train acc: 0.744\n",
      "loss: 1.639240849018097, train acc: 0.7468\n",
      "loss: 1.6204424262046815, train acc: 0.742\n",
      "epoch: 17, loss: 1.6359692811965942, train acc: 0.742, test acc: 0.7131\n",
      "loss: 1.599179983139038, train acc: 0.7415\n",
      "loss: 1.6696643471717834, train acc: 0.746\n",
      "loss: 1.636958122253418, train acc: 0.7394\n",
      "loss: 1.6613080263137818, train acc: 0.7491\n",
      "loss: 1.6795390844345093, train acc: 0.7512\n",
      "loss: 1.6717843532562255, train acc: 0.7524\n",
      "loss: 1.69251309633255, train acc: 0.7449\n",
      "loss: 1.6833237051963805, train acc: 0.7473\n",
      "epoch: 18, loss: 1.9693161249160767, train acc: 0.7473, test acc: 0.7173\n",
      "loss: 1.620282530784607, train acc: 0.7496\n",
      "loss: 1.616770601272583, train acc: 0.7421\n",
      "loss: 1.6824228286743164, train acc: 0.7454\n",
      "loss: 1.6962172985076904, train acc: 0.7463\n",
      "loss: 1.650077509880066, train acc: 0.7535\n",
      "loss: 1.6461685657501222, train acc: 0.7553\n",
      "loss: 1.6271520733833313, train acc: 0.7536\n",
      "loss: 1.6088072657585144, train acc: 0.7429\n",
      "epoch: 19, loss: 1.6687122583389282, train acc: 0.7429, test acc: 0.7223\n",
      "loss: 1.6969860792160034, train acc: 0.7524\n",
      "loss: 1.6367989301681518, train acc: 0.7445\n",
      "loss: 1.651234257221222, train acc: 0.747\n",
      "loss: 1.6577566385269165, train acc: 0.7474\n",
      "loss: 1.6510584712028504, train acc: 0.7601\n",
      "loss: 1.6004897236824036, train acc: 0.7602\n",
      "loss: 1.6512104868888855, train acc: 0.7543\n",
      "loss: 1.5881124496459962, train acc: 0.7475\n",
      "epoch: 20, loss: 1.7980636358261108, train acc: 0.7475, test acc: 0.7154\n",
      "loss: 1.651450276374817, train acc: 0.7501\n",
      "loss: 1.6430017828941346, train acc: 0.7495\n",
      "loss: 1.637818706035614, train acc: 0.7563\n",
      "loss: 1.6762783527374268, train acc: 0.7517\n",
      "loss: 1.710002613067627, train acc: 0.7562\n",
      "loss: 1.6542797684669495, train acc: 0.7591\n",
      "loss: 1.6732254862785338, train acc: 0.7559\n",
      "loss: 1.6041826248168944, train acc: 0.7532\n",
      "epoch: 21, loss: 1.7925972938537598, train acc: 0.7532, test acc: 0.7236\n",
      "loss: 1.6746313571929932, train acc: 0.7508\n",
      "loss: 1.6807962298393249, train acc: 0.7539\n",
      "loss: 1.6590207695961, train acc: 0.7503\n",
      "loss: 1.6900146365165711, train acc: 0.7498\n",
      "loss: 1.6447052478790283, train acc: 0.7608\n",
      "loss: 1.6725847125053406, train acc: 0.7629\n",
      "loss: 1.654390037059784, train acc: 0.763\n",
      "loss: 1.605164682865143, train acc: 0.7561\n",
      "epoch: 22, loss: 1.57323157787323, train acc: 0.7561, test acc: 0.7305\n",
      "loss: 1.6391000747680664, train acc: 0.7609\n",
      "loss: 1.6173559308052063, train acc: 0.7589\n",
      "loss: 1.6446617245674133, train acc: 0.7508\n",
      "loss: 1.659686541557312, train acc: 0.7547\n",
      "loss: 1.6879409313201905, train acc: 0.7615\n",
      "loss: 1.6536558270454407, train acc: 0.7577\n",
      "loss: 1.6601580500602722, train acc: 0.7579\n",
      "loss: 1.656531274318695, train acc: 0.7547\n",
      "epoch: 23, loss: 1.3961082696914673, train acc: 0.7547, test acc: 0.7199\n",
      "loss: 1.548270583152771, train acc: 0.7555\n",
      "loss: 1.623479926586151, train acc: 0.7616\n",
      "loss: 1.649806523323059, train acc: 0.759\n",
      "loss: 1.6443479299545287, train acc: 0.761\n",
      "loss: 1.6315390467643738, train acc: 0.7619\n",
      "loss: 1.6532330751419066, train acc: 0.7643\n",
      "loss: 1.6540694117546082, train acc: 0.7566\n",
      "loss: 1.6278836250305175, train acc: 0.7566\n",
      "epoch: 24, loss: 1.7081960439682007, train acc: 0.7566, test acc: 0.7212\n",
      "loss: 1.6748663187026978, train acc: 0.7567\n",
      "loss: 1.6475544571876526, train acc: 0.7626\n",
      "loss: 1.6667990803718566, train acc: 0.7593\n",
      "loss: 1.6802947163581847, train acc: 0.7585\n",
      "loss: 1.6795930981636047, train acc: 0.7674\n",
      "loss: 1.6548262119293213, train acc: 0.7691\n",
      "loss: 1.6634071707725524, train acc: 0.7622\n",
      "loss: 1.619482123851776, train acc: 0.7525\n",
      "epoch: 25, loss: 1.2926934957504272, train acc: 0.7525, test acc: 0.7302\n",
      "loss: 1.621922254562378, train acc: 0.763\n",
      "loss: 1.6659952998161316, train acc: 0.7573\n",
      "loss: 1.6728874683380126, train acc: 0.7569\n",
      "loss: 1.6874426126480102, train acc: 0.7559\n",
      "loss: 1.6783865571022034, train acc: 0.7696\n",
      "loss: 1.6278860688209533, train acc: 0.7704\n",
      "loss: 1.652576780319214, train acc: 0.7654\n",
      "loss: 1.6309974670410157, train acc: 0.7685\n",
      "epoch: 26, loss: 1.5327527523040771, train acc: 0.7685, test acc: 0.7314\n",
      "loss: 1.5092564821243286, train acc: 0.7695\n",
      "loss: 1.6318818092346192, train acc: 0.7607\n",
      "loss: 1.6194863796234131, train acc: 0.757\n",
      "loss: 1.642173409461975, train acc: 0.7619\n",
      "loss: 1.6519331097602845, train acc: 0.7677\n",
      "loss: 1.6284935355186463, train acc: 0.7704\n",
      "loss: 1.6843133330345155, train acc: 0.7637\n",
      "loss: 1.602971351146698, train acc: 0.7688\n",
      "epoch: 27, loss: 1.4416574239730835, train acc: 0.7688, test acc: 0.7309\n",
      "loss: 1.4168519973754883, train acc: 0.7701\n",
      "loss: 1.6419741988182068, train acc: 0.7661\n",
      "loss: 1.6307238698005677, train acc: 0.7636\n",
      "loss: 1.6816689372062683, train acc: 0.7576\n",
      "loss: 1.6811954855918885, train acc: 0.7714\n",
      "loss: 1.6564619302749635, train acc: 0.7721\n",
      "loss: 1.6604697108268738, train acc: 0.7738\n",
      "loss: 1.6068716645240784, train acc: 0.7597\n",
      "epoch: 28, loss: 1.9200941324234009, train acc: 0.7597, test acc: 0.7306\n",
      "loss: 1.6022281646728516, train acc: 0.7702\n",
      "loss: 1.6210196614265442, train acc: 0.7675\n",
      "loss: 1.607982099056244, train acc: 0.7647\n",
      "loss: 1.6170017719268799, train acc: 0.7708\n",
      "loss: 1.649057722091675, train acc: 0.7768\n",
      "loss: 1.5982841849327087, train acc: 0.7765\n",
      "loss: 1.6339091658592224, train acc: 0.7702\n",
      "loss: 1.6237653255462647, train acc: 0.7654\n",
      "epoch: 29, loss: 1.4946823120117188, train acc: 0.7654, test acc: 0.7368\n",
      "loss: 1.644561767578125, train acc: 0.7743\n",
      "loss: 1.5805512547492981, train acc: 0.7667\n",
      "loss: 1.6669244647026062, train acc: 0.7663\n",
      "loss: 1.6446251630783082, train acc: 0.7717\n",
      "loss: 1.6460867881774903, train acc: 0.775\n",
      "loss: 1.6317482709884643, train acc: 0.7765\n",
      "loss: 1.5968453764915467, train acc: 0.7687\n",
      "loss: 1.6302797317504882, train acc: 0.77\n",
      "epoch: 30, loss: 1.9200998544692993, train acc: 0.77, test acc: 0.734\n",
      "loss: 1.663664698600769, train acc: 0.7722\n",
      "loss: 1.6274277567863464, train acc: 0.7649\n",
      "loss: 1.6250371098518372, train acc: 0.771\n",
      "loss: 1.6638956308364867, train acc: 0.7623\n",
      "loss: 1.663152849674225, train acc: 0.776\n",
      "loss: 1.6318289756774902, train acc: 0.778\n",
      "loss: 1.6402199506759643, train acc: 0.7718\n",
      "loss: 1.647127592563629, train acc: 0.7692\n",
      "epoch: 31, loss: 1.1740037202835083, train acc: 0.7692, test acc: 0.7409\n",
      "loss: 1.667794942855835, train acc: 0.7789\n",
      "loss: 1.6165621638298036, train acc: 0.772\n",
      "loss: 1.5880466818809509, train acc: 0.7728\n",
      "loss: 1.5935802578926086, train acc: 0.7762\n",
      "loss: 1.6358408451080322, train acc: 0.7764\n",
      "loss: 1.6883031845092773, train acc: 0.7785\n",
      "loss: 1.6322576522827148, train acc: 0.7743\n",
      "loss: 1.6369343400001526, train acc: 0.7734\n",
      "epoch: 32, loss: 1.333155870437622, train acc: 0.7734, test acc: 0.7363\n",
      "loss: 1.5896196365356445, train acc: 0.7791\n",
      "loss: 1.5824063658714294, train acc: 0.7714\n",
      "loss: 1.6353607416152953, train acc: 0.771\n",
      "loss: 1.6837625741958617, train acc: 0.7713\n",
      "loss: 1.6380654096603393, train acc: 0.7736\n",
      "loss: 1.6469782948493958, train acc: 0.776\n",
      "loss: 1.6315810918807983, train acc: 0.7655\n",
      "loss: 1.6262914896011353, train acc: 0.7708\n",
      "epoch: 33, loss: 1.4354476928710938, train acc: 0.7708, test acc: 0.7429\n",
      "loss: 1.675578236579895, train acc: 0.7796\n",
      "loss: 1.6212738394737243, train acc: 0.7698\n",
      "loss: 1.6507256388664246, train acc: 0.7746\n",
      "loss: 1.645571494102478, train acc: 0.7729\n",
      "loss: 1.6553476333618165, train acc: 0.7767\n",
      "loss: 1.6097956538200378, train acc: 0.7794\n",
      "loss: 1.6109776496887207, train acc: 0.7775\n",
      "loss: 1.6204938173294068, train acc: 0.7632\n",
      "epoch: 34, loss: 1.9063981771469116, train acc: 0.7632, test acc: 0.7333\n",
      "loss: 1.697782039642334, train acc: 0.7807\n",
      "loss: 1.6211562037467957, train acc: 0.7765\n",
      "loss: 1.6750513792037964, train acc: 0.7744\n",
      "loss: 1.6536633491516113, train acc: 0.7615\n",
      "loss: 1.648977017402649, train acc: 0.7751\n",
      "loss: 1.6161683440208434, train acc: 0.7758\n",
      "loss: 1.6376312255859375, train acc: 0.7744\n",
      "loss: 1.6195060968399049, train acc: 0.769\n",
      "epoch: 35, loss: 1.6852917671203613, train acc: 0.769, test acc: 0.7371\n",
      "loss: 1.678917407989502, train acc: 0.7823\n",
      "loss: 1.6508878350257874, train acc: 0.7786\n",
      "loss: 1.6001195192337037, train acc: 0.7765\n",
      "loss: 1.6760182499885559, train acc: 0.7752\n",
      "loss: 1.5644601821899413, train acc: 0.7792\n",
      "loss: 1.6594520926475524, train acc: 0.7801\n",
      "loss: 1.6088807106018066, train acc: 0.7695\n",
      "loss: 1.6303317427635193, train acc: 0.7695\n",
      "epoch: 36, loss: 1.4943673610687256, train acc: 0.7695, test acc: 0.7303\n",
      "loss: 1.6984593868255615, train acc: 0.7749\n",
      "loss: 1.5923685669898986, train acc: 0.7747\n",
      "loss: 1.596154475212097, train acc: 0.7742\n",
      "loss: 1.6350983500480651, train acc: 0.7746\n",
      "loss: 1.619699192047119, train acc: 0.7776\n",
      "loss: 1.6575939893722533, train acc: 0.7775\n",
      "loss: 1.6763306379318237, train acc: 0.7773\n",
      "loss: 1.64422664642334, train acc: 0.7677\n",
      "epoch: 37, loss: 1.5446866750717163, train acc: 0.7677, test acc: 0.7417\n",
      "loss: 1.6957730054855347, train acc: 0.7831\n",
      "loss: 1.5854684710502625, train acc: 0.7791\n",
      "loss: 1.6267103791236877, train acc: 0.7727\n",
      "loss: 1.6336542606353759, train acc: 0.7773\n",
      "loss: 1.5936629891395568, train acc: 0.7759\n",
      "loss: 1.6307344913482666, train acc: 0.782\n",
      "loss: 1.586676013469696, train acc: 0.7731\n",
      "loss: 1.6086365461349488, train acc: 0.7713\n",
      "epoch: 38, loss: 1.7032454013824463, train acc: 0.7713, test acc: 0.7302\n",
      "loss: 1.743607997894287, train acc: 0.7721\n",
      "loss: 1.6543720245361329, train acc: 0.7748\n",
      "loss: 1.6434616327285767, train acc: 0.7745\n",
      "loss: 1.6621935963630676, train acc: 0.7712\n",
      "loss: 1.6668049573898316, train acc: 0.7803\n",
      "loss: 1.6157474040985107, train acc: 0.7761\n",
      "loss: 1.6468927264213562, train acc: 0.7704\n",
      "loss: 1.648292565345764, train acc: 0.7757\n",
      "epoch: 39, loss: 1.8580769300460815, train acc: 0.7757, test acc: 0.7381\n",
      "loss: 1.7085628509521484, train acc: 0.7793\n",
      "loss: 1.6177076697349548, train acc: 0.7719\n",
      "loss: 1.6690508365631103, train acc: 0.7749\n",
      "loss: 1.680731213092804, train acc: 0.7696\n",
      "loss: 1.656959342956543, train acc: 0.7827\n",
      "loss: 1.6056838870048522, train acc: 0.7859\n",
      "loss: 1.6028322339057923, train acc: 0.7802\n",
      "loss: 1.6296339750289917, train acc: 0.7792\n",
      "epoch: 40, loss: 1.1865222454071045, train acc: 0.7792, test acc: 0.7389\n",
      "loss: 1.5546207427978516, train acc: 0.7856\n",
      "loss: 1.6063749313354492, train acc: 0.7787\n",
      "loss: 1.6504811286926269, train acc: 0.7748\n",
      "loss: 1.6313469409942627, train acc: 0.7763\n",
      "loss: 1.6225272059440612, train acc: 0.7824\n",
      "loss: 1.601505982875824, train acc: 0.7804\n",
      "loss: 1.5816168785095215, train acc: 0.7751\n",
      "loss: 1.628778326511383, train acc: 0.7673\n",
      "epoch: 41, loss: 1.8040417432785034, train acc: 0.7673, test acc: 0.739\n",
      "loss: 1.68976891040802, train acc: 0.7815\n",
      "loss: 1.5966877222061158, train acc: 0.7819\n",
      "loss: 1.6288586735725403, train acc: 0.7682\n",
      "loss: 1.6361931800842284, train acc: 0.7763\n",
      "loss: 1.6587093353271485, train acc: 0.7864\n",
      "loss: 1.6238809943199157, train acc: 0.7788\n",
      "loss: 1.63681663274765, train acc: 0.7783\n",
      "loss: 1.61335232257843, train acc: 0.7788\n",
      "epoch: 42, loss: 1.5611428022384644, train acc: 0.7788, test acc: 0.7407\n",
      "loss: 1.6097911596298218, train acc: 0.7845\n",
      "loss: 1.6523040771484374, train acc: 0.7825\n",
      "loss: 1.6286886096000672, train acc: 0.7778\n",
      "loss: 1.641045129299164, train acc: 0.7774\n",
      "loss: 1.659881579875946, train acc: 0.7818\n",
      "loss: 1.615181601047516, train acc: 0.7844\n",
      "loss: 1.6550652742385865, train acc: 0.7757\n",
      "loss: 1.6527556657791138, train acc: 0.7733\n",
      "epoch: 43, loss: 1.6649597883224487, train acc: 0.7733, test acc: 0.7348\n",
      "loss: 1.6818840503692627, train acc: 0.7781\n",
      "loss: 1.6259212374687195, train acc: 0.7848\n",
      "loss: 1.6545383930206299, train acc: 0.7748\n",
      "loss: 1.6630745887756349, train acc: 0.7816\n",
      "loss: 1.652205228805542, train acc: 0.7801\n",
      "loss: 1.6278787851333618, train acc: 0.7838\n",
      "loss: 1.611669385433197, train acc: 0.7853\n",
      "loss: 1.5843022227287293, train acc: 0.7767\n",
      "epoch: 44, loss: 1.3731235265731812, train acc: 0.7767, test acc: 0.741\n",
      "loss: 1.623833417892456, train acc: 0.7858\n",
      "loss: 1.5989089727401733, train acc: 0.7865\n",
      "loss: 1.6139987468719483, train acc: 0.7835\n",
      "loss: 1.6255241274833678, train acc: 0.7802\n",
      "loss: 1.6445506930351257, train acc: 0.7888\n",
      "loss: 1.6372348427772523, train acc: 0.7857\n",
      "loss: 1.630377221107483, train acc: 0.7761\n",
      "loss: 1.6322434782981872, train acc: 0.7729\n",
      "epoch: 45, loss: 1.3416155576705933, train acc: 0.7729, test acc: 0.7391\n",
      "loss: 1.6631146669387817, train acc: 0.7877\n",
      "loss: 1.594391942024231, train acc: 0.7833\n",
      "loss: 1.6521870732307433, train acc: 0.7805\n",
      "loss: 1.628977632522583, train acc: 0.7789\n",
      "loss: 1.5939743876457215, train acc: 0.7897\n",
      "loss: 1.6447200536727906, train acc: 0.7878\n",
      "loss: 1.6479902625083924, train acc: 0.7779\n",
      "loss: 1.605911946296692, train acc: 0.7794\n",
      "epoch: 46, loss: 1.941429615020752, train acc: 0.7794, test acc: 0.742\n",
      "loss: 1.6346327066421509, train acc: 0.7877\n",
      "loss: 1.6015121817588807, train acc: 0.781\n",
      "loss: 1.6334800362586974, train acc: 0.7806\n",
      "loss: 1.6516554236412049, train acc: 0.7747\n",
      "loss: 1.6231842994689942, train acc: 0.7826\n",
      "loss: 1.6028908967971802, train acc: 0.7877\n",
      "loss: 1.6349400758743287, train acc: 0.7759\n",
      "loss: 1.6102138996124267, train acc: 0.7803\n",
      "epoch: 47, loss: 1.456415057182312, train acc: 0.7803, test acc: 0.7331\n",
      "loss: 1.6249526739120483, train acc: 0.7812\n",
      "loss: 1.6394113540649413, train acc: 0.7836\n",
      "loss: 1.6274237275123595, train acc: 0.7838\n",
      "loss: 1.65053551197052, train acc: 0.7694\n",
      "loss: 1.6581889033317565, train acc: 0.7856\n",
      "loss: 1.6518037080764771, train acc: 0.7847\n",
      "loss: 1.5804884791374207, train acc: 0.7759\n",
      "loss: 1.624723482131958, train acc: 0.7718\n",
      "epoch: 48, loss: 1.5034470558166504, train acc: 0.7718, test acc: 0.7362\n",
      "loss: 1.5509991645812988, train acc: 0.7837\n",
      "loss: 1.6338748335838318, train acc: 0.782\n",
      "loss: 1.609397292137146, train acc: 0.7805\n",
      "loss: 1.6423139929771424, train acc: 0.7768\n",
      "loss: 1.5877453446388246, train acc: 0.788\n",
      "loss: 1.5940027236938477, train acc: 0.7858\n",
      "loss: 1.6287025928497314, train acc: 0.7811\n",
      "loss: 1.5907618403434753, train acc: 0.7745\n",
      "epoch: 49, loss: 1.623895525932312, train acc: 0.7745, test acc: 0.7374\n",
      "loss: 1.6292518377304077, train acc: 0.7867\n",
      "loss: 1.61086163520813, train acc: 0.7893\n",
      "loss: 1.6447509050369262, train acc: 0.7782\n",
      "loss: 1.6852794289588928, train acc: 0.7807\n",
      "loss: 1.6504098176956177, train acc: 0.7856\n",
      "loss: 1.598978054523468, train acc: 0.7885\n",
      "loss: 1.6420859694480896, train acc: 0.7805\n",
      "loss: 1.6214199304580688, train acc: 0.7702\n",
      "epoch: 50, loss: 1.4839164018630981, train acc: 0.7702, test acc: 0.7416\n",
      "loss: 1.6846004724502563, train acc: 0.7874\n",
      "loss: 1.599788761138916, train acc: 0.7841\n",
      "loss: 1.6270800709724427, train acc: 0.7814\n",
      "loss: 1.6651975989341736, train acc: 0.7611\n",
      "loss: 1.6655468940734863, train acc: 0.7867\n",
      "loss: 1.584015166759491, train acc: 0.7854\n",
      "loss: 1.627428364753723, train acc: 0.7744\n",
      "loss: 1.6115492820739745, train acc: 0.7762\n",
      "epoch: 51, loss: 1.5917372703552246, train acc: 0.7762, test acc: 0.7372\n",
      "loss: 1.5457524061203003, train acc: 0.7895\n",
      "loss: 1.5603472709655761, train acc: 0.7884\n",
      "loss: 1.648137640953064, train acc: 0.7842\n",
      "loss: 1.6585527300834655, train acc: 0.786\n",
      "loss: 1.55828914642334, train acc: 0.7872\n",
      "loss: 1.6200453996658326, train acc: 0.7883\n",
      "loss: 1.6147215127944947, train acc: 0.7901\n",
      "loss: 1.609429955482483, train acc: 0.7825\n",
      "epoch: 52, loss: 1.4427810907363892, train acc: 0.7825, test acc: 0.7445\n",
      "loss: 1.5917125940322876, train acc: 0.7909\n",
      "loss: 1.5654193878173828, train acc: 0.7813\n",
      "loss: 1.588623547554016, train acc: 0.7833\n",
      "loss: 1.6232155561447144, train acc: 0.7771\n",
      "loss: 1.622716236114502, train acc: 0.79\n",
      "loss: 1.5947141647338867, train acc: 0.7934\n",
      "loss: 1.5891566753387452, train acc: 0.7827\n",
      "loss: 1.6378534317016602, train acc: 0.7783\n",
      "epoch: 53, loss: 1.482988953590393, train acc: 0.7783, test acc: 0.7404\n",
      "loss: 1.6370669603347778, train acc: 0.7931\n",
      "loss: 1.6483102321624756, train acc: 0.7862\n",
      "loss: 1.5772042632102967, train acc: 0.7762\n",
      "loss: 1.641460108757019, train acc: 0.7796\n",
      "loss: 1.6216601848602294, train acc: 0.7895\n",
      "loss: 1.66701842546463, train acc: 0.7885\n",
      "loss: 1.629208469390869, train acc: 0.7799\n",
      "loss: 1.6432519793510436, train acc: 0.7764\n",
      "epoch: 54, loss: 1.0314998626708984, train acc: 0.7764, test acc: 0.7369\n",
      "loss: 1.5712087154388428, train acc: 0.7864\n",
      "loss: 1.6476376056671143, train acc: 0.7848\n",
      "loss: 1.6065255403518677, train acc: 0.7831\n",
      "loss: 1.6511500597000122, train acc: 0.7851\n",
      "loss: 1.6254794359207154, train acc: 0.7905\n",
      "loss: 1.6133818864822387, train acc: 0.7878\n",
      "loss: 1.6234739661216735, train acc: 0.7884\n",
      "loss: 1.6125084519386292, train acc: 0.7814\n",
      "epoch: 55, loss: 2.0542848110198975, train acc: 0.7814, test acc: 0.7408\n",
      "loss: 1.719699501991272, train acc: 0.7892\n",
      "loss: 1.6314181327819823, train acc: 0.7841\n",
      "loss: 1.583033847808838, train acc: 0.7857\n",
      "loss: 1.6257941007614136, train acc: 0.7862\n",
      "loss: 1.6141464710235596, train acc: 0.7876\n",
      "loss: 1.6167808890342712, train acc: 0.7872\n",
      "loss: 1.6120947360992433, train acc: 0.7837\n",
      "loss: 1.6085108995437623, train acc: 0.7751\n",
      "epoch: 56, loss: 1.2144880294799805, train acc: 0.7751, test acc: 0.7356\n",
      "loss: 1.5884555578231812, train acc: 0.789\n",
      "loss: 1.6228671669960022, train acc: 0.7833\n",
      "loss: 1.6668492555618286, train acc: 0.7816\n",
      "loss: 1.6197906851768493, train acc: 0.7842\n",
      "loss: 1.6447616696357727, train acc: 0.786\n",
      "loss: 1.634563183784485, train acc: 0.7864\n",
      "loss: 1.6142833352088928, train acc: 0.7826\n",
      "loss: 1.6031376838684082, train acc: 0.7748\n",
      "epoch: 57, loss: 1.7787537574768066, train acc: 0.7748, test acc: 0.7371\n",
      "loss: 1.6688157320022583, train acc: 0.7891\n",
      "loss: 1.617967128753662, train acc: 0.7875\n",
      "loss: 1.6275396943092346, train acc: 0.7854\n",
      "loss: 1.6338610887527465, train acc: 0.7732\n",
      "loss: 1.642264437675476, train acc: 0.7884\n",
      "loss: 1.605563473701477, train acc: 0.7894\n",
      "loss: 1.600113534927368, train acc: 0.785\n",
      "loss: 1.6108047127723695, train acc: 0.7824\n",
      "epoch: 58, loss: 1.7302565574645996, train acc: 0.7824, test acc: 0.7354\n",
      "loss: 1.7104355096817017, train acc: 0.7833\n",
      "loss: 1.619323492050171, train acc: 0.7873\n",
      "loss: 1.6068846821784972, train acc: 0.782\n",
      "loss: 1.6284048318862916, train acc: 0.7808\n",
      "loss: 1.5972577571868896, train acc: 0.7884\n",
      "loss: 1.6585561275482177, train acc: 0.7901\n",
      "loss: 1.6125513195991517, train acc: 0.781\n",
      "loss: 1.6312536358833314, train acc: 0.7749\n",
      "epoch: 59, loss: 1.4815871715545654, train acc: 0.7749, test acc: 0.737\n",
      "loss: 1.594182014465332, train acc: 0.7911\n",
      "loss: 1.581840705871582, train acc: 0.7886\n",
      "loss: 1.624805736541748, train acc: 0.7846\n",
      "loss: 1.6553865194320678, train acc: 0.7797\n",
      "loss: 1.6231611609458922, train acc: 0.7926\n",
      "loss: 1.6335097193717956, train acc: 0.786\n",
      "loss: 1.615441131591797, train acc: 0.7823\n",
      "loss: 1.6224857091903686, train acc: 0.7768\n",
      "epoch: 60, loss: 1.425513744354248, train acc: 0.7768, test acc: 0.7319\n",
      "loss: 1.5942578315734863, train acc: 0.7884\n",
      "loss: 1.6066084623336792, train acc: 0.7886\n",
      "loss: 1.6034470915794372, train acc: 0.7878\n",
      "loss: 1.63034690618515, train acc: 0.7756\n",
      "loss: 1.6292190670967102, train acc: 0.7864\n",
      "loss: 1.565116012096405, train acc: 0.7925\n",
      "loss: 1.6313820600509643, train acc: 0.7829\n",
      "loss: 1.5828083872795105, train acc: 0.7834\n",
      "epoch: 61, loss: 1.3952147960662842, train acc: 0.7834, test acc: 0.7453\n",
      "loss: 1.6593389511108398, train acc: 0.7949\n",
      "loss: 1.606434953212738, train acc: 0.7825\n",
      "loss: 1.6441994547843932, train acc: 0.7833\n",
      "loss: 1.6528709053993225, train acc: 0.7857\n",
      "loss: 1.636921238899231, train acc: 0.791\n",
      "loss: 1.5866022825241088, train acc: 0.7907\n",
      "loss: 1.6187116503715515, train acc: 0.7794\n",
      "loss: 1.5961321592330933, train acc: 0.781\n",
      "epoch: 62, loss: 1.5033272504806519, train acc: 0.781, test acc: 0.7425\n",
      "loss: 1.5670982599258423, train acc: 0.7954\n",
      "loss: 1.5680525183677674, train acc: 0.7933\n",
      "loss: 1.5984071612358093, train acc: 0.785\n",
      "loss: 1.6710999131202697, train acc: 0.7694\n",
      "loss: 1.653227722644806, train acc: 0.7907\n",
      "loss: 1.6374318957328797, train acc: 0.7881\n",
      "loss: 1.59162894487381, train acc: 0.7807\n",
      "loss: 1.6097751140594483, train acc: 0.7747\n",
      "epoch: 63, loss: 1.647889256477356, train acc: 0.7747, test acc: 0.7278\n",
      "loss: 1.6466288566589355, train acc: 0.7858\n",
      "loss: 1.6139467239379883, train acc: 0.7867\n",
      "loss: 1.6024323225021362, train acc: 0.7879\n",
      "loss: 1.6167049050331115, train acc: 0.7775\n",
      "loss: 1.6287666797637939, train acc: 0.7837\n",
      "loss: 1.6243491768836975, train acc: 0.7856\n",
      "loss: 1.6275347590446472, train acc: 0.7657\n",
      "loss: 1.6135377764701844, train acc: 0.7795\n",
      "epoch: 64, loss: 1.1806613206863403, train acc: 0.7795, test acc: 0.738\n",
      "loss: 1.6558570861816406, train acc: 0.7921\n",
      "loss: 1.5959043145179748, train acc: 0.789\n",
      "loss: 1.5624501585960389, train acc: 0.7857\n",
      "loss: 1.5801135659217835, train acc: 0.7886\n",
      "loss: 1.6516306281089783, train acc: 0.7904\n",
      "loss: 1.57114018201828, train acc: 0.7922\n",
      "loss: 1.6466517567634582, train acc: 0.7783\n",
      "loss: 1.5940746545791626, train acc: 0.7742\n",
      "epoch: 65, loss: 1.4858207702636719, train acc: 0.7742, test acc: 0.7371\n",
      "loss: 1.579264760017395, train acc: 0.7893\n",
      "loss: 1.5874454379081726, train acc: 0.7904\n",
      "loss: 1.6005987763404845, train acc: 0.7903\n",
      "loss: 1.6414495825767517, train acc: 0.7849\n",
      "loss: 1.6355041980743408, train acc: 0.7845\n",
      "loss: 1.5890364527702332, train acc: 0.7954\n",
      "loss: 1.6120834350585938, train acc: 0.793\n",
      "loss: 1.6186936855316163, train acc: 0.7869\n",
      "epoch: 66, loss: 1.4542661905288696, train acc: 0.7869, test acc: 0.7419\n",
      "loss: 1.6789829730987549, train acc: 0.7942\n",
      "loss: 1.598082959651947, train acc: 0.7958\n",
      "loss: 1.6245993614196776, train acc: 0.7916\n",
      "loss: 1.645346713066101, train acc: 0.7873\n",
      "loss: 1.630253219604492, train acc: 0.7929\n",
      "loss: 1.6151101112365722, train acc: 0.7947\n",
      "loss: 1.6131203174591064, train acc: 0.7949\n",
      "loss: 1.623539352416992, train acc: 0.7796\n",
      "epoch: 67, loss: 1.289624571800232, train acc: 0.7796, test acc: 0.7423\n",
      "loss: 1.6127879619598389, train acc: 0.7978\n",
      "loss: 1.612405741214752, train acc: 0.7967\n",
      "loss: 1.625779962539673, train acc: 0.7888\n",
      "loss: 1.6719544172286986, train acc: 0.7908\n",
      "loss: 1.6393916606903076, train acc: 0.7911\n",
      "loss: 1.6056012272834779, train acc: 0.794\n",
      "loss: 1.6476386070251465, train acc: 0.7942\n",
      "loss: 1.5840533137321473, train acc: 0.7798\n",
      "epoch: 68, loss: 1.3592702150344849, train acc: 0.7798, test acc: 0.7348\n",
      "loss: 1.4734480381011963, train acc: 0.7903\n",
      "loss: 1.6018370032310485, train acc: 0.7945\n",
      "loss: 1.6433016657829285, train acc: 0.7909\n",
      "loss: 1.6196170687675475, train acc: 0.7908\n",
      "loss: 1.616832721233368, train acc: 0.7864\n",
      "loss: 1.5969890356063843, train acc: 0.7912\n",
      "loss: 1.6129706621170044, train acc: 0.7938\n",
      "loss: 1.5533313989639281, train acc: 0.7897\n",
      "epoch: 69, loss: 1.1538207530975342, train acc: 0.7897, test acc: 0.7398\n",
      "loss: 1.7541474103927612, train acc: 0.7945\n",
      "loss: 1.6190240502357482, train acc: 0.7913\n",
      "loss: 1.5899872660636902, train acc: 0.7834\n",
      "loss: 1.6283429145812989, train acc: 0.7831\n",
      "loss: 1.5993187785148621, train acc: 0.7919\n",
      "loss: 1.5873544216156006, train acc: 0.7884\n",
      "loss: 1.6103504061698914, train acc: 0.7811\n",
      "loss: 1.6279327034950257, train acc: 0.783\n",
      "epoch: 70, loss: 1.295875072479248, train acc: 0.783, test acc: 0.7452\n",
      "loss: 1.6971864700317383, train acc: 0.7999\n",
      "loss: 1.5999516725540162, train acc: 0.7963\n",
      "loss: 1.6459704756736755, train acc: 0.7934\n",
      "loss: 1.6461308360099793, train acc: 0.7874\n",
      "loss: 1.6793604016304016, train acc: 0.7952\n",
      "loss: 1.5905535459518432, train acc: 0.8005\n",
      "loss: 1.6022648811340332, train acc: 0.7942\n",
      "loss: 1.6121338605880737, train acc: 0.778\n",
      "epoch: 71, loss: 1.1476458311080933, train acc: 0.778, test acc: 0.7405\n",
      "loss: 1.5870020389556885, train acc: 0.7946\n",
      "loss: 1.5983049154281617, train acc: 0.7998\n",
      "loss: 1.6116770267486573, train acc: 0.7985\n",
      "loss: 1.6599104046821593, train acc: 0.794\n",
      "loss: 1.6337224245071411, train acc: 0.7988\n",
      "loss: 1.598022425174713, train acc: 0.7942\n",
      "loss: 1.6264845967292785, train acc: 0.7787\n",
      "loss: 1.611262822151184, train acc: 0.7908\n",
      "epoch: 72, loss: 1.57463800907135, train acc: 0.7908, test acc: 0.7521\n",
      "loss: 1.6168805360794067, train acc: 0.8022\n",
      "loss: 1.6205268979072571, train acc: 0.7977\n",
      "loss: 1.6036134481430053, train acc: 0.7911\n",
      "loss: 1.633752465248108, train acc: 0.7928\n",
      "loss: 1.651036834716797, train acc: 0.7975\n",
      "loss: 1.6081246376037597, train acc: 0.7897\n",
      "loss: 1.5993665218353272, train acc: 0.7905\n",
      "loss: 1.6029042959213258, train acc: 0.7846\n",
      "epoch: 73, loss: 1.60948646068573, train acc: 0.7846, test acc: 0.7351\n",
      "loss: 1.664200782775879, train acc: 0.7922\n",
      "loss: 1.6205541133880614, train acc: 0.7966\n",
      "loss: 1.6005775690078736, train acc: 0.7905\n",
      "loss: 1.6253656983375548, train acc: 0.7923\n",
      "loss: 1.6302479267120362, train acc: 0.7971\n",
      "loss: 1.6239089131355287, train acc: 0.7971\n",
      "loss: 1.6087438821792603, train acc: 0.78\n",
      "loss: 1.6230215072631835, train acc: 0.7715\n",
      "epoch: 74, loss: 1.6641677618026733, train acc: 0.7715, test acc: 0.7329\n",
      "loss: 1.4810595512390137, train acc: 0.7953\n",
      "loss: 1.5541270971298218, train acc: 0.7989\n",
      "loss: 1.6070991992950439, train acc: 0.7956\n",
      "loss: 1.6416907906532288, train acc: 0.7971\n",
      "loss: 1.6004894971847534, train acc: 0.7868\n",
      "loss: 1.6222115516662599, train acc: 0.7928\n",
      "loss: 1.6118962287902832, train acc: 0.7896\n",
      "loss: 1.584168553352356, train acc: 0.7825\n",
      "epoch: 75, loss: 1.51814603805542, train acc: 0.7825, test acc: 0.739\n",
      "loss: 1.6925283670425415, train acc: 0.795\n",
      "loss: 1.6143409252166747, train acc: 0.7965\n",
      "loss: 1.591547417640686, train acc: 0.7941\n",
      "loss: 1.6255702733993531, train acc: 0.7934\n",
      "loss: 1.626569104194641, train acc: 0.7964\n",
      "loss: 1.6139158725738525, train acc: 0.798\n",
      "loss: 1.629193162918091, train acc: 0.7913\n",
      "loss: 1.604256308078766, train acc: 0.774\n",
      "epoch: 76, loss: 1.3389509916305542, train acc: 0.774, test acc: 0.7436\n",
      "loss: 1.6241406202316284, train acc: 0.8005\n",
      "loss: 1.6069230437278748, train acc: 0.7949\n",
      "loss: 1.5845053315162658, train acc: 0.7886\n",
      "loss: 1.630697000026703, train acc: 0.784\n",
      "loss: 1.6565310835838318, train acc: 0.7956\n",
      "loss: 1.6080479741096496, train acc: 0.796\n",
      "loss: 1.643069851398468, train acc: 0.7868\n",
      "loss: 1.6078352451324462, train acc: 0.7832\n",
      "epoch: 77, loss: 1.7436258792877197, train acc: 0.7832, test acc: 0.7432\n",
      "loss: 1.5615187883377075, train acc: 0.7983\n",
      "loss: 1.5697989583015441, train acc: 0.7967\n",
      "loss: 1.6305093169212341, train acc: 0.7878\n",
      "loss: 1.6127636432647705, train acc: 0.7939\n",
      "loss: 1.6578043580055237, train acc: 0.7913\n",
      "loss: 1.6078830361366272, train acc: 0.7949\n",
      "loss: 1.6100102424621583, train acc: 0.7918\n",
      "loss: 1.6433060050010682, train acc: 0.7714\n",
      "epoch: 78, loss: 1.7465856075286865, train acc: 0.7714, test acc: 0.7395\n",
      "loss: 1.6244287490844727, train acc: 0.7936\n",
      "loss: 1.6139792680740357, train acc: 0.7954\n",
      "loss: 1.5945540189743042, train acc: 0.7964\n",
      "loss: 1.6532734751701355, train acc: 0.7884\n",
      "loss: 1.628934919834137, train acc: 0.7868\n",
      "loss: 1.6065128207206727, train acc: 0.7926\n",
      "loss: 1.608260416984558, train acc: 0.7931\n",
      "loss: 1.5805827021598815, train acc: 0.7879\n",
      "epoch: 79, loss: 1.3403971195220947, train acc: 0.7879, test acc: 0.741\n",
      "#####training and testing end with K:5, P:0.5######\n",
      "#####training and testing start with K:5, P:1######\n",
      "loss: 2.368656635284424, train acc: 0.1483\n",
      "loss: 2.2650732517242433, train acc: 0.2143\n",
      "loss: 2.1156062126159667, train acc: 0.2988\n",
      "loss: 2.0138365268707275, train acc: 0.3215\n",
      "loss: 1.9471498489379884, train acc: 0.3268\n",
      "loss: 1.820053517818451, train acc: 0.3683\n",
      "loss: 1.6783852100372314, train acc: 0.422\n",
      "loss: 1.5540172338485718, train acc: 0.467\n",
      "epoch: 0, loss: 1.4790818691253662, train acc: 0.467, test acc: 0.522\n",
      "loss: 1.3623963594436646, train acc: 0.5115\n",
      "loss: 1.386300003528595, train acc: 0.5621\n",
      "loss: 1.3011200070381164, train acc: 0.6002\n",
      "loss: 1.2211825132369996, train acc: 0.6116\n",
      "loss: 1.1726254224777222, train acc: 0.6486\n",
      "loss: 1.0779302775859834, train acc: 0.6702\n",
      "loss: 1.0346254706382751, train acc: 0.686\n",
      "loss: 0.9528641879558564, train acc: 0.703\n",
      "epoch: 1, loss: 0.9962708353996277, train acc: 0.703, test acc: 0.717\n",
      "loss: 0.8405464887619019, train acc: 0.7177\n",
      "loss: 0.898823493719101, train acc: 0.7297\n",
      "loss: 0.8618062615394593, train acc: 0.7428\n",
      "loss: 0.8493335366249084, train acc: 0.7481\n",
      "loss: 0.8378661155700684, train acc: 0.7556\n",
      "loss: 0.8011444807052612, train acc: 0.7686\n",
      "loss: 0.7962095499038696, train acc: 0.7736\n",
      "loss: 0.7336117565631867, train acc: 0.7847\n",
      "epoch: 2, loss: 0.7975438833236694, train acc: 0.7847, test acc: 0.7798\n",
      "loss: 0.6551099419593811, train acc: 0.7887\n",
      "loss: 0.717832225561142, train acc: 0.7969\n",
      "loss: 0.7022055268287659, train acc: 0.8034\n",
      "loss: 0.705129337310791, train acc: 0.8017\n",
      "loss: 0.6989693760871887, train acc: 0.8071\n",
      "loss: 0.675308096408844, train acc: 0.8121\n",
      "loss: 0.6771653234958649, train acc: 0.8147\n",
      "loss: 0.6259616076946258, train acc: 0.8203\n",
      "epoch: 3, loss: 0.6703996658325195, train acc: 0.8203, test acc: 0.8071\n",
      "loss: 0.5713273286819458, train acc: 0.8177\n",
      "loss: 0.6294887900352478, train acc: 0.8254\n",
      "loss: 0.6253734469413758, train acc: 0.8302\n",
      "loss: 0.6290086448192597, train acc: 0.8292\n",
      "loss: 0.6231475591659545, train acc: 0.8308\n",
      "loss: 0.6055066496133804, train acc: 0.8372\n",
      "loss: 0.6082440137863159, train acc: 0.8367\n",
      "loss: 0.5616444736719132, train acc: 0.8402\n",
      "epoch: 4, loss: 0.5956100225448608, train acc: 0.8402, test acc: 0.8211\n",
      "loss: 0.5210108160972595, train acc: 0.837\n",
      "loss: 0.5754382014274597, train acc: 0.8396\n",
      "loss: 0.5775567173957825, train acc: 0.8456\n",
      "loss: 0.5824337065219879, train acc: 0.8453\n",
      "loss: 0.576779380440712, train acc: 0.845\n",
      "loss: 0.5612230241298676, train acc: 0.8502\n",
      "loss: 0.5642376780509949, train acc: 0.8513\n",
      "loss: 0.51919626891613, train acc: 0.8515\n",
      "epoch: 5, loss: 0.545860767364502, train acc: 0.8515, test acc: 0.8333\n",
      "loss: 0.48271673917770386, train acc: 0.8498\n",
      "loss: 0.5399687170982361, train acc: 0.8517\n",
      "loss: 0.5460484385490417, train acc: 0.8549\n",
      "loss: 0.5508695483207703, train acc: 0.8561\n",
      "loss: 0.5456855058670044, train acc: 0.8572\n",
      "loss: 0.5299031525850296, train acc: 0.8591\n",
      "loss: 0.5338189452886581, train acc: 0.8584\n",
      "loss: 0.49007517099380493, train acc: 0.8586\n",
      "epoch: 6, loss: 0.5096762180328369, train acc: 0.8586, test acc: 0.8388\n",
      "loss: 0.4528948962688446, train acc: 0.858\n",
      "loss: 0.5135723501443863, train acc: 0.8601\n",
      "loss: 0.5219704300165177, train acc: 0.8631\n",
      "loss: 0.5277266085147858, train acc: 0.8632\n",
      "loss: 0.5231354773044586, train acc: 0.8645\n",
      "loss: 0.5061177283525466, train acc: 0.8653\n",
      "loss: 0.5117869883775711, train acc: 0.8663\n",
      "loss: 0.46865872740745546, train acc: 0.8656\n",
      "epoch: 7, loss: 0.4802253246307373, train acc: 0.8656, test acc: 0.8431\n",
      "loss: 0.428135484457016, train acc: 0.8622\n",
      "loss: 0.492334845662117, train acc: 0.8665\n",
      "loss: 0.5032819360494614, train acc: 0.8694\n",
      "loss: 0.5097774535417556, train acc: 0.869\n",
      "loss: 0.5066657453775406, train acc: 0.8695\n",
      "loss: 0.4876455903053284, train acc: 0.8709\n",
      "loss: 0.49414604902267456, train acc: 0.8713\n",
      "loss: 0.45205472111701966, train acc: 0.8701\n",
      "epoch: 8, loss: 0.4567755460739136, train acc: 0.8701, test acc: 0.8467\n",
      "loss: 0.4086006283760071, train acc: 0.868\n",
      "loss: 0.4753905177116394, train acc: 0.87\n",
      "loss: 0.4879653573036194, train acc: 0.8738\n",
      "loss: 0.4951839864253998, train acc: 0.8725\n",
      "loss: 0.4928574413061142, train acc: 0.8727\n",
      "loss: 0.47276727557182313, train acc: 0.8743\n",
      "loss: 0.47979262471199036, train acc: 0.8736\n",
      "loss: 0.438483926653862, train acc: 0.8737\n",
      "epoch: 9, loss: 0.43898794054985046, train acc: 0.8737, test acc: 0.8498\n",
      "loss: 0.3909907042980194, train acc: 0.8715\n",
      "loss: 0.46088172495365143, train acc: 0.8719\n",
      "loss: 0.47490530014038085, train acc: 0.8762\n",
      "loss: 0.48349678218364717, train acc: 0.8759\n",
      "loss: 0.48083567917346953, train acc: 0.876\n",
      "loss: 0.45930797457695005, train acc: 0.8771\n",
      "loss: 0.46672390401363373, train acc: 0.8771\n",
      "loss: 0.42621137797832487, train acc: 0.8764\n",
      "epoch: 10, loss: 0.42173388600349426, train acc: 0.8764, test acc: 0.852\n",
      "loss: 0.37801483273506165, train acc: 0.8746\n",
      "loss: 0.44754727780818937, train acc: 0.8752\n",
      "loss: 0.4635240018367767, train acc: 0.8801\n",
      "loss: 0.4730905741453171, train acc: 0.8799\n",
      "loss: 0.4706146210432053, train acc: 0.8795\n",
      "loss: 0.44787784218788146, train acc: 0.8801\n",
      "loss: 0.45534283816814425, train acc: 0.8808\n",
      "loss: 0.4155053436756134, train acc: 0.8791\n",
      "epoch: 11, loss: 0.4063795804977417, train acc: 0.8791, test acc: 0.8525\n",
      "loss: 0.3660123944282532, train acc: 0.8774\n",
      "loss: 0.43588647544384, train acc: 0.8779\n",
      "loss: 0.45256909132003786, train acc: 0.8823\n",
      "loss: 0.4633779972791672, train acc: 0.8838\n",
      "loss: 0.4610865592956543, train acc: 0.8814\n",
      "loss: 0.43795143365859984, train acc: 0.8823\n",
      "loss: 0.44461518824100493, train acc: 0.884\n",
      "loss: 0.40600105226039884, train acc: 0.882\n",
      "epoch: 12, loss: 0.3924300968647003, train acc: 0.882, test acc: 0.8536\n",
      "loss: 0.35651895403862, train acc: 0.8795\n",
      "loss: 0.42493037283420565, train acc: 0.8799\n",
      "loss: 0.4428719788789749, train acc: 0.8851\n",
      "loss: 0.45488837659358977, train acc: 0.8861\n",
      "loss: 0.45255421102046967, train acc: 0.8846\n",
      "loss: 0.4290627300739288, train acc: 0.8849\n",
      "loss: 0.43479966521263125, train acc: 0.8866\n",
      "loss: 0.3973821818828583, train acc: 0.8842\n",
      "epoch: 13, loss: 0.3806658089160919, train acc: 0.8842, test acc: 0.8555\n",
      "loss: 0.3486076295375824, train acc: 0.8823\n",
      "loss: 0.415058034658432, train acc: 0.8825\n",
      "loss: 0.43320935368537905, train acc: 0.8872\n",
      "loss: 0.4466683387756348, train acc: 0.8883\n",
      "loss: 0.44414915442466735, train acc: 0.8862\n",
      "loss: 0.4214791715145111, train acc: 0.8865\n",
      "loss: 0.4260511755943298, train acc: 0.8877\n",
      "loss: 0.38947550356388094, train acc: 0.8866\n",
      "epoch: 14, loss: 0.3732728660106659, train acc: 0.8866, test acc: 0.8565\n",
      "loss: 0.34142470359802246, train acc: 0.8837\n",
      "loss: 0.40648501813411714, train acc: 0.8848\n",
      "loss: 0.4248456746339798, train acc: 0.8894\n",
      "loss: 0.43988968431949615, train acc: 0.8901\n",
      "loss: 0.43651587069034575, train acc: 0.8879\n",
      "loss: 0.414058992266655, train acc: 0.8885\n",
      "loss: 0.4176666736602783, train acc: 0.8893\n",
      "loss: 0.3820493370294571, train acc: 0.8886\n",
      "epoch: 15, loss: 0.36378443241119385, train acc: 0.8886, test acc: 0.8578\n",
      "loss: 0.334613174200058, train acc: 0.8855\n",
      "loss: 0.39804607629776, train acc: 0.8869\n",
      "loss: 0.4178956180810928, train acc: 0.8918\n",
      "loss: 0.4339442431926727, train acc: 0.892\n",
      "loss: 0.42874211072921753, train acc: 0.8893\n",
      "loss: 0.40694365203380584, train acc: 0.8897\n",
      "loss: 0.41021490693092344, train acc: 0.8909\n",
      "loss: 0.37519376277923583, train acc: 0.8905\n",
      "epoch: 16, loss: 0.35616621375083923, train acc: 0.8905, test acc: 0.8578\n",
      "loss: 0.3284818232059479, train acc: 0.8869\n",
      "loss: 0.3905144274234772, train acc: 0.8876\n",
      "loss: 0.41070162653923037, train acc: 0.8928\n",
      "loss: 0.4279808193445206, train acc: 0.8931\n",
      "loss: 0.4215231418609619, train acc: 0.8908\n",
      "loss: 0.40013921707868577, train acc: 0.8913\n",
      "loss: 0.40298002362251284, train acc: 0.8926\n",
      "loss: 0.36859863698482515, train acc: 0.8919\n",
      "epoch: 17, loss: 0.3465743660926819, train acc: 0.8919, test acc: 0.8604\n",
      "loss: 0.3241807222366333, train acc: 0.8888\n",
      "loss: 0.3834080517292023, train acc: 0.889\n",
      "loss: 0.4041879892349243, train acc: 0.8944\n",
      "loss: 0.4220359355211258, train acc: 0.894\n",
      "loss: 0.4149220257997513, train acc: 0.8928\n",
      "loss: 0.39414740204811094, train acc: 0.8922\n",
      "loss: 0.3967736691236496, train acc: 0.8939\n",
      "loss: 0.3621663570404053, train acc: 0.8942\n",
      "epoch: 18, loss: 0.33736634254455566, train acc: 0.8942, test acc: 0.8612\n",
      "loss: 0.3191821873188019, train acc: 0.8901\n",
      "loss: 0.3762775003910065, train acc: 0.8907\n",
      "loss: 0.3983056366443634, train acc: 0.8953\n",
      "loss: 0.4168132424354553, train acc: 0.8955\n",
      "loss: 0.4075109392404556, train acc: 0.8939\n",
      "loss: 0.3874947905540466, train acc: 0.8933\n",
      "loss: 0.3907435953617096, train acc: 0.8959\n",
      "loss: 0.35583682656288146, train acc: 0.8959\n",
      "epoch: 19, loss: 0.329937607049942, train acc: 0.8959, test acc: 0.8633\n",
      "loss: 0.31464916467666626, train acc: 0.8921\n",
      "loss: 0.36962544173002243, train acc: 0.8924\n",
      "loss: 0.39308151602745056, train acc: 0.8956\n",
      "loss: 0.41034793853759766, train acc: 0.8969\n",
      "loss: 0.40134322047233584, train acc: 0.8954\n",
      "loss: 0.38130437284708024, train acc: 0.8955\n",
      "loss: 0.38462148904800414, train acc: 0.8972\n",
      "loss: 0.3499778926372528, train acc: 0.897\n",
      "epoch: 20, loss: 0.3226962089538574, train acc: 0.897, test acc: 0.8645\n",
      "loss: 0.31223055720329285, train acc: 0.8938\n",
      "loss: 0.36351384222507477, train acc: 0.8951\n",
      "loss: 0.3881175726652145, train acc: 0.8983\n",
      "loss: 0.40455047190189364, train acc: 0.8983\n",
      "loss: 0.3952987164258957, train acc: 0.8971\n",
      "loss: 0.3755022406578064, train acc: 0.8978\n",
      "loss: 0.3790613681077957, train acc: 0.8995\n",
      "loss: 0.34469321370124817, train acc: 0.8986\n",
      "epoch: 21, loss: 0.3156513571739197, train acc: 0.8986, test acc: 0.866\n",
      "loss: 0.3094310164451599, train acc: 0.8951\n",
      "loss: 0.3575108200311661, train acc: 0.8968\n",
      "loss: 0.3831925243139267, train acc: 0.8999\n",
      "loss: 0.39861106872558594, train acc: 0.8995\n",
      "loss: 0.3897246509790421, train acc: 0.8974\n",
      "loss: 0.37023771554231644, train acc: 0.8991\n",
      "loss: 0.3737941175699234, train acc: 0.9014\n",
      "loss: 0.3398328244686127, train acc: 0.8994\n",
      "epoch: 22, loss: 0.30822882056236267, train acc: 0.8994, test acc: 0.8664\n",
      "loss: 0.30587702989578247, train acc: 0.8966\n",
      "loss: 0.35173692256212236, train acc: 0.8977\n",
      "loss: 0.379046767950058, train acc: 0.9002\n",
      "loss: 0.39410924911499023, train acc: 0.9009\n",
      "loss: 0.38424873650074004, train acc: 0.9001\n",
      "loss: 0.3650688886642456, train acc: 0.8994\n",
      "loss: 0.3689980447292328, train acc: 0.902\n",
      "loss: 0.33522756695747374, train acc: 0.9004\n",
      "epoch: 23, loss: 0.30085182189941406, train acc: 0.9004, test acc: 0.8666\n",
      "loss: 0.3018305003643036, train acc: 0.8981\n",
      "loss: 0.34652832746505735, train acc: 0.898\n",
      "loss: 0.3749158412218094, train acc: 0.9017\n",
      "loss: 0.38938798308372496, train acc: 0.903\n",
      "loss: 0.37897672951221467, train acc: 0.9012\n",
      "loss: 0.3602599188685417, train acc: 0.9006\n",
      "loss: 0.36423043012619016, train acc: 0.9034\n",
      "loss: 0.3311495453119278, train acc: 0.9018\n",
      "epoch: 24, loss: 0.29467007517814636, train acc: 0.9018, test acc: 0.8671\n",
      "loss: 0.2976410686969757, train acc: 0.9007\n",
      "loss: 0.34108712077140807, train acc: 0.8996\n",
      "loss: 0.37076883018016815, train acc: 0.9033\n",
      "loss: 0.38480830788612364, train acc: 0.904\n",
      "loss: 0.37420512139797213, train acc: 0.9031\n",
      "loss: 0.35560155659914017, train acc: 0.9015\n",
      "loss: 0.36010736525058745, train acc: 0.9039\n",
      "loss: 0.32710700631141665, train acc: 0.9037\n",
      "epoch: 25, loss: 0.28979289531707764, train acc: 0.9037, test acc: 0.8666\n",
      "loss: 0.29446789622306824, train acc: 0.9021\n",
      "loss: 0.3360481157898903, train acc: 0.9009\n",
      "loss: 0.36697882413864136, train acc: 0.9048\n",
      "loss: 0.3808813735842705, train acc: 0.9052\n",
      "loss: 0.3695129811763763, train acc: 0.9046\n",
      "loss: 0.35145005136728286, train acc: 0.9037\n",
      "loss: 0.3561536014080048, train acc: 0.9053\n",
      "loss: 0.3232135087251663, train acc: 0.9053\n",
      "epoch: 26, loss: 0.28526896238327026, train acc: 0.9053, test acc: 0.8678\n",
      "loss: 0.29087862372398376, train acc: 0.9029\n",
      "loss: 0.3312316879630089, train acc: 0.9032\n",
      "loss: 0.3631699025630951, train acc: 0.9061\n",
      "loss: 0.3772541806101799, train acc: 0.9063\n",
      "loss: 0.36526181995868684, train acc: 0.9049\n",
      "loss: 0.3474769040942192, train acc: 0.9045\n",
      "loss: 0.352703320980072, train acc: 0.9068\n",
      "loss: 0.31974057853221893, train acc: 0.9064\n",
      "epoch: 27, loss: 0.2790042757987976, train acc: 0.9064, test acc: 0.8691\n",
      "loss: 0.2884437143802643, train acc: 0.9037\n",
      "loss: 0.32695740908384324, train acc: 0.9041\n",
      "loss: 0.3598369717597961, train acc: 0.907\n",
      "loss: 0.3737748935818672, train acc: 0.9066\n",
      "loss: 0.3612897485494614, train acc: 0.9055\n",
      "loss: 0.3434450954198837, train acc: 0.905\n",
      "loss: 0.3494118005037308, train acc: 0.9074\n",
      "loss: 0.31642448604106904, train acc: 0.9076\n",
      "epoch: 28, loss: 0.2730783224105835, train acc: 0.9076, test acc: 0.8685\n",
      "loss: 0.28612378239631653, train acc: 0.9038\n",
      "loss: 0.32315829396247864, train acc: 0.9047\n",
      "loss: 0.3569665879011154, train acc: 0.9067\n",
      "loss: 0.37094114124774935, train acc: 0.9077\n",
      "loss: 0.35736813843250276, train acc: 0.9064\n",
      "loss: 0.33976148068904877, train acc: 0.9056\n",
      "loss: 0.34610451459884645, train acc: 0.9083\n",
      "loss: 0.31372875571250913, train acc: 0.9078\n",
      "epoch: 29, loss: 0.2677735984325409, train acc: 0.9078, test acc: 0.8698\n",
      "loss: 0.2837250232696533, train acc: 0.9059\n",
      "loss: 0.31937598884105683, train acc: 0.9061\n",
      "loss: 0.3536070466041565, train acc: 0.908\n",
      "loss: 0.3680152714252472, train acc: 0.9093\n",
      "loss: 0.35391753613948823, train acc: 0.9082\n",
      "loss: 0.3362330228090286, train acc: 0.9072\n",
      "loss: 0.34272721111774446, train acc: 0.9099\n",
      "loss: 0.31088075041770935, train acc: 0.9086\n",
      "epoch: 30, loss: 0.2621363401412964, train acc: 0.9086, test acc: 0.8699\n",
      "loss: 0.2807447612285614, train acc: 0.9067\n",
      "loss: 0.31608384400606154, train acc: 0.9068\n",
      "loss: 0.35032679736614225, train acc: 0.9089\n",
      "loss: 0.36537414938211443, train acc: 0.9104\n",
      "loss: 0.3504138082265854, train acc: 0.9078\n",
      "loss: 0.3329292953014374, train acc: 0.908\n",
      "loss: 0.3395832598209381, train acc: 0.9101\n",
      "loss: 0.308635213971138, train acc: 0.9097\n",
      "epoch: 31, loss: 0.25578856468200684, train acc: 0.9097, test acc: 0.8703\n",
      "loss: 0.2789836823940277, train acc: 0.9073\n",
      "loss: 0.3129874378442764, train acc: 0.9075\n",
      "loss: 0.3476766735315323, train acc: 0.9104\n",
      "loss: 0.36237975358963015, train acc: 0.9108\n",
      "loss: 0.3466279566287994, train acc: 0.9089\n",
      "loss: 0.3297713935375214, train acc: 0.9084\n",
      "loss: 0.3366533249616623, train acc: 0.9108\n",
      "loss: 0.3066120058298111, train acc: 0.9108\n",
      "epoch: 32, loss: 0.2511644959449768, train acc: 0.9108, test acc: 0.8696\n",
      "loss: 0.27798885107040405, train acc: 0.9077\n",
      "loss: 0.30998114198446275, train acc: 0.9078\n",
      "loss: 0.345125213265419, train acc: 0.9109\n",
      "loss: 0.3604329004883766, train acc: 0.9108\n",
      "loss: 0.3434461623430252, train acc: 0.9105\n",
      "loss: 0.32651671171188357, train acc: 0.9098\n",
      "loss: 0.3336495339870453, train acc: 0.9117\n",
      "loss: 0.3047066956758499, train acc: 0.9117\n",
      "epoch: 33, loss: 0.24574309587478638, train acc: 0.9117, test acc: 0.8693\n",
      "loss: 0.27690479159355164, train acc: 0.908\n",
      "loss: 0.3069815397262573, train acc: 0.9083\n",
      "loss: 0.34240342676639557, train acc: 0.912\n",
      "loss: 0.35772895663976667, train acc: 0.9114\n",
      "loss: 0.3405620902776718, train acc: 0.9114\n",
      "loss: 0.3238962948322296, train acc: 0.9101\n",
      "loss: 0.3305654525756836, train acc: 0.9123\n",
      "loss: 0.3027738034725189, train acc: 0.9124\n",
      "epoch: 34, loss: 0.24069033563137054, train acc: 0.9124, test acc: 0.8696\n",
      "loss: 0.2747560143470764, train acc: 0.9093\n",
      "loss: 0.3038528740406036, train acc: 0.9093\n",
      "loss: 0.33921931982040404, train acc: 0.9129\n",
      "loss: 0.3553758665919304, train acc: 0.9118\n",
      "loss: 0.3378036588430405, train acc: 0.9126\n",
      "loss: 0.32071948051452637, train acc: 0.912\n",
      "loss: 0.32763445675373076, train acc: 0.9127\n",
      "loss: 0.3005451574921608, train acc: 0.9126\n",
      "epoch: 35, loss: 0.23707908391952515, train acc: 0.9126, test acc: 0.8698\n",
      "loss: 0.2710588276386261, train acc: 0.9097\n",
      "loss: 0.3011352583765984, train acc: 0.91\n",
      "loss: 0.33707609474658967, train acc: 0.9128\n",
      "loss: 0.35291028022766113, train acc: 0.9132\n",
      "loss: 0.3349799543619156, train acc: 0.913\n",
      "loss: 0.3181967154145241, train acc: 0.9124\n",
      "loss: 0.3249575361609459, train acc: 0.9139\n",
      "loss: 0.2986090824007988, train acc: 0.9139\n",
      "epoch: 36, loss: 0.2322595864534378, train acc: 0.9139, test acc: 0.8705\n",
      "loss: 0.26730814576148987, train acc: 0.9105\n",
      "loss: 0.2983269453048706, train acc: 0.911\n",
      "loss: 0.3345098257064819, train acc: 0.9128\n",
      "loss: 0.3509309709072113, train acc: 0.9135\n",
      "loss: 0.33228415697813035, train acc: 0.9131\n",
      "loss: 0.315873683989048, train acc: 0.913\n",
      "loss: 0.3225230351090431, train acc: 0.9142\n",
      "loss: 0.29696701616048815, train acc: 0.9141\n",
      "epoch: 37, loss: 0.2301565557718277, train acc: 0.9141, test acc: 0.8703\n",
      "loss: 0.2621709406375885, train acc: 0.9112\n",
      "loss: 0.2956865608692169, train acc: 0.9116\n",
      "loss: 0.3325088366866112, train acc: 0.9146\n",
      "loss: 0.34804577678442, train acc: 0.9142\n",
      "loss: 0.3294151544570923, train acc: 0.9136\n",
      "loss: 0.31397988796234133, train acc: 0.9135\n",
      "loss: 0.32008870840072634, train acc: 0.9142\n",
      "loss: 0.29501411467790606, train acc: 0.9148\n",
      "epoch: 38, loss: 0.22628659009933472, train acc: 0.9148, test acc: 0.8705\n",
      "loss: 0.25697943568229675, train acc: 0.9125\n",
      "loss: 0.29292465597391126, train acc: 0.9127\n",
      "loss: 0.32929694950580596, train acc: 0.9153\n",
      "loss: 0.34562471359968183, train acc: 0.914\n",
      "loss: 0.32661194652318953, train acc: 0.9145\n",
      "loss: 0.31180287450551986, train acc: 0.9134\n",
      "loss: 0.31767442375421523, train acc: 0.9146\n",
      "loss: 0.29326840192079545, train acc: 0.916\n",
      "epoch: 39, loss: 0.22336889803409576, train acc: 0.916, test acc: 0.8709\n",
      "loss: 0.25343087315559387, train acc: 0.9127\n",
      "loss: 0.29040977358818054, train acc: 0.9132\n",
      "loss: 0.32686080783605576, train acc: 0.9151\n",
      "loss: 0.34349554032087326, train acc: 0.9144\n",
      "loss: 0.32374753057956696, train acc: 0.915\n",
      "loss: 0.3094668224453926, train acc: 0.9137\n",
      "loss: 0.31551108211278917, train acc: 0.9153\n",
      "loss: 0.29211828112602234, train acc: 0.9161\n",
      "epoch: 40, loss: 0.21855832636356354, train acc: 0.9161, test acc: 0.8708\n",
      "loss: 0.24962717294692993, train acc: 0.9131\n",
      "loss: 0.28816955238580705, train acc: 0.9131\n",
      "loss: 0.32443919479846955, train acc: 0.9155\n",
      "loss: 0.34153801649808885, train acc: 0.9151\n",
      "loss: 0.32124945521354675, train acc: 0.915\n",
      "loss: 0.3074673652648926, train acc: 0.9136\n",
      "loss: 0.3130187153816223, train acc: 0.9155\n",
      "loss: 0.2902581736445427, train acc: 0.9161\n",
      "epoch: 41, loss: 0.21601837873458862, train acc: 0.9161, test acc: 0.8706\n",
      "loss: 0.24609366059303284, train acc: 0.913\n",
      "loss: 0.28606176674365996, train acc: 0.9135\n",
      "loss: 0.32234243750572206, train acc: 0.9156\n",
      "loss: 0.3394692450761795, train acc: 0.9163\n",
      "loss: 0.31857349872589114, train acc: 0.9161\n",
      "loss: 0.3049438953399658, train acc: 0.9142\n",
      "loss: 0.3112194508314133, train acc: 0.9158\n",
      "loss: 0.2885242447257042, train acc: 0.9165\n",
      "epoch: 42, loss: 0.21286331117153168, train acc: 0.9165, test acc: 0.8703\n",
      "loss: 0.24373199045658112, train acc: 0.9141\n",
      "loss: 0.2836454764008522, train acc: 0.914\n",
      "loss: 0.3202939838171005, train acc: 0.9161\n",
      "loss: 0.3376969948410988, train acc: 0.9164\n",
      "loss: 0.3155225977301598, train acc: 0.9162\n",
      "loss: 0.3027224913239479, train acc: 0.9148\n",
      "loss: 0.30938106030225754, train acc: 0.916\n",
      "loss: 0.2866642132401466, train acc: 0.9166\n",
      "epoch: 43, loss: 0.20993731915950775, train acc: 0.9166, test acc: 0.8704\n",
      "loss: 0.24141879379749298, train acc: 0.9137\n",
      "loss: 0.281993405520916, train acc: 0.9141\n",
      "loss: 0.3185147300362587, train acc: 0.9166\n",
      "loss: 0.33603414297103884, train acc: 0.9167\n",
      "loss: 0.3128189042210579, train acc: 0.9168\n",
      "loss: 0.300388877093792, train acc: 0.9156\n",
      "loss: 0.3075089693069458, train acc: 0.9164\n",
      "loss: 0.2852047085762024, train acc: 0.917\n",
      "epoch: 44, loss: 0.20667113363742828, train acc: 0.917, test acc: 0.8708\n",
      "loss: 0.23911428451538086, train acc: 0.9139\n",
      "loss: 0.2805150970816612, train acc: 0.9141\n",
      "loss: 0.31654050052165983, train acc: 0.9171\n",
      "loss: 0.3345028877258301, train acc: 0.9177\n",
      "loss: 0.3098682135343552, train acc: 0.9173\n",
      "loss: 0.29840134978294375, train acc: 0.917\n",
      "loss: 0.3055177807807922, train acc: 0.9174\n",
      "loss: 0.2837555259466171, train acc: 0.9171\n",
      "epoch: 45, loss: 0.20433089137077332, train acc: 0.9171, test acc: 0.8704\n",
      "loss: 0.2364027053117752, train acc: 0.9143\n",
      "loss: 0.27841251343488693, train acc: 0.9143\n",
      "loss: 0.3143202528357506, train acc: 0.9171\n",
      "loss: 0.3325296685099602, train acc: 0.918\n",
      "loss: 0.30728079229593275, train acc: 0.9176\n",
      "loss: 0.29646908193826677, train acc: 0.9175\n",
      "loss: 0.30390543639659884, train acc: 0.9178\n",
      "loss: 0.2825538396835327, train acc: 0.9174\n",
      "epoch: 46, loss: 0.20149630308151245, train acc: 0.9174, test acc: 0.8703\n",
      "loss: 0.23351658880710602, train acc: 0.9146\n",
      "loss: 0.2765242487192154, train acc: 0.9153\n",
      "loss: 0.3116737127304077, train acc: 0.918\n",
      "loss: 0.3305948689579964, train acc: 0.9177\n",
      "loss: 0.30501650720834733, train acc: 0.918\n",
      "loss: 0.2945602789521217, train acc: 0.9183\n",
      "loss: 0.3021890014410019, train acc: 0.9178\n",
      "loss: 0.2808252990245819, train acc: 0.9186\n",
      "epoch: 47, loss: 0.19736598432064056, train acc: 0.9186, test acc: 0.8708\n",
      "loss: 0.23279981315135956, train acc: 0.9152\n",
      "loss: 0.2751672685146332, train acc: 0.9148\n",
      "loss: 0.3104425311088562, train acc: 0.9185\n",
      "loss: 0.32916342914104463, train acc: 0.9184\n",
      "loss: 0.3023738771677017, train acc: 0.9189\n",
      "loss: 0.29286771863698957, train acc: 0.9186\n",
      "loss: 0.30052279084920885, train acc: 0.9181\n",
      "loss: 0.2798395246267319, train acc: 0.9187\n",
      "epoch: 48, loss: 0.1947358399629593, train acc: 0.9187, test acc: 0.8712\n",
      "loss: 0.23013629019260406, train acc: 0.9162\n",
      "loss: 0.27322058081626893, train acc: 0.9152\n",
      "loss: 0.30857104510068895, train acc: 0.9185\n",
      "loss: 0.32733037173748014, train acc: 0.9192\n",
      "loss: 0.3000465020537376, train acc: 0.9195\n",
      "loss: 0.2910387575626373, train acc: 0.9188\n",
      "loss: 0.2986442267894745, train acc: 0.9184\n",
      "loss: 0.27849519103765485, train acc: 0.9191\n",
      "epoch: 49, loss: 0.19100601971149445, train acc: 0.9191, test acc: 0.8712\n",
      "loss: 0.22935284674167633, train acc: 0.9161\n",
      "loss: 0.27210791409015656, train acc: 0.9159\n",
      "loss: 0.3066348671913147, train acc: 0.9185\n",
      "loss: 0.3255780950188637, train acc: 0.9199\n",
      "loss: 0.29789671003818513, train acc: 0.9203\n",
      "loss: 0.2892031103372574, train acc: 0.9191\n",
      "loss: 0.297476589679718, train acc: 0.9193\n",
      "loss: 0.27746431082487105, train acc: 0.9195\n",
      "epoch: 50, loss: 0.18839605152606964, train acc: 0.9195, test acc: 0.8704\n",
      "loss: 0.22820433974266052, train acc: 0.916\n",
      "loss: 0.2706388279795647, train acc: 0.9161\n",
      "loss: 0.30577288269996644, train acc: 0.9187\n",
      "loss: 0.32424457669258117, train acc: 0.9206\n",
      "loss: 0.2956537321209908, train acc: 0.9209\n",
      "loss: 0.28764151632785795, train acc: 0.9199\n",
      "loss: 0.29596866965293883, train acc: 0.9192\n",
      "loss: 0.27668790370225904, train acc: 0.9196\n",
      "epoch: 51, loss: 0.18607120215892792, train acc: 0.9196, test acc: 0.8706\n",
      "loss: 0.22682970762252808, train acc: 0.9164\n",
      "loss: 0.26883275359869, train acc: 0.9156\n",
      "loss: 0.30413442105054855, train acc: 0.9194\n",
      "loss: 0.32270911782979966, train acc: 0.9208\n",
      "loss: 0.2936356231570244, train acc: 0.9213\n",
      "loss: 0.286146380007267, train acc: 0.9204\n",
      "loss: 0.29431470632553103, train acc: 0.92\n",
      "loss: 0.275226928293705, train acc: 0.9205\n",
      "epoch: 52, loss: 0.18401752412319183, train acc: 0.9205, test acc: 0.8707\n",
      "loss: 0.2263043373823166, train acc: 0.9168\n",
      "loss: 0.26755411326885226, train acc: 0.9162\n",
      "loss: 0.30233552902936933, train acc: 0.9197\n",
      "loss: 0.321237912774086, train acc: 0.9216\n",
      "loss: 0.2914850920438766, train acc: 0.9213\n",
      "loss: 0.2844520017504692, train acc: 0.9206\n",
      "loss: 0.2927985370159149, train acc: 0.9206\n",
      "loss: 0.2741556853055954, train acc: 0.9205\n",
      "epoch: 53, loss: 0.18102124333381653, train acc: 0.9205, test acc: 0.8703\n",
      "loss: 0.22507670521736145, train acc: 0.9173\n",
      "loss: 0.26583103835582733, train acc: 0.9169\n",
      "loss: 0.3008308902382851, train acc: 0.9195\n",
      "loss: 0.31987339109182356, train acc: 0.9215\n",
      "loss: 0.28977093696594236, train acc: 0.9215\n",
      "loss: 0.28320480436086654, train acc: 0.9214\n",
      "loss: 0.2911389172077179, train acc: 0.9205\n",
      "loss: 0.2729444861412048, train acc: 0.9209\n",
      "epoch: 54, loss: 0.17849676311016083, train acc: 0.9209, test acc: 0.8703\n",
      "loss: 0.22361505031585693, train acc: 0.9179\n",
      "loss: 0.2644625842571259, train acc: 0.9174\n",
      "loss: 0.2992909178137779, train acc: 0.9203\n",
      "loss: 0.31856333911418916, train acc: 0.9217\n",
      "loss: 0.28812931627035143, train acc: 0.9212\n",
      "loss: 0.2816914692521095, train acc: 0.9213\n",
      "loss: 0.28962281793355943, train acc: 0.9211\n",
      "loss: 0.2716872438788414, train acc: 0.9218\n",
      "epoch: 55, loss: 0.175983265042305, train acc: 0.9218, test acc: 0.87\n",
      "loss: 0.22273175418376923, train acc: 0.9179\n",
      "loss: 0.2632800668478012, train acc: 0.9182\n",
      "loss: 0.29801858216524124, train acc: 0.9208\n",
      "loss: 0.31733836233615875, train acc: 0.9223\n",
      "loss: 0.28626419603824615, train acc: 0.9217\n",
      "loss: 0.2804257020354271, train acc: 0.9221\n",
      "loss: 0.28796211779117586, train acc: 0.9218\n",
      "loss: 0.2706511303782463, train acc: 0.9224\n",
      "epoch: 56, loss: 0.17333519458770752, train acc: 0.9224, test acc: 0.8701\n",
      "loss: 0.22102969884872437, train acc: 0.9191\n",
      "loss: 0.26194891184568403, train acc: 0.9189\n",
      "loss: 0.2967105969786644, train acc: 0.9209\n",
      "loss: 0.31589299589395525, train acc: 0.9227\n",
      "loss: 0.28463988155126574, train acc: 0.9215\n",
      "loss: 0.27919274270534516, train acc: 0.9221\n",
      "loss: 0.28643346577882767, train acc: 0.922\n",
      "loss: 0.2696522817015648, train acc: 0.9228\n",
      "epoch: 57, loss: 0.17067192494869232, train acc: 0.9228, test acc: 0.8702\n",
      "loss: 0.22016754746437073, train acc: 0.9191\n",
      "loss: 0.26088559776544573, train acc: 0.9198\n",
      "loss: 0.2951852917671204, train acc: 0.9213\n",
      "loss: 0.3145588144659996, train acc: 0.923\n",
      "loss: 0.2828748106956482, train acc: 0.922\n",
      "loss: 0.2778364449739456, train acc: 0.9224\n",
      "loss: 0.28512564301490784, train acc: 0.9225\n",
      "loss: 0.2687012568116188, train acc: 0.9236\n",
      "epoch: 58, loss: 0.1676255762577057, train acc: 0.9236, test acc: 0.87\n",
      "loss: 0.21932432055473328, train acc: 0.9191\n",
      "loss: 0.25954760015010836, train acc: 0.9199\n",
      "loss: 0.2940970718860626, train acc: 0.9216\n",
      "loss: 0.31337399035692215, train acc: 0.923\n",
      "loss: 0.28124849647283556, train acc: 0.9223\n",
      "loss: 0.27663920521736146, train acc: 0.9229\n",
      "loss: 0.28362474739551546, train acc: 0.9229\n",
      "loss: 0.2674850091338158, train acc: 0.9244\n",
      "epoch: 59, loss: 0.1652294099330902, train acc: 0.9244, test acc: 0.8697\n",
      "loss: 0.2179213911294937, train acc: 0.9197\n",
      "loss: 0.25840286910533905, train acc: 0.9202\n",
      "loss: 0.2926359921693802, train acc: 0.9225\n",
      "loss: 0.31238040775060655, train acc: 0.9242\n",
      "loss: 0.2800431057810783, train acc: 0.9228\n",
      "loss: 0.27545043975114825, train acc: 0.9234\n",
      "loss: 0.28228841423988343, train acc: 0.9226\n",
      "loss: 0.2664376452565193, train acc: 0.9246\n",
      "epoch: 60, loss: 0.16300860047340393, train acc: 0.9246, test acc: 0.8698\n",
      "loss: 0.21720173954963684, train acc: 0.9202\n",
      "loss: 0.2573480635881424, train acc: 0.9208\n",
      "loss: 0.2912333980202675, train acc: 0.9224\n",
      "loss: 0.31118754148483274, train acc: 0.9245\n",
      "loss: 0.27841612249612807, train acc: 0.923\n",
      "loss: 0.27424844205379484, train acc: 0.9238\n",
      "loss: 0.2808682590723038, train acc: 0.9229\n",
      "loss: 0.2653820350766182, train acc: 0.9245\n",
      "epoch: 61, loss: 0.16058018803596497, train acc: 0.9245, test acc: 0.8699\n",
      "loss: 0.21604755520820618, train acc: 0.9208\n",
      "loss: 0.25627497881650924, train acc: 0.9205\n",
      "loss: 0.28967697322368624, train acc: 0.9226\n",
      "loss: 0.3098438799381256, train acc: 0.9249\n",
      "loss: 0.27673792392015456, train acc: 0.9228\n",
      "loss: 0.2731246441602707, train acc: 0.924\n",
      "loss: 0.27937544137239456, train acc: 0.9229\n",
      "loss: 0.2645921289920807, train acc: 0.9247\n",
      "epoch: 62, loss: 0.15924738347530365, train acc: 0.9247, test acc: 0.8702\n",
      "loss: 0.214936301112175, train acc: 0.9204\n",
      "loss: 0.2546987131237984, train acc: 0.9208\n",
      "loss: 0.2881686106324196, train acc: 0.9232\n",
      "loss: 0.3086669951677322, train acc: 0.925\n",
      "loss: 0.2752483278512955, train acc: 0.923\n",
      "loss: 0.2721103265881538, train acc: 0.9239\n",
      "loss: 0.27764900773763657, train acc: 0.9232\n",
      "loss: 0.2636022880673409, train acc: 0.925\n",
      "epoch: 63, loss: 0.1579054892063141, train acc: 0.925, test acc: 0.8704\n",
      "loss: 0.2141009122133255, train acc: 0.9204\n",
      "loss: 0.2535558372735977, train acc: 0.9206\n",
      "loss: 0.2870107665657997, train acc: 0.9237\n",
      "loss: 0.30750531554222105, train acc: 0.9252\n",
      "loss: 0.27364361137151716, train acc: 0.9233\n",
      "loss: 0.2713208571076393, train acc: 0.9243\n",
      "loss: 0.27603034526109693, train acc: 0.9243\n",
      "loss: 0.26268027573823927, train acc: 0.925\n",
      "epoch: 64, loss: 0.1564626544713974, train acc: 0.925, test acc: 0.8704\n",
      "loss: 0.2134999781847, train acc: 0.9208\n",
      "loss: 0.2524977892637253, train acc: 0.9206\n",
      "loss: 0.28576472848653794, train acc: 0.9242\n",
      "loss: 0.30661966651678085, train acc: 0.9249\n",
      "loss: 0.27199687659740446, train acc: 0.9242\n",
      "loss: 0.27018763720989225, train acc: 0.9244\n",
      "loss: 0.2744073212146759, train acc: 0.9243\n",
      "loss: 0.26169347018003464, train acc: 0.9259\n",
      "epoch: 65, loss: 0.15460726618766785, train acc: 0.9259, test acc: 0.8703\n",
      "loss: 0.21252743899822235, train acc: 0.9208\n",
      "loss: 0.25123280435800555, train acc: 0.9214\n",
      "loss: 0.28446862697601316, train acc: 0.9237\n",
      "loss: 0.30584899336099625, train acc: 0.9258\n",
      "loss: 0.2706731751561165, train acc: 0.9248\n",
      "loss: 0.2690689623355865, train acc: 0.9243\n",
      "loss: 0.27268134504556657, train acc: 0.9246\n",
      "loss: 0.26040455251932143, train acc: 0.9262\n",
      "epoch: 66, loss: 0.15276029706001282, train acc: 0.9262, test acc: 0.8702\n",
      "loss: 0.21096457540988922, train acc: 0.9215\n",
      "loss: 0.24996592998504638, train acc: 0.9221\n",
      "loss: 0.2834992930293083, train acc: 0.9245\n",
      "loss: 0.30475121885538103, train acc: 0.9261\n",
      "loss: 0.26930044740438464, train acc: 0.9249\n",
      "loss: 0.26791548281908034, train acc: 0.9249\n",
      "loss: 0.2712730035185814, train acc: 0.9253\n",
      "loss: 0.2595043733716011, train acc: 0.9265\n",
      "epoch: 67, loss: 0.15096646547317505, train acc: 0.9265, test acc: 0.8702\n",
      "loss: 0.21023301780223846, train acc: 0.9211\n",
      "loss: 0.24896593242883683, train acc: 0.9222\n",
      "loss: 0.28232692629098893, train acc: 0.9251\n",
      "loss: 0.30373995304107665, train acc: 0.9269\n",
      "loss: 0.26791764944791796, train acc: 0.9253\n",
      "loss: 0.2670983955264091, train acc: 0.9252\n",
      "loss: 0.2696606457233429, train acc: 0.9254\n",
      "loss: 0.2584742814302444, train acc: 0.9269\n",
      "epoch: 68, loss: 0.14920656383037567, train acc: 0.9269, test acc: 0.8705\n",
      "loss: 0.20912045240402222, train acc: 0.9215\n",
      "loss: 0.24785023108124732, train acc: 0.9226\n",
      "loss: 0.281394898891449, train acc: 0.9251\n",
      "loss: 0.3028771117329597, train acc: 0.9267\n",
      "loss: 0.26667599081993104, train acc: 0.9256\n",
      "loss: 0.266326105594635, train acc: 0.9248\n",
      "loss: 0.2684267073869705, train acc: 0.9256\n",
      "loss: 0.25770689100027083, train acc: 0.9273\n",
      "epoch: 69, loss: 0.14753270149230957, train acc: 0.9273, test acc: 0.8707\n",
      "loss: 0.20836056768894196, train acc: 0.9218\n",
      "loss: 0.24687831699848176, train acc: 0.9232\n",
      "loss: 0.2801950216293335, train acc: 0.9255\n",
      "loss: 0.301905931532383, train acc: 0.9271\n",
      "loss: 0.2654371470212936, train acc: 0.926\n",
      "loss: 0.2650565907359123, train acc: 0.9253\n",
      "loss: 0.26706732660531995, train acc: 0.9259\n",
      "loss: 0.256835912168026, train acc: 0.9272\n",
      "epoch: 70, loss: 0.14630600810050964, train acc: 0.9272, test acc: 0.8707\n",
      "loss: 0.20788176357746124, train acc: 0.9212\n",
      "loss: 0.2459907479584217, train acc: 0.9228\n",
      "loss: 0.2794633969664574, train acc: 0.9263\n",
      "loss: 0.30124269872903825, train acc: 0.9276\n",
      "loss: 0.26411040872335434, train acc: 0.9264\n",
      "loss: 0.2642859876155853, train acc: 0.926\n",
      "loss: 0.2657056912779808, train acc: 0.9271\n",
      "loss: 0.255741024017334, train acc: 0.928\n",
      "epoch: 71, loss: 0.14409223198890686, train acc: 0.928, test acc: 0.871\n",
      "loss: 0.20681634545326233, train acc: 0.9216\n",
      "loss: 0.2449715219438076, train acc: 0.9239\n",
      "loss: 0.2784197390079498, train acc: 0.9261\n",
      "loss: 0.29987977594137194, train acc: 0.9278\n",
      "loss: 0.262912517786026, train acc: 0.927\n",
      "loss: 0.2633762165904045, train acc: 0.9261\n",
      "loss: 0.264585742354393, train acc: 0.9271\n",
      "loss: 0.254779215157032, train acc: 0.928\n",
      "epoch: 72, loss: 0.14245928823947906, train acc: 0.928, test acc: 0.8706\n",
      "loss: 0.2063496857881546, train acc: 0.9219\n",
      "loss: 0.2441570170223713, train acc: 0.9235\n",
      "loss: 0.2776352897286415, train acc: 0.9266\n",
      "loss: 0.29896829277276993, train acc: 0.9281\n",
      "loss: 0.26169012635946276, train acc: 0.9271\n",
      "loss: 0.2625939652323723, train acc: 0.9265\n",
      "loss: 0.26334038078784944, train acc: 0.9267\n",
      "loss: 0.25388640016317365, train acc: 0.9284\n",
      "epoch: 73, loss: 0.1397293359041214, train acc: 0.9284, test acc: 0.8709\n",
      "loss: 0.20535936951637268, train acc: 0.9223\n",
      "loss: 0.24329473972320556, train acc: 0.9237\n",
      "loss: 0.27659122347831727, train acc: 0.9268\n",
      "loss: 0.29773919135332105, train acc: 0.9284\n",
      "loss: 0.260592582821846, train acc: 0.9274\n",
      "loss: 0.2615764856338501, train acc: 0.9265\n",
      "loss: 0.2621580049395561, train acc: 0.9278\n",
      "loss: 0.2531148537993431, train acc: 0.9292\n",
      "epoch: 74, loss: 0.1385694295167923, train acc: 0.9292, test acc: 0.871\n",
      "loss: 0.20462121069431305, train acc: 0.923\n",
      "loss: 0.2423163488507271, train acc: 0.9242\n",
      "loss: 0.27557456195354463, train acc: 0.9272\n",
      "loss: 0.29701231271028516, train acc: 0.9285\n",
      "loss: 0.2596666947007179, train acc: 0.9277\n",
      "loss: 0.26035826057195666, train acc: 0.9273\n",
      "loss: 0.2609386593103409, train acc: 0.9276\n",
      "loss: 0.25218515694141386, train acc: 0.929\n",
      "epoch: 75, loss: 0.13597124814987183, train acc: 0.929, test acc: 0.8711\n",
      "loss: 0.20372650027275085, train acc: 0.9229\n",
      "loss: 0.24118752479553224, train acc: 0.9245\n",
      "loss: 0.2747568339109421, train acc: 0.928\n",
      "loss: 0.29595635533332826, train acc: 0.9288\n",
      "loss: 0.25829053223133086, train acc: 0.9278\n",
      "loss: 0.25962561666965484, train acc: 0.9277\n",
      "loss: 0.2597598731517792, train acc: 0.9279\n",
      "loss: 0.25135700553655627, train acc: 0.9288\n",
      "epoch: 76, loss: 0.1343141496181488, train acc: 0.9288, test acc: 0.871\n",
      "loss: 0.2029883712530136, train acc: 0.9232\n",
      "loss: 0.2404283605515957, train acc: 0.925\n",
      "loss: 0.27321498841047287, train acc: 0.9283\n",
      "loss: 0.29469904154539106, train acc: 0.9296\n",
      "loss: 0.25734966099262235, train acc: 0.9277\n",
      "loss: 0.2590403199195862, train acc: 0.9277\n",
      "loss: 0.2588249623775482, train acc: 0.9278\n",
      "loss: 0.2505943700671196, train acc: 0.9294\n",
      "epoch: 77, loss: 0.13295906782150269, train acc: 0.9294, test acc: 0.8708\n",
      "loss: 0.2024359554052353, train acc: 0.9232\n",
      "loss: 0.23951218500733376, train acc: 0.9251\n",
      "loss: 0.2726852223277092, train acc: 0.9287\n",
      "loss: 0.29391677379608155, train acc: 0.9291\n",
      "loss: 0.25643030256032945, train acc: 0.9283\n",
      "loss: 0.25802747905254364, train acc: 0.928\n",
      "loss: 0.257901993393898, train acc: 0.9287\n",
      "loss: 0.24977981448173522, train acc: 0.9293\n",
      "epoch: 78, loss: 0.131663978099823, train acc: 0.9293, test acc: 0.8706\n",
      "loss: 0.20152632892131805, train acc: 0.924\n",
      "loss: 0.23858779519796372, train acc: 0.9255\n",
      "loss: 0.27202132642269133, train acc: 0.9289\n",
      "loss: 0.29314598590135577, train acc: 0.9291\n",
      "loss: 0.25491291135549543, train acc: 0.9283\n",
      "loss: 0.2574276998639107, train acc: 0.9283\n",
      "loss: 0.2567087262868881, train acc: 0.9284\n",
      "loss: 0.24916013926267624, train acc: 0.9294\n",
      "epoch: 79, loss: 0.13146361708641052, train acc: 0.9294, test acc: 0.8704\n",
      "#####training and testing end with K:5, P:1######\n",
      "#####training and testing start with K:10, P:0.1######\n",
      "loss: 2.2751388549804688, train acc: 0.1442\n",
      "loss: 2.1522973656654356, train acc: 0.3067\n",
      "loss: 1.865415620803833, train acc: 0.4871\n",
      "loss: 1.615072500705719, train acc: 0.5635\n",
      "loss: 1.4468190312385558, train acc: 0.6146\n",
      "loss: 1.321091628074646, train acc: 0.682\n",
      "loss: 1.2470691323280334, train acc: 0.7123\n",
      "loss: 1.1476777851581574, train acc: 0.7421\n",
      "epoch: 0, loss: 1.5222240686416626, train acc: 0.7421, test acc: 0.7792\n",
      "loss: 1.0963560342788696, train acc: 0.7796\n",
      "loss: 1.0420564115047455, train acc: 0.7973\n",
      "loss: 0.9762805521488189, train acc: 0.8149\n",
      "loss: 0.9279172539710998, train acc: 0.822\n",
      "loss: 0.9006003379821778, train acc: 0.8333\n",
      "loss: 0.8862898528575898, train acc: 0.8397\n",
      "loss: 0.855652266740799, train acc: 0.8425\n",
      "loss: 0.7933068394660949, train acc: 0.8498\n",
      "epoch: 1, loss: 0.9826259016990662, train acc: 0.8498, test acc: 0.8516\n",
      "loss: 0.7642991542816162, train acc: 0.8588\n",
      "loss: 0.7742253184318543, train acc: 0.8621\n",
      "loss: 0.7245592534542084, train acc: 0.869\n",
      "loss: 0.7213598966598511, train acc: 0.8675\n",
      "loss: 0.6968413293361664, train acc: 0.8742\n",
      "loss: 0.673871886730194, train acc: 0.8726\n",
      "loss: 0.6795349389314651, train acc: 0.8775\n",
      "loss: 0.6639128804206849, train acc: 0.8814\n",
      "epoch: 2, loss: 0.6044793128967285, train acc: 0.8814, test acc: 0.8695\n",
      "loss: 0.6940236687660217, train acc: 0.885\n",
      "loss: 0.6534350752830506, train acc: 0.8856\n",
      "loss: 0.6094986915588378, train acc: 0.8855\n",
      "loss: 0.6417801439762115, train acc: 0.8873\n",
      "loss: 0.6052133798599243, train acc: 0.8884\n",
      "loss: 0.5944990992546082, train acc: 0.8875\n",
      "loss: 0.5996812760829926, train acc: 0.8907\n",
      "loss: 0.5785171389579773, train acc: 0.8942\n",
      "epoch: 3, loss: 0.5684473514556885, train acc: 0.8942, test acc: 0.878\n",
      "loss: 0.5176758766174316, train acc: 0.8956\n",
      "loss: 0.5762023359537125, train acc: 0.897\n",
      "loss: 0.559802022576332, train acc: 0.8972\n",
      "loss: 0.5971137166023255, train acc: 0.9\n",
      "loss: 0.5911232084035873, train acc: 0.8983\n",
      "loss: 0.5922732025384903, train acc: 0.8992\n",
      "loss: 0.5838391602039337, train acc: 0.8984\n",
      "loss: 0.5581141293048859, train acc: 0.9022\n",
      "epoch: 4, loss: 0.6700556874275208, train acc: 0.9022, test acc: 0.8838\n",
      "loss: 0.6923437118530273, train acc: 0.903\n",
      "loss: 0.5912145912647248, train acc: 0.9022\n",
      "loss: 0.4962173283100128, train acc: 0.9052\n",
      "loss: 0.5581988096237183, train acc: 0.9038\n",
      "loss: 0.5202402293682098, train acc: 0.9064\n",
      "loss: 0.5150593847036362, train acc: 0.9038\n",
      "loss: 0.548841679096222, train acc: 0.9057\n",
      "loss: 0.515236321091652, train acc: 0.9087\n",
      "epoch: 5, loss: 0.4087371230125427, train acc: 0.9087, test acc: 0.8875\n",
      "loss: 0.5817091464996338, train acc: 0.9083\n",
      "loss: 0.558054780960083, train acc: 0.9087\n",
      "loss: 0.4914810359477997, train acc: 0.9087\n",
      "loss: 0.5416354089975357, train acc: 0.9101\n",
      "loss: 0.5243040800094605, train acc: 0.9083\n",
      "loss: 0.5088446825742722, train acc: 0.9091\n",
      "loss: 0.5230156064033509, train acc: 0.9081\n",
      "loss: 0.4995622128248215, train acc: 0.9136\n",
      "epoch: 6, loss: 0.5087658762931824, train acc: 0.9136, test acc: 0.8951\n",
      "loss: 0.537111222743988, train acc: 0.9148\n",
      "loss: 0.4829209893941879, train acc: 0.9146\n",
      "loss: 0.4583091288805008, train acc: 0.9145\n",
      "loss: 0.5038560152053833, train acc: 0.9127\n",
      "loss: 0.5093466907739639, train acc: 0.9123\n",
      "loss: 0.47593882083892824, train acc: 0.9135\n",
      "loss: 0.5071568101644516, train acc: 0.9144\n",
      "loss: 0.4873466074466705, train acc: 0.9163\n",
      "epoch: 7, loss: 0.32448941469192505, train acc: 0.9163, test acc: 0.8954\n",
      "loss: 0.5203409790992737, train acc: 0.9154\n",
      "loss: 0.4826276212930679, train acc: 0.918\n",
      "loss: 0.4385300487279892, train acc: 0.9183\n",
      "loss: 0.47238083779811857, train acc: 0.9189\n",
      "loss: 0.4594253897666931, train acc: 0.9174\n",
      "loss: 0.483418208360672, train acc: 0.9199\n",
      "loss: 0.48052814304828645, train acc: 0.9181\n",
      "loss: 0.45895310640335085, train acc: 0.9201\n",
      "epoch: 8, loss: 0.3618674576282501, train acc: 0.9201, test acc: 0.8974\n",
      "loss: 0.4203583300113678, train acc: 0.9194\n",
      "loss: 0.4495097666978836, train acc: 0.9208\n",
      "loss: 0.4077588051557541, train acc: 0.9193\n",
      "loss: 0.47767545878887174, train acc: 0.9224\n",
      "loss: 0.46760794520378113, train acc: 0.9197\n",
      "loss: 0.4668735802173615, train acc: 0.9192\n",
      "loss: 0.4703845202922821, train acc: 0.921\n",
      "loss: 0.45303818583488464, train acc: 0.9237\n",
      "epoch: 9, loss: 0.32970958948135376, train acc: 0.9237, test acc: 0.8971\n",
      "loss: 0.46851658821105957, train acc: 0.9228\n",
      "loss: 0.47184661626815794, train acc: 0.923\n",
      "loss: 0.4216399759054184, train acc: 0.9227\n",
      "loss: 0.4949900954961777, train acc: 0.9245\n",
      "loss: 0.4442561358213425, train acc: 0.9213\n",
      "loss: 0.4640124022960663, train acc: 0.923\n",
      "loss: 0.47090956270694734, train acc: 0.9244\n",
      "loss: 0.4374587029218674, train acc: 0.9247\n",
      "epoch: 10, loss: 0.29037022590637207, train acc: 0.9247, test acc: 0.8995\n",
      "loss: 0.46231499314308167, train acc: 0.9279\n",
      "loss: 0.44405645728111265, train acc: 0.9261\n",
      "loss: 0.4351131856441498, train acc: 0.9238\n",
      "loss: 0.45433919727802274, train acc: 0.9268\n",
      "loss: 0.446848651766777, train acc: 0.9266\n",
      "loss: 0.44646877944469454, train acc: 0.9247\n",
      "loss: 0.46884421408176424, train acc: 0.9246\n",
      "loss: 0.41219023764133456, train acc: 0.927\n",
      "epoch: 11, loss: 0.354547381401062, train acc: 0.927, test acc: 0.9012\n",
      "loss: 0.41151461005210876, train acc: 0.9276\n",
      "loss: 0.40316933393478394, train acc: 0.9272\n",
      "loss: 0.40139298141002655, train acc: 0.9281\n",
      "loss: 0.4515860348939896, train acc: 0.9265\n",
      "loss: 0.4066365122795105, train acc: 0.9261\n",
      "loss: 0.43460998237133025, train acc: 0.927\n",
      "loss: 0.47035268843173983, train acc: 0.9244\n",
      "loss: 0.40489778071641924, train acc: 0.9274\n",
      "epoch: 12, loss: 0.3169579803943634, train acc: 0.9274, test acc: 0.9029\n",
      "loss: 0.4997701048851013, train acc: 0.9289\n",
      "loss: 0.42976059913635256, train acc: 0.9285\n",
      "loss: 0.39877004623413087, train acc: 0.9284\n",
      "loss: 0.46742860078811643, train acc: 0.9291\n",
      "loss: 0.4132775515317917, train acc: 0.9284\n",
      "loss: 0.4377691000699997, train acc: 0.9268\n",
      "loss: 0.41452743113040924, train acc: 0.9256\n",
      "loss: 0.4250733762979507, train acc: 0.927\n",
      "epoch: 13, loss: 0.458905965089798, train acc: 0.927, test acc: 0.9024\n",
      "loss: 0.5223371386528015, train acc: 0.9296\n",
      "loss: 0.4301440715789795, train acc: 0.9302\n",
      "loss: 0.388907316327095, train acc: 0.9295\n",
      "loss: 0.4487121790647507, train acc: 0.9299\n",
      "loss: 0.4246414303779602, train acc: 0.9303\n",
      "loss: 0.425522643327713, train acc: 0.9291\n",
      "loss: 0.4464767247438431, train acc: 0.9283\n",
      "loss: 0.4180052071809769, train acc: 0.9294\n",
      "epoch: 14, loss: 0.22654005885124207, train acc: 0.9294, test acc: 0.9039\n",
      "loss: 0.47870883345603943, train acc: 0.9309\n",
      "loss: 0.42866154909133913, train acc: 0.9301\n",
      "loss: 0.38980223834514616, train acc: 0.9315\n",
      "loss: 0.47196948528289795, train acc: 0.9313\n",
      "loss: 0.4311232030391693, train acc: 0.9304\n",
      "loss: 0.4128458142280579, train acc: 0.9288\n",
      "loss: 0.43556719422340395, train acc: 0.9284\n",
      "loss: 0.41332463920116425, train acc: 0.929\n",
      "epoch: 15, loss: 0.455463707447052, train acc: 0.929, test acc: 0.9014\n",
      "loss: 0.41627541184425354, train acc: 0.9298\n",
      "loss: 0.4074974536895752, train acc: 0.9301\n",
      "loss: 0.3819203764200211, train acc: 0.9303\n",
      "loss: 0.45903425216674804, train acc: 0.9322\n",
      "loss: 0.4205452144145966, train acc: 0.9304\n",
      "loss: 0.4139346033334732, train acc: 0.9306\n",
      "loss: 0.43783589005470275, train acc: 0.9306\n",
      "loss: 0.40686827152967453, train acc: 0.9305\n",
      "epoch: 16, loss: 0.2106613963842392, train acc: 0.9305, test acc: 0.904\n",
      "loss: 0.43890300393104553, train acc: 0.9319\n",
      "loss: 0.4353521168231964, train acc: 0.9336\n",
      "loss: 0.40714783370494845, train acc: 0.9308\n",
      "loss: 0.4189557820558548, train acc: 0.9331\n",
      "loss: 0.3894317805767059, train acc: 0.9324\n",
      "loss: 0.3922232687473297, train acc: 0.933\n",
      "loss: 0.43381133377552034, train acc: 0.9315\n",
      "loss: 0.40158894658088684, train acc: 0.9319\n",
      "epoch: 17, loss: 0.3302462100982666, train acc: 0.9319, test acc: 0.9024\n",
      "loss: 0.4607149362564087, train acc: 0.9335\n",
      "loss: 0.4185592383146286, train acc: 0.9334\n",
      "loss: 0.4082981199026108, train acc: 0.9299\n",
      "loss: 0.4227413088083267, train acc: 0.932\n",
      "loss: 0.40549311637878416, train acc: 0.9337\n",
      "loss: 0.41494579017162325, train acc: 0.9321\n",
      "loss: 0.4116013526916504, train acc: 0.9326\n",
      "loss: 0.3998419508337975, train acc: 0.933\n",
      "epoch: 18, loss: 0.22750788927078247, train acc: 0.933, test acc: 0.9011\n",
      "loss: 0.3693401515483856, train acc: 0.9349\n",
      "loss: 0.3693081051111221, train acc: 0.9346\n",
      "loss: 0.3918814718723297, train acc: 0.9351\n",
      "loss: 0.41647140979766845, train acc: 0.9347\n",
      "loss: 0.3976781964302063, train acc: 0.9335\n",
      "loss: 0.41839668452739714, train acc: 0.9336\n",
      "loss: 0.4327518314123154, train acc: 0.9326\n",
      "loss: 0.4093097001314163, train acc: 0.9329\n",
      "epoch: 19, loss: 0.34123730659484863, train acc: 0.9329, test acc: 0.901\n",
      "loss: 0.46251294016838074, train acc: 0.9348\n",
      "loss: 0.3991264641284943, train acc: 0.9365\n",
      "loss: 0.38034422397613527, train acc: 0.9331\n",
      "loss: 0.43463724851608276, train acc: 0.9347\n",
      "loss: 0.4002096265554428, train acc: 0.9348\n",
      "loss: 0.39497738182544706, train acc: 0.9336\n",
      "loss: 0.41442897617816926, train acc: 0.9329\n",
      "loss: 0.39250982254743577, train acc: 0.935\n",
      "epoch: 20, loss: 0.15182790160179138, train acc: 0.935, test acc: 0.9027\n",
      "loss: 0.4863659739494324, train acc: 0.9362\n",
      "loss: 0.40382749438285825, train acc: 0.9347\n",
      "loss: 0.35676823258399964, train acc: 0.9311\n",
      "loss: 0.40508125722408295, train acc: 0.9355\n",
      "loss: 0.38057093024253846, train acc: 0.9335\n",
      "loss: 0.396929657459259, train acc: 0.932\n",
      "loss: 0.41185156255960464, train acc: 0.9323\n",
      "loss: 0.3709300607442856, train acc: 0.9356\n",
      "epoch: 21, loss: 0.22219307720661163, train acc: 0.9356, test acc: 0.9018\n",
      "loss: 0.4575757682323456, train acc: 0.9372\n",
      "loss: 0.4110025703907013, train acc: 0.9365\n",
      "loss: 0.3656329721212387, train acc: 0.935\n",
      "loss: 0.43258485198020935, train acc: 0.9366\n",
      "loss: 0.3686257392168045, train acc: 0.9336\n",
      "loss: 0.3774472862482071, train acc: 0.9356\n",
      "loss: 0.4173608660697937, train acc: 0.9349\n",
      "loss: 0.38908938467502596, train acc: 0.9352\n",
      "epoch: 22, loss: 0.18976953625679016, train acc: 0.9352, test acc: 0.9017\n",
      "loss: 0.40306708216667175, train acc: 0.9376\n",
      "loss: 0.3770856261253357, train acc: 0.937\n",
      "loss: 0.37623471915721896, train acc: 0.9351\n",
      "loss: 0.4030432552099228, train acc: 0.9374\n",
      "loss: 0.38771742582321167, train acc: 0.9353\n",
      "loss: 0.40147501826286314, train acc: 0.9354\n",
      "loss: 0.4119788944721222, train acc: 0.9367\n",
      "loss: 0.3999298959970474, train acc: 0.9387\n",
      "epoch: 23, loss: 0.15753619372844696, train acc: 0.9387, test acc: 0.9002\n",
      "loss: 0.5713494420051575, train acc: 0.9371\n",
      "loss: 0.41139198243618014, train acc: 0.9387\n",
      "loss: 0.3762374609708786, train acc: 0.9363\n",
      "loss: 0.40835278034210204, train acc: 0.9387\n",
      "loss: 0.37506328225135804, train acc: 0.9358\n",
      "loss: 0.3983958661556244, train acc: 0.9375\n",
      "loss: 0.4088611274957657, train acc: 0.936\n",
      "loss: 0.37860060334205625, train acc: 0.9393\n",
      "epoch: 24, loss: 0.2872811555862427, train acc: 0.9393, test acc: 0.8995\n",
      "loss: 0.42554908990859985, train acc: 0.938\n",
      "loss: 0.3703849300742149, train acc: 0.9369\n",
      "loss: 0.36762390434741976, train acc: 0.9364\n",
      "loss: 0.4226076275110245, train acc: 0.9386\n",
      "loss: 0.3735850006341934, train acc: 0.9359\n",
      "loss: 0.37035698890686036, train acc: 0.9374\n",
      "loss: 0.39654885828495023, train acc: 0.9361\n",
      "loss: 0.39179202914237976, train acc: 0.9392\n",
      "epoch: 25, loss: 0.3286079168319702, train acc: 0.9392, test acc: 0.8995\n",
      "loss: 0.4546510577201843, train acc: 0.9373\n",
      "loss: 0.3694311797618866, train acc: 0.9385\n",
      "loss: 0.35964861810207366, train acc: 0.9355\n",
      "loss: 0.4019561856985092, train acc: 0.9392\n",
      "loss: 0.37037360668182373, train acc: 0.9352\n",
      "loss: 0.4096826732158661, train acc: 0.9369\n",
      "loss: 0.39851820170879365, train acc: 0.9379\n",
      "loss: 0.38603858947753905, train acc: 0.9391\n",
      "epoch: 26, loss: 0.24007081985473633, train acc: 0.9391, test acc: 0.8999\n",
      "loss: 0.4200921952724457, train acc: 0.9393\n",
      "loss: 0.39991213381290436, train acc: 0.9394\n",
      "loss: 0.3512219101190567, train acc: 0.9365\n",
      "loss: 0.3982428938150406, train acc: 0.9397\n",
      "loss: 0.3680195540189743, train acc: 0.9382\n",
      "loss: 0.38970260620117186, train acc: 0.9391\n",
      "loss: 0.3900999963283539, train acc: 0.9387\n",
      "loss: 0.35574272871017454, train acc: 0.9416\n",
      "epoch: 27, loss: 0.09136758744716644, train acc: 0.9416, test acc: 0.9017\n",
      "loss: 0.4932035803794861, train acc: 0.9399\n",
      "loss: 0.38509596288204195, train acc: 0.9385\n",
      "loss: 0.375700643658638, train acc: 0.9376\n",
      "loss: 0.4036303788423538, train acc: 0.9413\n",
      "loss: 0.36762177348136904, train acc: 0.9395\n",
      "loss: 0.37626601457595826, train acc: 0.9403\n",
      "loss: 0.40125525295734404, train acc: 0.9386\n",
      "loss: 0.39119728803634646, train acc: 0.9418\n",
      "epoch: 28, loss: 0.3029119372367859, train acc: 0.9418, test acc: 0.9003\n",
      "loss: 0.3316267728805542, train acc: 0.941\n",
      "loss: 0.3593914121389389, train acc: 0.9413\n",
      "loss: 0.3510142803192139, train acc: 0.9406\n",
      "loss: 0.3844240576028824, train acc: 0.941\n",
      "loss: 0.34354018568992617, train acc: 0.9398\n",
      "loss: 0.3973908692598343, train acc: 0.9415\n",
      "loss: 0.41566208004951477, train acc: 0.9385\n",
      "loss: 0.3578605502843857, train acc: 0.9411\n",
      "epoch: 29, loss: 0.24380633234977722, train acc: 0.9411, test acc: 0.8985\n",
      "loss: 0.485644668340683, train acc: 0.9404\n",
      "loss: 0.35733179450035096, train acc: 0.9422\n",
      "loss: 0.3724967479705811, train acc: 0.9392\n",
      "loss: 0.3732234984636307, train acc: 0.9423\n",
      "loss: 0.3603649646043777, train acc: 0.9402\n",
      "loss: 0.38276350498199463, train acc: 0.9419\n",
      "loss: 0.3703498512506485, train acc: 0.9397\n",
      "loss: 0.3691171854734421, train acc: 0.9414\n",
      "epoch: 30, loss: 0.27360203862190247, train acc: 0.9414, test acc: 0.9\n",
      "loss: 0.46523329615592957, train acc: 0.9423\n",
      "loss: 0.3760687351226807, train acc: 0.9414\n",
      "loss: 0.3553609251976013, train acc: 0.9378\n",
      "loss: 0.3935555785894394, train acc: 0.94\n",
      "loss: 0.37079316973686216, train acc: 0.9386\n",
      "loss: 0.3532652616500854, train acc: 0.9407\n",
      "loss: 0.4048842787742615, train acc: 0.9407\n",
      "loss: 0.38029564917087555, train acc: 0.9414\n",
      "epoch: 31, loss: 0.3438931107521057, train acc: 0.9414, test acc: 0.9004\n",
      "loss: 0.3394428789615631, train acc: 0.9423\n",
      "loss: 0.3564839541912079, train acc: 0.9417\n",
      "loss: 0.3401805028319359, train acc: 0.9422\n",
      "loss: 0.3668026149272919, train acc: 0.9426\n",
      "loss: 0.38528000116348265, train acc: 0.9399\n",
      "loss: 0.36457020342350005, train acc: 0.9431\n",
      "loss: 0.39417980015277865, train acc: 0.9406\n",
      "loss: 0.3428415209054947, train acc: 0.9421\n",
      "epoch: 32, loss: 0.22258056700229645, train acc: 0.9421, test acc: 0.8993\n",
      "loss: 0.5018160343170166, train acc: 0.9421\n",
      "loss: 0.36867975294589994, train acc: 0.9404\n",
      "loss: 0.33362480998039246, train acc: 0.9407\n",
      "loss: 0.38760623037815095, train acc: 0.9439\n",
      "loss: 0.34467801749706267, train acc: 0.9396\n",
      "loss: 0.34416641145944593, train acc: 0.9419\n",
      "loss: 0.3688517898321152, train acc: 0.9417\n",
      "loss: 0.3624575600028038, train acc: 0.9424\n",
      "epoch: 33, loss: 0.12732180953025818, train acc: 0.9424, test acc: 0.9007\n",
      "loss: 0.4027899205684662, train acc: 0.9438\n",
      "loss: 0.3750276565551758, train acc: 0.943\n",
      "loss: 0.36299082785844805, train acc: 0.9416\n",
      "loss: 0.36484187841415405, train acc: 0.9425\n",
      "loss: 0.3738124221563339, train acc: 0.9411\n",
      "loss: 0.3603620171546936, train acc: 0.9429\n",
      "loss: 0.40127725899219513, train acc: 0.9398\n",
      "loss: 0.3735744893550873, train acc: 0.9416\n",
      "epoch: 34, loss: 0.3184719383716583, train acc: 0.9416, test acc: 0.8987\n",
      "loss: 0.48276522755622864, train acc: 0.9421\n",
      "loss: 0.3671773195266724, train acc: 0.9402\n",
      "loss: 0.3430581957101822, train acc: 0.9419\n",
      "loss: 0.40077723264694215, train acc: 0.9406\n",
      "loss: 0.3216873526573181, train acc: 0.9381\n",
      "loss: 0.34494331628084185, train acc: 0.9419\n",
      "loss: 0.3665633976459503, train acc: 0.9416\n",
      "loss: 0.38630396127700806, train acc: 0.9429\n",
      "epoch: 35, loss: 0.1610487550497055, train acc: 0.9429, test acc: 0.9003\n",
      "loss: 0.3406056761741638, train acc: 0.9421\n",
      "loss: 0.35788227021694186, train acc: 0.944\n",
      "loss: 0.33035639524459837, train acc: 0.9418\n",
      "loss: 0.37702603340148927, train acc: 0.943\n",
      "loss: 0.3508670568466187, train acc: 0.9414\n",
      "loss: 0.36628370732069016, train acc: 0.9437\n",
      "loss: 0.3780804187059402, train acc: 0.9423\n",
      "loss: 0.34996541440486906, train acc: 0.944\n",
      "epoch: 36, loss: 0.11050243675708771, train acc: 0.944, test acc: 0.8993\n",
      "loss: 0.43411049246788025, train acc: 0.9454\n",
      "loss: 0.3448328971862793, train acc: 0.9443\n",
      "loss: 0.3437917947769165, train acc: 0.9423\n",
      "loss: 0.3608436495065689, train acc: 0.946\n",
      "loss: 0.33916087448596954, train acc: 0.9397\n",
      "loss: 0.3752755045890808, train acc: 0.9442\n",
      "loss: 0.38508826196193696, train acc: 0.9412\n",
      "loss: 0.3646546989679337, train acc: 0.9447\n",
      "epoch: 37, loss: 0.2172023057937622, train acc: 0.9447, test acc: 0.8996\n",
      "loss: 0.44482114911079407, train acc: 0.9423\n",
      "loss: 0.36770880818367, train acc: 0.9444\n",
      "loss: 0.3703455775976181, train acc: 0.9447\n",
      "loss: 0.383623331785202, train acc: 0.9422\n",
      "loss: 0.34196636378765105, train acc: 0.9421\n",
      "loss: 0.3372299402952194, train acc: 0.945\n",
      "loss: 0.36462565064430236, train acc: 0.9441\n",
      "loss: 0.34845014065504076, train acc: 0.9453\n",
      "epoch: 38, loss: 0.06249003857374191, train acc: 0.9453, test acc: 0.8985\n",
      "loss: 0.43697234988212585, train acc: 0.9429\n",
      "loss: 0.37872493267059326, train acc: 0.9438\n",
      "loss: 0.33081783950328825, train acc: 0.9446\n",
      "loss: 0.38888913094997407, train acc: 0.9454\n",
      "loss: 0.35082135796546937, train acc: 0.9412\n",
      "loss: 0.34266453683376313, train acc: 0.9449\n",
      "loss: 0.3901473432779312, train acc: 0.9422\n",
      "loss: 0.34465823173522947, train acc: 0.9462\n",
      "epoch: 39, loss: 0.2123376727104187, train acc: 0.9462, test acc: 0.8984\n",
      "loss: 0.4603024125099182, train acc: 0.9446\n",
      "loss: 0.35116236507892606, train acc: 0.9429\n",
      "loss: 0.3274781957268715, train acc: 0.9438\n",
      "loss: 0.38534251749515536, train acc: 0.9454\n",
      "loss: 0.3522426664829254, train acc: 0.9428\n",
      "loss: 0.34178213477134706, train acc: 0.9475\n",
      "loss: 0.35393311381340026, train acc: 0.9438\n",
      "loss: 0.35816633254289626, train acc: 0.9448\n",
      "epoch: 40, loss: 0.12391066551208496, train acc: 0.9448, test acc: 0.8982\n",
      "loss: 0.37839508056640625, train acc: 0.9451\n",
      "loss: 0.3169610261917114, train acc: 0.9454\n",
      "loss: 0.34198714047670364, train acc: 0.9436\n",
      "loss: 0.39684117436408994, train acc: 0.9443\n",
      "loss: 0.3228026956319809, train acc: 0.9399\n",
      "loss: 0.3463159829378128, train acc: 0.9454\n",
      "loss: 0.3845183804631233, train acc: 0.9442\n",
      "loss: 0.34365891218185424, train acc: 0.9466\n",
      "epoch: 41, loss: 0.11852476000785828, train acc: 0.9466, test acc: 0.8981\n",
      "loss: 0.46665674448013306, train acc: 0.9454\n",
      "loss: 0.3493151873350143, train acc: 0.9466\n",
      "loss: 0.36398234218358994, train acc: 0.9443\n",
      "loss: 0.373362734913826, train acc: 0.9465\n",
      "loss: 0.34356763660907746, train acc: 0.9433\n",
      "loss: 0.34726548194885254, train acc: 0.9467\n",
      "loss: 0.35178264379501345, train acc: 0.9446\n",
      "loss: 0.36235893666744234, train acc: 0.9467\n",
      "epoch: 42, loss: 0.21689540147781372, train acc: 0.9467, test acc: 0.8971\n",
      "loss: 0.41103440523147583, train acc: 0.9458\n",
      "loss: 0.3638134926557541, train acc: 0.9453\n",
      "loss: 0.34380815327167513, train acc: 0.9445\n",
      "loss: 0.33729063868522646, train acc: 0.9459\n",
      "loss: 0.3282521843910217, train acc: 0.943\n",
      "loss: 0.35385751724243164, train acc: 0.9449\n",
      "loss: 0.3645795464515686, train acc: 0.9439\n",
      "loss: 0.38167988657951357, train acc: 0.9455\n",
      "epoch: 43, loss: 0.13590788841247559, train acc: 0.9455, test acc: 0.8971\n",
      "loss: 0.4044599235057831, train acc: 0.9441\n",
      "loss: 0.36774030029773713, train acc: 0.9441\n",
      "loss: 0.34083409011363985, train acc: 0.9435\n",
      "loss: 0.3561885952949524, train acc: 0.9462\n",
      "loss: 0.34985559433698654, train acc: 0.944\n",
      "loss: 0.34792815446853637, train acc: 0.9456\n",
      "loss: 0.3753155738115311, train acc: 0.9433\n",
      "loss: 0.3369417190551758, train acc: 0.9458\n",
      "epoch: 44, loss: 0.2624606490135193, train acc: 0.9458, test acc: 0.8987\n",
      "loss: 0.49820294976234436, train acc: 0.9458\n",
      "loss: 0.3486025795340538, train acc: 0.9443\n",
      "loss: 0.33034937679767606, train acc: 0.9448\n",
      "loss: 0.3555593490600586, train acc: 0.9472\n",
      "loss: 0.33848476558923724, train acc: 0.9452\n",
      "loss: 0.33612947165966034, train acc: 0.9477\n",
      "loss: 0.33980660140514374, train acc: 0.9443\n",
      "loss: 0.3506534665822983, train acc: 0.9476\n",
      "epoch: 45, loss: 0.2671976387500763, train acc: 0.9476, test acc: 0.8985\n",
      "loss: 0.36500900983810425, train acc: 0.9467\n",
      "loss: 0.3520114839076996, train acc: 0.9466\n",
      "loss: 0.3183791011571884, train acc: 0.9455\n",
      "loss: 0.3185889691114426, train acc: 0.9476\n",
      "loss: 0.3280127629637718, train acc: 0.9458\n",
      "loss: 0.35315812528133395, train acc: 0.9486\n",
      "loss: 0.3499936193227768, train acc: 0.9466\n",
      "loss: 0.36266315579414365, train acc: 0.949\n",
      "epoch: 46, loss: 0.04039434343576431, train acc: 0.949, test acc: 0.8986\n",
      "loss: 0.3953961431980133, train acc: 0.9471\n",
      "loss: 0.35138603746891023, train acc: 0.9471\n",
      "loss: 0.3393790051341057, train acc: 0.9481\n",
      "loss: 0.36145141422748567, train acc: 0.949\n",
      "loss: 0.3572083920240402, train acc: 0.9448\n",
      "loss: 0.33764311522245405, train acc: 0.9483\n",
      "loss: 0.3394317954778671, train acc: 0.9459\n",
      "loss: 0.3197616383433342, train acc: 0.9477\n",
      "epoch: 47, loss: 0.19146625697612762, train acc: 0.9477, test acc: 0.8963\n",
      "loss: 0.4480816721916199, train acc: 0.9467\n",
      "loss: 0.3596909523010254, train acc: 0.9471\n",
      "loss: 0.31808624267578123, train acc: 0.9469\n",
      "loss: 0.34558798372745514, train acc: 0.9472\n",
      "loss: 0.3116989955306053, train acc: 0.9476\n",
      "loss: 0.3515655636787415, train acc: 0.9501\n",
      "loss: 0.3807167083024979, train acc: 0.9475\n",
      "loss: 0.3298810452222824, train acc: 0.9489\n",
      "epoch: 48, loss: 0.1315086930990219, train acc: 0.9489, test acc: 0.8983\n",
      "loss: 0.542145848274231, train acc: 0.9465\n",
      "loss: 0.36358526945114134, train acc: 0.9476\n",
      "loss: 0.31690849363803864, train acc: 0.9471\n",
      "loss: 0.356915608048439, train acc: 0.9491\n",
      "loss: 0.3417271047830582, train acc: 0.9452\n",
      "loss: 0.3402115270495415, train acc: 0.9497\n",
      "loss: 0.33950412571430205, train acc: 0.948\n",
      "loss: 0.330584354698658, train acc: 0.9474\n",
      "epoch: 49, loss: 0.5095982551574707, train acc: 0.9474, test acc: 0.8974\n",
      "loss: 0.4132041037082672, train acc: 0.9486\n",
      "loss: 0.36248833537101743, train acc: 0.9471\n",
      "loss: 0.34036368429660796, train acc: 0.9471\n",
      "loss: 0.3499780595302582, train acc: 0.949\n",
      "loss: 0.30200809240341187, train acc: 0.9458\n",
      "loss: 0.33621522039175034, train acc: 0.9504\n",
      "loss: 0.3564928963780403, train acc: 0.9474\n",
      "loss: 0.33463114500045776, train acc: 0.9494\n",
      "epoch: 50, loss: 0.12770435214042664, train acc: 0.9494, test acc: 0.898\n",
      "loss: 0.3875288665294647, train acc: 0.9488\n",
      "loss: 0.32599294036626814, train acc: 0.9497\n",
      "loss: 0.3186391994357109, train acc: 0.9496\n",
      "loss: 0.36444330513477324, train acc: 0.9503\n",
      "loss: 0.3309785708785057, train acc: 0.948\n",
      "loss: 0.3453540563583374, train acc: 0.951\n",
      "loss: 0.35119284838438036, train acc: 0.9474\n",
      "loss: 0.33859739899635316, train acc: 0.9486\n",
      "epoch: 51, loss: 0.14068642258644104, train acc: 0.9486, test acc: 0.8982\n",
      "loss: 0.4085853099822998, train acc: 0.9485\n",
      "loss: 0.3444654643535614, train acc: 0.9485\n",
      "loss: 0.31723388433456423, train acc: 0.9468\n",
      "loss: 0.3641886174678802, train acc: 0.9502\n",
      "loss: 0.3510368049144745, train acc: 0.9466\n",
      "loss: 0.30665532499551773, train acc: 0.9506\n",
      "loss: 0.34317007213830947, train acc: 0.9497\n",
      "loss: 0.30747637152671814, train acc: 0.9502\n",
      "epoch: 52, loss: 0.188538059592247, train acc: 0.9502, test acc: 0.8948\n",
      "loss: 0.39960524439811707, train acc: 0.9485\n",
      "loss: 0.34164920449256897, train acc: 0.9516\n",
      "loss: 0.34064648300409317, train acc: 0.9494\n",
      "loss: 0.3548318013548851, train acc: 0.9477\n",
      "loss: 0.33416991829872134, train acc: 0.9471\n",
      "loss: 0.32305646389722825, train acc: 0.9508\n",
      "loss: 0.35605986416339874, train acc: 0.9477\n",
      "loss: 0.34574526846408843, train acc: 0.9488\n",
      "epoch: 53, loss: 0.14334122836589813, train acc: 0.9488, test acc: 0.8966\n",
      "loss: 0.38697224855422974, train acc: 0.95\n",
      "loss: 0.31776322722434996, train acc: 0.9506\n",
      "loss: 0.3368198424577713, train acc: 0.9477\n",
      "loss: 0.3502217590808868, train acc: 0.9498\n",
      "loss: 0.35642260760068895, train acc: 0.9482\n",
      "loss: 0.324813149869442, train acc: 0.9511\n",
      "loss: 0.3449918434023857, train acc: 0.9494\n",
      "loss: 0.3361065074801445, train acc: 0.9503\n",
      "epoch: 54, loss: 0.1729622632265091, train acc: 0.9503, test acc: 0.8976\n",
      "loss: 0.33718183636665344, train acc: 0.9508\n",
      "loss: 0.3157703191041946, train acc: 0.9499\n",
      "loss: 0.31325237452983856, train acc: 0.951\n",
      "loss: 0.3647441744804382, train acc: 0.9515\n",
      "loss: 0.3224378705024719, train acc: 0.9477\n",
      "loss: 0.3188414812088013, train acc: 0.9525\n",
      "loss: 0.3504338890314102, train acc: 0.9494\n",
      "loss: 0.3171511322259903, train acc: 0.9512\n",
      "epoch: 55, loss: 0.1391131728887558, train acc: 0.9512, test acc: 0.8959\n",
      "loss: 0.32376840710639954, train acc: 0.9481\n",
      "loss: 0.313616506755352, train acc: 0.9514\n",
      "loss: 0.34695973098278043, train acc: 0.9514\n",
      "loss: 0.314566408097744, train acc: 0.9506\n",
      "loss: 0.32416949421167374, train acc: 0.9481\n",
      "loss: 0.33204969465732576, train acc: 0.9526\n",
      "loss: 0.3483189821243286, train acc: 0.9491\n",
      "loss: 0.34108144193887713, train acc: 0.9521\n",
      "epoch: 56, loss: 0.33240950107574463, train acc: 0.9521, test acc: 0.8979\n",
      "loss: 0.31296202540397644, train acc: 0.9524\n",
      "loss: 0.33475832641124725, train acc: 0.9516\n",
      "loss: 0.34304447323083875, train acc: 0.9489\n",
      "loss: 0.34804491996765136, train acc: 0.9529\n",
      "loss: 0.31999851167202, train acc: 0.9481\n",
      "loss: 0.3335290729999542, train acc: 0.9503\n",
      "loss: 0.35050955712795256, train acc: 0.9486\n",
      "loss: 0.3262114480137825, train acc: 0.9519\n",
      "epoch: 57, loss: 0.23724162578582764, train acc: 0.9519, test acc: 0.8963\n",
      "loss: 0.427119642496109, train acc: 0.9511\n",
      "loss: 0.3662399262189865, train acc: 0.9522\n",
      "loss: 0.3167551100254059, train acc: 0.9506\n",
      "loss: 0.3359111413359642, train acc: 0.95\n",
      "loss: 0.34332508146762847, train acc: 0.9452\n",
      "loss: 0.3309236690402031, train acc: 0.9502\n",
      "loss: 0.34300554990768434, train acc: 0.9479\n",
      "loss: 0.3321067377924919, train acc: 0.952\n",
      "epoch: 58, loss: 0.250311017036438, train acc: 0.952, test acc: 0.8975\n",
      "loss: 0.3031662702560425, train acc: 0.9478\n",
      "loss: 0.3187307432293892, train acc: 0.9508\n",
      "loss: 0.32239278256893156, train acc: 0.9497\n",
      "loss: 0.34779628217220304, train acc: 0.9527\n",
      "loss: 0.3099491760134697, train acc: 0.9485\n",
      "loss: 0.3223156318068504, train acc: 0.9516\n",
      "loss: 0.34738204032182696, train acc: 0.9496\n",
      "loss: 0.3270584687590599, train acc: 0.9524\n",
      "epoch: 59, loss: 0.07587158679962158, train acc: 0.9524, test acc: 0.8963\n",
      "loss: 0.3374936878681183, train acc: 0.9521\n",
      "loss: 0.3126727342605591, train acc: 0.9522\n",
      "loss: 0.31868058890104295, train acc: 0.9517\n",
      "loss: 0.3177260994911194, train acc: 0.9509\n",
      "loss: 0.3440804585814476, train acc: 0.9513\n",
      "loss: 0.3099896237254143, train acc: 0.9534\n",
      "loss: 0.341186186671257, train acc: 0.9501\n",
      "loss: 0.3322959735989571, train acc: 0.9526\n",
      "epoch: 60, loss: 0.26134341955184937, train acc: 0.9526, test acc: 0.8965\n",
      "loss: 0.41213324666023254, train acc: 0.951\n",
      "loss: 0.304218789935112, train acc: 0.9527\n",
      "loss: 0.3202527478337288, train acc: 0.9503\n",
      "loss: 0.36222003698349, train acc: 0.9529\n",
      "loss: 0.29058215469121934, train acc: 0.9501\n",
      "loss: 0.3143197789788246, train acc: 0.9503\n",
      "loss: 0.3341949597001076, train acc: 0.9481\n",
      "loss: 0.31429391354322433, train acc: 0.9519\n",
      "epoch: 61, loss: 0.17325200140476227, train acc: 0.9519, test acc: 0.8969\n",
      "loss: 0.43980327248573303, train acc: 0.9516\n",
      "loss: 0.37037564516067506, train acc: 0.9509\n",
      "loss: 0.32115896344184874, train acc: 0.9518\n",
      "loss: 0.32923831343650817, train acc: 0.9518\n",
      "loss: 0.3259447678923607, train acc: 0.95\n",
      "loss: 0.3123947739601135, train acc: 0.9543\n",
      "loss: 0.3311918333172798, train acc: 0.9513\n",
      "loss: 0.30631028562784196, train acc: 0.9528\n",
      "epoch: 62, loss: 0.08566000312566757, train acc: 0.9528, test acc: 0.8982\n",
      "loss: 0.37342649698257446, train acc: 0.9525\n",
      "loss: 0.3049929112195969, train acc: 0.952\n",
      "loss: 0.3019395425915718, train acc: 0.9539\n",
      "loss: 0.3416819840669632, train acc: 0.9523\n",
      "loss: 0.31319694221019745, train acc: 0.9488\n",
      "loss: 0.3108645275235176, train acc: 0.9553\n",
      "loss: 0.32374760657548907, train acc: 0.9529\n",
      "loss: 0.329106742143631, train acc: 0.9525\n",
      "epoch: 63, loss: 0.12823978066444397, train acc: 0.9525, test acc: 0.896\n",
      "loss: 0.5270383954048157, train acc: 0.9528\n",
      "loss: 0.3514307916164398, train acc: 0.9521\n",
      "loss: 0.3242497190833092, train acc: 0.9535\n",
      "loss: 0.339247839152813, train acc: 0.9543\n",
      "loss: 0.308434396982193, train acc: 0.9525\n",
      "loss: 0.3152577877044678, train acc: 0.9537\n",
      "loss: 0.35010765194892884, train acc: 0.9518\n",
      "loss: 0.3179553419351578, train acc: 0.954\n",
      "epoch: 64, loss: 0.18741342425346375, train acc: 0.954, test acc: 0.8949\n",
      "loss: 0.4301621913909912, train acc: 0.9501\n",
      "loss: 0.32359171509742735, train acc: 0.9524\n",
      "loss: 0.3203089490532875, train acc: 0.953\n",
      "loss: 0.33936900198459624, train acc: 0.9532\n",
      "loss: 0.2999210342764854, train acc: 0.9479\n",
      "loss: 0.31334250420331955, train acc: 0.9527\n",
      "loss: 0.3551167592406273, train acc: 0.9524\n",
      "loss: 0.31339559853076937, train acc: 0.9505\n",
      "epoch: 65, loss: 0.22381818294525146, train acc: 0.9505, test acc: 0.8945\n",
      "loss: 0.293413907289505, train acc: 0.9515\n",
      "loss: 0.3327605053782463, train acc: 0.9506\n",
      "loss: 0.3039971128106117, train acc: 0.9521\n",
      "loss: 0.33604193925857545, train acc: 0.953\n",
      "loss: 0.30703106373548505, train acc: 0.9466\n",
      "loss: 0.32617281675338744, train acc: 0.9543\n",
      "loss: 0.32249304950237273, train acc: 0.9506\n",
      "loss: 0.32908845096826556, train acc: 0.9533\n",
      "epoch: 66, loss: 0.036035217344760895, train acc: 0.9533, test acc: 0.8955\n",
      "loss: 0.5014175772666931, train acc: 0.9523\n",
      "loss: 0.35471659898757935, train acc: 0.9525\n",
      "loss: 0.29592361897230146, train acc: 0.9522\n",
      "loss: 0.30665906965732576, train acc: 0.9522\n",
      "loss: 0.29088657200336454, train acc: 0.9486\n",
      "loss: 0.3078289687633514, train acc: 0.9519\n",
      "loss: 0.32544103264808655, train acc: 0.9509\n",
      "loss: 0.3309458911418915, train acc: 0.9516\n",
      "epoch: 67, loss: 0.17652073502540588, train acc: 0.9516, test acc: 0.8976\n",
      "loss: 0.36789247393608093, train acc: 0.9543\n",
      "loss: 0.31826083809137345, train acc: 0.9547\n",
      "loss: 0.31788202226161955, train acc: 0.9547\n",
      "loss: 0.3286463364958763, train acc: 0.9554\n",
      "loss: 0.2927222788333893, train acc: 0.9491\n",
      "loss: 0.32976661771535876, train acc: 0.954\n",
      "loss: 0.3193624272942543, train acc: 0.9551\n",
      "loss: 0.27230980545282363, train acc: 0.9543\n",
      "epoch: 68, loss: 0.060076646506786346, train acc: 0.9543, test acc: 0.8946\n",
      "loss: 0.38513442873954773, train acc: 0.953\n",
      "loss: 0.3428252175450325, train acc: 0.9555\n",
      "loss: 0.29352631270885465, train acc: 0.9551\n",
      "loss: 0.3412309274077415, train acc: 0.9545\n",
      "loss: 0.3125514224171638, train acc: 0.9523\n",
      "loss: 0.32865050733089446, train acc: 0.9542\n",
      "loss: 0.34347537755966184, train acc: 0.9527\n",
      "loss: 0.31310125142335893, train acc: 0.9538\n",
      "epoch: 69, loss: 0.187864288687706, train acc: 0.9538, test acc: 0.8974\n",
      "loss: 0.388640433549881, train acc: 0.9528\n",
      "loss: 0.3498842179775238, train acc: 0.9539\n",
      "loss: 0.2849492847919464, train acc: 0.9561\n",
      "loss: 0.32079256176948545, train acc: 0.9535\n",
      "loss: 0.30244559943675997, train acc: 0.952\n",
      "loss: 0.33827573955059054, train acc: 0.954\n",
      "loss: 0.3401707485318184, train acc: 0.9512\n",
      "loss: 0.324409057199955, train acc: 0.9534\n",
      "epoch: 70, loss: 0.17279651761054993, train acc: 0.9534, test acc: 0.8959\n",
      "loss: 0.42145225405693054, train acc: 0.9561\n",
      "loss: 0.32666280269622805, train acc: 0.9556\n",
      "loss: 0.3171059817075729, train acc: 0.954\n",
      "loss: 0.3343635231256485, train acc: 0.9543\n",
      "loss: 0.3024185225367546, train acc: 0.9492\n",
      "loss: 0.32000774443149566, train acc: 0.9527\n",
      "loss: 0.3420084074139595, train acc: 0.9542\n",
      "loss: 0.3372940018773079, train acc: 0.9555\n",
      "epoch: 71, loss: 0.060700852423906326, train acc: 0.9555, test acc: 0.8953\n",
      "loss: 0.32353484630584717, train acc: 0.9548\n",
      "loss: 0.2897219076752663, train acc: 0.9527\n",
      "loss: 0.3122838944196701, train acc: 0.9539\n",
      "loss: 0.3270400106906891, train acc: 0.9548\n",
      "loss: 0.3066451206803322, train acc: 0.9534\n",
      "loss: 0.2968402341008186, train acc: 0.9561\n",
      "loss: 0.3017755150794983, train acc: 0.9514\n",
      "loss: 0.32106117606163026, train acc: 0.9542\n",
      "epoch: 72, loss: 0.29395559430122375, train acc: 0.9542, test acc: 0.8939\n",
      "loss: 0.4409375488758087, train acc: 0.9545\n",
      "loss: 0.3227104917168617, train acc: 0.9529\n",
      "loss: 0.3146915152668953, train acc: 0.9524\n",
      "loss: 0.35149645507335664, train acc: 0.9522\n",
      "loss: 0.28985542356967925, train acc: 0.9491\n",
      "loss: 0.32334821224212645, train acc: 0.9525\n",
      "loss: 0.3790624991059303, train acc: 0.9487\n",
      "loss: 0.2997973710298538, train acc: 0.9547\n",
      "epoch: 73, loss: 0.09481889754533768, train acc: 0.9547, test acc: 0.8928\n",
      "loss: 0.43910112977027893, train acc: 0.9553\n",
      "loss: 0.31528915017843245, train acc: 0.9535\n",
      "loss: 0.3088522911071777, train acc: 0.9543\n",
      "loss: 0.3272038847208023, train acc: 0.9521\n",
      "loss: 0.2971972391009331, train acc: 0.9515\n",
      "loss: 0.3262919276952744, train acc: 0.9534\n",
      "loss: 0.31001305431127546, train acc: 0.9527\n",
      "loss: 0.3046392261981964, train acc: 0.9553\n",
      "epoch: 74, loss: 0.22868601977825165, train acc: 0.9553, test acc: 0.8941\n",
      "loss: 0.3554314374923706, train acc: 0.9558\n",
      "loss: 0.29850953072309494, train acc: 0.9544\n",
      "loss: 0.30898030698299406, train acc: 0.9548\n",
      "loss: 0.3324959933757782, train acc: 0.9566\n",
      "loss: 0.31424379646778106, train acc: 0.9528\n",
      "loss: 0.31139270663261415, train acc: 0.9566\n",
      "loss: 0.336434543132782, train acc: 0.9534\n",
      "loss: 0.3103653252124786, train acc: 0.9542\n",
      "epoch: 75, loss: 0.11849142611026764, train acc: 0.9542, test acc: 0.8962\n",
      "loss: 0.4100463092327118, train acc: 0.9563\n",
      "loss: 0.31158908307552335, train acc: 0.9559\n",
      "loss: 0.28939120918512345, train acc: 0.9547\n",
      "loss: 0.3401439502835274, train acc: 0.9564\n",
      "loss: 0.30974326431751253, train acc: 0.9507\n",
      "loss: 0.31608969271183013, train acc: 0.9561\n",
      "loss: 0.32067337781190874, train acc: 0.9545\n",
      "loss: 0.3038696989417076, train acc: 0.9562\n",
      "epoch: 76, loss: 0.13234743475914001, train acc: 0.9562, test acc: 0.8942\n",
      "loss: 0.3210180401802063, train acc: 0.9559\n",
      "loss: 0.3302903652191162, train acc: 0.9576\n",
      "loss: 0.29606392830610273, train acc: 0.9571\n",
      "loss: 0.31169463247060775, train acc: 0.9559\n",
      "loss: 0.2996514767408371, train acc: 0.9509\n",
      "loss: 0.3153970956802368, train acc: 0.9536\n",
      "loss: 0.3231607794761658, train acc: 0.9543\n",
      "loss: 0.30056632310152054, train acc: 0.9519\n",
      "epoch: 77, loss: 0.12109901010990143, train acc: 0.9519, test acc: 0.8942\n",
      "loss: 0.41973766684532166, train acc: 0.9563\n",
      "loss: 0.328332906961441, train acc: 0.9557\n",
      "loss: 0.30154141038656235, train acc: 0.9557\n",
      "loss: 0.31644284129142763, train acc: 0.9555\n",
      "loss: 0.3047080174088478, train acc: 0.9531\n",
      "loss: 0.3005199894309044, train acc: 0.9558\n",
      "loss: 0.3300434693694115, train acc: 0.9552\n",
      "loss: 0.3301480859518051, train acc: 0.9555\n",
      "epoch: 78, loss: 0.11738526821136475, train acc: 0.9555, test acc: 0.8941\n",
      "loss: 0.3137272596359253, train acc: 0.9557\n",
      "loss: 0.3104810699820518, train acc: 0.9559\n",
      "loss: 0.3154635041952133, train acc: 0.9563\n",
      "loss: 0.31154593974351885, train acc: 0.9549\n",
      "loss: 0.2922346621751785, train acc: 0.9543\n",
      "loss: 0.3035362333059311, train acc: 0.9539\n",
      "loss: 0.30608980506658556, train acc: 0.9572\n",
      "loss: 0.2891068562865257, train acc: 0.9544\n",
      "epoch: 79, loss: 0.14199402928352356, train acc: 0.9544, test acc: 0.8928\n",
      "#####training and testing end with K:10, P:0.1######\n",
      "#####training and testing start with K:10, P:0.5######\n",
      "loss: 2.4378345012664795, train acc: 0.0767\n",
      "loss: 2.2707983016967774, train acc: 0.2444\n",
      "loss: 2.073368453979492, train acc: 0.3415\n",
      "loss: 1.953605318069458, train acc: 0.3598\n",
      "loss: 1.8552460074424744, train acc: 0.368\n",
      "loss: 1.7730060935020446, train acc: 0.3749\n",
      "loss: 1.7111856579780578, train acc: 0.3793\n",
      "loss: 1.7134733200073242, train acc: 0.3972\n",
      "epoch: 0, loss: 1.4763092994689941, train acc: 0.3972, test acc: 0.4329\n",
      "loss: 1.6368924379348755, train acc: 0.4231\n",
      "loss: 1.63991117477417, train acc: 0.4702\n",
      "loss: 1.6093524098396301, train acc: 0.546\n",
      "loss: 1.5580358505249023, train acc: 0.5916\n",
      "loss: 1.5587335586547852, train acc: 0.6182\n",
      "loss: 1.5431265830993652, train acc: 0.6512\n",
      "loss: 1.4946032285690307, train acc: 0.6729\n",
      "loss: 1.500006949901581, train acc: 0.6618\n",
      "epoch: 1, loss: 1.0938441753387451, train acc: 0.6618, test acc: 0.6761\n",
      "loss: 1.5443387031555176, train acc: 0.6767\n",
      "loss: 1.4470577478408813, train acc: 0.6857\n",
      "loss: 1.4638983488082886, train acc: 0.6929\n",
      "loss: 1.4055119156837463, train acc: 0.701\n",
      "loss: 1.4021446347236632, train acc: 0.7082\n",
      "loss: 1.4148884773254395, train acc: 0.7478\n",
      "loss: 1.4159966468811036, train acc: 0.7867\n",
      "loss: 1.4003184199333192, train acc: 0.771\n",
      "epoch: 2, loss: 1.088047742843628, train acc: 0.771, test acc: 0.7874\n",
      "loss: 1.4528764486312866, train acc: 0.7903\n",
      "loss: 1.3994208812713622, train acc: 0.8152\n",
      "loss: 1.3952060103416444, train acc: 0.8296\n",
      "loss: 1.3917221426963806, train acc: 0.8374\n",
      "loss: 1.3045305132865905, train acc: 0.8444\n",
      "loss: 1.353093934059143, train acc: 0.846\n",
      "loss: 1.3062467336654664, train acc: 0.8599\n",
      "loss: 1.3381485223770142, train acc: 0.857\n",
      "epoch: 3, loss: 1.1505273580551147, train acc: 0.857, test acc: 0.8338\n",
      "loss: 1.2116397619247437, train acc: 0.839\n",
      "loss: 1.2766505599021911, train acc: 0.8542\n",
      "loss: 1.2685245871543884, train acc: 0.8622\n",
      "loss: 1.2946712374687195, train acc: 0.8595\n",
      "loss: 1.27997624874115, train acc: 0.8661\n",
      "loss: 1.3306352138519286, train acc: 0.8674\n",
      "loss: 1.295609450340271, train acc: 0.8626\n",
      "loss: 1.2972867012023925, train acc: 0.8617\n",
      "epoch: 4, loss: 1.1999790668487549, train acc: 0.8617, test acc: 0.8474\n",
      "loss: 1.2406439781188965, train acc: 0.8619\n",
      "loss: 1.2787208557128906, train acc: 0.8703\n",
      "loss: 1.2795219540596008, train acc: 0.8727\n",
      "loss: 1.2522807955741881, train acc: 0.8693\n",
      "loss: 1.258005440235138, train acc: 0.8743\n",
      "loss: 1.2550391912460328, train acc: 0.8776\n",
      "loss: 1.2250597715377807, train acc: 0.8723\n",
      "loss: 1.258543312549591, train acc: 0.8777\n",
      "epoch: 5, loss: 1.4663540124893188, train acc: 0.8777, test acc: 0.8601\n",
      "loss: 1.2134552001953125, train acc: 0.8757\n",
      "loss: 1.2900187253952027, train acc: 0.8762\n",
      "loss: 1.1975877165794373, train acc: 0.8803\n",
      "loss: 1.244401466846466, train acc: 0.8778\n",
      "loss: 1.1792534589767456, train acc: 0.8758\n",
      "loss: 1.2746196031570434, train acc: 0.883\n",
      "loss: 1.1973016142845154, train acc: 0.8802\n",
      "loss: 1.2319231271743774, train acc: 0.8817\n",
      "epoch: 6, loss: 0.7706098556518555, train acc: 0.8817, test acc: 0.8597\n",
      "loss: 1.159119963645935, train acc: 0.8734\n",
      "loss: 1.2023964524269104, train acc: 0.8764\n",
      "loss: 1.2377145528793334, train acc: 0.8804\n",
      "loss: 1.2228671312332153, train acc: 0.883\n",
      "loss: 1.1859201073646546, train acc: 0.8745\n",
      "loss: 1.2066619038581847, train acc: 0.8807\n",
      "loss: 1.1687866866588592, train acc: 0.8802\n",
      "loss: 1.1631252586841583, train acc: 0.8835\n",
      "epoch: 7, loss: 1.1921377182006836, train acc: 0.8835, test acc: 0.8607\n",
      "loss: 1.3513171672821045, train acc: 0.8796\n",
      "loss: 1.2564735531806945, train acc: 0.8838\n",
      "loss: 1.2342687129974366, train acc: 0.8803\n",
      "loss: 1.219302248954773, train acc: 0.8837\n",
      "loss: 1.220660400390625, train acc: 0.8842\n",
      "loss: 1.1591985821723938, train acc: 0.8835\n",
      "loss: 1.207999050617218, train acc: 0.8821\n",
      "loss: 1.2185215711593629, train acc: 0.8819\n",
      "epoch: 8, loss: 1.2539727687835693, train acc: 0.8819, test acc: 0.8644\n",
      "loss: 1.2440215349197388, train acc: 0.8845\n",
      "loss: 1.200856590270996, train acc: 0.8852\n",
      "loss: 1.1853159546852112, train acc: 0.8886\n",
      "loss: 1.1765756726264953, train acc: 0.8872\n",
      "loss: 1.1670770287513732, train acc: 0.8871\n",
      "loss: 1.2299035310745239, train acc: 0.8866\n",
      "loss: 1.1847370624542237, train acc: 0.8907\n",
      "loss: 1.2083357334136964, train acc: 0.8894\n",
      "epoch: 9, loss: 1.1019291877746582, train acc: 0.8894, test acc: 0.8664\n",
      "loss: 1.1635019779205322, train acc: 0.8874\n",
      "loss: 1.13051735162735, train acc: 0.894\n",
      "loss: 1.163863229751587, train acc: 0.8933\n",
      "loss: 1.1500541925430299, train acc: 0.8899\n",
      "loss: 1.2066283822059631, train acc: 0.89\n",
      "loss: 1.2374282360076905, train acc: 0.8923\n",
      "loss: 1.1473623633384704, train acc: 0.8862\n",
      "loss: 1.2012744843959808, train acc: 0.8897\n",
      "epoch: 10, loss: 0.8803820610046387, train acc: 0.8897, test acc: 0.8683\n",
      "loss: 1.3174089193344116, train acc: 0.889\n",
      "loss: 1.1960205435752869, train acc: 0.8941\n",
      "loss: 1.1408967852592469, train acc: 0.8974\n",
      "loss: 1.1727777481079102, train acc: 0.8937\n",
      "loss: 1.1449932217597962, train acc: 0.8928\n",
      "loss: 1.1947152733802795, train acc: 0.8956\n",
      "loss: 1.1607266545295716, train acc: 0.8915\n",
      "loss: 1.176878386735916, train acc: 0.894\n",
      "epoch: 11, loss: 0.9709238409996033, train acc: 0.894, test acc: 0.8724\n",
      "loss: 1.130663275718689, train acc: 0.8929\n",
      "loss: 1.1504765748977661, train acc: 0.8967\n",
      "loss: 1.1239854514598846, train acc: 0.8972\n",
      "loss: 1.1674491047859192, train acc: 0.8969\n",
      "loss: 1.1786084532737733, train acc: 0.8952\n",
      "loss: 1.1563546657562256, train acc: 0.8985\n",
      "loss: 1.1735835552215577, train acc: 0.8988\n",
      "loss: 1.1857466340065002, train acc: 0.8956\n",
      "epoch: 12, loss: 0.6847618818283081, train acc: 0.8956, test acc: 0.8723\n",
      "loss: 1.1323730945587158, train acc: 0.8934\n",
      "loss: 1.160024917125702, train acc: 0.8975\n",
      "loss: 1.1328784584999085, train acc: 0.8928\n",
      "loss: 1.1500835597515107, train acc: 0.8965\n",
      "loss: 1.1358335375785829, train acc: 0.8986\n",
      "loss: 1.1921497762203217, train acc: 0.8997\n",
      "loss: 1.1447895407676696, train acc: 0.8949\n",
      "loss: 1.1699790120124818, train acc: 0.8991\n",
      "epoch: 13, loss: 0.7746424078941345, train acc: 0.8991, test acc: 0.8723\n",
      "loss: 1.0736569166183472, train acc: 0.9\n",
      "loss: 1.111846423149109, train acc: 0.8983\n",
      "loss: 1.14743914604187, train acc: 0.8968\n",
      "loss: 1.1587788820266725, train acc: 0.9003\n",
      "loss: 1.1270546674728394, train acc: 0.8996\n",
      "loss: 1.1566302299499511, train acc: 0.9016\n",
      "loss: 1.1354662656784058, train acc: 0.8976\n",
      "loss: 1.1545120239257813, train acc: 0.8983\n",
      "epoch: 14, loss: 0.8351451754570007, train acc: 0.8983, test acc: 0.8741\n",
      "loss: 1.219942331314087, train acc: 0.8979\n",
      "loss: 1.1615679025650025, train acc: 0.9011\n",
      "loss: 1.0971261978149414, train acc: 0.8989\n",
      "loss: 1.1448116302490234, train acc: 0.9041\n",
      "loss: 1.0960035383701325, train acc: 0.9033\n",
      "loss: 1.161397111415863, train acc: 0.9004\n",
      "loss: 1.1225927472114563, train acc: 0.902\n",
      "loss: 1.1313325464725494, train acc: 0.9009\n",
      "epoch: 15, loss: 0.5872687697410583, train acc: 0.9009, test acc: 0.8737\n",
      "loss: 1.1895323991775513, train acc: 0.8979\n",
      "loss: 1.1570478558540345, train acc: 0.9031\n",
      "loss: 1.1305479049682616, train acc: 0.9011\n",
      "loss: 1.1120646953582765, train acc: 0.9016\n",
      "loss: 1.1335915327072144, train acc: 0.9006\n",
      "loss: 1.1721602916717528, train acc: 0.9022\n",
      "loss: 1.1378819346427917, train acc: 0.8998\n",
      "loss: 1.1480492949485779, train acc: 0.9023\n",
      "epoch: 16, loss: 1.0097095966339111, train acc: 0.9023, test acc: 0.8757\n",
      "loss: 1.1388959884643555, train acc: 0.9033\n",
      "loss: 1.1560127139091492, train acc: 0.8999\n",
      "loss: 1.1235447525978088, train acc: 0.901\n",
      "loss: 1.1735339164733887, train acc: 0.9051\n",
      "loss: 1.1056890845298768, train acc: 0.9005\n",
      "loss: 1.1152196645736694, train acc: 0.904\n",
      "loss: 1.1142700433731079, train acc: 0.9011\n",
      "loss: 1.1356997787952423, train acc: 0.9008\n",
      "epoch: 17, loss: 0.5434755086898804, train acc: 0.9008, test acc: 0.8737\n",
      "loss: 1.0263986587524414, train acc: 0.9034\n",
      "loss: 1.1131351053714753, train acc: 0.9031\n",
      "loss: 1.1030947506427764, train acc: 0.8955\n",
      "loss: 1.1291477918624877, train acc: 0.9064\n",
      "loss: 1.1166488647460937, train acc: 0.9064\n",
      "loss: 1.1394222974777222, train acc: 0.9055\n",
      "loss: 1.1294960856437684, train acc: 0.9047\n",
      "loss: 1.0994616270065307, train acc: 0.9037\n",
      "epoch: 18, loss: 0.8420006036758423, train acc: 0.9037, test acc: 0.8766\n",
      "loss: 1.0400129556655884, train acc: 0.9024\n",
      "loss: 1.1040774643421174, train acc: 0.9017\n",
      "loss: 1.1165928483009337, train acc: 0.8997\n",
      "loss: 1.1205715596675874, train acc: 0.9032\n",
      "loss: 1.093525493144989, train acc: 0.9016\n",
      "loss: 1.14233455657959, train acc: 0.9035\n",
      "loss: 1.0757960677146912, train acc: 0.9017\n",
      "loss: 1.1288065612316132, train acc: 0.9046\n",
      "epoch: 19, loss: 1.1291710138320923, train acc: 0.9046, test acc: 0.8786\n",
      "loss: 1.2124484777450562, train acc: 0.905\n",
      "loss: 1.1653141617774962, train acc: 0.9033\n",
      "loss: 1.099567210674286, train acc: 0.9021\n",
      "loss: 1.1131843626499176, train acc: 0.9036\n",
      "loss: 1.1080880641937256, train acc: 0.9062\n",
      "loss: 1.116153347492218, train acc: 0.9048\n",
      "loss: 1.077027940750122, train acc: 0.9059\n",
      "loss: 1.116364699602127, train acc: 0.9024\n",
      "epoch: 20, loss: 0.9029821157455444, train acc: 0.9024, test acc: 0.8753\n",
      "loss: 1.061429500579834, train acc: 0.9028\n",
      "loss: 1.0756167829036714, train acc: 0.9023\n",
      "loss: 1.1186316668987275, train acc: 0.9015\n",
      "loss: 1.1183899879455566, train acc: 0.904\n",
      "loss: 1.1493745386600493, train acc: 0.9021\n",
      "loss: 1.1540775418281555, train acc: 0.9018\n",
      "loss: 1.0790745139122009, train acc: 0.9063\n",
      "loss: 1.1891823887825013, train acc: 0.9042\n",
      "epoch: 21, loss: 1.1256288290023804, train acc: 0.9042, test acc: 0.8767\n",
      "loss: 1.2557836771011353, train acc: 0.9039\n",
      "loss: 1.1551243782043457, train acc: 0.9055\n",
      "loss: 1.1149097979068756, train acc: 0.9057\n",
      "loss: 1.1259001970291138, train acc: 0.9051\n",
      "loss: 1.1211177051067351, train acc: 0.9047\n",
      "loss: 1.1111363291740417, train acc: 0.9065\n",
      "loss: 1.0746929168701171, train acc: 0.9037\n",
      "loss: 1.1462306678295135, train acc: 0.9036\n",
      "epoch: 22, loss: 0.9052177667617798, train acc: 0.9036, test acc: 0.8766\n",
      "loss: 1.0370888710021973, train acc: 0.907\n",
      "loss: 1.0990976691246033, train acc: 0.9057\n",
      "loss: 1.096243441104889, train acc: 0.9025\n",
      "loss: 1.124356734752655, train acc: 0.9049\n",
      "loss: 1.1258715212345123, train acc: 0.9048\n",
      "loss: 1.1431545853614806, train acc: 0.906\n",
      "loss: 1.0705520987510682, train acc: 0.9049\n",
      "loss: 1.1048962712287902, train acc: 0.9072\n",
      "epoch: 23, loss: 0.9731428623199463, train acc: 0.9072, test acc: 0.8772\n",
      "loss: 1.1819908618927002, train acc: 0.9065\n",
      "loss: 1.0705788731575012, train acc: 0.9045\n",
      "loss: 1.0858329236507416, train acc: 0.9046\n",
      "loss: 1.0698026657104491, train acc: 0.9052\n",
      "loss: 1.1372505605220795, train acc: 0.9029\n",
      "loss: 1.1294891595840455, train acc: 0.9058\n",
      "loss: 1.0843042135238647, train acc: 0.9056\n",
      "loss: 1.0988376557826995, train acc: 0.9077\n",
      "epoch: 24, loss: 0.8960536122322083, train acc: 0.9077, test acc: 0.8761\n",
      "loss: 0.9826066493988037, train acc: 0.9038\n",
      "loss: 1.072299498319626, train acc: 0.9062\n",
      "loss: 1.1089203119277955, train acc: 0.9079\n",
      "loss: 1.100404781103134, train acc: 0.9058\n",
      "loss: 1.113546073436737, train acc: 0.9064\n",
      "loss: 1.114947098493576, train acc: 0.9058\n",
      "loss: 1.093125194311142, train acc: 0.9077\n",
      "loss: 1.112723422050476, train acc: 0.908\n",
      "epoch: 25, loss: 0.8505046963691711, train acc: 0.908, test acc: 0.8765\n",
      "loss: 1.0747640132904053, train acc: 0.9079\n",
      "loss: 1.1388255119323731, train acc: 0.9058\n",
      "loss: 1.1111559391021728, train acc: 0.908\n",
      "loss: 1.0838321268558502, train acc: 0.9052\n",
      "loss: 1.1188952684402467, train acc: 0.9064\n",
      "loss: 1.1420410752296448, train acc: 0.906\n",
      "loss: 1.0693991780281067, train acc: 0.905\n",
      "loss: 1.124945592880249, train acc: 0.9088\n",
      "epoch: 26, loss: 1.0666403770446777, train acc: 0.9088, test acc: 0.8775\n",
      "loss: 1.1182284355163574, train acc: 0.9085\n",
      "loss: 1.1174297511577607, train acc: 0.9078\n",
      "loss: 1.0893691301345825, train acc: 0.906\n",
      "loss: 1.09779332280159, train acc: 0.9073\n",
      "loss: 1.0753486692905425, train acc: 0.9087\n",
      "loss: 1.1387956380844115, train acc: 0.9077\n",
      "loss: 1.053165316581726, train acc: 0.9113\n",
      "loss: 1.1312587440013886, train acc: 0.9107\n",
      "epoch: 27, loss: 0.7429980635643005, train acc: 0.9107, test acc: 0.8752\n",
      "loss: 1.1046112775802612, train acc: 0.907\n",
      "loss: 1.099579644203186, train acc: 0.9082\n",
      "loss: 1.0963306546211242, train acc: 0.9095\n",
      "loss: 1.132483994960785, train acc: 0.9091\n",
      "loss: 1.0903665363788604, train acc: 0.9082\n",
      "loss: 1.0550287902355193, train acc: 0.9081\n",
      "loss: 1.05477676987648, train acc: 0.9122\n",
      "loss: 1.1161991655826569, train acc: 0.9089\n",
      "epoch: 28, loss: 0.6040245294570923, train acc: 0.9089, test acc: 0.8766\n",
      "loss: 1.0598089694976807, train acc: 0.9077\n",
      "loss: 1.117966091632843, train acc: 0.9056\n",
      "loss: 1.1170556664466857, train acc: 0.9072\n",
      "loss: 1.1097341656684876, train acc: 0.9073\n",
      "loss: 1.100458127260208, train acc: 0.9072\n",
      "loss: 1.1132145464420318, train acc: 0.908\n",
      "loss: 1.0845819294452668, train acc: 0.908\n",
      "loss: 1.0796339750289916, train acc: 0.9066\n",
      "epoch: 29, loss: 0.6773751378059387, train acc: 0.9066, test acc: 0.8753\n",
      "loss: 0.9971113204956055, train acc: 0.9075\n",
      "loss: 1.0458877742290498, train acc: 0.9024\n",
      "loss: 1.1124139726161957, train acc: 0.9068\n",
      "loss: 1.0990407943725586, train acc: 0.9091\n",
      "loss: 1.1267755150794982, train acc: 0.9086\n",
      "loss: 1.1135177850723266, train acc: 0.9036\n",
      "loss: 1.0553839325904846, train acc: 0.9108\n",
      "loss: 1.112465512752533, train acc: 0.9056\n",
      "epoch: 30, loss: 1.2284704446792603, train acc: 0.9056, test acc: 0.8788\n",
      "loss: 1.1217457056045532, train acc: 0.9091\n",
      "loss: 1.0867736637592316, train acc: 0.9069\n",
      "loss: 1.0963820934295654, train acc: 0.9078\n",
      "loss: 1.1402700960636138, train acc: 0.91\n",
      "loss: 1.1195055544376373, train acc: 0.9088\n",
      "loss: 1.0610583424568176, train acc: 0.9093\n",
      "loss: 1.0286626100540162, train acc: 0.9075\n",
      "loss: 1.0803762674331665, train acc: 0.911\n",
      "epoch: 31, loss: 0.694362461566925, train acc: 0.911, test acc: 0.8784\n",
      "loss: 1.0717118978500366, train acc: 0.9098\n",
      "loss: 1.1035577774047851, train acc: 0.9053\n",
      "loss: 1.0750500500202178, train acc: 0.9088\n",
      "loss: 1.1142221510410308, train acc: 0.909\n",
      "loss: 1.1221136391162871, train acc: 0.9089\n",
      "loss: 1.1145553112030029, train acc: 0.9033\n",
      "loss: 1.0762671291828156, train acc: 0.9091\n",
      "loss: 1.1498464226722718, train acc: 0.9003\n",
      "epoch: 32, loss: 1.0576348304748535, train acc: 0.9003, test acc: 0.8758\n",
      "loss: 1.1095234155654907, train acc: 0.9099\n",
      "loss: 1.0895423710346221, train acc: 0.9002\n",
      "loss: 1.123436725139618, train acc: 0.9096\n",
      "loss: 1.0778655588626862, train acc: 0.9063\n",
      "loss: 1.076867365837097, train acc: 0.9096\n",
      "loss: 1.1251138389110564, train acc: 0.9065\n",
      "loss: 1.0723182380199432, train acc: 0.9086\n",
      "loss: 1.1342627108097076, train acc: 0.9085\n",
      "epoch: 33, loss: 0.6153542399406433, train acc: 0.9085, test acc: 0.8754\n",
      "loss: 1.057795524597168, train acc: 0.9083\n",
      "loss: 1.0850887775421143, train acc: 0.9087\n",
      "loss: 1.0951589226722718, train acc: 0.911\n",
      "loss: 1.0742573738098145, train acc: 0.9102\n",
      "loss: 1.057461154460907, train acc: 0.9096\n",
      "loss: 1.0735547721385956, train acc: 0.9109\n",
      "loss: 1.0889851987361907, train acc: 0.9119\n",
      "loss: 1.104474925994873, train acc: 0.9094\n",
      "epoch: 34, loss: 0.7972622513771057, train acc: 0.9094, test acc: 0.8766\n",
      "loss: 1.0983794927597046, train acc: 0.9092\n",
      "loss: 1.109890592098236, train acc: 0.9114\n",
      "loss: 1.1144335627555848, train acc: 0.9115\n",
      "loss: 1.1215711832046509, train acc: 0.9108\n",
      "loss: 1.1045909225940704, train acc: 0.9108\n",
      "loss: 1.0941039860248565, train acc: 0.9116\n",
      "loss: 1.0660154163837432, train acc: 0.9119\n",
      "loss: 1.0839517951011657, train acc: 0.9075\n",
      "epoch: 35, loss: 0.7062358856201172, train acc: 0.9075, test acc: 0.8751\n",
      "loss: 1.1326051950454712, train acc: 0.9112\n",
      "loss: 1.128152358531952, train acc: 0.908\n",
      "loss: 1.094500058889389, train acc: 0.9087\n",
      "loss: 1.0867154896259308, train acc: 0.9094\n",
      "loss: 1.11127051115036, train acc: 0.9088\n",
      "loss: 1.0649566233158112, train acc: 0.909\n",
      "loss: 1.0636335492134095, train acc: 0.9091\n",
      "loss: 1.100555968284607, train acc: 0.9108\n",
      "epoch: 36, loss: 0.6173153519630432, train acc: 0.9108, test acc: 0.8761\n",
      "loss: 1.1041779518127441, train acc: 0.9097\n",
      "loss: 1.1167058646678925, train acc: 0.9075\n",
      "loss: 1.084849363565445, train acc: 0.9087\n",
      "loss: 1.0864581227302552, train acc: 0.9106\n",
      "loss: 1.0781026303768158, train acc: 0.9075\n",
      "loss: 1.1253061532974242, train acc: 0.9074\n",
      "loss: 1.0703965544700622, train acc: 0.9093\n",
      "loss: 1.0862384498119355, train acc: 0.9105\n",
      "epoch: 37, loss: 0.8845443725585938, train acc: 0.9105, test acc: 0.8767\n",
      "loss: 0.9551628828048706, train acc: 0.9106\n",
      "loss: 1.088436883687973, train acc: 0.9073\n",
      "loss: 1.1125088214874268, train acc: 0.9095\n",
      "loss: 1.0979621350765227, train acc: 0.9074\n",
      "loss: 1.0314501404762269, train acc: 0.9102\n",
      "loss: 1.1003464877605438, train acc: 0.9095\n",
      "loss: 1.1179586350917816, train acc: 0.9094\n",
      "loss: 1.1074426889419555, train acc: 0.9141\n",
      "epoch: 38, loss: 0.8808814287185669, train acc: 0.9141, test acc: 0.8744\n",
      "loss: 1.1905286312103271, train acc: 0.9121\n",
      "loss: 1.099159049987793, train acc: 0.9092\n",
      "loss: 1.0662635087966919, train acc: 0.9113\n",
      "loss: 1.124162757396698, train acc: 0.9118\n",
      "loss: 1.096250593662262, train acc: 0.9133\n",
      "loss: 1.0716393768787384, train acc: 0.9142\n",
      "loss: 1.0262432932853698, train acc: 0.9111\n",
      "loss: 1.146096795797348, train acc: 0.912\n",
      "epoch: 39, loss: 1.3072314262390137, train acc: 0.912, test acc: 0.8751\n",
      "loss: 1.1110684871673584, train acc: 0.9146\n",
      "loss: 1.1097623348236083, train acc: 0.9131\n",
      "loss: 1.0762016355991364, train acc: 0.912\n",
      "loss: 1.074994432926178, train acc: 0.9137\n",
      "loss: 1.0709582328796388, train acc: 0.9118\n",
      "loss: 1.0975088834762574, train acc: 0.912\n",
      "loss: 1.0681957006454468, train acc: 0.909\n",
      "loss: 1.1101876020431518, train acc: 0.9145\n",
      "epoch: 40, loss: 0.8162652850151062, train acc: 0.9145, test acc: 0.8737\n",
      "loss: 0.9853595495223999, train acc: 0.9115\n",
      "loss: 1.0852308869361877, train acc: 0.9102\n",
      "loss: 1.0396158814430236, train acc: 0.9124\n",
      "loss: 1.0822829127311706, train acc: 0.9084\n",
      "loss: 1.1183431565761566, train acc: 0.9101\n",
      "loss: 1.0960761606693268, train acc: 0.9102\n",
      "loss: 1.0785865545272828, train acc: 0.9135\n",
      "loss: 1.0969667196273805, train acc: 0.9121\n",
      "epoch: 41, loss: 1.0287258625030518, train acc: 0.9121, test acc: 0.8768\n",
      "loss: 0.9679505825042725, train acc: 0.9142\n",
      "loss: 1.1039690852165223, train acc: 0.9156\n",
      "loss: 1.103369837999344, train acc: 0.9127\n",
      "loss: 1.114871233701706, train acc: 0.9097\n",
      "loss: 1.0528829574584961, train acc: 0.9146\n",
      "loss: 1.0888661086559295, train acc: 0.9135\n",
      "loss: 1.09995596408844, train acc: 0.9132\n",
      "loss: 1.1036589682102202, train acc: 0.9113\n",
      "epoch: 42, loss: 0.8301911354064941, train acc: 0.9113, test acc: 0.874\n",
      "loss: 1.0243850946426392, train acc: 0.9136\n",
      "loss: 1.0721850275993348, train acc: 0.9059\n",
      "loss: 1.099476158618927, train acc: 0.9114\n",
      "loss: 1.1028494656085968, train acc: 0.9103\n",
      "loss: 1.0448279321193694, train acc: 0.9123\n",
      "loss: 1.0951975107192993, train acc: 0.9096\n",
      "loss: 1.0757477521896361, train acc: 0.9124\n",
      "loss: 1.0872736990451812, train acc: 0.9128\n",
      "epoch: 43, loss: 1.0672026872634888, train acc: 0.9128, test acc: 0.8723\n",
      "loss: 1.0956776142120361, train acc: 0.9131\n",
      "loss: 1.0958489179611206, train acc: 0.9143\n",
      "loss: 1.1010249555110931, train acc: 0.9111\n",
      "loss: 1.0562985360622406, train acc: 0.913\n",
      "loss: 1.0608767569065094, train acc: 0.913\n",
      "loss: 1.1040323972702026, train acc: 0.9051\n",
      "loss: 1.0938507854938506, train acc: 0.9106\n",
      "loss: 1.1159207046031951, train acc: 0.9118\n",
      "epoch: 44, loss: 0.9378883838653564, train acc: 0.9118, test acc: 0.8697\n",
      "loss: 1.1308923959732056, train acc: 0.9107\n",
      "loss: 1.0757792234420775, train acc: 0.9118\n",
      "loss: 1.0871853888034821, train acc: 0.9137\n",
      "loss: 1.082966649532318, train acc: 0.9132\n",
      "loss: 1.0957981467247009, train acc: 0.9129\n",
      "loss: 1.065050482749939, train acc: 0.913\n",
      "loss: 1.0788049876689911, train acc: 0.9126\n",
      "loss: 1.089788007736206, train acc: 0.9122\n",
      "epoch: 45, loss: 1.035387396812439, train acc: 0.9122, test acc: 0.8756\n",
      "loss: 1.0472298860549927, train acc: 0.9134\n",
      "loss: 1.0999033987522124, train acc: 0.912\n",
      "loss: 1.069124311208725, train acc: 0.9135\n",
      "loss: 1.0670421898365021, train acc: 0.9144\n",
      "loss: 1.0328674733638763, train acc: 0.9145\n",
      "loss: 1.1400002717971802, train acc: 0.9133\n",
      "loss: 1.0695019602775573, train acc: 0.9126\n",
      "loss: 1.0809607207775116, train acc: 0.9131\n",
      "epoch: 46, loss: 0.5780904293060303, train acc: 0.9131, test acc: 0.8751\n",
      "loss: 1.108760952949524, train acc: 0.9131\n",
      "loss: 1.0504633128643035, train acc: 0.9145\n",
      "loss: 1.0424698531627654, train acc: 0.9142\n",
      "loss: 1.0794450044631958, train acc: 0.9131\n",
      "loss: 1.0644946575164795, train acc: 0.9138\n",
      "loss: 1.122523409128189, train acc: 0.9109\n",
      "loss: 1.0596724927425385, train acc: 0.9128\n",
      "loss: 1.1083140194416046, train acc: 0.9143\n",
      "epoch: 47, loss: 0.7218442559242249, train acc: 0.9143, test acc: 0.8755\n",
      "loss: 1.1798526048660278, train acc: 0.9138\n",
      "loss: 1.0438893556594848, train acc: 0.9143\n",
      "loss: 1.0942498803138734, train acc: 0.9143\n",
      "loss: 1.0519319295883178, train acc: 0.9173\n",
      "loss: 1.0563283383846283, train acc: 0.9142\n",
      "loss: 1.0792179226875305, train acc: 0.9126\n",
      "loss: 1.0442565202713012, train acc: 0.916\n",
      "loss: 1.0986856281757356, train acc: 0.9152\n",
      "epoch: 48, loss: 0.8668763041496277, train acc: 0.9152, test acc: 0.8763\n",
      "loss: 1.167185664176941, train acc: 0.9154\n",
      "loss: 1.0727880597114563, train acc: 0.9173\n",
      "loss: 1.0936141610145569, train acc: 0.915\n",
      "loss: 1.0745681464672088, train acc: 0.9157\n",
      "loss: 1.0730329692363738, train acc: 0.9153\n",
      "loss: 1.0921702623367309, train acc: 0.9133\n",
      "loss: 1.0428611278533935, train acc: 0.9156\n",
      "loss: 1.0928247451782227, train acc: 0.9153\n",
      "epoch: 49, loss: 0.9233099818229675, train acc: 0.9153, test acc: 0.8735\n",
      "loss: 1.1678571701049805, train acc: 0.9161\n",
      "loss: 1.0702499806880952, train acc: 0.9156\n",
      "loss: 1.0567713379859924, train acc: 0.9164\n",
      "loss: 1.0505732238292693, train acc: 0.9136\n",
      "loss: 1.0810500383377075, train acc: 0.9156\n",
      "loss: 1.0971641898155213, train acc: 0.9161\n",
      "loss: 1.0092356741428374, train acc: 0.9176\n",
      "loss: 1.122094213962555, train acc: 0.9179\n",
      "epoch: 50, loss: 1.1757092475891113, train acc: 0.9179, test acc: 0.874\n",
      "loss: 1.0891706943511963, train acc: 0.914\n",
      "loss: 1.0582116723060608, train acc: 0.9123\n",
      "loss: 1.1064884424209596, train acc: 0.9109\n",
      "loss: 1.0916188180446624, train acc: 0.9146\n",
      "loss: 1.027704620361328, train acc: 0.9153\n",
      "loss: 1.0662553131580352, train acc: 0.9151\n",
      "loss: 1.0177769720554353, train acc: 0.9144\n",
      "loss: 1.0958255887031556, train acc: 0.9169\n",
      "epoch: 51, loss: 0.5035480856895447, train acc: 0.9169, test acc: 0.8726\n",
      "loss: 1.1062242984771729, train acc: 0.9121\n",
      "loss: 1.0499060153961182, train acc: 0.9168\n",
      "loss: 1.0713488280773162, train acc: 0.9141\n",
      "loss: 1.0858098268508911, train acc: 0.916\n",
      "loss: 1.0238239884376525, train acc: 0.9161\n",
      "loss: 1.0466561913490295, train acc: 0.9165\n",
      "loss: 1.0309427976608276, train acc: 0.9147\n",
      "loss: 1.0597277522087096, train acc: 0.9163\n",
      "epoch: 52, loss: 0.7402769327163696, train acc: 0.9163, test acc: 0.875\n",
      "loss: 1.0089741945266724, train acc: 0.9175\n",
      "loss: 1.077267724275589, train acc: 0.9148\n",
      "loss: 1.0284380555152892, train acc: 0.9142\n",
      "loss: 1.0806064367294312, train acc: 0.9162\n",
      "loss: 1.0621883511543273, train acc: 0.9141\n",
      "loss: 1.058357435464859, train acc: 0.917\n",
      "loss: 1.063381451368332, train acc: 0.9125\n",
      "loss: 1.0975905418395997, train acc: 0.9134\n",
      "epoch: 53, loss: 0.647074818611145, train acc: 0.9134, test acc: 0.874\n",
      "loss: 1.0905005931854248, train acc: 0.9133\n",
      "loss: 1.0656893014907838, train acc: 0.9142\n",
      "loss: 1.026658195257187, train acc: 0.9155\n",
      "loss: 1.0768987357616424, train acc: 0.9142\n",
      "loss: 1.0737152457237245, train acc: 0.9141\n",
      "loss: 1.0954181849956512, train acc: 0.9149\n",
      "loss: 1.0327912390232086, train acc: 0.9161\n",
      "loss: 1.071832275390625, train acc: 0.9148\n",
      "epoch: 54, loss: 1.0783485174179077, train acc: 0.9148, test acc: 0.8758\n",
      "loss: 1.0733901262283325, train acc: 0.9155\n",
      "loss: 1.076092541217804, train acc: 0.9141\n",
      "loss: 1.0665140330791474, train acc: 0.9129\n",
      "loss: 1.0818389236927033, train acc: 0.9122\n",
      "loss: 1.0553376615047454, train acc: 0.9158\n",
      "loss: 1.0987150967121124, train acc: 0.9114\n",
      "loss: 1.0918497323989869, train acc: 0.9158\n",
      "loss: 1.1026262044906616, train acc: 0.9153\n",
      "epoch: 55, loss: 0.6869795918464661, train acc: 0.9153, test acc: 0.8751\n",
      "loss: 1.083762764930725, train acc: 0.9165\n",
      "loss: 1.0676019132137298, train acc: 0.9172\n",
      "loss: 1.1000184953212737, train acc: 0.917\n",
      "loss: 1.0801207423210144, train acc: 0.9167\n",
      "loss: 1.0959644377231599, train acc: 0.9159\n",
      "loss: 1.064265990257263, train acc: 0.9144\n",
      "loss: 1.0984094262123107, train acc: 0.9165\n",
      "loss: 1.1211298823356628, train acc: 0.9172\n",
      "epoch: 56, loss: 0.7968547344207764, train acc: 0.9172, test acc: 0.8749\n",
      "loss: 0.9745558500289917, train acc: 0.9165\n",
      "loss: 1.0224055826663971, train acc: 0.9177\n",
      "loss: 1.0319006085395812, train acc: 0.9136\n",
      "loss: 1.06455300450325, train acc: 0.919\n",
      "loss: 1.0179622948169709, train acc: 0.918\n",
      "loss: 1.082157415151596, train acc: 0.9157\n",
      "loss: 1.0321927309036254, train acc: 0.9166\n",
      "loss: 1.0986481189727784, train acc: 0.9186\n",
      "epoch: 57, loss: 0.8600242137908936, train acc: 0.9186, test acc: 0.875\n",
      "loss: 1.1808397769927979, train acc: 0.915\n",
      "loss: 1.0935381174087524, train acc: 0.9119\n",
      "loss: 1.1302663385868073, train acc: 0.9159\n",
      "loss: 1.0412919640541076, train acc: 0.9153\n",
      "loss: 1.073695170879364, train acc: 0.9149\n",
      "loss: 1.0476392745971679, train acc: 0.9131\n",
      "loss: 1.0528691053390502, train acc: 0.9132\n",
      "loss: 1.0758471608161926, train acc: 0.9143\n",
      "epoch: 58, loss: 0.8247745037078857, train acc: 0.9143, test acc: 0.8701\n",
      "loss: 0.9655962586402893, train acc: 0.9143\n",
      "loss: 1.0520801544189453, train acc: 0.9156\n",
      "loss: 1.0667170643806458, train acc: 0.9124\n",
      "loss: 1.0608602285385131, train acc: 0.914\n",
      "loss: 1.0548789560794831, train acc: 0.9147\n",
      "loss: 1.0712694942951202, train acc: 0.9108\n",
      "loss: 1.0534951448440553, train acc: 0.912\n",
      "loss: 1.0714608490467072, train acc: 0.9152\n",
      "epoch: 59, loss: 0.5345175862312317, train acc: 0.9152, test acc: 0.8718\n",
      "loss: 1.2493970394134521, train acc: 0.9162\n",
      "loss: 1.046544075012207, train acc: 0.9163\n",
      "loss: 1.0164182364940644, train acc: 0.9148\n",
      "loss: 1.0518554329872132, train acc: 0.9182\n",
      "loss: 1.0504971385002135, train acc: 0.9157\n",
      "loss: 1.099972814321518, train acc: 0.9179\n",
      "loss: 1.039275324344635, train acc: 0.9149\n",
      "loss: 1.0835099041461944, train acc: 0.9157\n",
      "epoch: 60, loss: 0.938603937625885, train acc: 0.9157, test acc: 0.8726\n",
      "loss: 1.16831636428833, train acc: 0.9156\n",
      "loss: 1.0618522882461547, train acc: 0.9141\n",
      "loss: 1.06240713596344, train acc: 0.9168\n",
      "loss: 1.0709753274917602, train acc: 0.9153\n",
      "loss: 1.0524854481220245, train acc: 0.9136\n",
      "loss: 1.046262127161026, train acc: 0.9135\n",
      "loss: 1.0521246254444123, train acc: 0.9127\n",
      "loss: 1.071904981136322, train acc: 0.9155\n",
      "epoch: 61, loss: 0.910162091255188, train acc: 0.9155, test acc: 0.8716\n",
      "loss: 1.1644254922866821, train acc: 0.9136\n",
      "loss: 1.0935222864151002, train acc: 0.9142\n",
      "loss: 1.038242644071579, train acc: 0.9167\n",
      "loss: 1.0442278742790223, train acc: 0.9161\n",
      "loss: 1.0850648939609528, train acc: 0.9152\n",
      "loss: 1.1037404417991639, train acc: 0.9134\n",
      "loss: 1.0396045744419098, train acc: 0.9149\n",
      "loss: 1.0886442124843598, train acc: 0.9154\n",
      "epoch: 62, loss: 0.4850855767726898, train acc: 0.9154, test acc: 0.8735\n",
      "loss: 1.0933918952941895, train acc: 0.9177\n",
      "loss: 1.0431520581245421, train acc: 0.9155\n",
      "loss: 1.0545906007289887, train acc: 0.9155\n",
      "loss: 1.0542287886142732, train acc: 0.9163\n",
      "loss: 0.9888443112373352, train acc: 0.914\n",
      "loss: 1.0231048583984375, train acc: 0.9156\n",
      "loss: 1.0970173120498656, train acc: 0.9156\n",
      "loss: 1.0550786018371583, train acc: 0.9163\n",
      "epoch: 63, loss: 0.8458396792411804, train acc: 0.9163, test acc: 0.8693\n",
      "loss: 1.1218653917312622, train acc: 0.9155\n",
      "loss: 1.0704742252826691, train acc: 0.9134\n",
      "loss: 1.0371845483779907, train acc: 0.9138\n",
      "loss: 0.9996768295764923, train acc: 0.9175\n",
      "loss: 1.0637834131717683, train acc: 0.9149\n",
      "loss: 1.02508664727211, train acc: 0.9173\n",
      "loss: 1.0270878553390503, train acc: 0.9162\n",
      "loss: 1.0563997030258179, train acc: 0.9157\n",
      "epoch: 64, loss: 0.8260214924812317, train acc: 0.9157, test acc: 0.869\n",
      "loss: 1.0869674682617188, train acc: 0.9151\n",
      "loss: 1.0504639804363252, train acc: 0.9094\n",
      "loss: 1.056380045413971, train acc: 0.9162\n",
      "loss: 1.0161867558956146, train acc: 0.9141\n",
      "loss: 1.0892405927181243, train acc: 0.9137\n",
      "loss: 1.0849416375160217, train acc: 0.9152\n",
      "loss: 1.048242211341858, train acc: 0.9152\n",
      "loss: 1.0179805994033813, train acc: 0.9166\n",
      "epoch: 65, loss: 0.8373133540153503, train acc: 0.9166, test acc: 0.8715\n",
      "loss: 0.9935291409492493, train acc: 0.9159\n",
      "loss: 1.066301417350769, train acc: 0.9162\n",
      "loss: 1.0775094270706176, train acc: 0.9137\n",
      "loss: 1.0669614791870117, train acc: 0.9162\n",
      "loss: 1.0533122420310974, train acc: 0.9144\n",
      "loss: 1.0977725267410279, train acc: 0.9154\n",
      "loss: 1.0126153469085692, train acc: 0.9175\n",
      "loss: 1.0935456335544587, train acc: 0.9182\n",
      "epoch: 66, loss: 0.6191380620002747, train acc: 0.9182, test acc: 0.8715\n",
      "loss: 1.089471459388733, train acc: 0.9163\n",
      "loss: 1.0782054901123046, train acc: 0.9116\n",
      "loss: 1.0521885931491852, train acc: 0.9146\n",
      "loss: 1.030708420276642, train acc: 0.9168\n",
      "loss: 1.0752311587333678, train acc: 0.917\n",
      "loss: 1.0429252803325653, train acc: 0.9108\n",
      "loss: 1.0725547134876252, train acc: 0.9166\n",
      "loss: 1.0762657523155212, train acc: 0.9158\n",
      "epoch: 67, loss: 0.670075535774231, train acc: 0.9158, test acc: 0.8722\n",
      "loss: 0.9828659892082214, train acc: 0.9193\n",
      "loss: 1.0351664066314696, train acc: 0.9149\n",
      "loss: 1.0767282664775848, train acc: 0.9178\n",
      "loss: 1.0401148676872254, train acc: 0.917\n",
      "loss: 1.0514574110507966, train acc: 0.9127\n",
      "loss: 1.026844185590744, train acc: 0.9166\n",
      "loss: 1.0592121005058288, train acc: 0.9149\n",
      "loss: 1.0987630128860473, train acc: 0.9163\n",
      "epoch: 68, loss: 0.6280357241630554, train acc: 0.9163, test acc: 0.867\n",
      "loss: 0.9368386268615723, train acc: 0.9157\n",
      "loss: 1.023836225271225, train acc: 0.9179\n",
      "loss: 1.027765268087387, train acc: 0.9164\n",
      "loss: 1.0251391112804413, train acc: 0.9161\n",
      "loss: 1.0536215603351593, train acc: 0.9192\n",
      "loss: 1.078742790222168, train acc: 0.9159\n",
      "loss: 1.0384368598461151, train acc: 0.9164\n",
      "loss: 1.0461824595928193, train acc: 0.9159\n",
      "epoch: 69, loss: 1.1133726835250854, train acc: 0.9159, test acc: 0.87\n",
      "loss: 1.191253423690796, train acc: 0.919\n",
      "loss: 1.0656644344329833, train acc: 0.9072\n",
      "loss: 1.0254489302635192, train acc: 0.9165\n",
      "loss: 0.9859804093837738, train acc: 0.9173\n",
      "loss: 1.0469927370548249, train acc: 0.9169\n",
      "loss: 1.0712763905525207, train acc: 0.9183\n",
      "loss: 1.0596325755119325, train acc: 0.918\n",
      "loss: 1.0674727618694306, train acc: 0.915\n",
      "epoch: 70, loss: 0.8128862977027893, train acc: 0.915, test acc: 0.8711\n",
      "loss: 0.9968863725662231, train acc: 0.9205\n",
      "loss: 1.0843683362007142, train acc: 0.9204\n",
      "loss: 1.0538644552230836, train acc: 0.9142\n",
      "loss: 1.0773330688476563, train acc: 0.9161\n",
      "loss: 1.0917598128318786, train acc: 0.9162\n",
      "loss: 1.0498008906841279, train acc: 0.9177\n",
      "loss: 1.0317531764507293, train acc: 0.917\n",
      "loss: 1.0480682313442231, train acc: 0.9153\n",
      "epoch: 71, loss: 1.066279411315918, train acc: 0.9153, test acc: 0.8724\n",
      "loss: 1.1311020851135254, train acc: 0.921\n",
      "loss: 1.0384867429733275, train acc: 0.9182\n",
      "loss: 1.0451601803302766, train acc: 0.913\n",
      "loss: 1.0557187378406525, train acc: 0.9147\n",
      "loss: 1.04268336892128, train acc: 0.9203\n",
      "loss: 1.0368610739707946, train acc: 0.9136\n",
      "loss: 1.045430028438568, train acc: 0.9205\n",
      "loss: 1.0454039871692657, train acc: 0.9151\n",
      "epoch: 72, loss: 0.6716474890708923, train acc: 0.9151, test acc: 0.8716\n",
      "loss: 1.002756118774414, train acc: 0.9192\n",
      "loss: 1.0669123172760009, train acc: 0.9113\n",
      "loss: 1.0559989213943481, train acc: 0.9153\n",
      "loss: 1.0562577724456788, train acc: 0.9175\n",
      "loss: 0.9802992463111877, train acc: 0.9177\n",
      "loss: 1.0550893068313598, train acc: 0.9184\n",
      "loss: 1.0522717237472534, train acc: 0.9154\n",
      "loss: 1.0680713176727294, train acc: 0.9185\n",
      "epoch: 73, loss: 0.8209072947502136, train acc: 0.9185, test acc: 0.8698\n",
      "loss: 1.1003938913345337, train acc: 0.9176\n",
      "loss: 1.034090518951416, train acc: 0.9098\n",
      "loss: 1.0173272788524628, train acc: 0.9176\n",
      "loss: 1.054003393650055, train acc: 0.9165\n",
      "loss: 0.9893599390983582, train acc: 0.9169\n",
      "loss: 1.0625226140022277, train acc: 0.9184\n",
      "loss: 0.9817525029182435, train acc: 0.9202\n",
      "loss: 1.0478680193424226, train acc: 0.919\n",
      "epoch: 74, loss: 1.0465344190597534, train acc: 0.919, test acc: 0.8725\n",
      "loss: 1.0620421171188354, train acc: 0.9204\n",
      "loss: 1.0245195627212524, train acc: 0.9159\n",
      "loss: 1.022046113014221, train acc: 0.916\n",
      "loss: 1.0440031945705415, train acc: 0.9166\n",
      "loss: 1.041415411233902, train acc: 0.9174\n",
      "loss: 1.084016740322113, train acc: 0.9196\n",
      "loss: 1.047691524028778, train acc: 0.9169\n",
      "loss: 1.050695389509201, train acc: 0.9196\n",
      "epoch: 75, loss: 0.6441726684570312, train acc: 0.9196, test acc: 0.8697\n",
      "loss: 1.2341870069503784, train acc: 0.9207\n",
      "loss: 1.1232713401317596, train acc: 0.9171\n",
      "loss: 1.0314736783504486, train acc: 0.9156\n",
      "loss: 1.0840006172657013, train acc: 0.9186\n",
      "loss: 1.046396166086197, train acc: 0.9171\n",
      "loss: 1.0857693314552308, train acc: 0.9122\n",
      "loss: 1.0844422698020935, train acc: 0.916\n",
      "loss: 1.0985025465488434, train acc: 0.9168\n",
      "epoch: 76, loss: 0.7767063975334167, train acc: 0.9168, test acc: 0.8673\n",
      "loss: 1.056631326675415, train acc: 0.9159\n",
      "loss: 1.075067162513733, train acc: 0.9211\n",
      "loss: 1.0681131303310394, train acc: 0.9176\n",
      "loss: 0.9841720044612885, train acc: 0.9184\n",
      "loss: 1.044680619239807, train acc: 0.9157\n",
      "loss: 1.0569796502590179, train acc: 0.9132\n",
      "loss: 1.0273464262485503, train acc: 0.9177\n",
      "loss: 1.006346482038498, train acc: 0.9185\n",
      "epoch: 77, loss: 0.6286401748657227, train acc: 0.9185, test acc: 0.8702\n",
      "loss: 0.9563974142074585, train acc: 0.9167\n",
      "loss: 1.0442260682582856, train acc: 0.9138\n",
      "loss: 1.069777286052704, train acc: 0.917\n",
      "loss: 1.0534188985824584, train acc: 0.9176\n",
      "loss: 1.0441535830497741, train acc: 0.9146\n",
      "loss: 1.0735385477542878, train acc: 0.9164\n",
      "loss: 1.018965047597885, train acc: 0.9153\n",
      "loss: 1.1201360046863555, train acc: 0.9194\n",
      "epoch: 78, loss: 0.609809160232544, train acc: 0.9194, test acc: 0.8691\n",
      "loss: 1.0245627164840698, train acc: 0.9178\n",
      "loss: 1.072163164615631, train acc: 0.9171\n",
      "loss: 1.0138670086860657, train acc: 0.916\n",
      "loss: 1.0991737008094788, train acc: 0.9159\n",
      "loss: 1.0172043859958648, train acc: 0.9204\n",
      "loss: 1.0161712050437928, train acc: 0.9165\n",
      "loss: 1.0442040026187898, train acc: 0.9168\n",
      "loss: 1.0900180160999298, train acc: 0.9158\n",
      "epoch: 79, loss: 0.7841010093688965, train acc: 0.9158, test acc: 0.8697\n",
      "#####training and testing end with K:10, P:0.5######\n",
      "#####training and testing start with K:10, P:1######\n",
      "loss: 2.379467248916626, train acc: 0.109\n",
      "loss: 2.23245484828949, train acc: 0.2521\n",
      "loss: 1.9771977186203002, train acc: 0.3665\n",
      "loss: 1.7202701091766357, train acc: 0.4826\n",
      "loss: 1.5058937430381776, train acc: 0.5641\n",
      "loss: 1.3883812069892882, train acc: 0.6221\n",
      "loss: 1.2159345507621766, train acc: 0.6761\n",
      "loss: 1.0689660549163817, train acc: 0.7051\n",
      "epoch: 0, loss: 0.9063535332679749, train acc: 0.7051, test acc: 0.7678\n",
      "loss: 0.833784282207489, train acc: 0.7623\n",
      "loss: 0.8795120120048523, train acc: 0.7869\n",
      "loss: 0.772158396244049, train acc: 0.8005\n",
      "loss: 0.7262367010116577, train acc: 0.8118\n",
      "loss: 0.6385530650615692, train acc: 0.8273\n",
      "loss: 0.6644552052021027, train acc: 0.8353\n",
      "loss: 0.6192021191120147, train acc: 0.8434\n",
      "loss: 0.5871184557676316, train acc: 0.8443\n",
      "epoch: 1, loss: 0.4511246085166931, train acc: 0.8443, test acc: 0.8448\n",
      "loss: 0.47839003801345825, train acc: 0.8506\n",
      "loss: 0.573235148191452, train acc: 0.849\n",
      "loss: 0.5128538727760314, train acc: 0.8521\n",
      "loss: 0.528622642159462, train acc: 0.8599\n",
      "loss: 0.47014050781726835, train acc: 0.8659\n",
      "loss: 0.5211542218923568, train acc: 0.8678\n",
      "loss: 0.49988958835601804, train acc: 0.8709\n",
      "loss: 0.47295697331428527, train acc: 0.8738\n",
      "epoch: 2, loss: 0.32508385181427, train acc: 0.8738, test acc: 0.8668\n",
      "loss: 0.37862682342529297, train acc: 0.8777\n",
      "loss: 0.47920125126838686, train acc: 0.8743\n",
      "loss: 0.4192811667919159, train acc: 0.8755\n",
      "loss: 0.45441147685050964, train acc: 0.8808\n",
      "loss: 0.39408406615257263, train acc: 0.8842\n",
      "loss: 0.4507197290658951, train acc: 0.8855\n",
      "loss: 0.43969769179821017, train acc: 0.8885\n",
      "loss: 0.41111346781253816, train acc: 0.8901\n",
      "epoch: 3, loss: 0.25243932008743286, train acc: 0.8901, test acc: 0.8785\n",
      "loss: 0.32143449783325195, train acc: 0.8932\n",
      "loss: 0.4244735449552536, train acc: 0.8896\n",
      "loss: 0.36199903935194017, train acc: 0.8919\n",
      "loss: 0.4074668437242508, train acc: 0.8941\n",
      "loss: 0.3449994370341301, train acc: 0.8962\n",
      "loss: 0.4042262494564056, train acc: 0.897\n",
      "loss: 0.3980811685323715, train acc: 0.8982\n",
      "loss: 0.36952504217624665, train acc: 0.8998\n",
      "epoch: 4, loss: 0.2033129781484604, train acc: 0.8998, test acc: 0.885\n",
      "loss: 0.2822817265987396, train acc: 0.9046\n",
      "loss: 0.38696849048137666, train acc: 0.8987\n",
      "loss: 0.322086825966835, train acc: 0.9031\n",
      "loss: 0.3752647086977959, train acc: 0.9048\n",
      "loss: 0.31228922456502917, train acc: 0.9046\n",
      "loss: 0.3719386875629425, train acc: 0.9069\n",
      "loss: 0.36843070983886717, train acc: 0.9088\n",
      "loss: 0.3412156865000725, train acc: 0.9088\n",
      "epoch: 5, loss: 0.16798602044582367, train acc: 0.9088, test acc: 0.8927\n",
      "loss: 0.2551010549068451, train acc: 0.9133\n",
      "loss: 0.3603813320398331, train acc: 0.9065\n",
      "loss: 0.29405427575111387, train acc: 0.9089\n",
      "loss: 0.35231009423732756, train acc: 0.9119\n",
      "loss: 0.2898610457777977, train acc: 0.912\n",
      "loss: 0.34841897487640383, train acc: 0.9134\n",
      "loss: 0.3463700503110886, train acc: 0.9147\n",
      "loss: 0.3212294399738312, train acc: 0.9149\n",
      "epoch: 6, loss: 0.14285209774971008, train acc: 0.9149, test acc: 0.8972\n",
      "loss: 0.234771266579628, train acc: 0.9171\n",
      "loss: 0.34068893641233444, train acc: 0.9125\n",
      "loss: 0.2734560951590538, train acc: 0.9139\n",
      "loss: 0.33593371957540513, train acc: 0.9176\n",
      "loss: 0.2734702989459038, train acc: 0.9175\n",
      "loss: 0.3301300048828125, train acc: 0.9184\n",
      "loss: 0.32957326620817184, train acc: 0.9201\n",
      "loss: 0.30722464621067047, train acc: 0.9198\n",
      "epoch: 7, loss: 0.12555386126041412, train acc: 0.9198, test acc: 0.8995\n",
      "loss: 0.2191956639289856, train acc: 0.9205\n",
      "loss: 0.32556777000427245, train acc: 0.9172\n",
      "loss: 0.25741438418626783, train acc: 0.9195\n",
      "loss: 0.3234316289424896, train acc: 0.9211\n",
      "loss: 0.2609973013401031, train acc: 0.9212\n",
      "loss: 0.31630018055438996, train acc: 0.9214\n",
      "loss: 0.31601288467645644, train acc: 0.9233\n",
      "loss: 0.29643125683069227, train acc: 0.924\n",
      "epoch: 8, loss: 0.11247789859771729, train acc: 0.924, test acc: 0.9016\n",
      "loss: 0.20687608420848846, train acc: 0.923\n",
      "loss: 0.31361940652132037, train acc: 0.9193\n",
      "loss: 0.2447101041674614, train acc: 0.9228\n",
      "loss: 0.3135501191020012, train acc: 0.9236\n",
      "loss: 0.2512535214424133, train acc: 0.9236\n",
      "loss: 0.30492244064807894, train acc: 0.9247\n",
      "loss: 0.3051686853170395, train acc: 0.9251\n",
      "loss: 0.2875332996249199, train acc: 0.9258\n",
      "epoch: 9, loss: 0.1021815836429596, train acc: 0.9258, test acc: 0.9019\n",
      "loss: 0.19640575349330902, train acc: 0.9243\n",
      "loss: 0.3037093058228493, train acc: 0.9228\n",
      "loss: 0.23424647897481918, train acc: 0.9254\n",
      "loss: 0.30544513314962385, train acc: 0.9263\n",
      "loss: 0.2430822104215622, train acc: 0.926\n",
      "loss: 0.2954982936382294, train acc: 0.9262\n",
      "loss: 0.2955960348248482, train acc: 0.9266\n",
      "loss: 0.28022567331790926, train acc: 0.9273\n",
      "epoch: 10, loss: 0.09311933070421219, train acc: 0.9273, test acc: 0.903\n",
      "loss: 0.18683554232120514, train acc: 0.9266\n",
      "loss: 0.29547925144433973, train acc: 0.9257\n",
      "loss: 0.2257322408258915, train acc: 0.927\n",
      "loss: 0.2988728046417236, train acc: 0.9286\n",
      "loss: 0.23587124198675155, train acc: 0.9276\n",
      "loss: 0.28705747574567797, train acc: 0.9276\n",
      "loss: 0.28707204312086104, train acc: 0.9291\n",
      "loss: 0.2739164762198925, train acc: 0.9291\n",
      "epoch: 11, loss: 0.08572793006896973, train acc: 0.9291, test acc: 0.9036\n",
      "loss: 0.17849984765052795, train acc: 0.9282\n",
      "loss: 0.28833515048027036, train acc: 0.9275\n",
      "loss: 0.218283262103796, train acc: 0.9288\n",
      "loss: 0.292955382168293, train acc: 0.93\n",
      "loss: 0.23005537688732147, train acc: 0.9308\n",
      "loss: 0.2800188079476357, train acc: 0.9289\n",
      "loss: 0.27941561937332154, train acc: 0.9309\n",
      "loss: 0.2683953709900379, train acc: 0.9306\n",
      "epoch: 12, loss: 0.07848304510116577, train acc: 0.9306, test acc: 0.9041\n",
      "loss: 0.17061612010002136, train acc: 0.9303\n",
      "loss: 0.2819366753101349, train acc: 0.929\n",
      "loss: 0.21201566085219384, train acc: 0.9308\n",
      "loss: 0.28767071664333344, train acc: 0.932\n",
      "loss: 0.2243368461728096, train acc: 0.9323\n",
      "loss: 0.2735570102930069, train acc: 0.9309\n",
      "loss: 0.2726362094283104, train acc: 0.9331\n",
      "loss: 0.26337705850601195, train acc: 0.9318\n",
      "epoch: 13, loss: 0.07261598855257034, train acc: 0.9318, test acc: 0.9042\n",
      "loss: 0.16325245797634125, train acc: 0.9319\n",
      "loss: 0.2764085426926613, train acc: 0.9311\n",
      "loss: 0.2061570756137371, train acc: 0.9312\n",
      "loss: 0.2826583057641983, train acc: 0.9333\n",
      "loss: 0.2193577244877815, train acc: 0.934\n",
      "loss: 0.2676884815096855, train acc: 0.9325\n",
      "loss: 0.2664852410554886, train acc: 0.9335\n",
      "loss: 0.25862530767917635, train acc: 0.9329\n",
      "epoch: 14, loss: 0.06777162849903107, train acc: 0.9329, test acc: 0.9038\n",
      "loss: 0.15686769783496857, train acc: 0.9327\n",
      "loss: 0.27125714123249056, train acc: 0.9331\n",
      "loss: 0.20101563334465028, train acc: 0.9324\n",
      "loss: 0.2781384393572807, train acc: 0.9347\n",
      "loss: 0.21474871039390564, train acc: 0.9347\n",
      "loss: 0.2624718636274338, train acc: 0.9334\n",
      "loss: 0.26090522259473803, train acc: 0.935\n",
      "loss: 0.2544844850897789, train acc: 0.9341\n",
      "epoch: 15, loss: 0.06446334719657898, train acc: 0.9341, test acc: 0.9041\n",
      "loss: 0.1511521339416504, train acc: 0.9345\n",
      "loss: 0.2664697155356407, train acc: 0.9341\n",
      "loss: 0.19626143500208854, train acc: 0.9338\n",
      "loss: 0.2741365015506744, train acc: 0.9359\n",
      "loss: 0.2106181040406227, train acc: 0.9357\n",
      "loss: 0.2573883220553398, train acc: 0.9349\n",
      "loss: 0.2561895251274109, train acc: 0.9365\n",
      "loss: 0.25056944563984873, train acc: 0.9358\n",
      "epoch: 16, loss: 0.060798339545726776, train acc: 0.9358, test acc: 0.9037\n",
      "loss: 0.14566870033740997, train acc: 0.9355\n",
      "loss: 0.2621019586920738, train acc: 0.9351\n",
      "loss: 0.19212788790464402, train acc: 0.935\n",
      "loss: 0.2701964691281319, train acc: 0.9374\n",
      "loss: 0.20650277584791182, train acc: 0.9369\n",
      "loss: 0.2526844397187233, train acc: 0.9363\n",
      "loss: 0.2515582725405693, train acc: 0.9374\n",
      "loss: 0.2470826156437397, train acc: 0.9374\n",
      "epoch: 17, loss: 0.057681821286678314, train acc: 0.9374, test acc: 0.9032\n",
      "loss: 0.1402072161436081, train acc: 0.9366\n",
      "loss: 0.2579501822590828, train acc: 0.9362\n",
      "loss: 0.18827712237834932, train acc: 0.9359\n",
      "loss: 0.26633960753679276, train acc: 0.9384\n",
      "loss: 0.2028411477804184, train acc: 0.9386\n",
      "loss: 0.24852642863988877, train acc: 0.9365\n",
      "loss: 0.24715315997600557, train acc: 0.9377\n",
      "loss: 0.24364922493696212, train acc: 0.9378\n",
      "epoch: 18, loss: 0.054520778357982635, train acc: 0.9378, test acc: 0.9036\n",
      "loss: 0.1353992223739624, train acc: 0.9376\n",
      "loss: 0.253872275352478, train acc: 0.9369\n",
      "loss: 0.1847460649907589, train acc: 0.9369\n",
      "loss: 0.26278121024370193, train acc: 0.9391\n",
      "loss: 0.19963231086730956, train acc: 0.9398\n",
      "loss: 0.2447801411151886, train acc: 0.9375\n",
      "loss: 0.24274001121520997, train acc: 0.9387\n",
      "loss: 0.2404710866510868, train acc: 0.9394\n",
      "epoch: 19, loss: 0.051945388317108154, train acc: 0.9394, test acc: 0.9036\n",
      "loss: 0.13120226562023163, train acc: 0.9384\n",
      "loss: 0.25019395649433135, train acc: 0.938\n",
      "loss: 0.18142583295702935, train acc: 0.9376\n",
      "loss: 0.25902021378278733, train acc: 0.9389\n",
      "loss: 0.1962185189127922, train acc: 0.9409\n",
      "loss: 0.24113767445087433, train acc: 0.9378\n",
      "loss: 0.23886511772871016, train acc: 0.9396\n",
      "loss: 0.2374544009566307, train acc: 0.9405\n",
      "epoch: 20, loss: 0.04921489208936691, train acc: 0.9405, test acc: 0.9033\n",
      "loss: 0.12753835320472717, train acc: 0.9392\n",
      "loss: 0.24662242829799652, train acc: 0.9386\n",
      "loss: 0.178248480707407, train acc: 0.9383\n",
      "loss: 0.2554773300886154, train acc: 0.9403\n",
      "loss: 0.19330273568630219, train acc: 0.9416\n",
      "loss: 0.23780032098293305, train acc: 0.9386\n",
      "loss: 0.23494806587696077, train acc: 0.9397\n",
      "loss: 0.23473041132092476, train acc: 0.941\n",
      "epoch: 21, loss: 0.04661506786942482, train acc: 0.941, test acc: 0.9031\n",
      "loss: 0.12371153384447098, train acc: 0.94\n",
      "loss: 0.2432200275361538, train acc: 0.9398\n",
      "loss: 0.17516928762197495, train acc: 0.9387\n",
      "loss: 0.25227175652980804, train acc: 0.9407\n",
      "loss: 0.19023164063692094, train acc: 0.9418\n",
      "loss: 0.23457007855176926, train acc: 0.9396\n",
      "loss: 0.2315494865179062, train acc: 0.9402\n",
      "loss: 0.23196714520454406, train acc: 0.9417\n",
      "epoch: 22, loss: 0.044394124299287796, train acc: 0.9417, test acc: 0.9034\n",
      "loss: 0.12047716230154037, train acc: 0.9409\n",
      "loss: 0.23979857340455055, train acc: 0.9405\n",
      "loss: 0.17232729494571686, train acc: 0.9393\n",
      "loss: 0.24900174587965013, train acc: 0.9415\n",
      "loss: 0.18757617399096488, train acc: 0.9423\n",
      "loss: 0.2313453420996666, train acc: 0.9404\n",
      "loss: 0.2277609884738922, train acc: 0.9408\n",
      "loss: 0.2295277699828148, train acc: 0.9422\n",
      "epoch: 23, loss: 0.04267701506614685, train acc: 0.9422, test acc: 0.9028\n",
      "loss: 0.11739437282085419, train acc: 0.942\n",
      "loss: 0.2365918517112732, train acc: 0.9408\n",
      "loss: 0.16979183554649352, train acc: 0.9393\n",
      "loss: 0.24583781957626344, train acc: 0.942\n",
      "loss: 0.1847962573170662, train acc: 0.9422\n",
      "loss: 0.22840128540992738, train acc: 0.9413\n",
      "loss: 0.22445107400417327, train acc: 0.9417\n",
      "loss: 0.22712327241897584, train acc: 0.943\n",
      "epoch: 24, loss: 0.04117737337946892, train acc: 0.943, test acc: 0.9022\n",
      "loss: 0.11478149145841599, train acc: 0.942\n",
      "loss: 0.23358906880021096, train acc: 0.9414\n",
      "loss: 0.16708432957530023, train acc: 0.9396\n",
      "loss: 0.24277930408716203, train acc: 0.9424\n",
      "loss: 0.18241236433386804, train acc: 0.9426\n",
      "loss: 0.22559454590082167, train acc: 0.9418\n",
      "loss: 0.22086622565984726, train acc: 0.9423\n",
      "loss: 0.2248593360185623, train acc: 0.9439\n",
      "epoch: 25, loss: 0.03947441279888153, train acc: 0.9439, test acc: 0.9028\n",
      "loss: 0.11226110905408859, train acc: 0.9429\n",
      "loss: 0.2306159518659115, train acc: 0.9415\n",
      "loss: 0.16478415429592133, train acc: 0.9401\n",
      "loss: 0.23986670672893523, train acc: 0.9429\n",
      "loss: 0.18003946617245675, train acc: 0.943\n",
      "loss: 0.22289150804281235, train acc: 0.9426\n",
      "loss: 0.21812873929739, train acc: 0.9436\n",
      "loss: 0.22232768386602403, train acc: 0.9444\n",
      "epoch: 26, loss: 0.03787974268198013, train acc: 0.9444, test acc: 0.9033\n",
      "loss: 0.1099470853805542, train acc: 0.9437\n",
      "loss: 0.22761407643556594, train acc: 0.9421\n",
      "loss: 0.1623761013150215, train acc: 0.941\n",
      "loss: 0.23703404068946837, train acc: 0.9433\n",
      "loss: 0.17760526314377784, train acc: 0.9433\n",
      "loss: 0.22034161165356636, train acc: 0.943\n",
      "loss: 0.21494100838899613, train acc: 0.944\n",
      "loss: 0.22016244977712632, train acc: 0.9451\n",
      "epoch: 27, loss: 0.0365329384803772, train acc: 0.9451, test acc: 0.9036\n",
      "loss: 0.10769515484571457, train acc: 0.9443\n",
      "loss: 0.22470843121409417, train acc: 0.9426\n",
      "loss: 0.16023753583431244, train acc: 0.9414\n",
      "loss: 0.23427364528179168, train acc: 0.9436\n",
      "loss: 0.17530836611986161, train acc: 0.944\n",
      "loss: 0.2176882028579712, train acc: 0.9437\n",
      "loss: 0.21213962882757187, train acc: 0.9452\n",
      "loss: 0.21768178418278694, train acc: 0.9458\n",
      "epoch: 28, loss: 0.035407654941082, train acc: 0.9458, test acc: 0.9034\n",
      "loss: 0.10591249912977219, train acc: 0.9449\n",
      "loss: 0.22215558663010598, train acc: 0.9425\n",
      "loss: 0.1580868050456047, train acc: 0.9423\n",
      "loss: 0.23155787736177444, train acc: 0.9442\n",
      "loss: 0.17260478660464287, train acc: 0.9442\n",
      "loss: 0.21539268866181374, train acc: 0.9444\n",
      "loss: 0.20939049869775772, train acc: 0.9458\n",
      "loss: 0.21542819812893868, train acc: 0.946\n",
      "epoch: 29, loss: 0.03419123589992523, train acc: 0.946, test acc: 0.9035\n",
      "loss: 0.10327482223510742, train acc: 0.9461\n",
      "loss: 0.21951934844255447, train acc: 0.9432\n",
      "loss: 0.156329295784235, train acc: 0.943\n",
      "loss: 0.22918484061956407, train acc: 0.9443\n",
      "loss: 0.1703992448747158, train acc: 0.9454\n",
      "loss: 0.21281955540180206, train acc: 0.9449\n",
      "loss: 0.20710209086537362, train acc: 0.9462\n",
      "loss: 0.21313119158148766, train acc: 0.9464\n",
      "epoch: 30, loss: 0.033045124262571335, train acc: 0.9464, test acc: 0.9039\n",
      "loss: 0.10150720179080963, train acc: 0.946\n",
      "loss: 0.2171840563416481, train acc: 0.9437\n",
      "loss: 0.15416260957717895, train acc: 0.9437\n",
      "loss: 0.2265338584780693, train acc: 0.9449\n",
      "loss: 0.16810716614127158, train acc: 0.9458\n",
      "loss: 0.21058545187115668, train acc: 0.9453\n",
      "loss: 0.20443331077694893, train acc: 0.9459\n",
      "loss: 0.2111000582575798, train acc: 0.9464\n",
      "epoch: 31, loss: 0.03183204308152199, train acc: 0.9464, test acc: 0.9038\n",
      "loss: 0.09963297843933105, train acc: 0.9465\n",
      "loss: 0.21471965610980986, train acc: 0.9442\n",
      "loss: 0.15251204520463943, train acc: 0.944\n",
      "loss: 0.22426531612873077, train acc: 0.9449\n",
      "loss: 0.1660496287047863, train acc: 0.9472\n",
      "loss: 0.20825036987662315, train acc: 0.9456\n",
      "loss: 0.2022308275103569, train acc: 0.9463\n",
      "loss: 0.2089502327144146, train acc: 0.9468\n",
      "epoch: 32, loss: 0.030949709936976433, train acc: 0.9468, test acc: 0.904\n",
      "loss: 0.09850431233644485, train acc: 0.9468\n",
      "loss: 0.21263337209820748, train acc: 0.9445\n",
      "loss: 0.15029402151703836, train acc: 0.9443\n",
      "loss: 0.22200219035148622, train acc: 0.9454\n",
      "loss: 0.1639831021428108, train acc: 0.9475\n",
      "loss: 0.20617515295743943, train acc: 0.9462\n",
      "loss: 0.19996213391423226, train acc: 0.9467\n",
      "loss: 0.20677945166826248, train acc: 0.948\n",
      "epoch: 33, loss: 0.030615264549851418, train acc: 0.948, test acc: 0.9036\n",
      "loss: 0.0969112291932106, train acc: 0.9474\n",
      "loss: 0.21038710251450538, train acc: 0.9454\n",
      "loss: 0.14875799790024757, train acc: 0.9447\n",
      "loss: 0.21988273710012435, train acc: 0.9459\n",
      "loss: 0.1622301794588566, train acc: 0.9478\n",
      "loss: 0.2040656328201294, train acc: 0.9469\n",
      "loss: 0.19776114374399184, train acc: 0.9476\n",
      "loss: 0.2049596391618252, train acc: 0.9478\n",
      "epoch: 34, loss: 0.029494646936655045, train acc: 0.9478, test acc: 0.9039\n",
      "loss: 0.09518003463745117, train acc: 0.9481\n",
      "loss: 0.20804485082626342, train acc: 0.9461\n",
      "loss: 0.14732528999447822, train acc: 0.9453\n",
      "loss: 0.2176031306385994, train acc: 0.9471\n",
      "loss: 0.15990763306617736, train acc: 0.9482\n",
      "loss: 0.20211700052022935, train acc: 0.947\n",
      "loss: 0.19571632295846939, train acc: 0.9478\n",
      "loss: 0.2031177319586277, train acc: 0.9487\n",
      "epoch: 35, loss: 0.029115229845046997, train acc: 0.9487, test acc: 0.9033\n",
      "loss: 0.0942702516913414, train acc: 0.9483\n",
      "loss: 0.20623064860701562, train acc: 0.9467\n",
      "loss: 0.1452472247183323, train acc: 0.9457\n",
      "loss: 0.21549053117632866, train acc: 0.9484\n",
      "loss: 0.1580898307263851, train acc: 0.9484\n",
      "loss: 0.20030533224344255, train acc: 0.9476\n",
      "loss: 0.19338230192661285, train acc: 0.9486\n",
      "loss: 0.20105026215314864, train acc: 0.9494\n",
      "epoch: 36, loss: 0.02723652869462967, train acc: 0.9494, test acc: 0.9041\n",
      "loss: 0.09314195066690445, train acc: 0.9488\n",
      "loss: 0.20426856800913812, train acc: 0.9469\n",
      "loss: 0.14376347959041597, train acc: 0.9463\n",
      "loss: 0.2136494278907776, train acc: 0.9483\n",
      "loss: 0.1562374636530876, train acc: 0.9492\n",
      "loss: 0.19901492223143577, train acc: 0.9478\n",
      "loss: 0.19157010614871978, train acc: 0.9489\n",
      "loss: 0.19923413917422295, train acc: 0.95\n",
      "epoch: 37, loss: 0.02718832716345787, train acc: 0.95, test acc: 0.9035\n",
      "loss: 0.09175685793161392, train acc: 0.9493\n",
      "loss: 0.20230443850159646, train acc: 0.9475\n",
      "loss: 0.14209583550691604, train acc: 0.947\n",
      "loss: 0.21146435290575027, train acc: 0.9488\n",
      "loss: 0.15450384020805358, train acc: 0.9494\n",
      "loss: 0.19699181094765664, train acc: 0.9484\n",
      "loss: 0.1897626578807831, train acc: 0.9491\n",
      "loss: 0.1974941112101078, train acc: 0.9503\n",
      "epoch: 38, loss: 0.026273982599377632, train acc: 0.9503, test acc: 0.9035\n",
      "loss: 0.09053000062704086, train acc: 0.9493\n",
      "loss: 0.20042771920561792, train acc: 0.948\n",
      "loss: 0.14036635980010032, train acc: 0.9476\n",
      "loss: 0.20972524359822273, train acc: 0.9499\n",
      "loss: 0.15242816582322122, train acc: 0.9497\n",
      "loss: 0.19534285366535187, train acc: 0.9497\n",
      "loss: 0.18756842762231826, train acc: 0.9498\n",
      "loss: 0.19582636281847954, train acc: 0.9511\n",
      "epoch: 39, loss: 0.02589753083884716, train acc: 0.9511, test acc: 0.9032\n",
      "loss: 0.08971013873815536, train acc: 0.9494\n",
      "loss: 0.19870002195239067, train acc: 0.9486\n",
      "loss: 0.1392790973186493, train acc: 0.948\n",
      "loss: 0.20754960924386978, train acc: 0.9495\n",
      "loss: 0.15084134563803672, train acc: 0.9501\n",
      "loss: 0.193773852288723, train acc: 0.9499\n",
      "loss: 0.18611383959650993, train acc: 0.9497\n",
      "loss: 0.19399244710803032, train acc: 0.9514\n",
      "epoch: 40, loss: 0.02519228309392929, train acc: 0.9514, test acc: 0.9029\n",
      "loss: 0.08867830783128738, train acc: 0.9501\n",
      "loss: 0.19678360596299171, train acc: 0.949\n",
      "loss: 0.13761318251490592, train acc: 0.9488\n",
      "loss: 0.20559544265270233, train acc: 0.9501\n",
      "loss: 0.14914401695132257, train acc: 0.9507\n",
      "loss: 0.19210139885544777, train acc: 0.9505\n",
      "loss: 0.1842960737645626, train acc: 0.9503\n",
      "loss: 0.1921881504356861, train acc: 0.9517\n",
      "epoch: 41, loss: 0.024644648656249046, train acc: 0.9517, test acc: 0.9029\n",
      "loss: 0.0874032974243164, train acc: 0.9504\n",
      "loss: 0.19481550008058549, train acc: 0.9498\n",
      "loss: 0.13634086549282073, train acc: 0.9487\n",
      "loss: 0.20377834066748618, train acc: 0.9507\n",
      "loss: 0.1473907448351383, train acc: 0.9508\n",
      "loss: 0.19053758457303047, train acc: 0.9511\n",
      "loss: 0.18249246180057527, train acc: 0.9505\n",
      "loss: 0.19064359366893768, train acc: 0.9516\n",
      "epoch: 42, loss: 0.024218309670686722, train acc: 0.9516, test acc: 0.903\n",
      "loss: 0.08635681867599487, train acc: 0.9505\n",
      "loss: 0.19295459538698195, train acc: 0.9498\n",
      "loss: 0.13494386449456214, train acc: 0.9496\n",
      "loss: 0.20187081769108772, train acc: 0.9508\n",
      "loss: 0.1457131028175354, train acc: 0.9512\n",
      "loss: 0.18904466256499292, train acc: 0.9518\n",
      "loss: 0.18087499514222144, train acc: 0.9512\n",
      "loss: 0.18878084197640418, train acc: 0.9523\n",
      "epoch: 43, loss: 0.023697659373283386, train acc: 0.9523, test acc: 0.9031\n",
      "loss: 0.08606800436973572, train acc: 0.9511\n",
      "loss: 0.19166786074638367, train acc: 0.9504\n",
      "loss: 0.13369657546281816, train acc: 0.9502\n",
      "loss: 0.2000330664217472, train acc: 0.9517\n",
      "loss: 0.1441577486693859, train acc: 0.9511\n",
      "loss: 0.1878989227116108, train acc: 0.9522\n",
      "loss: 0.17932739853858948, train acc: 0.9515\n",
      "loss: 0.1871146857738495, train acc: 0.9525\n",
      "epoch: 44, loss: 0.02317206561565399, train acc: 0.9525, test acc: 0.903\n",
      "loss: 0.08433868736028671, train acc: 0.9519\n",
      "loss: 0.1893877498805523, train acc: 0.9506\n",
      "loss: 0.13254926800727845, train acc: 0.9508\n",
      "loss: 0.19831929504871368, train acc: 0.952\n",
      "loss: 0.14274989441037178, train acc: 0.9516\n",
      "loss: 0.18631974160671233, train acc: 0.9523\n",
      "loss: 0.17764685153961182, train acc: 0.952\n",
      "loss: 0.18533306941390038, train acc: 0.9525\n",
      "epoch: 45, loss: 0.022623561322689056, train acc: 0.9525, test acc: 0.9031\n",
      "loss: 0.08391745388507843, train acc: 0.9521\n",
      "loss: 0.18815476149320604, train acc: 0.9517\n",
      "loss: 0.1309381764382124, train acc: 0.9515\n",
      "loss: 0.19669484719634056, train acc: 0.9527\n",
      "loss: 0.14098177924752237, train acc: 0.9522\n",
      "loss: 0.1851062633097172, train acc: 0.9531\n",
      "loss: 0.1758293516933918, train acc: 0.9526\n",
      "loss: 0.18362636864185333, train acc: 0.9533\n",
      "epoch: 46, loss: 0.022364770993590355, train acc: 0.9533, test acc: 0.9026\n",
      "loss: 0.08275087177753448, train acc: 0.9524\n",
      "loss: 0.18626727610826493, train acc: 0.9519\n",
      "loss: 0.12999505512416362, train acc: 0.9514\n",
      "loss: 0.1948798932135105, train acc: 0.9535\n",
      "loss: 0.139903724193573, train acc: 0.9524\n",
      "loss: 0.18378122076392173, train acc: 0.9532\n",
      "loss: 0.17437469437718392, train acc: 0.9531\n",
      "loss: 0.18198373317718505, train acc: 0.9538\n",
      "epoch: 47, loss: 0.021782491356134415, train acc: 0.9538, test acc: 0.9027\n",
      "loss: 0.08174970746040344, train acc: 0.953\n",
      "loss: 0.18472065180540084, train acc: 0.9518\n",
      "loss: 0.12895769998431206, train acc: 0.9518\n",
      "loss: 0.1931934468448162, train acc: 0.9536\n",
      "loss: 0.1382959522306919, train acc: 0.9526\n",
      "loss: 0.1824724942445755, train acc: 0.9536\n",
      "loss: 0.1729093499481678, train acc: 0.9537\n",
      "loss: 0.18015804067254065, train acc: 0.9537\n",
      "epoch: 48, loss: 0.021724596619606018, train acc: 0.9537, test acc: 0.9029\n",
      "loss: 0.08098156005144119, train acc: 0.9536\n",
      "loss: 0.18303167745471, train acc: 0.952\n",
      "loss: 0.12781935408711434, train acc: 0.952\n",
      "loss: 0.19140138998627662, train acc: 0.9537\n",
      "loss: 0.13677504658699036, train acc: 0.9534\n",
      "loss: 0.18102676868438722, train acc: 0.9536\n",
      "loss: 0.1717366024851799, train acc: 0.9539\n",
      "loss: 0.17886746525764466, train acc: 0.9549\n",
      "epoch: 49, loss: 0.020993197336792946, train acc: 0.9549, test acc: 0.903\n",
      "loss: 0.08005999028682709, train acc: 0.9539\n",
      "loss: 0.18179981857538224, train acc: 0.952\n",
      "loss: 0.12643771916627883, train acc: 0.9526\n",
      "loss: 0.18978905603289603, train acc: 0.9544\n",
      "loss: 0.13511898294091224, train acc: 0.9535\n",
      "loss: 0.17978907898068427, train acc: 0.9538\n",
      "loss: 0.1697748512029648, train acc: 0.9547\n",
      "loss: 0.1775476075708866, train acc: 0.9553\n",
      "epoch: 50, loss: 0.020787011831998825, train acc: 0.9553, test acc: 0.9029\n",
      "loss: 0.07906446605920792, train acc: 0.9548\n",
      "loss: 0.17979034557938575, train acc: 0.9529\n",
      "loss: 0.1257415793836117, train acc: 0.9532\n",
      "loss: 0.1880175106227398, train acc: 0.9538\n",
      "loss: 0.13430382162332535, train acc: 0.9539\n",
      "loss: 0.17859410345554352, train acc: 0.9539\n",
      "loss: 0.1688048705458641, train acc: 0.955\n",
      "loss: 0.17588049322366714, train acc: 0.9555\n",
      "epoch: 51, loss: 0.020218461751937866, train acc: 0.9555, test acc: 0.9029\n",
      "loss: 0.07835888862609863, train acc: 0.9554\n",
      "loss: 0.178291979432106, train acc: 0.9533\n",
      "loss: 0.12415424399077893, train acc: 0.9535\n",
      "loss: 0.18652951568365098, train acc: 0.9541\n",
      "loss: 0.13318303525447844, train acc: 0.954\n",
      "loss: 0.1771833822131157, train acc: 0.9547\n",
      "loss: 0.16715387776494026, train acc: 0.9551\n",
      "loss: 0.17452419102191924, train acc: 0.9559\n",
      "epoch: 52, loss: 0.01999776065349579, train acc: 0.9559, test acc: 0.9026\n",
      "loss: 0.07706926763057709, train acc: 0.9555\n",
      "loss: 0.176864805072546, train acc: 0.9537\n",
      "loss: 0.12347660735249519, train acc: 0.954\n",
      "loss: 0.18518360257148742, train acc: 0.9548\n",
      "loss: 0.13167328760027885, train acc: 0.9544\n",
      "loss: 0.17603988498449324, train acc: 0.9545\n",
      "loss: 0.16608364060521125, train acc: 0.9553\n",
      "loss: 0.1729663535952568, train acc: 0.9566\n",
      "epoch: 53, loss: 0.0196258295327425, train acc: 0.9566, test acc: 0.9023\n",
      "loss: 0.07640080899000168, train acc: 0.9561\n",
      "loss: 0.17554381042718886, train acc: 0.9546\n",
      "loss: 0.1220842245966196, train acc: 0.9545\n",
      "loss: 0.18371501490473746, train acc: 0.9559\n",
      "loss: 0.13025437891483307, train acc: 0.9545\n",
      "loss: 0.17480845302343367, train acc: 0.9556\n",
      "loss: 0.1644781470298767, train acc: 0.9555\n",
      "loss: 0.17144001722335817, train acc: 0.9563\n",
      "epoch: 54, loss: 0.01944238692522049, train acc: 0.9563, test acc: 0.9024\n",
      "loss: 0.07548826187849045, train acc: 0.9562\n",
      "loss: 0.17373499944806098, train acc: 0.9547\n",
      "loss: 0.1214560765773058, train acc: 0.9544\n",
      "loss: 0.18217057287693023, train acc: 0.9558\n",
      "loss: 0.12974899411201476, train acc: 0.9547\n",
      "loss: 0.17364090159535409, train acc: 0.9555\n",
      "loss: 0.1637015961110592, train acc: 0.9559\n",
      "loss: 0.17030516713857652, train acc: 0.9568\n",
      "epoch: 55, loss: 0.018911270424723625, train acc: 0.9568, test acc: 0.9019\n",
      "loss: 0.0742446631193161, train acc: 0.9566\n",
      "loss: 0.17218181043863295, train acc: 0.9553\n",
      "loss: 0.12039987929165363, train acc: 0.9549\n",
      "loss: 0.18074544817209243, train acc: 0.9564\n",
      "loss: 0.12835385501384736, train acc: 0.9555\n",
      "loss: 0.17241639122366906, train acc: 0.9558\n",
      "loss: 0.16201608031988143, train acc: 0.9566\n",
      "loss: 0.16868140622973443, train acc: 0.9576\n",
      "epoch: 56, loss: 0.01874581165611744, train acc: 0.9576, test acc: 0.9024\n",
      "loss: 0.07319553941488266, train acc: 0.9568\n",
      "loss: 0.17028985321521758, train acc: 0.9557\n",
      "loss: 0.11952489092946053, train acc: 0.9551\n",
      "loss: 0.17913992702960968, train acc: 0.9568\n",
      "loss: 0.12736647501587867, train acc: 0.9556\n",
      "loss: 0.17110234275460243, train acc: 0.9561\n",
      "loss: 0.1612364798784256, train acc: 0.9565\n",
      "loss: 0.1675334818661213, train acc: 0.9579\n",
      "epoch: 57, loss: 0.017983706668019295, train acc: 0.9579, test acc: 0.9018\n",
      "loss: 0.07261303812265396, train acc: 0.9574\n",
      "loss: 0.1692766435444355, train acc: 0.9561\n",
      "loss: 0.11804784312844277, train acc: 0.9557\n",
      "loss: 0.17751646637916565, train acc: 0.9575\n",
      "loss: 0.12547726333141326, train acc: 0.9562\n",
      "loss: 0.16983859315514566, train acc: 0.9564\n",
      "loss: 0.159486074000597, train acc: 0.9566\n",
      "loss: 0.16620049327611924, train acc: 0.9583\n",
      "epoch: 58, loss: 0.018286272883415222, train acc: 0.9583, test acc: 0.9018\n",
      "loss: 0.0720306932926178, train acc: 0.9578\n",
      "loss: 0.16750814020633698, train acc: 0.9563\n",
      "loss: 0.11756076887249947, train acc: 0.9556\n",
      "loss: 0.17592794001102446, train acc: 0.9577\n",
      "loss: 0.12533247098326683, train acc: 0.9568\n",
      "loss: 0.16898767203092574, train acc: 0.9567\n",
      "loss: 0.15876371785998344, train acc: 0.9568\n",
      "loss: 0.16446287110447882, train acc: 0.9591\n",
      "epoch: 59, loss: 0.01787242665886879, train acc: 0.9591, test acc: 0.9022\n",
      "loss: 0.07096671313047409, train acc: 0.9581\n",
      "loss: 0.16590826958417892, train acc: 0.9569\n",
      "loss: 0.11628547236323357, train acc: 0.9565\n",
      "loss: 0.17460064664483071, train acc: 0.9579\n",
      "loss: 0.1238108031451702, train acc: 0.9567\n",
      "loss: 0.1680796816945076, train acc: 0.9577\n",
      "loss: 0.15719830989837646, train acc: 0.9572\n",
      "loss: 0.16339718475937842, train acc: 0.9589\n",
      "epoch: 60, loss: 0.01825779117643833, train acc: 0.9589, test acc: 0.9026\n",
      "loss: 0.07034613192081451, train acc: 0.9582\n",
      "loss: 0.16479275077581407, train acc: 0.9572\n",
      "loss: 0.11573541648685932, train acc: 0.9561\n",
      "loss: 0.1730596736073494, train acc: 0.9586\n",
      "loss: 0.12309390604496002, train acc: 0.9579\n",
      "loss: 0.16671281084418296, train acc: 0.9573\n",
      "loss: 0.15617818310856818, train acc: 0.9574\n",
      "loss: 0.16185357496142389, train acc: 0.9595\n",
      "epoch: 61, loss: 0.01730312407016754, train acc: 0.9595, test acc: 0.9022\n",
      "loss: 0.0694354698061943, train acc: 0.959\n",
      "loss: 0.16253823265433312, train acc: 0.9574\n",
      "loss: 0.11474362201988697, train acc: 0.9571\n",
      "loss: 0.17182328179478645, train acc: 0.9583\n",
      "loss: 0.1220717541873455, train acc: 0.9578\n",
      "loss: 0.16552756801247598, train acc: 0.9582\n",
      "loss: 0.15489336475729942, train acc: 0.9578\n",
      "loss: 0.16054978519678115, train acc: 0.9596\n",
      "epoch: 62, loss: 0.01719764806330204, train acc: 0.9596, test acc: 0.9019\n",
      "loss: 0.06848598271608353, train acc: 0.959\n",
      "loss: 0.16142406314611435, train acc: 0.9582\n",
      "loss: 0.11402003318071366, train acc: 0.9566\n",
      "loss: 0.17037550136446952, train acc: 0.9591\n",
      "loss: 0.12072226628661156, train acc: 0.9579\n",
      "loss: 0.16455266177654265, train acc: 0.958\n",
      "loss: 0.15384722724556923, train acc: 0.9581\n",
      "loss: 0.15916282385587693, train acc: 0.9603\n",
      "epoch: 63, loss: 0.017350729554891586, train acc: 0.9603, test acc: 0.9019\n",
      "loss: 0.06733760237693787, train acc: 0.9595\n",
      "loss: 0.159593915194273, train acc: 0.9581\n",
      "loss: 0.11337648630142212, train acc: 0.9572\n",
      "loss: 0.1690836362540722, train acc: 0.9593\n",
      "loss: 0.12005931958556175, train acc: 0.9581\n",
      "loss: 0.16370433270931245, train acc: 0.9587\n",
      "loss: 0.15319545045495034, train acc: 0.9584\n",
      "loss: 0.1583517849445343, train acc: 0.9602\n",
      "epoch: 64, loss: 0.0167849063873291, train acc: 0.9602, test acc: 0.9013\n",
      "loss: 0.06657766550779343, train acc: 0.9597\n",
      "loss: 0.1584718979895115, train acc: 0.9584\n",
      "loss: 0.11238456852734088, train acc: 0.9584\n",
      "loss: 0.16766204684972763, train acc: 0.9594\n",
      "loss: 0.11919557303190231, train acc: 0.9585\n",
      "loss: 0.1624597914516926, train acc: 0.9591\n",
      "loss: 0.1520415097475052, train acc: 0.9585\n",
      "loss: 0.1567985974252224, train acc: 0.9607\n",
      "epoch: 65, loss: 0.01689338870346546, train acc: 0.9607, test acc: 0.9014\n",
      "loss: 0.06544052064418793, train acc: 0.96\n",
      "loss: 0.15717549473047257, train acc: 0.959\n",
      "loss: 0.11113153770565987, train acc: 0.9589\n",
      "loss: 0.16631704121828078, train acc: 0.9595\n",
      "loss: 0.11742071881890297, train acc: 0.9588\n",
      "loss: 0.16139343082904817, train acc: 0.9597\n",
      "loss: 0.1505641907453537, train acc: 0.9591\n",
      "loss: 0.15586950331926347, train acc: 0.9612\n",
      "epoch: 66, loss: 0.016600187867879868, train acc: 0.9612, test acc: 0.9011\n",
      "loss: 0.06463504582643509, train acc: 0.9608\n",
      "loss: 0.15581678375601768, train acc: 0.9594\n",
      "loss: 0.11061914749443531, train acc: 0.9586\n",
      "loss: 0.16481410786509515, train acc: 0.9598\n",
      "loss: 0.11728083789348602, train acc: 0.9587\n",
      "loss: 0.1604129396378994, train acc: 0.96\n",
      "loss: 0.15002084374427796, train acc: 0.9587\n",
      "loss: 0.1545661360025406, train acc: 0.9609\n",
      "epoch: 67, loss: 0.0164018664509058, train acc: 0.9609, test acc: 0.9016\n",
      "loss: 0.06416070461273193, train acc: 0.961\n",
      "loss: 0.15443722754716874, train acc: 0.9594\n",
      "loss: 0.10979756638407707, train acc: 0.9595\n",
      "loss: 0.16396082937717438, train acc: 0.9603\n",
      "loss: 0.11585921198129653, train acc: 0.959\n",
      "loss: 0.15940625593066216, train acc: 0.9606\n",
      "loss: 0.1486922800540924, train acc: 0.9596\n",
      "loss: 0.153903279453516, train acc: 0.9614\n",
      "epoch: 68, loss: 0.016042400151491165, train acc: 0.9614, test acc: 0.9012\n",
      "loss: 0.06363514065742493, train acc: 0.9613\n",
      "loss: 0.15327516049146653, train acc: 0.9599\n",
      "loss: 0.1087461456656456, train acc: 0.9598\n",
      "loss: 0.16258933544158935, train acc: 0.9599\n",
      "loss: 0.11585536599159241, train acc: 0.9599\n",
      "loss: 0.15848133191466332, train acc: 0.9607\n",
      "loss: 0.14761170521378517, train acc: 0.9605\n",
      "loss: 0.1522567316889763, train acc: 0.9618\n",
      "epoch: 69, loss: 0.01601344160735607, train acc: 0.9618, test acc: 0.9011\n",
      "loss: 0.06289979815483093, train acc: 0.9617\n",
      "loss: 0.152038886398077, train acc: 0.9597\n",
      "loss: 0.10826128646731377, train acc: 0.9598\n",
      "loss: 0.16143233925104142, train acc: 0.9607\n",
      "loss: 0.11458595916628837, train acc: 0.9598\n",
      "loss: 0.15725461393594742, train acc: 0.9606\n",
      "loss: 0.1469496913254261, train acc: 0.9607\n",
      "loss: 0.15139624401926993, train acc: 0.9617\n",
      "epoch: 70, loss: 0.016005028039216995, train acc: 0.9617, test acc: 0.9007\n",
      "loss: 0.06275791674852371, train acc: 0.9624\n",
      "loss: 0.15092266574501992, train acc: 0.96\n",
      "loss: 0.10769973173737526, train acc: 0.96\n",
      "loss: 0.1602166011929512, train acc: 0.9607\n",
      "loss: 0.11356342658400535, train acc: 0.9595\n",
      "loss: 0.15660135075449944, train acc: 0.9612\n",
      "loss: 0.14572271779179574, train acc: 0.9608\n",
      "loss: 0.1501743584871292, train acc: 0.9622\n",
      "epoch: 71, loss: 0.015664586797356606, train acc: 0.9622, test acc: 0.9008\n",
      "loss: 0.061667826026678085, train acc: 0.9626\n",
      "loss: 0.14928142614662648, train acc: 0.9606\n",
      "loss: 0.10645722225308418, train acc: 0.9607\n",
      "loss: 0.15919731482863425, train acc: 0.9614\n",
      "loss: 0.11257235705852509, train acc: 0.96\n",
      "loss: 0.15546873807907105, train acc: 0.9613\n",
      "loss: 0.1447124697268009, train acc: 0.9617\n",
      "loss: 0.14957957118749618, train acc: 0.9618\n",
      "epoch: 72, loss: 0.015546154230833054, train acc: 0.9618, test acc: 0.9005\n",
      "loss: 0.06208420917391777, train acc: 0.9627\n",
      "loss: 0.14859897457063198, train acc: 0.9611\n",
      "loss: 0.10584629364311696, train acc: 0.9604\n",
      "loss: 0.15776746794581414, train acc: 0.9609\n",
      "loss: 0.11259101331233978, train acc: 0.9614\n",
      "loss: 0.154666481167078, train acc: 0.9613\n",
      "loss: 0.14401055350899697, train acc: 0.9613\n",
      "loss: 0.1477404683828354, train acc: 0.9631\n",
      "epoch: 73, loss: 0.015582376159727573, train acc: 0.9631, test acc: 0.9007\n",
      "loss: 0.0605013333261013, train acc: 0.9632\n",
      "loss: 0.1469342175871134, train acc: 0.9606\n",
      "loss: 0.10544535927474499, train acc: 0.9611\n",
      "loss: 0.15652408227324485, train acc: 0.9613\n",
      "loss: 0.1110425814986229, train acc: 0.9608\n",
      "loss: 0.1536615990102291, train acc: 0.9619\n",
      "loss: 0.14297249019145966, train acc: 0.9618\n",
      "loss: 0.1470525696873665, train acc: 0.9629\n",
      "epoch: 74, loss: 0.01524544507265091, train acc: 0.9629, test acc: 0.9008\n",
      "loss: 0.060111574828624725, train acc: 0.9632\n",
      "loss: 0.14591074883937835, train acc: 0.9614\n",
      "loss: 0.10430296473205089, train acc: 0.9618\n",
      "loss: 0.15533204078674318, train acc: 0.9625\n",
      "loss: 0.11028174310922623, train acc: 0.9612\n",
      "loss: 0.15285181105136872, train acc: 0.9619\n",
      "loss: 0.14212398156523703, train acc: 0.9617\n",
      "loss: 0.14593136981129645, train acc: 0.9633\n",
      "epoch: 75, loss: 0.014932598918676376, train acc: 0.9633, test acc: 0.9\n",
      "loss: 0.05975114554166794, train acc: 0.9632\n",
      "loss: 0.14469623044133187, train acc: 0.9612\n",
      "loss: 0.10363295264542102, train acc: 0.962\n",
      "loss: 0.1545598790049553, train acc: 0.9618\n",
      "loss: 0.1097988285124302, train acc: 0.9615\n",
      "loss: 0.15187451913952826, train acc: 0.9618\n",
      "loss: 0.14124736860394477, train acc: 0.9624\n",
      "loss: 0.14481005370616912, train acc: 0.964\n",
      "epoch: 76, loss: 0.014970237389206886, train acc: 0.964, test acc: 0.9006\n",
      "loss: 0.05935008078813553, train acc: 0.9634\n",
      "loss: 0.14310977309942247, train acc: 0.9613\n",
      "loss: 0.10328374467790127, train acc: 0.9621\n",
      "loss: 0.15315692275762557, train acc: 0.9624\n",
      "loss: 0.10858052298426628, train acc: 0.9618\n",
      "loss: 0.15088209733366967, train acc: 0.9622\n",
      "loss: 0.1405737243592739, train acc: 0.9621\n",
      "loss: 0.1441367194056511, train acc: 0.964\n",
      "epoch: 77, loss: 0.014582188799977303, train acc: 0.964, test acc: 0.8999\n",
      "loss: 0.058750662952661514, train acc: 0.9634\n",
      "loss: 0.14249823428690434, train acc: 0.9618\n",
      "loss: 0.1019802812486887, train acc: 0.9622\n",
      "loss: 0.1522492602467537, train acc: 0.9628\n",
      "loss: 0.10790477469563484, train acc: 0.9619\n",
      "loss: 0.15007507130503656, train acc: 0.9628\n",
      "loss: 0.1392820470035076, train acc: 0.9622\n",
      "loss: 0.1426908489316702, train acc: 0.9647\n",
      "epoch: 78, loss: 0.014501360245049, train acc: 0.9647, test acc: 0.9003\n",
      "loss: 0.05790748819708824, train acc: 0.964\n",
      "loss: 0.14085555486381054, train acc: 0.9619\n",
      "loss: 0.10155748054385186, train acc: 0.9626\n",
      "loss: 0.1512056455016136, train acc: 0.9631\n",
      "loss: 0.10717680901288987, train acc: 0.9628\n",
      "loss: 0.14903874173760415, train acc: 0.9628\n",
      "loss: 0.13889356404542924, train acc: 0.9627\n",
      "loss: 0.14199198782444, train acc: 0.9651\n",
      "epoch: 79, loss: 0.014420907013118267, train acc: 0.9651, test acc: 0.9004\n",
      "#####training and testing end with K:10, P:1######\n",
      "#####training and testing start with K:20, P:0.1######\n",
      "loss: 2.3548479080200195, train acc: 0.1811\n",
      "loss: 2.1142379522323607, train acc: 0.4277\n",
      "loss: 1.6484560370445251, train acc: 0.6809\n",
      "loss: 1.2959893941879272, train acc: 0.784\n",
      "loss: 1.0381196796894074, train acc: 0.8315\n",
      "loss: 0.86309734582901, train acc: 0.8444\n",
      "loss: 0.7610391795635223, train acc: 0.8554\n",
      "loss: 0.7034921944141388, train acc: 0.8647\n",
      "epoch: 0, loss: 0.3787922263145447, train acc: 0.8647, test acc: 0.8657\n",
      "loss: 0.6537724137306213, train acc: 0.87\n",
      "loss: 0.615534919500351, train acc: 0.8757\n",
      "loss: 0.6020266890525818, train acc: 0.8808\n",
      "loss: 0.5924124538898468, train acc: 0.8881\n",
      "loss: 0.5645353555679321, train acc: 0.886\n",
      "loss: 0.5200143247842789, train acc: 0.8892\n",
      "loss: 0.4852922886610031, train acc: 0.8952\n",
      "loss: 0.5010570585727692, train acc: 0.896\n",
      "epoch: 1, loss: 0.3877442181110382, train acc: 0.896, test acc: 0.8902\n",
      "loss: 0.39948615431785583, train acc: 0.8973\n",
      "loss: 0.4609088271856308, train acc: 0.8991\n",
      "loss: 0.46746754348278047, train acc: 0.9061\n",
      "loss: 0.48081417977809904, train acc: 0.9071\n",
      "loss: 0.45844743251800535, train acc: 0.9016\n",
      "loss: 0.43804502189159394, train acc: 0.9072\n",
      "loss: 0.4225414633750916, train acc: 0.9109\n",
      "loss: 0.41994012594223024, train acc: 0.9094\n",
      "epoch: 2, loss: 0.17227938771247864, train acc: 0.9094, test acc: 0.9016\n",
      "loss: 0.36691752076148987, train acc: 0.9134\n",
      "loss: 0.38322395384311675, train acc: 0.9127\n",
      "loss: 0.4150452524423599, train acc: 0.9166\n",
      "loss: 0.39514689743518827, train acc: 0.9181\n",
      "loss: 0.38942553400993346, train acc: 0.9132\n",
      "loss: 0.39339776933193205, train acc: 0.9145\n",
      "loss: 0.37396361827850344, train acc: 0.9197\n",
      "loss: 0.36781087815761565, train acc: 0.92\n",
      "epoch: 3, loss: 0.09448999911546707, train acc: 0.92, test acc: 0.9056\n",
      "loss: 0.2917303442955017, train acc: 0.9206\n",
      "loss: 0.3476049929857254, train acc: 0.9218\n",
      "loss: 0.3558747947216034, train acc: 0.923\n",
      "loss: 0.3815416470170021, train acc: 0.9253\n",
      "loss: 0.37282874584198, train acc: 0.9226\n",
      "loss: 0.3551615685224533, train acc: 0.9222\n",
      "loss: 0.32692570239305496, train acc: 0.926\n",
      "loss: 0.3775410562753677, train acc: 0.9264\n",
      "epoch: 4, loss: 0.06555765867233276, train acc: 0.9264, test acc: 0.91\n",
      "loss: 0.3131539821624756, train acc: 0.9259\n",
      "loss: 0.3332650184631348, train acc: 0.9274\n",
      "loss: 0.3679227277636528, train acc: 0.9276\n",
      "loss: 0.35587117820978165, train acc: 0.9301\n",
      "loss: 0.3548460155725479, train acc: 0.9285\n",
      "loss: 0.3312304198741913, train acc: 0.9287\n",
      "loss: 0.32769615203142166, train acc: 0.9309\n",
      "loss: 0.3358141928911209, train acc: 0.9318\n",
      "epoch: 5, loss: 0.21946878731250763, train acc: 0.9318, test acc: 0.913\n",
      "loss: 0.2721588611602783, train acc: 0.9331\n",
      "loss: 0.3061165764927864, train acc: 0.933\n",
      "loss: 0.335154527425766, train acc: 0.9313\n",
      "loss: 0.3385485738515854, train acc: 0.9358\n",
      "loss: 0.33163770139217374, train acc: 0.9328\n",
      "loss: 0.29835211485624313, train acc: 0.9322\n",
      "loss: 0.30165975987911225, train acc: 0.9353\n",
      "loss: 0.3121004059910774, train acc: 0.9365\n",
      "epoch: 6, loss: 0.05590679123997688, train acc: 0.9365, test acc: 0.9169\n",
      "loss: 0.22856038808822632, train acc: 0.9369\n",
      "loss: 0.2956836387515068, train acc: 0.9363\n",
      "loss: 0.30215785652399063, train acc: 0.938\n",
      "loss: 0.31994210332632067, train acc: 0.9403\n",
      "loss: 0.32511372268199923, train acc: 0.9365\n",
      "loss: 0.29707403630018236, train acc: 0.9368\n",
      "loss: 0.28293891102075575, train acc: 0.9393\n",
      "loss: 0.3054977387189865, train acc: 0.9394\n",
      "epoch: 7, loss: 0.22657501697540283, train acc: 0.9394, test acc: 0.9178\n",
      "loss: 0.21671009063720703, train acc: 0.94\n",
      "loss: 0.2796726986765862, train acc: 0.9374\n",
      "loss: 0.3124152973294258, train acc: 0.9388\n",
      "loss: 0.2900877848267555, train acc: 0.943\n",
      "loss: 0.3102453827857971, train acc: 0.9388\n",
      "loss: 0.2607636243104935, train acc: 0.9418\n",
      "loss: 0.28152979761362074, train acc: 0.9423\n",
      "loss: 0.304812528192997, train acc: 0.9442\n",
      "epoch: 8, loss: 0.053724922239780426, train acc: 0.9442, test acc: 0.9202\n",
      "loss: 0.27289217710494995, train acc: 0.9448\n",
      "loss: 0.26750459223985673, train acc: 0.9419\n",
      "loss: 0.2838361382484436, train acc: 0.9441\n",
      "loss: 0.28621922731399535, train acc: 0.9458\n",
      "loss: 0.2960077628493309, train acc: 0.9446\n",
      "loss: 0.2643678918480873, train acc: 0.9436\n",
      "loss: 0.26724876910448075, train acc: 0.9453\n",
      "loss: 0.2589185446500778, train acc: 0.9452\n",
      "epoch: 9, loss: 0.2741154730319977, train acc: 0.9452, test acc: 0.9224\n",
      "loss: 0.20530910789966583, train acc: 0.9452\n",
      "loss: 0.24166302531957626, train acc: 0.9431\n",
      "loss: 0.2841052606701851, train acc: 0.9453\n",
      "loss: 0.26403132528066636, train acc: 0.9467\n",
      "loss: 0.26753917038440705, train acc: 0.9454\n",
      "loss: 0.24224329739809036, train acc: 0.9442\n",
      "loss: 0.2387000471353531, train acc: 0.9468\n",
      "loss: 0.25551387667655945, train acc: 0.9488\n",
      "epoch: 10, loss: 0.12613262236118317, train acc: 0.9488, test acc: 0.923\n",
      "loss: 0.16206860542297363, train acc: 0.9491\n",
      "loss: 0.2308871105313301, train acc: 0.9477\n",
      "loss: 0.26724139750003817, train acc: 0.9498\n",
      "loss: 0.2669050216674805, train acc: 0.949\n",
      "loss: 0.2733298450708389, train acc: 0.9478\n",
      "loss: 0.26711371541023254, train acc: 0.948\n",
      "loss: 0.23311963081359863, train acc: 0.9503\n",
      "loss: 0.25286169424653054, train acc: 0.9505\n",
      "epoch: 11, loss: 0.2885049879550934, train acc: 0.9505, test acc: 0.924\n",
      "loss: 0.188177227973938, train acc: 0.9518\n",
      "loss: 0.22939373850822448, train acc: 0.9485\n",
      "loss: 0.23926232010126114, train acc: 0.952\n",
      "loss: 0.23502152562141418, train acc: 0.9528\n",
      "loss: 0.2541221797466278, train acc: 0.9499\n",
      "loss: 0.22283729463815688, train acc: 0.9498\n",
      "loss: 0.22256791293621064, train acc: 0.9517\n",
      "loss: 0.2452498584985733, train acc: 0.9535\n",
      "epoch: 12, loss: 0.0490688756108284, train acc: 0.9535, test acc: 0.9243\n",
      "loss: 0.22079330682754517, train acc: 0.9544\n",
      "loss: 0.22142010629177095, train acc: 0.9528\n",
      "loss: 0.2526431530714035, train acc: 0.9529\n",
      "loss: 0.2434884190559387, train acc: 0.9539\n",
      "loss: 0.2582005739212036, train acc: 0.9522\n",
      "loss: 0.23707066029310225, train acc: 0.9531\n",
      "loss: 0.216037318110466, train acc: 0.9545\n",
      "loss: 0.22490979880094528, train acc: 0.9544\n",
      "epoch: 13, loss: 0.10090009868144989, train acc: 0.9544, test acc: 0.9266\n",
      "loss: 0.1641961634159088, train acc: 0.9572\n",
      "loss: 0.21134057492017747, train acc: 0.9543\n",
      "loss: 0.23017189651727676, train acc: 0.9556\n",
      "loss: 0.23038330078125, train acc: 0.9574\n",
      "loss: 0.25854566544294355, train acc: 0.9551\n",
      "loss: 0.2265152558684349, train acc: 0.9558\n",
      "loss: 0.2000147245824337, train acc: 0.9568\n",
      "loss: 0.2301322564482689, train acc: 0.958\n",
      "epoch: 14, loss: 0.03327192738652229, train acc: 0.958, test acc: 0.9262\n",
      "loss: 0.14951400458812714, train acc: 0.9585\n",
      "loss: 0.20792778730392455, train acc: 0.9562\n",
      "loss: 0.22813174426555632, train acc: 0.9589\n",
      "loss: 0.23235129117965697, train acc: 0.957\n",
      "loss: 0.24785076826810837, train acc: 0.9566\n",
      "loss: 0.20871862322092055, train acc: 0.9563\n",
      "loss: 0.1910238556563854, train acc: 0.9596\n",
      "loss: 0.21182279363274575, train acc: 0.9598\n",
      "epoch: 15, loss: 0.08279068768024445, train acc: 0.9598, test acc: 0.9282\n",
      "loss: 0.12627361714839935, train acc: 0.9598\n",
      "loss: 0.1960136905312538, train acc: 0.9586\n",
      "loss: 0.21432282626628876, train acc: 0.9593\n",
      "loss: 0.21122727692127227, train acc: 0.9605\n",
      "loss: 0.23316824585199356, train acc: 0.9595\n",
      "loss: 0.20021567791700362, train acc: 0.9583\n",
      "loss: 0.1872285097837448, train acc: 0.9604\n",
      "loss: 0.2034688785672188, train acc: 0.9614\n",
      "epoch: 16, loss: 0.031572308391332626, train acc: 0.9614, test acc: 0.9282\n",
      "loss: 0.16420221328735352, train acc: 0.9621\n",
      "loss: 0.19430915862321854, train acc: 0.9604\n",
      "loss: 0.221216382086277, train acc: 0.9611\n",
      "loss: 0.2126304939389229, train acc: 0.9629\n",
      "loss: 0.22959033995866776, train acc: 0.9617\n",
      "loss: 0.20855108574032782, train acc: 0.9594\n",
      "loss: 0.19588448405265807, train acc: 0.9629\n",
      "loss: 0.21514938473701478, train acc: 0.9629\n",
      "epoch: 17, loss: 0.014898396097123623, train acc: 0.9629, test acc: 0.9277\n",
      "loss: 0.148992121219635, train acc: 0.9643\n",
      "loss: 0.17358437478542327, train acc: 0.9614\n",
      "loss: 0.20636708289384842, train acc: 0.9638\n",
      "loss: 0.19643352776765824, train acc: 0.9636\n",
      "loss: 0.22951761335134507, train acc: 0.9632\n",
      "loss: 0.20220205038785935, train acc: 0.9618\n",
      "loss: 0.2003630816936493, train acc: 0.9637\n",
      "loss: 0.20589915663003922, train acc: 0.9643\n",
      "epoch: 18, loss: 0.05699348449707031, train acc: 0.9643, test acc: 0.9262\n",
      "loss: 0.16257339715957642, train acc: 0.9648\n",
      "loss: 0.19377074986696244, train acc: 0.962\n",
      "loss: 0.19541101157665253, train acc: 0.9632\n",
      "loss: 0.21194640696048736, train acc: 0.9643\n",
      "loss: 0.22659596800804138, train acc: 0.966\n",
      "loss: 0.1878693200647831, train acc: 0.964\n",
      "loss: 0.18981744796037675, train acc: 0.9645\n",
      "loss: 0.19155801981687545, train acc: 0.9651\n",
      "epoch: 19, loss: 0.13135409355163574, train acc: 0.9651, test acc: 0.9284\n",
      "loss: 0.13724717497825623, train acc: 0.9666\n",
      "loss: 0.20241855680942536, train acc: 0.9643\n",
      "loss: 0.19494078904390336, train acc: 0.9674\n",
      "loss: 0.19227693006396293, train acc: 0.9675\n",
      "loss: 0.22915254086256026, train acc: 0.9656\n",
      "loss: 0.1905383661389351, train acc: 0.9657\n",
      "loss: 0.18682427927851677, train acc: 0.9663\n",
      "loss: 0.18998305797576903, train acc: 0.9658\n",
      "epoch: 20, loss: 0.01583356224000454, train acc: 0.9658, test acc: 0.9295\n",
      "loss: 0.13807334005832672, train acc: 0.9692\n",
      "loss: 0.16745009869337082, train acc: 0.9655\n",
      "loss: 0.19618748128414154, train acc: 0.9666\n",
      "loss: 0.1948821723461151, train acc: 0.9675\n",
      "loss: 0.2215199314057827, train acc: 0.9683\n",
      "loss: 0.17994872778654097, train acc: 0.9685\n",
      "loss: 0.1755513906478882, train acc: 0.968\n",
      "loss: 0.19533571600914001, train acc: 0.9691\n",
      "epoch: 21, loss: 0.05660223215818405, train acc: 0.9691, test acc: 0.929\n",
      "loss: 0.13356001675128937, train acc: 0.9693\n",
      "loss: 0.17724057286977768, train acc: 0.9683\n",
      "loss: 0.1884828343987465, train acc: 0.9685\n",
      "loss: 0.17111006528139114, train acc: 0.9681\n",
      "loss: 0.1927863523364067, train acc: 0.9675\n",
      "loss: 0.1644568219780922, train acc: 0.969\n",
      "loss: 0.1612730860710144, train acc: 0.9701\n",
      "loss: 0.18196908682584761, train acc: 0.9706\n",
      "epoch: 22, loss: 0.03851427510380745, train acc: 0.9706, test acc: 0.9295\n",
      "loss: 0.14414332807064056, train acc: 0.9705\n",
      "loss: 0.16390873938798906, train acc: 0.9707\n",
      "loss: 0.16317589953541756, train acc: 0.9692\n",
      "loss: 0.1717540666460991, train acc: 0.9697\n",
      "loss: 0.19096351265907288, train acc: 0.9723\n",
      "loss: 0.16259773448109627, train acc: 0.9726\n",
      "loss: 0.15993261486291885, train acc: 0.971\n",
      "loss: 0.18657065629959108, train acc: 0.9704\n",
      "epoch: 23, loss: 0.02002820558845997, train acc: 0.9704, test acc: 0.9285\n",
      "loss: 0.16698375344276428, train acc: 0.9723\n",
      "loss: 0.16559916883707046, train acc: 0.971\n",
      "loss: 0.17175512686371802, train acc: 0.971\n",
      "loss: 0.16925911605358124, train acc: 0.9711\n",
      "loss: 0.19012306332588197, train acc: 0.9716\n",
      "loss: 0.1640687845647335, train acc: 0.9724\n",
      "loss: 0.13971415385603905, train acc: 0.9718\n",
      "loss: 0.17126187682151794, train acc: 0.9723\n",
      "epoch: 24, loss: 0.04691152647137642, train acc: 0.9723, test acc: 0.9303\n",
      "loss: 0.1452961415052414, train acc: 0.9739\n",
      "loss: 0.15540391057729722, train acc: 0.9707\n",
      "loss: 0.17014009803533553, train acc: 0.9724\n",
      "loss: 0.16765913367271423, train acc: 0.9726\n",
      "loss: 0.19687777012586594, train acc: 0.9723\n",
      "loss: 0.16085522398352622, train acc: 0.9706\n",
      "loss: 0.15036168992519378, train acc: 0.9738\n",
      "loss: 0.16557612270116806, train acc: 0.9726\n",
      "epoch: 25, loss: 0.03514599800109863, train acc: 0.9726, test acc: 0.9298\n",
      "loss: 0.15815384685993195, train acc: 0.974\n",
      "loss: 0.16953110173344613, train acc: 0.9703\n",
      "loss: 0.19331053346395494, train acc: 0.9736\n",
      "loss: 0.161477430164814, train acc: 0.9736\n",
      "loss: 0.19998025819659232, train acc: 0.9739\n",
      "loss: 0.1640610344707966, train acc: 0.974\n",
      "loss: 0.1540423020720482, train acc: 0.9719\n",
      "loss: 0.16745098382234574, train acc: 0.9726\n",
      "epoch: 26, loss: 0.01918713003396988, train acc: 0.9726, test acc: 0.9301\n",
      "loss: 0.1675650179386139, train acc: 0.9757\n",
      "loss: 0.16239970922470093, train acc: 0.972\n",
      "loss: 0.16766702383756638, train acc: 0.9741\n",
      "loss: 0.1623403824865818, train acc: 0.9737\n",
      "loss: 0.19433603733778, train acc: 0.9751\n",
      "loss: 0.1458820194005966, train acc: 0.9733\n",
      "loss: 0.15226237401366233, train acc: 0.9738\n",
      "loss: 0.18671998381614685, train acc: 0.9759\n",
      "epoch: 27, loss: 0.02048046514391899, train acc: 0.9759, test acc: 0.9306\n",
      "loss: 0.11870341747999191, train acc: 0.977\n",
      "loss: 0.14053295701742172, train acc: 0.9761\n",
      "loss: 0.14281207025051118, train acc: 0.9745\n",
      "loss: 0.1730991467833519, train acc: 0.9755\n",
      "loss: 0.17580412700772285, train acc: 0.9749\n",
      "loss: 0.15613626092672347, train acc: 0.9744\n",
      "loss: 0.1561611019074917, train acc: 0.9762\n",
      "loss: 0.16147751957178116, train acc: 0.9769\n",
      "epoch: 28, loss: 0.013201610185205936, train acc: 0.9769, test acc: 0.9304\n",
      "loss: 0.08936382830142975, train acc: 0.9769\n",
      "loss: 0.1365548200905323, train acc: 0.9748\n",
      "loss: 0.1698227658867836, train acc: 0.976\n",
      "loss: 0.14905285239219665, train acc: 0.9756\n",
      "loss: 0.18239032626152038, train acc: 0.9736\n",
      "loss: 0.1480795130133629, train acc: 0.9774\n",
      "loss: 0.14694038331508635, train acc: 0.9785\n",
      "loss: 0.16742205619812012, train acc: 0.9788\n",
      "epoch: 29, loss: 0.04852630943059921, train acc: 0.9788, test acc: 0.9304\n",
      "loss: 0.09509570151567459, train acc: 0.9768\n",
      "loss: 0.14275347515940667, train acc: 0.9755\n",
      "loss: 0.16216573119163513, train acc: 0.9781\n",
      "loss: 0.15434026196599007, train acc: 0.9771\n",
      "loss: 0.17999578937888144, train acc: 0.9781\n",
      "loss: 0.15091451033949851, train acc: 0.9774\n",
      "loss: 0.13945587798953057, train acc: 0.979\n",
      "loss: 0.16028666868805885, train acc: 0.9786\n",
      "epoch: 30, loss: 0.025265462696552277, train acc: 0.9786, test acc: 0.9291\n",
      "loss: 0.09074708074331284, train acc: 0.9794\n",
      "loss: 0.14961619526147843, train acc: 0.9784\n",
      "loss: 0.1416734240949154, train acc: 0.9769\n",
      "loss: 0.1675818532705307, train acc: 0.9781\n",
      "loss: 0.17670958265662193, train acc: 0.9769\n",
      "loss: 0.14464323073625565, train acc: 0.978\n",
      "loss: 0.14231377243995666, train acc: 0.9792\n",
      "loss: 0.15114498808979987, train acc: 0.9782\n",
      "epoch: 31, loss: 0.02135704644024372, train acc: 0.9782, test acc: 0.931\n",
      "loss: 0.08544687926769257, train acc: 0.9789\n",
      "loss: 0.1493223451077938, train acc: 0.9788\n",
      "loss: 0.14734143316745757, train acc: 0.978\n",
      "loss: 0.16375875398516654, train acc: 0.9775\n",
      "loss: 0.16653406098484994, train acc: 0.9749\n",
      "loss: 0.14983128681778907, train acc: 0.9778\n",
      "loss: 0.12406069189310073, train acc: 0.9784\n",
      "loss: 0.15037576779723166, train acc: 0.9799\n",
      "epoch: 32, loss: 0.06774064898490906, train acc: 0.9799, test acc: 0.9318\n",
      "loss: 0.11768561601638794, train acc: 0.9813\n",
      "loss: 0.14725841730833053, train acc: 0.9783\n",
      "loss: 0.1471077524125576, train acc: 0.9793\n",
      "loss: 0.1573969192802906, train acc: 0.9792\n",
      "loss: 0.17019804194569588, train acc: 0.9792\n",
      "loss: 0.16220744661986827, train acc: 0.9794\n",
      "loss: 0.1342710219323635, train acc: 0.978\n",
      "loss: 0.14567579254508017, train acc: 0.9798\n",
      "epoch: 33, loss: 0.0843396857380867, train acc: 0.9798, test acc: 0.9302\n",
      "loss: 0.12592163681983948, train acc: 0.9806\n",
      "loss: 0.14719835072755813, train acc: 0.9782\n",
      "loss: 0.14874788224697114, train acc: 0.9811\n",
      "loss: 0.13478080928325653, train acc: 0.979\n",
      "loss: 0.15161922574043274, train acc: 0.9794\n",
      "loss: 0.13700968772172928, train acc: 0.9813\n",
      "loss: 0.1385629266500473, train acc: 0.9801\n",
      "loss: 0.16103458404541016, train acc: 0.9824\n",
      "epoch: 34, loss: 0.03130657598376274, train acc: 0.9824, test acc: 0.9317\n",
      "loss: 0.13642290234565735, train acc: 0.9829\n",
      "loss: 0.1266745038330555, train acc: 0.981\n",
      "loss: 0.14793477430939675, train acc: 0.9801\n",
      "loss: 0.13503339365124703, train acc: 0.9788\n",
      "loss: 0.18334902077913284, train acc: 0.9804\n",
      "loss: 0.13844268321990966, train acc: 0.9796\n",
      "loss: 0.13306520506739616, train acc: 0.9802\n",
      "loss: 0.14108123257756233, train acc: 0.9782\n",
      "epoch: 35, loss: 0.027272360399365425, train acc: 0.9782, test acc: 0.9309\n",
      "loss: 0.10404606908559799, train acc: 0.9825\n",
      "loss: 0.12633633837103844, train acc: 0.9811\n",
      "loss: 0.1392367921769619, train acc: 0.9813\n",
      "loss: 0.13754763528704644, train acc: 0.9812\n",
      "loss: 0.17187903970479965, train acc: 0.9803\n",
      "loss: 0.13939237967133522, train acc: 0.9806\n",
      "loss: 0.133060934394598, train acc: 0.9839\n",
      "loss: 0.141332433372736, train acc: 0.983\n",
      "epoch: 36, loss: 0.021501919254660606, train acc: 0.983, test acc: 0.9312\n",
      "loss: 0.10377128422260284, train acc: 0.9828\n",
      "loss: 0.12870924100279807, train acc: 0.9814\n",
      "loss: 0.12267797812819481, train acc: 0.9821\n",
      "loss: 0.15796894058585167, train acc: 0.9795\n",
      "loss: 0.15416252240538597, train acc: 0.979\n",
      "loss: 0.13749620094895362, train acc: 0.9821\n",
      "loss: 0.12792754173278809, train acc: 0.9824\n",
      "loss: 0.15120596885681153, train acc: 0.9832\n",
      "epoch: 37, loss: 0.03092711791396141, train acc: 0.9832, test acc: 0.9321\n",
      "loss: 0.07839477807283401, train acc: 0.9842\n",
      "loss: 0.1204167477786541, train acc: 0.9828\n",
      "loss: 0.12623822391033174, train acc: 0.9827\n",
      "loss: 0.14338964596390724, train acc: 0.9818\n",
      "loss: 0.16215312629938125, train acc: 0.9836\n",
      "loss: 0.11910249553620815, train acc: 0.9838\n",
      "loss: 0.13504497483372688, train acc: 0.9846\n",
      "loss: 0.15148800536990165, train acc: 0.9849\n",
      "epoch: 38, loss: 0.035936642438173294, train acc: 0.9849, test acc: 0.9306\n",
      "loss: 0.15015412867069244, train acc: 0.9857\n",
      "loss: 0.12644589319825172, train acc: 0.9854\n",
      "loss: 0.14078741669654846, train acc: 0.985\n",
      "loss: 0.12571891471743585, train acc: 0.9836\n",
      "loss: 0.16254691630601883, train acc: 0.9857\n",
      "loss: 0.12049193456768989, train acc: 0.9829\n",
      "loss: 0.11646129116415978, train acc: 0.9835\n",
      "loss: 0.1381177470088005, train acc: 0.9843\n",
      "epoch: 39, loss: 0.017506485804915428, train acc: 0.9843, test acc: 0.9312\n",
      "loss: 0.0799584835767746, train acc: 0.9853\n",
      "loss: 0.12009133398532867, train acc: 0.9839\n",
      "loss: 0.13018365651369096, train acc: 0.9831\n",
      "loss: 0.113815026730299, train acc: 0.9814\n",
      "loss: 0.15080687925219535, train acc: 0.9837\n",
      "loss: 0.12875984609127045, train acc: 0.9833\n",
      "loss: 0.1237258866429329, train acc: 0.9837\n",
      "loss: 0.12867822572588922, train acc: 0.9853\n",
      "epoch: 40, loss: 0.12506307661533356, train acc: 0.9853, test acc: 0.9296\n",
      "loss: 0.08193965256214142, train acc: 0.9839\n",
      "loss: 0.12314652726054191, train acc: 0.9804\n",
      "loss: 0.11883340552449226, train acc: 0.9818\n",
      "loss: 0.12666604444384574, train acc: 0.9829\n",
      "loss: 0.1488019846379757, train acc: 0.9845\n",
      "loss: 0.12651579976081848, train acc: 0.9833\n",
      "loss: 0.12581176459789276, train acc: 0.9832\n",
      "loss: 0.12879138365387915, train acc: 0.9854\n",
      "epoch: 41, loss: 0.008744897320866585, train acc: 0.9854, test acc: 0.9303\n",
      "loss: 0.09216342866420746, train acc: 0.9851\n",
      "loss: 0.11566792502999305, train acc: 0.984\n",
      "loss: 0.12882482558488845, train acc: 0.9852\n",
      "loss: 0.11936770677566529, train acc: 0.9808\n",
      "loss: 0.152267487347126, train acc: 0.982\n",
      "loss: 0.12289887815713882, train acc: 0.9843\n",
      "loss: 0.11275513768196106, train acc: 0.9854\n",
      "loss: 0.13601070567965506, train acc: 0.9839\n",
      "epoch: 42, loss: 0.013889826834201813, train acc: 0.9839, test acc: 0.9308\n",
      "loss: 0.07506528496742249, train acc: 0.9861\n",
      "loss: 0.11368975937366485, train acc: 0.9839\n",
      "loss: 0.10972049944102764, train acc: 0.985\n",
      "loss: 0.12261052206158637, train acc: 0.9844\n",
      "loss: 0.12863471657037734, train acc: 0.9853\n",
      "loss: 0.12118791788816452, train acc: 0.9854\n",
      "loss: 0.11349092870950699, train acc: 0.9853\n",
      "loss: 0.12298069745302201, train acc: 0.9876\n",
      "epoch: 43, loss: 0.016621839255094528, train acc: 0.9876, test acc: 0.9309\n",
      "loss: 0.06253962963819504, train acc: 0.9858\n",
      "loss: 0.11753863468766212, train acc: 0.9867\n",
      "loss: 0.12453582882881165, train acc: 0.9848\n",
      "loss: 0.11926960051059723, train acc: 0.9846\n",
      "loss: 0.15871383845806122, train acc: 0.985\n",
      "loss: 0.13420133143663407, train acc: 0.9834\n",
      "loss: 0.1460650347173214, train acc: 0.9849\n",
      "loss: 0.12802357971668243, train acc: 0.9845\n",
      "epoch: 44, loss: 0.02910858392715454, train acc: 0.9845, test acc: 0.931\n",
      "loss: 0.08377580344676971, train acc: 0.9873\n",
      "loss: 0.10336756780743599, train acc: 0.9856\n",
      "loss: 0.1160000279545784, train acc: 0.9865\n",
      "loss: 0.11154146492481232, train acc: 0.9824\n",
      "loss: 0.1265072740614414, train acc: 0.9844\n",
      "loss: 0.1184965081512928, train acc: 0.9868\n",
      "loss: 0.12218748033046722, train acc: 0.9869\n",
      "loss: 0.12920265644788742, train acc: 0.9867\n",
      "epoch: 45, loss: 0.011717787012457848, train acc: 0.9867, test acc: 0.9292\n",
      "loss: 0.09868722409009933, train acc: 0.9854\n",
      "loss: 0.12185317277908325, train acc: 0.9843\n",
      "loss: 0.10558953769505024, train acc: 0.9876\n",
      "loss: 0.11747078746557235, train acc: 0.9852\n",
      "loss: 0.14994785338640212, train acc: 0.988\n",
      "loss: 0.12286872118711471, train acc: 0.9863\n",
      "loss: 0.1262308329343796, train acc: 0.9864\n",
      "loss: 0.11016846969723701, train acc: 0.9879\n",
      "epoch: 46, loss: 0.0063398308120667934, train acc: 0.9879, test acc: 0.9317\n",
      "loss: 0.08315219730138779, train acc: 0.9877\n",
      "loss: 0.10568294040858746, train acc: 0.9853\n",
      "loss: 0.11971504688262939, train acc: 0.9866\n",
      "loss: 0.11382866874337197, train acc: 0.9851\n",
      "loss: 0.1432446226477623, train acc: 0.9855\n",
      "loss: 0.11394573971629143, train acc: 0.9872\n",
      "loss: 0.1012861780822277, train acc: 0.9863\n",
      "loss: 0.11993543691933155, train acc: 0.9878\n",
      "epoch: 47, loss: 0.08259245753288269, train acc: 0.9878, test acc: 0.9303\n",
      "loss: 0.0879855677485466, train acc: 0.9869\n",
      "loss: 0.11902602091431617, train acc: 0.9876\n",
      "loss: 0.10164599865674973, train acc: 0.9872\n",
      "loss: 0.1300411492586136, train acc: 0.9861\n",
      "loss: 0.14774852991104126, train acc: 0.9865\n",
      "loss: 0.12111915908753872, train acc: 0.9857\n",
      "loss: 0.11532614082098007, train acc: 0.9869\n",
      "loss: 0.11730446815490722, train acc: 0.9836\n",
      "epoch: 48, loss: 0.008199297823011875, train acc: 0.9836, test acc: 0.9303\n",
      "loss: 0.07147809863090515, train acc: 0.9873\n",
      "loss: 0.09174601063132286, train acc: 0.9873\n",
      "loss: 0.10926478952169419, train acc: 0.9886\n",
      "loss: 0.12977450340986252, train acc: 0.9879\n",
      "loss: 0.13993695601820946, train acc: 0.9871\n",
      "loss: 0.11262353099882602, train acc: 0.9889\n",
      "loss: 0.12740077897906305, train acc: 0.989\n",
      "loss: 0.1170330572873354, train acc: 0.9874\n",
      "epoch: 49, loss: 0.012185717932879925, train acc: 0.9874, test acc: 0.9298\n",
      "loss: 0.06535951048135757, train acc: 0.9891\n",
      "loss: 0.11011290326714515, train acc: 0.9873\n",
      "loss: 0.10922702699899674, train acc: 0.9877\n",
      "loss: 0.12058485932648182, train acc: 0.9868\n",
      "loss: 0.14614074379205705, train acc: 0.9877\n",
      "loss: 0.11463142335414886, train acc: 0.9878\n",
      "loss: 0.10690983161330223, train acc: 0.9882\n",
      "loss: 0.12248323224484921, train acc: 0.9867\n",
      "epoch: 50, loss: 0.006692051887512207, train acc: 0.9867, test acc: 0.9319\n",
      "loss: 0.10723783820867538, train acc: 0.989\n",
      "loss: 0.11151182800531387, train acc: 0.9881\n",
      "loss: 0.1085299514234066, train acc: 0.9887\n",
      "loss: 0.11234283670783043, train acc: 0.9873\n",
      "loss: 0.13601767867803574, train acc: 0.9865\n",
      "loss: 0.10497556962072849, train acc: 0.9893\n",
      "loss: 0.11756823249161244, train acc: 0.988\n",
      "loss: 0.12458048686385155, train acc: 0.9894\n",
      "epoch: 51, loss: 0.006609855219721794, train acc: 0.9894, test acc: 0.9312\n",
      "loss: 0.10225903242826462, train acc: 0.9881\n",
      "loss: 0.1382061429321766, train acc: 0.9881\n",
      "loss: 0.12614167109131813, train acc: 0.987\n",
      "loss: 0.13235489800572395, train acc: 0.9846\n",
      "loss: 0.1324504904448986, train acc: 0.9885\n",
      "loss: 0.11743433214724064, train acc: 0.9881\n",
      "loss: 0.09529163017868995, train acc: 0.9886\n",
      "loss: 0.11122162714600563, train acc: 0.9886\n",
      "epoch: 52, loss: 0.0030409914907068014, train acc: 0.9886, test acc: 0.9317\n",
      "loss: 0.07013072073459625, train acc: 0.9878\n",
      "loss: 0.11447482779622078, train acc: 0.9883\n",
      "loss: 0.11327015608549118, train acc: 0.9873\n",
      "loss: 0.11949883252382279, train acc: 0.9872\n",
      "loss: 0.12660151422023774, train acc: 0.9858\n",
      "loss: 0.0942494560033083, train acc: 0.9877\n",
      "loss: 0.08896121792495251, train acc: 0.9878\n",
      "loss: 0.12920272275805472, train acc: 0.9873\n",
      "epoch: 53, loss: 0.012507009319961071, train acc: 0.9873, test acc: 0.93\n",
      "loss: 0.10631753504276276, train acc: 0.9891\n",
      "loss: 0.10650677271187306, train acc: 0.9868\n",
      "loss: 0.1189528837800026, train acc: 0.9886\n",
      "loss: 0.11188103780150413, train acc: 0.9878\n",
      "loss: 0.1268281154334545, train acc: 0.9888\n",
      "loss: 0.10778907798230648, train acc: 0.9874\n",
      "loss: 0.10706481188535691, train acc: 0.9877\n",
      "loss: 0.10930648259818554, train acc: 0.9897\n",
      "epoch: 54, loss: 0.02212866209447384, train acc: 0.9897, test acc: 0.9302\n",
      "loss: 0.0636618509888649, train acc: 0.989\n",
      "loss: 0.10107443742454052, train acc: 0.9869\n",
      "loss: 0.1173962600529194, train acc: 0.9874\n",
      "loss: 0.10259675979614258, train acc: 0.9875\n",
      "loss: 0.13775793612003326, train acc: 0.9896\n",
      "loss: 0.11285908371210099, train acc: 0.9892\n",
      "loss: 0.12499615177512169, train acc: 0.989\n",
      "loss: 0.10975338518619537, train acc: 0.9905\n",
      "epoch: 55, loss: 0.009961960837244987, train acc: 0.9905, test acc: 0.9317\n",
      "loss: 0.09482919424772263, train acc: 0.9883\n",
      "loss: 0.1051656175404787, train acc: 0.9889\n",
      "loss: 0.12809450477361678, train acc: 0.9888\n",
      "loss: 0.10106422565877438, train acc: 0.9883\n",
      "loss: 0.1238492950797081, train acc: 0.9877\n",
      "loss: 0.11086678560823202, train acc: 0.9882\n",
      "loss: 0.10769945979118348, train acc: 0.9899\n",
      "loss: 0.12173856422305107, train acc: 0.9891\n",
      "epoch: 56, loss: 0.05163449048995972, train acc: 0.9891, test acc: 0.9319\n",
      "loss: 0.056171465665102005, train acc: 0.9898\n",
      "loss: 0.09956503957509995, train acc: 0.9875\n",
      "loss: 0.10826774127781391, train acc: 0.9891\n",
      "loss: 0.10300169885158539, train acc: 0.9864\n",
      "loss: 0.14105553328990936, train acc: 0.9896\n",
      "loss: 0.11540097519755363, train acc: 0.989\n",
      "loss: 0.0943904995918274, train acc: 0.9892\n",
      "loss: 0.12322600558400154, train acc: 0.9903\n",
      "epoch: 57, loss: 0.007201519329100847, train acc: 0.9903, test acc: 0.9306\n",
      "loss: 0.07082902640104294, train acc: 0.9899\n",
      "loss: 0.09294085800647736, train acc: 0.9894\n",
      "loss: 0.09298284575343133, train acc: 0.9887\n",
      "loss: 0.11291982047259808, train acc: 0.9897\n",
      "loss: 0.12414721697568894, train acc: 0.9882\n",
      "loss: 0.10297904759645463, train acc: 0.9888\n",
      "loss: 0.1156703419983387, train acc: 0.988\n",
      "loss: 0.12744444608688354, train acc: 0.9912\n",
      "epoch: 58, loss: 0.01614195853471756, train acc: 0.9912, test acc: 0.9318\n",
      "loss: 0.10670983046293259, train acc: 0.9911\n",
      "loss: 0.10255989357829094, train acc: 0.9909\n",
      "loss: 0.11654839068651199, train acc: 0.9902\n",
      "loss: 0.10483811199665069, train acc: 0.9874\n",
      "loss: 0.11337044090032578, train acc: 0.9875\n",
      "loss: 0.11395901478827, train acc: 0.9905\n",
      "loss: 0.08827514722943305, train acc: 0.9898\n",
      "loss: 0.11501830816268921, train acc: 0.9903\n",
      "epoch: 59, loss: 0.02171744778752327, train acc: 0.9903, test acc: 0.9316\n",
      "loss: 0.08107493817806244, train acc: 0.989\n",
      "loss: 0.09161227457225322, train acc: 0.9899\n",
      "loss: 0.1083462905138731, train acc: 0.9893\n",
      "loss: 0.1042919285595417, train acc: 0.9882\n",
      "loss: 0.1330621987581253, train acc: 0.9901\n",
      "loss: 0.10771278776228428, train acc: 0.9905\n",
      "loss: 0.09595935121178627, train acc: 0.9891\n",
      "loss: 0.1011708989739418, train acc: 0.9907\n",
      "epoch: 60, loss: 0.006518383044749498, train acc: 0.9907, test acc: 0.9309\n",
      "loss: 0.09056548029184341, train acc: 0.9888\n",
      "loss: 0.10521073117852212, train acc: 0.9898\n",
      "loss: 0.12617952078580857, train acc: 0.9883\n",
      "loss: 0.09115242213010788, train acc: 0.9898\n",
      "loss: 0.135034416615963, train acc: 0.9882\n",
      "loss: 0.11142763830721378, train acc: 0.9904\n",
      "loss: 0.09639835879206657, train acc: 0.9897\n",
      "loss: 0.11310496143996715, train acc: 0.9909\n",
      "epoch: 61, loss: 0.014384742826223373, train acc: 0.9909, test acc: 0.9306\n",
      "loss: 0.056046824902296066, train acc: 0.99\n",
      "loss: 0.09668098650872707, train acc: 0.9907\n",
      "loss: 0.11373632214963436, train acc: 0.991\n",
      "loss: 0.09926539137959481, train acc: 0.9897\n",
      "loss: 0.12998570054769515, train acc: 0.9909\n",
      "loss: 0.10402518212795257, train acc: 0.9906\n",
      "loss: 0.09829562343657017, train acc: 0.9917\n",
      "loss: 0.09363652765750885, train acc: 0.9905\n",
      "epoch: 62, loss: 0.005033106077462435, train acc: 0.9905, test acc: 0.9314\n",
      "loss: 0.10165108740329742, train acc: 0.9913\n",
      "loss: 0.10274795480072499, train acc: 0.9912\n",
      "loss: 0.08547086268663406, train acc: 0.9898\n",
      "loss: 0.1083182841539383, train acc: 0.9893\n",
      "loss: 0.11810518875718116, train acc: 0.9899\n",
      "loss: 0.0937753576785326, train acc: 0.9919\n",
      "loss: 0.10645932666957378, train acc: 0.9918\n",
      "loss: 0.10405723005533218, train acc: 0.992\n",
      "epoch: 63, loss: 0.013468362390995026, train acc: 0.992, test acc: 0.9322\n",
      "loss: 0.0778510645031929, train acc: 0.9919\n",
      "loss: 0.08611518628895283, train acc: 0.9912\n",
      "loss: 0.09843393191695213, train acc: 0.9915\n",
      "loss: 0.07841618731617928, train acc: 0.991\n",
      "loss: 0.12778947204351426, train acc: 0.9892\n",
      "loss: 0.10523195639252662, train acc: 0.9918\n",
      "loss: 0.104437904804945, train acc: 0.9903\n",
      "loss: 0.10260297805070877, train acc: 0.991\n",
      "epoch: 64, loss: 0.0032271472737193108, train acc: 0.991, test acc: 0.9309\n",
      "loss: 0.1085585430264473, train acc: 0.9914\n",
      "loss: 0.10072179734706879, train acc: 0.9908\n",
      "loss: 0.09870781302452088, train acc: 0.9918\n",
      "loss: 0.09952396340668201, train acc: 0.9905\n",
      "loss: 0.12461883611977101, train acc: 0.9916\n",
      "loss: 0.10205022767186164, train acc: 0.9917\n",
      "loss: 0.10255740322172642, train acc: 0.9918\n",
      "loss: 0.08844491243362426, train acc: 0.9921\n",
      "epoch: 65, loss: 0.007246334105730057, train acc: 0.9921, test acc: 0.9304\n",
      "loss: 0.08114222437143326, train acc: 0.9918\n",
      "loss: 0.0963713988661766, train acc: 0.9919\n",
      "loss: 0.09324327930808067, train acc: 0.9914\n",
      "loss: 0.07485362850129604, train acc: 0.9909\n",
      "loss: 0.10322562530636788, train acc: 0.9914\n",
      "loss: 0.09548795893788338, train acc: 0.9919\n",
      "loss: 0.10982227250933647, train acc: 0.9926\n",
      "loss: 0.11062581837177277, train acc: 0.9919\n",
      "epoch: 66, loss: 0.002464825054630637, train acc: 0.9919, test acc: 0.9295\n",
      "loss: 0.10779628902673721, train acc: 0.9912\n",
      "loss: 0.10330477803945541, train acc: 0.9917\n",
      "loss: 0.0942919459193945, train acc: 0.9915\n",
      "loss: 0.1014907605946064, train acc: 0.9901\n",
      "loss: 0.12618107348680496, train acc: 0.9907\n",
      "loss: 0.09215981103479862, train acc: 0.9917\n",
      "loss: 0.10255995988845826, train acc: 0.9924\n",
      "loss: 0.09204002097249031, train acc: 0.9933\n",
      "epoch: 67, loss: 0.01998649351298809, train acc: 0.9933, test acc: 0.9294\n",
      "loss: 0.05379144847393036, train acc: 0.9928\n",
      "loss: 0.07451100833714008, train acc: 0.9912\n",
      "loss: 0.10211015939712524, train acc: 0.9914\n",
      "loss: 0.09527689032256603, train acc: 0.9895\n",
      "loss: 0.10955639369785786, train acc: 0.9923\n",
      "loss: 0.08019122630357742, train acc: 0.9908\n",
      "loss: 0.09415245354175568, train acc: 0.9922\n",
      "loss: 0.11680529415607452, train acc: 0.9926\n",
      "epoch: 68, loss: 0.005539285484701395, train acc: 0.9926, test acc: 0.9301\n",
      "loss: 0.11684572696685791, train acc: 0.9926\n",
      "loss: 0.09046004265546799, train acc: 0.993\n",
      "loss: 0.09817027114331722, train acc: 0.992\n",
      "loss: 0.07658201362937689, train acc: 0.9919\n",
      "loss: 0.11219366379082203, train acc: 0.9932\n",
      "loss: 0.09478049017488957, train acc: 0.9929\n",
      "loss: 0.08871739134192466, train acc: 0.994\n",
      "loss: 0.08441671319305896, train acc: 0.9933\n",
      "epoch: 69, loss: 0.011415635235607624, train acc: 0.9933, test acc: 0.9309\n",
      "loss: 0.10284621268510818, train acc: 0.9922\n",
      "loss: 0.10769403651356697, train acc: 0.9938\n",
      "loss: 0.08915413469076157, train acc: 0.9933\n",
      "loss: 0.0855090368539095, train acc: 0.9925\n",
      "loss: 0.1046451285481453, train acc: 0.9924\n",
      "loss: 0.09103973619639874, train acc: 0.9924\n",
      "loss: 0.08210179135203362, train acc: 0.9922\n",
      "loss: 0.08623468950390815, train acc: 0.9935\n",
      "epoch: 70, loss: 0.022092651575803757, train acc: 0.9935, test acc: 0.9302\n",
      "loss: 0.06663794815540314, train acc: 0.9924\n",
      "loss: 0.08127613738179207, train acc: 0.9921\n",
      "loss: 0.07673933953046799, train acc: 0.9934\n",
      "loss: 0.08462231047451496, train acc: 0.9914\n",
      "loss: 0.10255692861974239, train acc: 0.9923\n",
      "loss: 0.07979265563189983, train acc: 0.9913\n",
      "loss: 0.0827767014503479, train acc: 0.9918\n",
      "loss: 0.09197128266096115, train acc: 0.9924\n",
      "epoch: 71, loss: 0.017946599051356316, train acc: 0.9924, test acc: 0.9321\n",
      "loss: 0.08161816000938416, train acc: 0.9934\n",
      "loss: 0.08065650910139084, train acc: 0.9933\n",
      "loss: 0.09422866478562356, train acc: 0.9937\n",
      "loss: 0.08677498511970043, train acc: 0.9924\n",
      "loss: 0.11330777108669281, train acc: 0.9931\n",
      "loss: 0.0860046986490488, train acc: 0.9937\n",
      "loss: 0.07670356333255768, train acc: 0.9933\n",
      "loss: 0.11471960768103599, train acc: 0.9931\n",
      "epoch: 72, loss: 0.0040200562216341496, train acc: 0.9931, test acc: 0.9306\n",
      "loss: 0.06028648093342781, train acc: 0.9917\n",
      "loss: 0.10008039958775043, train acc: 0.993\n",
      "loss: 0.09223103001713753, train acc: 0.9939\n",
      "loss: 0.08726600036025048, train acc: 0.9932\n",
      "loss: 0.10769651681184769, train acc: 0.9934\n",
      "loss: 0.08943072780966758, train acc: 0.9924\n",
      "loss: 0.09336447753012181, train acc: 0.9923\n",
      "loss: 0.10484247133135796, train acc: 0.9934\n",
      "epoch: 73, loss: 0.006961036939173937, train acc: 0.9934, test acc: 0.9311\n",
      "loss: 0.06491673737764359, train acc: 0.9914\n",
      "loss: 0.0941390223801136, train acc: 0.993\n",
      "loss: 0.08439923748373986, train acc: 0.9934\n",
      "loss: 0.09103829935193061, train acc: 0.9935\n",
      "loss: 0.0962166167795658, train acc: 0.9933\n",
      "loss: 0.0906225150451064, train acc: 0.9945\n",
      "loss: 0.09236272647976876, train acc: 0.9923\n",
      "loss: 0.09239896535873413, train acc: 0.9941\n",
      "epoch: 74, loss: 0.011748573742806911, train acc: 0.9941, test acc: 0.9306\n",
      "loss: 0.05367415398359299, train acc: 0.9923\n",
      "loss: 0.07713296338915825, train acc: 0.9936\n",
      "loss: 0.08723331242799759, train acc: 0.9928\n",
      "loss: 0.10052975118160248, train acc: 0.9906\n",
      "loss: 0.1284079819917679, train acc: 0.9919\n",
      "loss: 0.08400769531726837, train acc: 0.9937\n",
      "loss: 0.09715309627354145, train acc: 0.9929\n",
      "loss: 0.10523944087326527, train acc: 0.9943\n",
      "epoch: 75, loss: 0.05364266782999039, train acc: 0.9943, test acc: 0.9303\n",
      "loss: 0.13082443177700043, train acc: 0.9937\n",
      "loss: 0.08791936673223973, train acc: 0.9939\n",
      "loss: 0.09636968448758125, train acc: 0.9939\n",
      "loss: 0.09126129448413849, train acc: 0.9922\n",
      "loss: 0.10826306119561195, train acc: 0.9919\n",
      "loss: 0.08368941321969033, train acc: 0.9938\n",
      "loss: 0.07435268238186836, train acc: 0.9933\n",
      "loss: 0.0881212119013071, train acc: 0.9933\n",
      "epoch: 76, loss: 0.010556715540587902, train acc: 0.9933, test acc: 0.9316\n",
      "loss: 0.08718270063400269, train acc: 0.9921\n",
      "loss: 0.08187677040696144, train acc: 0.9935\n",
      "loss: 0.07800603583455086, train acc: 0.9932\n",
      "loss: 0.11609464138746262, train acc: 0.993\n",
      "loss: 0.0885618768632412, train acc: 0.9927\n",
      "loss: 0.09249919094145298, train acc: 0.9923\n",
      "loss: 0.10318941175937653, train acc: 0.9939\n",
      "loss: 0.09521714448928834, train acc: 0.9949\n",
      "epoch: 77, loss: 0.0022307806648314, train acc: 0.9949, test acc: 0.9313\n",
      "loss: 0.07468540966510773, train acc: 0.9942\n",
      "loss: 0.08503413498401642, train acc: 0.9941\n",
      "loss: 0.08873009346425534, train acc: 0.9916\n",
      "loss: 0.08931639492511749, train acc: 0.9908\n",
      "loss: 0.11272879019379616, train acc: 0.9932\n",
      "loss: 0.0921018373221159, train acc: 0.9939\n",
      "loss: 0.08383625634014606, train acc: 0.9932\n",
      "loss: 0.09274906367063522, train acc: 0.9952\n",
      "epoch: 78, loss: 0.005053212400525808, train acc: 0.9952, test acc: 0.9305\n",
      "loss: 0.06032514572143555, train acc: 0.9937\n",
      "loss: 0.0875795029103756, train acc: 0.9944\n",
      "loss: 0.09741428941488266, train acc: 0.994\n",
      "loss: 0.07053689397871495, train acc: 0.9932\n",
      "loss: 0.09844661466777324, train acc: 0.9933\n",
      "loss: 0.08910083770751953, train acc: 0.9949\n",
      "loss: 0.08259840719401837, train acc: 0.9945\n",
      "loss: 0.0968089036643505, train acc: 0.9941\n",
      "epoch: 79, loss: 0.11927317082881927, train acc: 0.9941, test acc: 0.933\n",
      "#####training and testing end with K:20, P:0.1######\n",
      "#####training and testing start with K:20, P:0.5######\n",
      "loss: 2.334759473800659, train acc: 0.1206\n",
      "loss: 2.1964686632156374, train acc: 0.3792\n",
      "loss: 1.948762345314026, train acc: 0.532\n",
      "loss: 1.7460471153259278, train acc: 0.6153\n",
      "loss: 1.6150982975959778, train acc: 0.6734\n",
      "loss: 1.5045135855674743, train acc: 0.7516\n",
      "loss: 1.4424140810966493, train acc: 0.7907\n",
      "loss: 1.3009074807167054, train acc: 0.794\n",
      "epoch: 0, loss: 1.3583261966705322, train acc: 0.794, test acc: 0.8309\n",
      "loss: 1.2916243076324463, train acc: 0.8353\n",
      "loss: 1.210206937789917, train acc: 0.849\n",
      "loss: 1.1600437760353088, train acc: 0.844\n",
      "loss: 1.1188664138317108, train acc: 0.862\n",
      "loss: 1.1050304532051087, train acc: 0.8626\n",
      "loss: 1.0983391106128693, train acc: 0.8623\n",
      "loss: 1.0370989620685578, train acc: 0.8721\n",
      "loss: 1.0427262961864472, train acc: 0.8757\n",
      "epoch: 1, loss: 1.0378062725067139, train acc: 0.8757, test acc: 0.8748\n",
      "loss: 1.127367377281189, train acc: 0.8853\n",
      "loss: 0.9868854343891144, train acc: 0.8848\n",
      "loss: 1.0015006244182587, train acc: 0.8807\n",
      "loss: 0.9711318135261535, train acc: 0.8886\n",
      "loss: 0.9908395886421204, train acc: 0.8901\n",
      "loss: 0.9761454880237579, train acc: 0.8895\n",
      "loss: 0.9331677377223968, train acc: 0.8934\n",
      "loss: 0.8704171776771545, train acc: 0.8942\n",
      "epoch: 2, loss: 0.7488534450531006, train acc: 0.8942, test acc: 0.8864\n",
      "loss: 0.9584597945213318, train acc: 0.9005\n",
      "loss: 0.937112283706665, train acc: 0.8975\n",
      "loss: 0.9102029800415039, train acc: 0.8921\n",
      "loss: 0.8985575377941132, train acc: 0.9009\n",
      "loss: 0.9368347585201263, train acc: 0.9013\n",
      "loss: 0.8945176482200623, train acc: 0.901\n",
      "loss: 0.8762136399745941, train acc: 0.9026\n",
      "loss: 0.8527794122695923, train acc: 0.9043\n",
      "epoch: 3, loss: 0.9747322797775269, train acc: 0.9043, test acc: 0.8888\n",
      "loss: 1.101728081703186, train acc: 0.9034\n",
      "loss: 0.8772518754005432, train acc: 0.9049\n",
      "loss: 0.8496217608451844, train acc: 0.905\n",
      "loss: 0.8366094708442688, train acc: 0.9078\n",
      "loss: 0.8445162117481232, train acc: 0.9093\n",
      "loss: 0.803183513879776, train acc: 0.908\n",
      "loss: 0.8334355354309082, train acc: 0.9077\n",
      "loss: 0.7973707318305969, train acc: 0.9123\n",
      "epoch: 4, loss: 0.8704661130905151, train acc: 0.9123, test acc: 0.8968\n",
      "loss: 1.0293245315551758, train acc: 0.913\n",
      "loss: 0.8149631440639495, train acc: 0.9103\n",
      "loss: 0.8584406197071075, train acc: 0.9093\n",
      "loss: 0.7974910318851471, train acc: 0.913\n",
      "loss: 0.807655668258667, train acc: 0.9138\n",
      "loss: 0.8100096583366394, train acc: 0.9141\n",
      "loss: 0.7991504192352294, train acc: 0.9105\n",
      "loss: 0.7335342764854431, train acc: 0.9142\n",
      "epoch: 5, loss: 1.0145705938339233, train acc: 0.9142, test acc: 0.8962\n",
      "loss: 1.0034689903259277, train acc: 0.9158\n",
      "loss: 0.7982806742191315, train acc: 0.9205\n",
      "loss: 0.8359217822551728, train acc: 0.9169\n",
      "loss: 0.8101273238658905, train acc: 0.9138\n",
      "loss: 0.7911765456199646, train acc: 0.9177\n",
      "loss: 0.7851426362991333, train acc: 0.9163\n",
      "loss: 0.7817084789276123, train acc: 0.9173\n",
      "loss: 0.7322205364704132, train acc: 0.917\n",
      "epoch: 6, loss: 1.074811339378357, train acc: 0.917, test acc: 0.8958\n",
      "loss: 0.9454158544540405, train acc: 0.9198\n",
      "loss: 0.8109689056873322, train acc: 0.9194\n",
      "loss: 0.8336675703525543, train acc: 0.9158\n",
      "loss: 0.7665517270565033, train acc: 0.9182\n",
      "loss: 0.8503291368484497, train acc: 0.9221\n",
      "loss: 0.7463921308517456, train acc: 0.9202\n",
      "loss: 0.7828163266181946, train acc: 0.9203\n",
      "loss: 0.7529848098754883, train acc: 0.9234\n",
      "epoch: 7, loss: 0.7885165810585022, train acc: 0.9234, test acc: 0.8997\n",
      "loss: 0.8615366220474243, train acc: 0.9238\n",
      "loss: 0.726399153470993, train acc: 0.9218\n",
      "loss: 0.7881704926490783, train acc: 0.9194\n",
      "loss: 0.8031218588352204, train acc: 0.9206\n",
      "loss: 0.7473699748516083, train acc: 0.9196\n",
      "loss: 0.7630951404571533, train acc: 0.9219\n",
      "loss: 0.7554166793823243, train acc: 0.9207\n",
      "loss: 0.7298917770385742, train acc: 0.9211\n",
      "epoch: 8, loss: 0.6868767142295837, train acc: 0.9211, test acc: 0.8962\n",
      "loss: 0.9505488276481628, train acc: 0.9189\n",
      "loss: 0.7710223317146301, train acc: 0.922\n",
      "loss: 0.7631817162036896, train acc: 0.9209\n",
      "loss: 0.7697337627410888, train acc: 0.921\n",
      "loss: 0.7892246305942535, train acc: 0.9236\n",
      "loss: 0.7248197138309479, train acc: 0.9235\n",
      "loss: 0.7274032056331634, train acc: 0.9232\n",
      "loss: 0.719606077671051, train acc: 0.9223\n",
      "epoch: 9, loss: 0.5815922021865845, train acc: 0.9223, test acc: 0.8995\n",
      "loss: 0.8835128545761108, train acc: 0.9225\n",
      "loss: 0.733885657787323, train acc: 0.9246\n",
      "loss: 0.7483489751815796, train acc: 0.921\n",
      "loss: 0.6934286296367645, train acc: 0.9231\n",
      "loss: 0.7600966036319733, train acc: 0.9256\n",
      "loss: 0.7281591534614563, train acc: 0.9257\n",
      "loss: 0.7018028497695923, train acc: 0.9243\n",
      "loss: 0.7049596786499024, train acc: 0.9273\n",
      "epoch: 10, loss: 1.026479721069336, train acc: 0.9273, test acc: 0.902\n",
      "loss: 0.8561448454856873, train acc: 0.926\n",
      "loss: 0.7313914358615875, train acc: 0.9262\n",
      "loss: 0.738328492641449, train acc: 0.9232\n",
      "loss: 0.698505836725235, train acc: 0.9256\n",
      "loss: 0.7261060118675232, train acc: 0.9256\n",
      "loss: 0.711430960893631, train acc: 0.9254\n",
      "loss: 0.7163973808288574, train acc: 0.9258\n",
      "loss: 0.6944039583206176, train acc: 0.9258\n",
      "epoch: 11, loss: 0.8039734959602356, train acc: 0.9258, test acc: 0.901\n",
      "loss: 0.9959406852722168, train acc: 0.9281\n",
      "loss: 0.7536361217498779, train acc: 0.9285\n",
      "loss: 0.7470088601112366, train acc: 0.9258\n",
      "loss: 0.7038177907466888, train acc: 0.9285\n",
      "loss: 0.7485092401504516, train acc: 0.928\n",
      "loss: 0.6918626427650452, train acc: 0.9272\n",
      "loss: 0.7104141592979432, train acc: 0.9254\n",
      "loss: 0.6908006489276886, train acc: 0.9289\n",
      "epoch: 12, loss: 1.1235201358795166, train acc: 0.9289, test acc: 0.9017\n",
      "loss: 0.8700814843177795, train acc: 0.9259\n",
      "loss: 0.7298926472663879, train acc: 0.9278\n",
      "loss: 0.7392882227897644, train acc: 0.9281\n",
      "loss: 0.7207037031650543, train acc: 0.9263\n",
      "loss: 0.737735801935196, train acc: 0.929\n",
      "loss: 0.6982530057430267, train acc: 0.9294\n",
      "loss: 0.6918775141239166, train acc: 0.9291\n",
      "loss: 0.6656948804855347, train acc: 0.9297\n",
      "epoch: 13, loss: 0.5748269557952881, train acc: 0.9297, test acc: 0.9041\n",
      "loss: 0.8103741407394409, train acc: 0.9297\n",
      "loss: 0.7388595461845398, train acc: 0.9339\n",
      "loss: 0.7543056845664978, train acc: 0.929\n",
      "loss: 0.6612128973007202, train acc: 0.9312\n",
      "loss: 0.7237438499927521, train acc: 0.9314\n",
      "loss: 0.6772838175296784, train acc: 0.9282\n",
      "loss: 0.695252513885498, train acc: 0.9309\n",
      "loss: 0.6322769194841384, train acc: 0.9306\n",
      "epoch: 14, loss: 0.41522306203842163, train acc: 0.9306, test acc: 0.9027\n",
      "loss: 0.9475719332695007, train acc: 0.93\n",
      "loss: 0.7041496753692627, train acc: 0.9325\n",
      "loss: 0.7155679881572723, train acc: 0.9302\n",
      "loss: 0.7137171804904938, train acc: 0.9295\n",
      "loss: 0.7292883515357971, train acc: 0.9326\n",
      "loss: 0.6670204043388367, train acc: 0.9314\n",
      "loss: 0.6901348412036896, train acc: 0.9333\n",
      "loss: 0.6921438813209534, train acc: 0.9337\n",
      "epoch: 15, loss: 0.8535888195037842, train acc: 0.9337, test acc: 0.9034\n",
      "loss: 0.8269197344779968, train acc: 0.9314\n",
      "loss: 0.6970075190067291, train acc: 0.9331\n",
      "loss: 0.7182753443717956, train acc: 0.9339\n",
      "loss: 0.6929286360740662, train acc: 0.9321\n",
      "loss: 0.667971408367157, train acc: 0.9324\n",
      "loss: 0.6640347898006439, train acc: 0.9343\n",
      "loss: 0.6801178455352783, train acc: 0.9314\n",
      "loss: 0.6385366916656494, train acc: 0.9318\n",
      "epoch: 16, loss: 0.6314327120780945, train acc: 0.9318, test acc: 0.9042\n",
      "loss: 1.018812894821167, train acc: 0.9316\n",
      "loss: 0.7249725818634033, train acc: 0.9318\n",
      "loss: 0.7439269661903382, train acc: 0.9326\n",
      "loss: 0.6887464344501495, train acc: 0.9332\n",
      "loss: 0.6874014139175415, train acc: 0.9331\n",
      "loss: 0.64494309425354, train acc: 0.9339\n",
      "loss: 0.6834345698356629, train acc: 0.9365\n",
      "loss: 0.6563887655735016, train acc: 0.934\n",
      "epoch: 17, loss: 0.4099263846874237, train acc: 0.934, test acc: 0.9042\n",
      "loss: 0.7573758959770203, train acc: 0.9354\n",
      "loss: 0.6768602013587952, train acc: 0.935\n",
      "loss: 0.7463934123516083, train acc: 0.9304\n",
      "loss: 0.6841760098934173, train acc: 0.9342\n",
      "loss: 0.6954154908657074, train acc: 0.9324\n",
      "loss: 0.6728970408439636, train acc: 0.9356\n",
      "loss: 0.6896878182888031, train acc: 0.9345\n",
      "loss: 0.621950751543045, train acc: 0.9372\n",
      "epoch: 18, loss: 0.6626663208007812, train acc: 0.9372, test acc: 0.9044\n",
      "loss: 0.8847560882568359, train acc: 0.9346\n",
      "loss: 0.6510679244995117, train acc: 0.9355\n",
      "loss: 0.7379610598087311, train acc: 0.9325\n",
      "loss: 0.7200494050979614, train acc: 0.9371\n",
      "loss: 0.7192006409168243, train acc: 0.935\n",
      "loss: 0.6648975491523743, train acc: 0.935\n",
      "loss: 0.6298144161701202, train acc: 0.9361\n",
      "loss: 0.6645768254995346, train acc: 0.936\n",
      "epoch: 19, loss: 0.30611279606819153, train acc: 0.936, test acc: 0.9031\n",
      "loss: 0.7853402495384216, train acc: 0.9356\n",
      "loss: 0.6555846035480499, train acc: 0.9372\n",
      "loss: 0.7414354145526886, train acc: 0.9357\n",
      "loss: 0.6673013627529144, train acc: 0.9354\n",
      "loss: 0.7350151538848877, train acc: 0.9361\n",
      "loss: 0.6429481983184815, train acc: 0.9361\n",
      "loss: 0.6728790521621704, train acc: 0.9354\n",
      "loss: 0.6411523967981339, train acc: 0.9359\n",
      "epoch: 20, loss: 0.6605225801467896, train acc: 0.9359, test acc: 0.9023\n",
      "loss: 0.9212628602981567, train acc: 0.9348\n",
      "loss: 0.6994178891181946, train acc: 0.9354\n",
      "loss: 0.7151705384254455, train acc: 0.9369\n",
      "loss: 0.6740861356258392, train acc: 0.9373\n",
      "loss: 0.6795916557312012, train acc: 0.938\n",
      "loss: 0.6530432879924775, train acc: 0.9396\n",
      "loss: 0.6553714036941528, train acc: 0.9368\n",
      "loss: 0.6680776476860046, train acc: 0.936\n",
      "epoch: 21, loss: 0.4630820155143738, train acc: 0.936, test acc: 0.9061\n",
      "loss: 0.807748019695282, train acc: 0.9376\n",
      "loss: 0.6478094756603241, train acc: 0.937\n",
      "loss: 0.6852023303508759, train acc: 0.9356\n",
      "loss: 0.6762221574783325, train acc: 0.9377\n",
      "loss: 0.649725866317749, train acc: 0.9378\n",
      "loss: 0.6591673791408539, train acc: 0.9398\n",
      "loss: 0.6867745220661163, train acc: 0.9388\n",
      "loss: 0.6183450222015381, train acc: 0.9388\n",
      "epoch: 22, loss: 0.5728330612182617, train acc: 0.9388, test acc: 0.9042\n",
      "loss: 1.00136137008667, train acc: 0.937\n",
      "loss: 0.7046184837818146, train acc: 0.9376\n",
      "loss: 0.7107356727123261, train acc: 0.9381\n",
      "loss: 0.6813108325004578, train acc: 0.9379\n",
      "loss: 0.6787992894649506, train acc: 0.9394\n",
      "loss: 0.6229550242424011, train acc: 0.9388\n",
      "loss: 0.6314581722021103, train acc: 0.9377\n",
      "loss: 0.6341475427150727, train acc: 0.9388\n",
      "epoch: 23, loss: 0.44165658950805664, train acc: 0.9388, test acc: 0.9048\n",
      "loss: 0.7462379932403564, train acc: 0.9396\n",
      "loss: 0.6552877008914948, train acc: 0.9403\n",
      "loss: 0.6697619706392288, train acc: 0.9385\n",
      "loss: 0.6916799008846283, train acc: 0.9375\n",
      "loss: 0.6770234823226928, train acc: 0.9376\n",
      "loss: 0.6723250150680542, train acc: 0.9399\n",
      "loss: 0.6869343042373657, train acc: 0.9383\n",
      "loss: 0.6488612949848175, train acc: 0.9396\n",
      "epoch: 24, loss: 0.7786320447921753, train acc: 0.9396, test acc: 0.9025\n",
      "loss: 0.9770079851150513, train acc: 0.9376\n",
      "loss: 0.7310283780097961, train acc: 0.9395\n",
      "loss: 0.6578266650438309, train acc: 0.9392\n",
      "loss: 0.6487571120262146, train acc: 0.9413\n",
      "loss: 0.6611412465572357, train acc: 0.9417\n",
      "loss: 0.6616618156433105, train acc: 0.939\n",
      "loss: 0.6742661595344543, train acc: 0.941\n",
      "loss: 0.6064349055290222, train acc: 0.94\n",
      "epoch: 25, loss: 0.6355428695678711, train acc: 0.94, test acc: 0.907\n",
      "loss: 0.6795797348022461, train acc: 0.9408\n",
      "loss: 0.673268610239029, train acc: 0.941\n",
      "loss: 0.7094279766082764, train acc: 0.9397\n",
      "loss: 0.648427101969719, train acc: 0.9395\n",
      "loss: 0.6610264658927918, train acc: 0.9392\n",
      "loss: 0.6082813531160355, train acc: 0.9387\n",
      "loss: 0.6511601150035858, train acc: 0.9394\n",
      "loss: 0.6131465315818787, train acc: 0.9391\n",
      "epoch: 26, loss: 0.8187907934188843, train acc: 0.9391, test acc: 0.904\n",
      "loss: 0.8680446147918701, train acc: 0.9392\n",
      "loss: 0.6614571809768677, train acc: 0.9408\n",
      "loss: 0.6317782908678055, train acc: 0.9397\n",
      "loss: 0.6478880941867828, train acc: 0.9394\n",
      "loss: 0.6544761896133423, train acc: 0.9415\n",
      "loss: 0.6588728666305542, train acc: 0.9385\n",
      "loss: 0.6556723892688752, train acc: 0.9409\n",
      "loss: 0.6190791189670563, train acc: 0.9418\n",
      "epoch: 27, loss: 0.4511873722076416, train acc: 0.9418, test acc: 0.9049\n",
      "loss: 0.7974805235862732, train acc: 0.9409\n",
      "loss: 0.6711996376514435, train acc: 0.9415\n",
      "loss: 0.6695063978433609, train acc: 0.9415\n",
      "loss: 0.6614302396774292, train acc: 0.94\n",
      "loss: 0.6640455007553101, train acc: 0.9414\n",
      "loss: 0.6306935787200928, train acc: 0.9392\n",
      "loss: 0.6443852722644806, train acc: 0.9376\n",
      "loss: 0.631521362066269, train acc: 0.9417\n",
      "epoch: 28, loss: 0.33359915018081665, train acc: 0.9417, test acc: 0.9044\n",
      "loss: 0.8217095136642456, train acc: 0.9423\n",
      "loss: 0.6359742999076843, train acc: 0.9416\n",
      "loss: 0.6678444087505341, train acc: 0.9413\n",
      "loss: 0.6595992147922516, train acc: 0.9423\n",
      "loss: 0.6727594554424285, train acc: 0.9408\n",
      "loss: 0.6369388043880463, train acc: 0.942\n",
      "loss: 0.6600264966487884, train acc: 0.9411\n",
      "loss: 0.6521437227725982, train acc: 0.9417\n",
      "epoch: 29, loss: 0.31939196586608887, train acc: 0.9417, test acc: 0.9056\n",
      "loss: 0.8601067662239075, train acc: 0.9434\n",
      "loss: 0.6527056396007538, train acc: 0.9431\n",
      "loss: 0.6824400454759598, train acc: 0.941\n",
      "loss: 0.6567491173744202, train acc: 0.9426\n",
      "loss: 0.6749854922294617, train acc: 0.944\n",
      "loss: 0.6301762342453003, train acc: 0.9429\n",
      "loss: 0.6628336906433105, train acc: 0.9446\n",
      "loss: 0.6141108125448227, train acc: 0.9445\n",
      "epoch: 30, loss: 0.7902539372444153, train acc: 0.9445, test acc: 0.9056\n",
      "loss: 0.9504291415214539, train acc: 0.9423\n",
      "loss: 0.6746304333209991, train acc: 0.944\n",
      "loss: 0.6718711674213409, train acc: 0.9434\n",
      "loss: 0.5976689517498016, train acc: 0.944\n",
      "loss: 0.6180037975311279, train acc: 0.9407\n",
      "loss: 0.6143697619438171, train acc: 0.943\n",
      "loss: 0.6314137637615204, train acc: 0.943\n",
      "loss: 0.6093739986419677, train acc: 0.9432\n",
      "epoch: 31, loss: 0.3035111129283905, train acc: 0.9432, test acc: 0.9075\n",
      "loss: 0.8754650354385376, train acc: 0.9439\n",
      "loss: 0.6306636750698089, train acc: 0.9433\n",
      "loss: 0.7028245449066162, train acc: 0.9407\n",
      "loss: 0.6335260331630707, train acc: 0.9434\n",
      "loss: 0.6355629026889801, train acc: 0.9445\n",
      "loss: 0.6402830660343171, train acc: 0.9421\n",
      "loss: 0.6723306596279144, train acc: 0.9427\n",
      "loss: 0.5827250003814697, train acc: 0.9439\n",
      "epoch: 32, loss: 0.30864810943603516, train acc: 0.9439, test acc: 0.908\n",
      "loss: 0.8312719464302063, train acc: 0.9445\n",
      "loss: 0.6544859170913696, train acc: 0.9434\n",
      "loss: 0.6860033392906189, train acc: 0.9415\n",
      "loss: 0.6186280369758606, train acc: 0.9452\n",
      "loss: 0.6676100552082062, train acc: 0.9455\n",
      "loss: 0.6486321687698364, train acc: 0.9432\n",
      "loss: 0.6469859778881073, train acc: 0.9448\n",
      "loss: 0.6009277075529098, train acc: 0.9452\n",
      "epoch: 33, loss: 0.22449159622192383, train acc: 0.9452, test acc: 0.9056\n",
      "loss: 0.8945909142494202, train acc: 0.9444\n",
      "loss: 0.6834786415100098, train acc: 0.9443\n",
      "loss: 0.6831846594810486, train acc: 0.9442\n",
      "loss: 0.6020483136177063, train acc: 0.9465\n",
      "loss: 0.7045133590698243, train acc: 0.9465\n",
      "loss: 0.6554375290870667, train acc: 0.9456\n",
      "loss: 0.607752013206482, train acc: 0.9433\n",
      "loss: 0.6485995173454284, train acc: 0.9463\n",
      "epoch: 34, loss: 0.6425554752349854, train acc: 0.9463, test acc: 0.908\n",
      "loss: 0.8005008697509766, train acc: 0.9455\n",
      "loss: 0.670452231168747, train acc: 0.9463\n",
      "loss: 0.6467894166707993, train acc: 0.9426\n",
      "loss: 0.6493102163076401, train acc: 0.9439\n",
      "loss: 0.662155133485794, train acc: 0.9449\n",
      "loss: 0.6042497813701629, train acc: 0.9447\n",
      "loss: 0.6447829306125641, train acc: 0.9411\n",
      "loss: 0.5889657765626908, train acc: 0.9442\n",
      "epoch: 35, loss: 0.27210766077041626, train acc: 0.9442, test acc: 0.9066\n",
      "loss: 0.8026039600372314, train acc: 0.9454\n",
      "loss: 0.6472325444221496, train acc: 0.9446\n",
      "loss: 0.6859642058610916, train acc: 0.9456\n",
      "loss: 0.6536997199058533, train acc: 0.9426\n",
      "loss: 0.6369442820549012, train acc: 0.9456\n",
      "loss: 0.5907775342464447, train acc: 0.9442\n",
      "loss: 0.6273105442523956, train acc: 0.9442\n",
      "loss: 0.5781627058982849, train acc: 0.9439\n",
      "epoch: 36, loss: 0.24333472549915314, train acc: 0.9439, test acc: 0.9055\n",
      "loss: 1.0042331218719482, train acc: 0.9425\n",
      "loss: 0.61219020485878, train acc: 0.9453\n",
      "loss: 0.6233951210975647, train acc: 0.9451\n",
      "loss: 0.6549030065536499, train acc: 0.9464\n",
      "loss: 0.6253465712070465, train acc: 0.9456\n",
      "loss: 0.6178686797618866, train acc: 0.9479\n",
      "loss: 0.6013464778661728, train acc: 0.9468\n",
      "loss: 0.652136480808258, train acc: 0.9469\n",
      "epoch: 37, loss: 0.5798503160476685, train acc: 0.9469, test acc: 0.908\n",
      "loss: 0.8680860996246338, train acc: 0.9458\n",
      "loss: 0.6775612950325012, train acc: 0.9445\n",
      "loss: 0.6773943543434143, train acc: 0.9464\n",
      "loss: 0.6442734807729721, train acc: 0.9478\n",
      "loss: 0.6339777112007141, train acc: 0.9478\n",
      "loss: 0.6429011702537537, train acc: 0.9467\n",
      "loss: 0.6279539048671723, train acc: 0.9464\n",
      "loss: 0.6297865808010101, train acc: 0.947\n",
      "epoch: 38, loss: 0.362702339887619, train acc: 0.947, test acc: 0.9086\n",
      "loss: 0.7280967235565186, train acc: 0.9481\n",
      "loss: 0.6217793881893158, train acc: 0.9428\n",
      "loss: 0.6616857945919037, train acc: 0.9438\n",
      "loss: 0.635766726732254, train acc: 0.9442\n",
      "loss: 0.6512864470481873, train acc: 0.9464\n",
      "loss: 0.6220071494579316, train acc: 0.9467\n",
      "loss: 0.625677290558815, train acc: 0.9449\n",
      "loss: 0.5950876414775849, train acc: 0.9467\n",
      "epoch: 39, loss: 0.2450646162033081, train acc: 0.9467, test acc: 0.9053\n",
      "loss: 0.7860180735588074, train acc: 0.9459\n",
      "loss: 0.6260354697704316, train acc: 0.9465\n",
      "loss: 0.6684053063392639, train acc: 0.9456\n",
      "loss: 0.6276762902736663, train acc: 0.9466\n",
      "loss: 0.653036254644394, train acc: 0.9468\n",
      "loss: 0.6146609842777252, train acc: 0.9448\n",
      "loss: 0.626950454711914, train acc: 0.9458\n",
      "loss: 0.5812032401561738, train acc: 0.9467\n",
      "epoch: 40, loss: 0.18978425860404968, train acc: 0.9467, test acc: 0.907\n",
      "loss: 0.8551095128059387, train acc: 0.9471\n",
      "loss: 0.6461964994668961, train acc: 0.9476\n",
      "loss: 0.6708385944366455, train acc: 0.947\n",
      "loss: 0.5875312566757203, train acc: 0.9467\n",
      "loss: 0.656038510799408, train acc: 0.9482\n",
      "loss: 0.5924598693847656, train acc: 0.9488\n",
      "loss: 0.6225364625453949, train acc: 0.9464\n",
      "loss: 0.5554686695337295, train acc: 0.9469\n",
      "epoch: 41, loss: 0.43436527252197266, train acc: 0.9469, test acc: 0.9058\n",
      "loss: 0.7559301853179932, train acc: 0.945\n",
      "loss: 0.6102014541625976, train acc: 0.9468\n",
      "loss: 0.6662562429904938, train acc: 0.9455\n",
      "loss: 0.6248907417058944, train acc: 0.9467\n",
      "loss: 0.6724559426307678, train acc: 0.9454\n",
      "loss: 0.6123895585536957, train acc: 0.9462\n",
      "loss: 0.6305517524480819, train acc: 0.9469\n",
      "loss: 0.5696441411972046, train acc: 0.948\n",
      "epoch: 42, loss: 0.3201642334461212, train acc: 0.948, test acc: 0.9087\n",
      "loss: 0.8758599162101746, train acc: 0.9481\n",
      "loss: 0.6680217206478118, train acc: 0.9463\n",
      "loss: 0.6727830320596695, train acc: 0.9466\n",
      "loss: 0.600920170545578, train acc: 0.9466\n",
      "loss: 0.6372384876012802, train acc: 0.9477\n",
      "loss: 0.6009302496910095, train acc: 0.9468\n",
      "loss: 0.5965676724910736, train acc: 0.9463\n",
      "loss: 0.5801104068756103, train acc: 0.948\n",
      "epoch: 43, loss: 0.22711563110351562, train acc: 0.948, test acc: 0.9081\n",
      "loss: 0.8220192790031433, train acc: 0.9479\n",
      "loss: 0.6334741562604904, train acc: 0.949\n",
      "loss: 0.6634549587965012, train acc: 0.9481\n",
      "loss: 0.63760005235672, train acc: 0.9484\n",
      "loss: 0.6186921417713165, train acc: 0.9504\n",
      "loss: 0.6160432875156403, train acc: 0.9457\n",
      "loss: 0.6551751017570495, train acc: 0.9484\n",
      "loss: 0.6089249044656754, train acc: 0.9472\n",
      "epoch: 44, loss: 0.4497070014476776, train acc: 0.9472, test acc: 0.9083\n",
      "loss: 0.8387461304664612, train acc: 0.9475\n",
      "loss: 0.6189401924610138, train acc: 0.9474\n",
      "loss: 0.6532498002052307, train acc: 0.9474\n",
      "loss: 0.6288298308849335, train acc: 0.949\n",
      "loss: 0.6394684076309204, train acc: 0.948\n",
      "loss: 0.5991528183221817, train acc: 0.9491\n",
      "loss: 0.6005484819412231, train acc: 0.9479\n",
      "loss: 0.6412886083126068, train acc: 0.9492\n",
      "epoch: 45, loss: 0.25574395060539246, train acc: 0.9492, test acc: 0.9078\n",
      "loss: 0.7611265182495117, train acc: 0.9481\n",
      "loss: 0.639307302236557, train acc: 0.9484\n",
      "loss: 0.6713844001293182, train acc: 0.9461\n",
      "loss: 0.6308570981025696, train acc: 0.9499\n",
      "loss: 0.6485404640436172, train acc: 0.9502\n",
      "loss: 0.6151698112487793, train acc: 0.9487\n",
      "loss: 0.6158186852931976, train acc: 0.9492\n",
      "loss: 0.5933167219161988, train acc: 0.9481\n",
      "epoch: 46, loss: 0.6542129516601562, train acc: 0.9481, test acc: 0.9065\n",
      "loss: 0.7664297223091125, train acc: 0.9483\n",
      "loss: 0.6106518179178237, train acc: 0.9448\n",
      "loss: 0.6747737795114517, train acc: 0.9488\n",
      "loss: 0.6102045893669128, train acc: 0.9482\n",
      "loss: 0.6199566721916199, train acc: 0.949\n",
      "loss: 0.6359481453895569, train acc: 0.9482\n",
      "loss: 0.6247950851917267, train acc: 0.9472\n",
      "loss: 0.5914602696895599, train acc: 0.9476\n",
      "epoch: 47, loss: 0.7248062491416931, train acc: 0.9476, test acc: 0.9104\n",
      "loss: 0.7944625616073608, train acc: 0.9499\n",
      "loss: 0.6373014688491822, train acc: 0.9493\n",
      "loss: 0.6406936824321747, train acc: 0.9493\n",
      "loss: 0.6086383670568466, train acc: 0.9486\n",
      "loss: 0.660691037774086, train acc: 0.949\n",
      "loss: 0.6280718922615052, train acc: 0.9499\n",
      "loss: 0.6121156930923461, train acc: 0.9503\n",
      "loss: 0.5970865488052368, train acc: 0.9494\n",
      "epoch: 48, loss: 0.23749549686908722, train acc: 0.9494, test acc: 0.9087\n",
      "loss: 0.8085712194442749, train acc: 0.9487\n",
      "loss: 0.6309374034404754, train acc: 0.9479\n",
      "loss: 0.6403067499399185, train acc: 0.9503\n",
      "loss: 0.6592991709709167, train acc: 0.951\n",
      "loss: 0.6450426578521729, train acc: 0.9487\n",
      "loss: 0.6583149552345275, train acc: 0.9489\n",
      "loss: 0.6020355701446534, train acc: 0.9488\n",
      "loss: 0.6093911677598953, train acc: 0.95\n",
      "epoch: 49, loss: 0.2642374634742737, train acc: 0.95, test acc: 0.9096\n",
      "loss: 0.7737470269203186, train acc: 0.9498\n",
      "loss: 0.6060872375965118, train acc: 0.9488\n",
      "loss: 0.6683218896389007, train acc: 0.9479\n",
      "loss: 0.595730772614479, train acc: 0.9491\n",
      "loss: 0.6160964012145996, train acc: 0.9497\n",
      "loss: 0.606722754240036, train acc: 0.949\n",
      "loss: 0.5946343868970871, train acc: 0.9473\n",
      "loss: 0.5970183044672013, train acc: 0.9492\n",
      "epoch: 50, loss: 0.46836674213409424, train acc: 0.9492, test acc: 0.9074\n",
      "loss: 0.7344244122505188, train acc: 0.9476\n",
      "loss: 0.6273695468902588, train acc: 0.9498\n",
      "loss: 0.6790271520614624, train acc: 0.9498\n",
      "loss: 0.5869231432676315, train acc: 0.9504\n",
      "loss: 0.6305009067058563, train acc: 0.9507\n",
      "loss: 0.594250065088272, train acc: 0.9512\n",
      "loss: 0.5713660776615143, train acc: 0.9512\n",
      "loss: 0.5407347321510315, train acc: 0.9494\n",
      "epoch: 51, loss: 0.40553390979766846, train acc: 0.9494, test acc: 0.9091\n",
      "loss: 0.7298542857170105, train acc: 0.9501\n",
      "loss: 0.6097166121006012, train acc: 0.9483\n",
      "loss: 0.6734866976737977, train acc: 0.9487\n",
      "loss: 0.6267399251461029, train acc: 0.9509\n",
      "loss: 0.6290097236633301, train acc: 0.9494\n",
      "loss: 0.5782946556806564, train acc: 0.9532\n",
      "loss: 0.6345944941043854, train acc: 0.9507\n",
      "loss: 0.5736532270908355, train acc: 0.9533\n",
      "epoch: 52, loss: 0.36103856563568115, train acc: 0.9533, test acc: 0.9094\n",
      "loss: 0.9024344682693481, train acc: 0.9528\n",
      "loss: 0.6162533730268478, train acc: 0.952\n",
      "loss: 0.6478327929973602, train acc: 0.9532\n",
      "loss: 0.584761843085289, train acc: 0.9525\n",
      "loss: 0.6865605294704438, train acc: 0.9527\n",
      "loss: 0.6195007264614105, train acc: 0.9534\n",
      "loss: 0.5678113549947739, train acc: 0.9509\n",
      "loss: 0.5597819983959198, train acc: 0.953\n",
      "epoch: 53, loss: 0.30381450057029724, train acc: 0.953, test acc: 0.9087\n",
      "loss: 0.797923743724823, train acc: 0.9529\n",
      "loss: 0.6267561197280884, train acc: 0.9502\n",
      "loss: 0.6334661334753037, train acc: 0.953\n",
      "loss: 0.6470836043357849, train acc: 0.9527\n",
      "loss: 0.6577020645141601, train acc: 0.9517\n",
      "loss: 0.5667822033166885, train acc: 0.952\n",
      "loss: 0.6059645295143128, train acc: 0.9529\n",
      "loss: 0.5825133860111237, train acc: 0.9524\n",
      "epoch: 54, loss: 0.3414793312549591, train acc: 0.9524, test acc: 0.9081\n",
      "loss: 0.7757839560508728, train acc: 0.9517\n",
      "loss: 0.6226081550121307, train acc: 0.9529\n",
      "loss: 0.6115230947732926, train acc: 0.9497\n",
      "loss: 0.6019057929515839, train acc: 0.9538\n",
      "loss: 0.5890763401985168, train acc: 0.9526\n",
      "loss: 0.6288739085197449, train acc: 0.9486\n",
      "loss: 0.6292003512382507, train acc: 0.952\n",
      "loss: 0.5918328046798706, train acc: 0.9528\n",
      "epoch: 55, loss: 0.1458940953016281, train acc: 0.9528, test acc: 0.9072\n",
      "loss: 0.7934582829475403, train acc: 0.9522\n",
      "loss: 0.5706115573644638, train acc: 0.9528\n",
      "loss: 0.6006929278373718, train acc: 0.9536\n",
      "loss: 0.5829871863126754, train acc: 0.9547\n",
      "loss: 0.6452149152755737, train acc: 0.9496\n",
      "loss: 0.5979141235351563, train acc: 0.9529\n",
      "loss: 0.585661667585373, train acc: 0.952\n",
      "loss: 0.5763910800218582, train acc: 0.9527\n",
      "epoch: 56, loss: 0.4456162750720978, train acc: 0.9527, test acc: 0.9093\n",
      "loss: 0.7967261672019958, train acc: 0.9503\n",
      "loss: 0.6233629643917084, train acc: 0.9521\n",
      "loss: 0.6350973904132843, train acc: 0.9506\n",
      "loss: 0.6158805280923844, train acc: 0.9537\n",
      "loss: 0.6350933581590652, train acc: 0.9528\n",
      "loss: 0.6337001979351043, train acc: 0.9518\n",
      "loss: 0.5876138150691986, train acc: 0.9516\n",
      "loss: 0.5678061872720719, train acc: 0.951\n",
      "epoch: 57, loss: 0.543576180934906, train acc: 0.951, test acc: 0.9079\n",
      "loss: 0.8188135623931885, train acc: 0.9533\n",
      "loss: 0.6388694405555725, train acc: 0.9514\n",
      "loss: 0.6671835720539093, train acc: 0.9503\n",
      "loss: 0.6168480038642883, train acc: 0.9518\n",
      "loss: 0.6428787171840668, train acc: 0.9525\n",
      "loss: 0.5994650036096573, train acc: 0.9524\n",
      "loss: 0.6127289175987244, train acc: 0.9526\n",
      "loss: 0.5772166907787323, train acc: 0.952\n",
      "epoch: 58, loss: 0.26508426666259766, train acc: 0.952, test acc: 0.909\n",
      "loss: 0.8613886833190918, train acc: 0.9524\n",
      "loss: 0.6269181847572327, train acc: 0.9515\n",
      "loss: 0.6562348306179047, train acc: 0.9496\n",
      "loss: 0.5910616427659988, train acc: 0.952\n",
      "loss: 0.6350167453289032, train acc: 0.9535\n",
      "loss: 0.5871353894472122, train acc: 0.9525\n",
      "loss: 0.6127135038375855, train acc: 0.9531\n",
      "loss: 0.5719403713941574, train acc: 0.9522\n",
      "epoch: 59, loss: 0.34275397658348083, train acc: 0.9522, test acc: 0.9052\n",
      "loss: 0.8873023986816406, train acc: 0.95\n",
      "loss: 0.626524668931961, train acc: 0.9513\n",
      "loss: 0.6791344225406647, train acc: 0.951\n",
      "loss: 0.5717911630868912, train acc: 0.9523\n",
      "loss: 0.613393384218216, train acc: 0.9523\n",
      "loss: 0.5931223422288895, train acc: 0.9512\n",
      "loss: 0.6099270939826965, train acc: 0.9521\n",
      "loss: 0.5524050623178482, train acc: 0.9531\n",
      "epoch: 60, loss: 0.23924165964126587, train acc: 0.9531, test acc: 0.9062\n",
      "loss: 0.7936756014823914, train acc: 0.9529\n",
      "loss: 0.59539754986763, train acc: 0.9536\n",
      "loss: 0.6416784584522247, train acc: 0.9528\n",
      "loss: 0.5675740897655487, train acc: 0.9553\n",
      "loss: 0.6060790896415711, train acc: 0.9533\n",
      "loss: 0.5469990819692612, train acc: 0.9536\n",
      "loss: 0.5686611503362655, train acc: 0.9529\n",
      "loss: 0.5738141417503357, train acc: 0.953\n",
      "epoch: 61, loss: 0.6777937412261963, train acc: 0.953, test acc: 0.9071\n",
      "loss: 0.7808067798614502, train acc: 0.9521\n",
      "loss: 0.617093950510025, train acc: 0.9522\n",
      "loss: 0.6551351606845855, train acc: 0.9536\n",
      "loss: 0.6134906798601151, train acc: 0.9541\n",
      "loss: 0.6174128711223602, train acc: 0.9532\n",
      "loss: 0.6052056401968002, train acc: 0.9507\n",
      "loss: 0.6114228725433349, train acc: 0.9527\n",
      "loss: 0.586118221282959, train acc: 0.954\n",
      "epoch: 62, loss: 0.277093768119812, train acc: 0.954, test acc: 0.9067\n",
      "loss: 0.8028868436813354, train acc: 0.955\n",
      "loss: 0.6022156149148941, train acc: 0.9556\n",
      "loss: 0.6362243890762329, train acc: 0.9543\n",
      "loss: 0.6146033585071564, train acc: 0.9544\n",
      "loss: 0.6494072079658508, train acc: 0.9529\n",
      "loss: 0.598285335302353, train acc: 0.9522\n",
      "loss: 0.5870790868997574, train acc: 0.9554\n",
      "loss: 0.5975094705820083, train acc: 0.9554\n",
      "epoch: 63, loss: 0.2416227012872696, train acc: 0.9554, test acc: 0.9093\n",
      "loss: 0.864672839641571, train acc: 0.953\n",
      "loss: 0.6221305012702942, train acc: 0.9536\n",
      "loss: 0.6167848467826843, train acc: 0.9536\n",
      "loss: 0.6191122710704804, train acc: 0.9564\n",
      "loss: 0.5677357316017151, train acc: 0.954\n",
      "loss: 0.5495265305042267, train acc: 0.9546\n",
      "loss: 0.5918228447437286, train acc: 0.9548\n",
      "loss: 0.5665932476520539, train acc: 0.956\n",
      "epoch: 64, loss: 0.42137590050697327, train acc: 0.956, test acc: 0.9083\n",
      "loss: 0.7511942982673645, train acc: 0.9548\n",
      "loss: 0.6344752877950668, train acc: 0.9513\n",
      "loss: 0.6651224017143249, train acc: 0.9525\n",
      "loss: 0.6194759607315063, train acc: 0.954\n",
      "loss: 0.5926525115966796, train acc: 0.9528\n",
      "loss: 0.6022132098674774, train acc: 0.9549\n",
      "loss: 0.5813122987747192, train acc: 0.9557\n",
      "loss: 0.5766189217567443, train acc: 0.9548\n",
      "epoch: 65, loss: 0.37943539023399353, train acc: 0.9548, test acc: 0.9081\n",
      "loss: 0.878332257270813, train acc: 0.9548\n",
      "loss: 0.6328290283679963, train acc: 0.9553\n",
      "loss: 0.6367409765720368, train acc: 0.9546\n",
      "loss: 0.5789205431938171, train acc: 0.9562\n",
      "loss: 0.5976773381233216, train acc: 0.9552\n",
      "loss: 0.5901029944419861, train acc: 0.9559\n",
      "loss: 0.5608329474925995, train acc: 0.9552\n",
      "loss: 0.5732547372579575, train acc: 0.9556\n",
      "epoch: 66, loss: 0.45463407039642334, train acc: 0.9556, test acc: 0.9086\n",
      "loss: 0.8442090153694153, train acc: 0.9539\n",
      "loss: 0.6348501861095428, train acc: 0.9544\n",
      "loss: 0.5698361456394195, train acc: 0.9544\n",
      "loss: 0.6117184817790985, train acc: 0.9549\n",
      "loss: 0.6214523494243622, train acc: 0.956\n",
      "loss: 0.5650697201490402, train acc: 0.9549\n",
      "loss: 0.582612544298172, train acc: 0.9543\n",
      "loss: 0.6005804985761642, train acc: 0.9566\n",
      "epoch: 67, loss: 0.20155195891857147, train acc: 0.9566, test acc: 0.9075\n",
      "loss: 0.7681407928466797, train acc: 0.9548\n",
      "loss: 0.6007320612668992, train acc: 0.9558\n",
      "loss: 0.6214784860610962, train acc: 0.9545\n",
      "loss: 0.617343419790268, train acc: 0.9552\n",
      "loss: 0.6403207182884216, train acc: 0.9563\n",
      "loss: 0.569850766658783, train acc: 0.9558\n",
      "loss: 0.5872050374746323, train acc: 0.9547\n",
      "loss: 0.5532491385936738, train acc: 0.9544\n",
      "epoch: 68, loss: 0.3605760931968689, train acc: 0.9544, test acc: 0.9072\n",
      "loss: 0.8292511105537415, train acc: 0.9546\n",
      "loss: 0.6277509957551957, train acc: 0.955\n",
      "loss: 0.6325194597244262, train acc: 0.9538\n",
      "loss: 0.5776385337114334, train acc: 0.9574\n",
      "loss: 0.636725252866745, train acc: 0.9537\n",
      "loss: 0.5555518090724945, train acc: 0.955\n",
      "loss: 0.5955858618021012, train acc: 0.9537\n",
      "loss: 0.5466995447874069, train acc: 0.9566\n",
      "epoch: 69, loss: 0.3686050772666931, train acc: 0.9566, test acc: 0.9062\n",
      "loss: 0.8747801184654236, train acc: 0.955\n",
      "loss: 0.5978940606117249, train acc: 0.9558\n",
      "loss: 0.6360435217618943, train acc: 0.9538\n",
      "loss: 0.6019916206598281, train acc: 0.9567\n",
      "loss: 0.5898384094238281, train acc: 0.956\n",
      "loss: 0.5682217091321945, train acc: 0.9546\n",
      "loss: 0.6224406242370606, train acc: 0.9561\n",
      "loss: 0.5799010455608368, train acc: 0.9533\n",
      "epoch: 70, loss: 0.7090142369270325, train acc: 0.9533, test acc: 0.9057\n",
      "loss: 0.8718491196632385, train acc: 0.9561\n",
      "loss: 0.641836941242218, train acc: 0.9522\n",
      "loss: 0.6209446102380752, train acc: 0.9535\n",
      "loss: 0.5707815259695053, train acc: 0.956\n",
      "loss: 0.602583134174347, train acc: 0.9557\n",
      "loss: 0.5756563514471054, train acc: 0.9567\n",
      "loss: 0.6408499419689179, train acc: 0.9557\n",
      "loss: 0.5610953807830811, train acc: 0.9582\n",
      "epoch: 71, loss: 0.20218448340892792, train acc: 0.9582, test acc: 0.9086\n",
      "loss: 0.7687821984291077, train acc: 0.9553\n",
      "loss: 0.5625074863433838, train acc: 0.9553\n",
      "loss: 0.6448588132858276, train acc: 0.9557\n",
      "loss: 0.6310061514377594, train acc: 0.9569\n",
      "loss: 0.6264490842819214, train acc: 0.9563\n",
      "loss: 0.5945127785205842, train acc: 0.9542\n",
      "loss: 0.5851777613162994, train acc: 0.956\n",
      "loss: 0.569293662905693, train acc: 0.9554\n",
      "epoch: 72, loss: 0.46530452370643616, train acc: 0.9554, test acc: 0.9067\n",
      "loss: 0.7345841526985168, train acc: 0.9565\n",
      "loss: 0.6055354863405228, train acc: 0.9549\n",
      "loss: 0.6330459952354431, train acc: 0.9564\n",
      "loss: 0.6049878835678101, train acc: 0.9566\n",
      "loss: 0.5981381356716156, train acc: 0.9553\n",
      "loss: 0.5513822585344315, train acc: 0.9546\n",
      "loss: 0.5957895994186402, train acc: 0.9553\n",
      "loss: 0.5552913069725036, train acc: 0.9551\n",
      "epoch: 73, loss: 0.27718159556388855, train acc: 0.9551, test acc: 0.9063\n",
      "loss: 0.911064088344574, train acc: 0.9532\n",
      "loss: 0.618364155292511, train acc: 0.955\n",
      "loss: 0.5953033655881882, train acc: 0.9536\n",
      "loss: 0.6105858743190765, train acc: 0.9581\n",
      "loss: 0.6247267961502075, train acc: 0.9572\n",
      "loss: 0.5727298051118851, train acc: 0.9557\n",
      "loss: 0.5772634208202362, train acc: 0.9572\n",
      "loss: 0.5568026810884475, train acc: 0.9571\n",
      "epoch: 74, loss: 0.3550415337085724, train acc: 0.9571, test acc: 0.9079\n",
      "loss: 0.7592989802360535, train acc: 0.9557\n",
      "loss: 0.6334100186824798, train acc: 0.9568\n",
      "loss: 0.6225707560777665, train acc: 0.9568\n",
      "loss: 0.5710470706224442, train acc: 0.9582\n",
      "loss: 0.5762324750423431, train acc: 0.9577\n",
      "loss: 0.5866793096065521, train acc: 0.9574\n",
      "loss: 0.5872633844614029, train acc: 0.9568\n",
      "loss: 0.5337108194828033, train acc: 0.9572\n",
      "epoch: 75, loss: 0.5320168733596802, train acc: 0.9572, test acc: 0.908\n",
      "loss: 0.8865336775779724, train acc: 0.9582\n",
      "loss: 0.5937942326068878, train acc: 0.956\n",
      "loss: 0.5892558544874191, train acc: 0.9568\n",
      "loss: 0.549174576997757, train acc: 0.9579\n",
      "loss: 0.6289155721664429, train acc: 0.9553\n",
      "loss: 0.585104015469551, train acc: 0.9564\n",
      "loss: 0.564529013633728, train acc: 0.9563\n",
      "loss: 0.5644617855548859, train acc: 0.9581\n",
      "epoch: 76, loss: 0.5103686451911926, train acc: 0.9581, test acc: 0.9063\n",
      "loss: 0.8292539119720459, train acc: 0.9557\n",
      "loss: 0.5981679201126099, train acc: 0.957\n",
      "loss: 0.6474839836359024, train acc: 0.9567\n",
      "loss: 0.5942254155874253, train acc: 0.957\n",
      "loss: 0.5931791841983796, train acc: 0.9577\n",
      "loss: 0.5586959272623062, train acc: 0.955\n",
      "loss: 0.5966635465621948, train acc: 0.9569\n",
      "loss: 0.5589396357536316, train acc: 0.9578\n",
      "epoch: 77, loss: 0.303210586309433, train acc: 0.9578, test acc: 0.9041\n",
      "loss: 0.9947470426559448, train acc: 0.9567\n",
      "loss: 0.6184745132923126, train acc: 0.9573\n",
      "loss: 0.6197034806013108, train acc: 0.9567\n",
      "loss: 0.5783198654651642, train acc: 0.9567\n",
      "loss: 0.6051324248313904, train acc: 0.957\n",
      "loss: 0.575473690032959, train acc: 0.9565\n",
      "loss: 0.5457191914319992, train acc: 0.9549\n",
      "loss: 0.5945794880390167, train acc: 0.9556\n",
      "epoch: 78, loss: 0.5557893514633179, train acc: 0.9556, test acc: 0.904\n",
      "loss: 0.8161762952804565, train acc: 0.9555\n",
      "loss: 0.5831965237855912, train acc: 0.9563\n",
      "loss: 0.6426998198032379, train acc: 0.9562\n",
      "loss: 0.6065766096115113, train acc: 0.9559\n",
      "loss: 0.6066792368888855, train acc: 0.9572\n",
      "loss: 0.5898208528757095, train acc: 0.9582\n",
      "loss: 0.6013756185770035, train acc: 0.957\n",
      "loss: 0.4968698680400848, train acc: 0.9593\n",
      "epoch: 79, loss: 0.3198954164981842, train acc: 0.9593, test acc: 0.9061\n",
      "#####training and testing end with K:20, P:0.5######\n",
      "#####training and testing start with K:20, P:1######\n",
      "loss: 2.3835740089416504, train acc: 0.1041\n",
      "loss: 2.138881540298462, train acc: 0.3989\n",
      "loss: 1.7012645483016968, train acc: 0.5873\n",
      "loss: 1.3185495376586913, train acc: 0.6935\n",
      "loss: 1.0339166164398192, train acc: 0.7472\n",
      "loss: 0.8790374875068665, train acc: 0.8234\n",
      "loss: 0.7089549720287323, train acc: 0.8434\n",
      "loss: 0.6054882645606995, train acc: 0.8636\n",
      "epoch: 0, loss: 0.4792744815349579, train acc: 0.8636, test acc: 0.8693\n",
      "loss: 0.42653071880340576, train acc: 0.8749\n",
      "loss: 0.4874031007289886, train acc: 0.8765\n",
      "loss: 0.49556330442428587, train acc: 0.8855\n",
      "loss: 0.47937263548374176, train acc: 0.8889\n",
      "loss: 0.43174706399440765, train acc: 0.8917\n",
      "loss: 0.4318405270576477, train acc: 0.893\n",
      "loss: 0.4022780030965805, train acc: 0.8936\n",
      "loss: 0.36550458073616027, train acc: 0.9032\n",
      "epoch: 1, loss: 0.19462022185325623, train acc: 0.9032, test acc: 0.896\n",
      "loss: 0.28256046772003174, train acc: 0.907\n",
      "loss: 0.3387322723865509, train acc: 0.9042\n",
      "loss: 0.36372735500335696, train acc: 0.9089\n",
      "loss: 0.3741752773523331, train acc: 0.9093\n",
      "loss: 0.33459561616182326, train acc: 0.9117\n",
      "loss: 0.3424422860145569, train acc: 0.9132\n",
      "loss: 0.33305283188819884, train acc: 0.9108\n",
      "loss: 0.29791889190673826, train acc: 0.9193\n",
      "epoch: 2, loss: 0.10627727955579758, train acc: 0.9193, test acc: 0.9056\n",
      "loss: 0.23646287620067596, train acc: 0.923\n",
      "loss: 0.2839893251657486, train acc: 0.9171\n",
      "loss: 0.3077685222029686, train acc: 0.9223\n",
      "loss: 0.32660229206085206, train acc: 0.9205\n",
      "loss: 0.29015120714902876, train acc: 0.9212\n",
      "loss: 0.2960324451327324, train acc: 0.9238\n",
      "loss: 0.2984466522932053, train acc: 0.9218\n",
      "loss: 0.26278032958507536, train acc: 0.9279\n",
      "epoch: 3, loss: 0.07717271149158478, train acc: 0.9279, test acc: 0.9106\n",
      "loss: 0.21163921058177948, train acc: 0.9321\n",
      "loss: 0.2519522562623024, train acc: 0.9248\n",
      "loss: 0.27446684390306475, train acc: 0.9289\n",
      "loss: 0.2946114644408226, train acc: 0.927\n",
      "loss: 0.26255144774913786, train acc: 0.9285\n",
      "loss: 0.2671828240156174, train acc: 0.9295\n",
      "loss: 0.275787490606308, train acc: 0.9305\n",
      "loss: 0.23996707648038865, train acc: 0.9329\n",
      "epoch: 4, loss: 0.06195088475942612, train acc: 0.9329, test acc: 0.9144\n",
      "loss: 0.19396735727787018, train acc: 0.9366\n",
      "loss: 0.2298315465450287, train acc: 0.9311\n",
      "loss: 0.2509063631296158, train acc: 0.9331\n",
      "loss: 0.2710904225707054, train acc: 0.9321\n",
      "loss: 0.24207987636327744, train acc: 0.9342\n",
      "loss: 0.24615902528166772, train acc: 0.9335\n",
      "loss: 0.25822790116071703, train acc: 0.9355\n",
      "loss: 0.22391934990882872, train acc: 0.9378\n",
      "epoch: 5, loss: 0.05122152343392372, train acc: 0.9378, test acc: 0.916\n",
      "loss: 0.17956314980983734, train acc: 0.9403\n",
      "loss: 0.21327797025442125, train acc: 0.9357\n",
      "loss: 0.2333369418978691, train acc: 0.9386\n",
      "loss: 0.2532853871583939, train acc: 0.9367\n",
      "loss: 0.2263132870197296, train acc: 0.9375\n",
      "loss: 0.2297177106142044, train acc: 0.9365\n",
      "loss: 0.24440327733755113, train acc: 0.9387\n",
      "loss: 0.21091482192277908, train acc: 0.9406\n",
      "epoch: 6, loss: 0.04340824484825134, train acc: 0.9406, test acc: 0.9173\n",
      "loss: 0.1676628738641739, train acc: 0.9435\n",
      "loss: 0.19986958503723146, train acc: 0.9394\n",
      "loss: 0.21974851787090302, train acc: 0.9426\n",
      "loss: 0.2377815455198288, train acc: 0.9408\n",
      "loss: 0.21315009742975236, train acc: 0.9409\n",
      "loss: 0.21569220200181008, train acc: 0.9413\n",
      "loss: 0.2317919686436653, train acc: 0.942\n",
      "loss: 0.1993429571390152, train acc: 0.945\n",
      "epoch: 7, loss: 0.03729648143053055, train acc: 0.945, test acc: 0.918\n",
      "loss: 0.15720698237419128, train acc: 0.9463\n",
      "loss: 0.1879362314939499, train acc: 0.9416\n",
      "loss: 0.20840175747871398, train acc: 0.9453\n",
      "loss: 0.22434450984001159, train acc: 0.9423\n",
      "loss: 0.2021249495446682, train acc: 0.9445\n",
      "loss: 0.20383183807134628, train acc: 0.9435\n",
      "loss: 0.22146429419517516, train acc: 0.9443\n",
      "loss: 0.18983468562364578, train acc: 0.9473\n",
      "epoch: 8, loss: 0.033754996955394745, train acc: 0.9473, test acc: 0.9178\n",
      "loss: 0.14994105696678162, train acc: 0.9492\n",
      "loss: 0.17803369462490082, train acc: 0.9443\n",
      "loss: 0.19780652821063996, train acc: 0.9467\n",
      "loss: 0.21203683614730834, train acc: 0.9453\n",
      "loss: 0.19183622896671296, train acc: 0.9475\n",
      "loss: 0.192345529794693, train acc: 0.9462\n",
      "loss: 0.21217896044254303, train acc: 0.948\n",
      "loss: 0.18068820238113403, train acc: 0.9496\n",
      "epoch: 9, loss: 0.029444288462400436, train acc: 0.9496, test acc: 0.9193\n",
      "loss: 0.14238782227039337, train acc: 0.9515\n",
      "loss: 0.1688225358724594, train acc: 0.948\n",
      "loss: 0.18833670467138292, train acc: 0.9492\n",
      "loss: 0.20133812949061394, train acc: 0.9481\n",
      "loss: 0.18222097381949426, train acc: 0.9501\n",
      "loss: 0.18216374665498733, train acc: 0.9487\n",
      "loss: 0.2034808337688446, train acc: 0.9505\n",
      "loss: 0.17232338935136796, train acc: 0.9518\n",
      "epoch: 10, loss: 0.026641251519322395, train acc: 0.9518, test acc: 0.9211\n",
      "loss: 0.1357116848230362, train acc: 0.9538\n",
      "loss: 0.1607564814388752, train acc: 0.9515\n",
      "loss: 0.1792238973081112, train acc: 0.9509\n",
      "loss: 0.19209477975964545, train acc: 0.9492\n",
      "loss: 0.17368854582309723, train acc: 0.9525\n",
      "loss: 0.17358034774661063, train acc: 0.9508\n",
      "loss: 0.19594691395759584, train acc: 0.9526\n",
      "loss: 0.1644354358315468, train acc: 0.9548\n",
      "epoch: 11, loss: 0.024830570444464684, train acc: 0.9548, test acc: 0.9209\n",
      "loss: 0.12913191318511963, train acc: 0.9565\n",
      "loss: 0.153049723058939, train acc: 0.954\n",
      "loss: 0.17147637233138086, train acc: 0.953\n",
      "loss: 0.18354267701506616, train acc: 0.9517\n",
      "loss: 0.16569649949669837, train acc: 0.9549\n",
      "loss: 0.1657080203294754, train acc: 0.9528\n",
      "loss: 0.18897702991962434, train acc: 0.9549\n",
      "loss: 0.15766413435339927, train acc: 0.9568\n",
      "epoch: 12, loss: 0.022879287600517273, train acc: 0.9568, test acc: 0.9222\n",
      "loss: 0.1234045997262001, train acc: 0.9581\n",
      "loss: 0.14569490849971772, train acc: 0.9558\n",
      "loss: 0.163692469894886, train acc: 0.9563\n",
      "loss: 0.1753934681415558, train acc: 0.9545\n",
      "loss: 0.15808635875582694, train acc: 0.9565\n",
      "loss: 0.15831380859017372, train acc: 0.9544\n",
      "loss: 0.18230041563510896, train acc: 0.9571\n",
      "loss: 0.15077416747808456, train acc: 0.9577\n",
      "epoch: 13, loss: 0.021670173853635788, train acc: 0.9577, test acc: 0.9226\n",
      "loss: 0.1172204539179802, train acc: 0.9596\n",
      "loss: 0.13958250358700752, train acc: 0.9589\n",
      "loss: 0.15703089237213136, train acc: 0.958\n",
      "loss: 0.1675848953425884, train acc: 0.9553\n",
      "loss: 0.15156791880726814, train acc: 0.958\n",
      "loss: 0.15148221738636494, train acc: 0.9562\n",
      "loss: 0.17629688531160354, train acc: 0.9601\n",
      "loss: 0.14464467540383338, train acc: 0.9605\n",
      "epoch: 14, loss: 0.019695045426487923, train acc: 0.9605, test acc: 0.9233\n",
      "loss: 0.11147549748420715, train acc: 0.9616\n",
      "loss: 0.13310430347919464, train acc: 0.9611\n",
      "loss: 0.15057276636362077, train acc: 0.9599\n",
      "loss: 0.16000936999917031, train acc: 0.9579\n",
      "loss: 0.14501348435878753, train acc: 0.9591\n",
      "loss: 0.14481025822460653, train acc: 0.9571\n",
      "loss: 0.17010383903980256, train acc: 0.9624\n",
      "loss: 0.13809554949402808, train acc: 0.9625\n",
      "epoch: 15, loss: 0.018205242231488228, train acc: 0.9625, test acc: 0.9228\n",
      "loss: 0.10654833912849426, train acc: 0.9629\n",
      "loss: 0.12786922305822374, train acc: 0.9624\n",
      "loss: 0.14495130851864815, train acc: 0.9618\n",
      "loss: 0.1528906248509884, train acc: 0.9598\n",
      "loss: 0.13952732011675834, train acc: 0.9611\n",
      "loss: 0.13909438848495484, train acc: 0.9589\n",
      "loss: 0.16401179283857345, train acc: 0.9637\n",
      "loss: 0.13135283216834068, train acc: 0.9648\n",
      "epoch: 16, loss: 0.016610881313681602, train acc: 0.9648, test acc: 0.9233\n",
      "loss: 0.10165537148714066, train acc: 0.9646\n",
      "loss: 0.12255936712026597, train acc: 0.964\n",
      "loss: 0.13937364816665648, train acc: 0.9637\n",
      "loss: 0.14655468091368676, train acc: 0.9608\n",
      "loss: 0.1339258186519146, train acc: 0.963\n",
      "loss: 0.1334072094410658, train acc: 0.9598\n",
      "loss: 0.15869984403252602, train acc: 0.965\n",
      "loss: 0.12657236009836198, train acc: 0.9656\n",
      "epoch: 17, loss: 0.015394169837236404, train acc: 0.9656, test acc: 0.923\n",
      "loss: 0.09676968306303024, train acc: 0.9657\n",
      "loss: 0.11778828725218773, train acc: 0.965\n",
      "loss: 0.13445224985480309, train acc: 0.9655\n",
      "loss: 0.14015352129936218, train acc: 0.9617\n",
      "loss: 0.1292091004550457, train acc: 0.9648\n",
      "loss: 0.1286838885396719, train acc: 0.9619\n",
      "loss: 0.1533572793006897, train acc: 0.9669\n",
      "loss: 0.12089594453573227, train acc: 0.9671\n",
      "epoch: 18, loss: 0.014122470282018185, train acc: 0.9671, test acc: 0.9231\n",
      "loss: 0.0933167040348053, train acc: 0.9677\n",
      "loss: 0.11339110359549523, train acc: 0.9666\n",
      "loss: 0.12985222935676574, train acc: 0.9661\n",
      "loss: 0.1342990506440401, train acc: 0.9634\n",
      "loss: 0.12417529225349426, train acc: 0.966\n",
      "loss: 0.12377145029604435, train acc: 0.9635\n",
      "loss: 0.14787859097123146, train acc: 0.9684\n",
      "loss: 0.11635489985346795, train acc: 0.9684\n",
      "epoch: 19, loss: 0.013034095987677574, train acc: 0.9684, test acc: 0.9235\n",
      "loss: 0.08992671966552734, train acc: 0.9683\n",
      "loss: 0.1087553545832634, train acc: 0.9678\n",
      "loss: 0.125300320237875, train acc: 0.9685\n",
      "loss: 0.12898416593670844, train acc: 0.9652\n",
      "loss: 0.1197535190731287, train acc: 0.9672\n",
      "loss: 0.11916833817958832, train acc: 0.9647\n",
      "loss: 0.14264630228281022, train acc: 0.9693\n",
      "loss: 0.1106674239039421, train acc: 0.9702\n",
      "epoch: 20, loss: 0.012694930657744408, train acc: 0.9702, test acc: 0.9235\n",
      "loss: 0.0868813619017601, train acc: 0.9698\n",
      "loss: 0.1050430491566658, train acc: 0.9689\n",
      "loss: 0.12062039151787758, train acc: 0.9691\n",
      "loss: 0.12338736802339553, train acc: 0.9672\n",
      "loss: 0.11480319611728192, train acc: 0.9687\n",
      "loss: 0.1145511120557785, train acc: 0.966\n",
      "loss: 0.1373578391969204, train acc: 0.9708\n",
      "loss: 0.10598050132393837, train acc: 0.9711\n",
      "epoch: 21, loss: 0.011834859848022461, train acc: 0.9711, test acc: 0.9232\n",
      "loss: 0.0840703696012497, train acc: 0.971\n",
      "loss: 0.10156484991312027, train acc: 0.9702\n",
      "loss: 0.1163004118949175, train acc: 0.9708\n",
      "loss: 0.11831324845552445, train acc: 0.9686\n",
      "loss: 0.11080823838710785, train acc: 0.9701\n",
      "loss: 0.11036293357610702, train acc: 0.968\n",
      "loss: 0.1327502593398094, train acc: 0.972\n",
      "loss: 0.10126205459237099, train acc: 0.9721\n",
      "epoch: 22, loss: 0.010830731131136417, train acc: 0.9721, test acc: 0.9234\n",
      "loss: 0.07967377454042435, train acc: 0.9715\n",
      "loss: 0.09772024154663086, train acc: 0.9713\n",
      "loss: 0.11238176710903644, train acc: 0.9711\n",
      "loss: 0.11324108056724072, train acc: 0.9695\n",
      "loss: 0.10635735541582107, train acc: 0.9707\n",
      "loss: 0.10624751597642898, train acc: 0.9694\n",
      "loss: 0.12833342477679252, train acc: 0.9735\n",
      "loss: 0.0961201824247837, train acc: 0.9731\n",
      "epoch: 23, loss: 0.010596662759780884, train acc: 0.9731, test acc: 0.9228\n",
      "loss: 0.07636094093322754, train acc: 0.9725\n",
      "loss: 0.09396117180585861, train acc: 0.9722\n",
      "loss: 0.10846745930612087, train acc: 0.9719\n",
      "loss: 0.10893049351871013, train acc: 0.9711\n",
      "loss: 0.10298944301903248, train acc: 0.9714\n",
      "loss: 0.10212895534932613, train acc: 0.9703\n",
      "loss: 0.12350812256336212, train acc: 0.9737\n",
      "loss: 0.09215208739042283, train acc: 0.9741\n",
      "epoch: 24, loss: 0.010305400937795639, train acc: 0.9741, test acc: 0.9239\n",
      "loss: 0.0747838169336319, train acc: 0.9731\n",
      "loss: 0.09066575393080711, train acc: 0.9736\n",
      "loss: 0.10474018976092339, train acc: 0.9734\n",
      "loss: 0.10397561006247998, train acc: 0.9722\n",
      "loss: 0.09891270361840725, train acc: 0.9726\n",
      "loss: 0.09844425357878209, train acc: 0.9714\n",
      "loss: 0.11918246671557427, train acc: 0.9763\n",
      "loss: 0.08783497884869576, train acc: 0.975\n",
      "epoch: 25, loss: 0.009316337294876575, train acc: 0.975, test acc: 0.9235\n",
      "loss: 0.07121213525533676, train acc: 0.9738\n",
      "loss: 0.08730718418955803, train acc: 0.9749\n",
      "loss: 0.1011391431093216, train acc: 0.9745\n",
      "loss: 0.09973447546362876, train acc: 0.9736\n",
      "loss: 0.09573515914380551, train acc: 0.9727\n",
      "loss: 0.09518571607768536, train acc: 0.9722\n",
      "loss: 0.1153646968305111, train acc: 0.9767\n",
      "loss: 0.08365865796804428, train acc: 0.976\n",
      "epoch: 26, loss: 0.00900463480502367, train acc: 0.976, test acc: 0.9233\n",
      "loss: 0.06871144473552704, train acc: 0.9747\n",
      "loss: 0.08381112515926362, train acc: 0.9762\n",
      "loss: 0.09803580157458783, train acc: 0.9757\n",
      "loss: 0.0957253783941269, train acc: 0.9745\n",
      "loss: 0.09261699430644513, train acc: 0.9736\n",
      "loss: 0.09193897396326065, train acc: 0.9727\n",
      "loss: 0.11206963062286376, train acc: 0.9768\n",
      "loss: 0.08020997606217861, train acc: 0.9769\n",
      "epoch: 27, loss: 0.00914590060710907, train acc: 0.9769, test acc: 0.9228\n",
      "loss: 0.06588667631149292, train acc: 0.9751\n",
      "loss: 0.08130774833261967, train acc: 0.9771\n",
      "loss: 0.09457221962511539, train acc: 0.9762\n",
      "loss: 0.09155171923339367, train acc: 0.9757\n",
      "loss: 0.08948457986116409, train acc: 0.974\n",
      "loss: 0.08876763507723809, train acc: 0.9737\n",
      "loss: 0.10802402198314667, train acc: 0.9778\n",
      "loss: 0.07664791531860829, train acc: 0.9783\n",
      "epoch: 28, loss: 0.008356799371540546, train acc: 0.9783, test acc: 0.9228\n",
      "loss: 0.06329931318759918, train acc: 0.9765\n",
      "loss: 0.07816821932792664, train acc: 0.9782\n",
      "loss: 0.09121295511722564, train acc: 0.9776\n",
      "loss: 0.08793030269443988, train acc: 0.9766\n",
      "loss: 0.08601363301277161, train acc: 0.9749\n",
      "loss: 0.08527614921331406, train acc: 0.9745\n",
      "loss: 0.10427814945578576, train acc: 0.9786\n",
      "loss: 0.07335928976535797, train acc: 0.9799\n",
      "epoch: 29, loss: 0.007837709970772266, train acc: 0.9799, test acc: 0.9226\n",
      "loss: 0.060921330004930496, train acc: 0.9776\n",
      "loss: 0.07549949176609516, train acc: 0.9798\n",
      "loss: 0.08815037831664085, train acc: 0.9786\n",
      "loss: 0.0841366458684206, train acc: 0.9775\n",
      "loss: 0.08343288749456405, train acc: 0.9756\n",
      "loss: 0.08253281451761722, train acc: 0.9753\n",
      "loss: 0.10114186108112336, train acc: 0.9801\n",
      "loss: 0.06983642727136612, train acc: 0.9812\n",
      "epoch: 30, loss: 0.0075147817842662334, train acc: 0.9812, test acc: 0.9228\n",
      "loss: 0.058734264224767685, train acc: 0.9782\n",
      "loss: 0.07279012128710746, train acc: 0.9806\n",
      "loss: 0.08567117527127266, train acc: 0.9792\n",
      "loss: 0.08059207983314991, train acc: 0.9778\n",
      "loss: 0.08112738393247128, train acc: 0.9763\n",
      "loss: 0.07942190542817115, train acc: 0.976\n",
      "loss: 0.09740125685930252, train acc: 0.9805\n",
      "loss: 0.06663906797766686, train acc: 0.9819\n",
      "epoch: 31, loss: 0.007017697673290968, train acc: 0.9819, test acc: 0.923\n",
      "loss: 0.05630296841263771, train acc: 0.9782\n",
      "loss: 0.07048606127500534, train acc: 0.9813\n",
      "loss: 0.08299256563186645, train acc: 0.9797\n",
      "loss: 0.07732797302305698, train acc: 0.9789\n",
      "loss: 0.0781975258141756, train acc: 0.9775\n",
      "loss: 0.07671475149691105, train acc: 0.9776\n",
      "loss: 0.09406681880354881, train acc: 0.9816\n",
      "loss: 0.06418682374060154, train acc: 0.9827\n",
      "epoch: 32, loss: 0.006800875533372164, train acc: 0.9827, test acc: 0.9226\n",
      "loss: 0.05346141755580902, train acc: 0.9797\n",
      "loss: 0.0680842012166977, train acc: 0.9819\n",
      "loss: 0.07996584810316562, train acc: 0.9803\n",
      "loss: 0.0743174534291029, train acc: 0.9789\n",
      "loss: 0.07556978315114975, train acc: 0.9774\n",
      "loss: 0.07365735881030559, train acc: 0.9786\n",
      "loss: 0.09194300174713135, train acc: 0.9825\n",
      "loss: 0.061803430691361426, train acc: 0.9831\n",
      "epoch: 33, loss: 0.006349546834826469, train acc: 0.9831, test acc: 0.9228\n",
      "loss: 0.05152250453829765, train acc: 0.9798\n",
      "loss: 0.0658756110817194, train acc: 0.9823\n",
      "loss: 0.07732139155268669, train acc: 0.981\n",
      "loss: 0.07122491002082824, train acc: 0.9802\n",
      "loss: 0.07339909970760346, train acc: 0.9775\n",
      "loss: 0.07126367762684822, train acc: 0.9788\n",
      "loss: 0.08799731843173504, train acc: 0.9837\n",
      "loss: 0.05855801776051521, train acc: 0.9838\n",
      "epoch: 34, loss: 0.006005565170198679, train acc: 0.9838, test acc: 0.9233\n",
      "loss: 0.04908234626054764, train acc: 0.9807\n",
      "loss: 0.06332776583731174, train acc: 0.9835\n",
      "loss: 0.07438668943941593, train acc: 0.9816\n",
      "loss: 0.06797595024108886, train acc: 0.9809\n",
      "loss: 0.07120620831847191, train acc: 0.9781\n",
      "loss: 0.06883829012513161, train acc: 0.9796\n",
      "loss: 0.08559260368347169, train acc: 0.984\n",
      "loss: 0.05599402710795402, train acc: 0.9847\n",
      "epoch: 35, loss: 0.005905944388359785, train acc: 0.9847, test acc: 0.9227\n",
      "loss: 0.04827585071325302, train acc: 0.9815\n",
      "loss: 0.06164894551038742, train acc: 0.9841\n",
      "loss: 0.07213361822068691, train acc: 0.9822\n",
      "loss: 0.0651662727817893, train acc: 0.9817\n",
      "loss: 0.06898984797298909, train acc: 0.9786\n",
      "loss: 0.06605268679559231, train acc: 0.9801\n",
      "loss: 0.08272576481103897, train acc: 0.9843\n",
      "loss: 0.05381307862699032, train acc: 0.9848\n",
      "epoch: 36, loss: 0.005699844565242529, train acc: 0.9848, test acc: 0.9218\n",
      "loss: 0.04642345383763313, train acc: 0.9823\n",
      "loss: 0.05944287218153477, train acc: 0.9844\n",
      "loss: 0.06971954517066478, train acc: 0.9827\n",
      "loss: 0.06254656575620174, train acc: 0.9826\n",
      "loss: 0.06616058647632599, train acc: 0.9791\n",
      "loss: 0.06317392215132714, train acc: 0.9809\n",
      "loss: 0.0792771641165018, train acc: 0.9852\n",
      "loss: 0.051134541630744934, train acc: 0.9859\n",
      "epoch: 37, loss: 0.0054022641852498055, train acc: 0.9859, test acc: 0.9214\n",
      "loss: 0.04503174498677254, train acc: 0.982\n",
      "loss: 0.05768302083015442, train acc: 0.9851\n",
      "loss: 0.0668522385880351, train acc: 0.9837\n",
      "loss: 0.06006596870720386, train acc: 0.9828\n",
      "loss: 0.06435304544866086, train acc: 0.9801\n",
      "loss: 0.060975033044815066, train acc: 0.9809\n",
      "loss: 0.07699057683348656, train acc: 0.9861\n",
      "loss: 0.049361510574817656, train acc: 0.9869\n",
      "epoch: 38, loss: 0.005615466274321079, train acc: 0.9869, test acc: 0.9216\n",
      "loss: 0.04351450502872467, train acc: 0.9829\n",
      "loss: 0.05538833178579807, train acc: 0.9859\n",
      "loss: 0.06488562654703856, train acc: 0.984\n",
      "loss: 0.0576485937461257, train acc: 0.9839\n",
      "loss: 0.061815021373331544, train acc: 0.9808\n",
      "loss: 0.05871984213590622, train acc: 0.982\n",
      "loss: 0.07385916076600552, train acc: 0.986\n",
      "loss: 0.04763079211115837, train acc: 0.9871\n",
      "epoch: 39, loss: 0.005183594301342964, train acc: 0.9871, test acc: 0.9206\n",
      "loss: 0.04126127064228058, train acc: 0.9832\n",
      "loss: 0.05412068702280522, train acc: 0.9862\n",
      "loss: 0.062960603274405, train acc: 0.9839\n",
      "loss: 0.05547942146658898, train acc: 0.9844\n",
      "loss: 0.059729805961251256, train acc: 0.9811\n",
      "loss: 0.055986050702631476, train acc: 0.9829\n",
      "loss: 0.07133027762174607, train acc: 0.9866\n",
      "loss: 0.045511622726917264, train acc: 0.9882\n",
      "epoch: 40, loss: 0.004764418583363295, train acc: 0.9882, test acc: 0.9203\n",
      "loss: 0.03974982723593712, train acc: 0.9837\n",
      "loss: 0.05238502211868763, train acc: 0.9872\n",
      "loss: 0.06065024491399527, train acc: 0.9848\n",
      "loss: 0.052651497162878515, train acc: 0.9855\n",
      "loss: 0.05717444084584713, train acc: 0.9813\n",
      "loss: 0.053889557532966134, train acc: 0.9838\n",
      "loss: 0.06852078922092915, train acc: 0.9868\n",
      "loss: 0.0431294459849596, train acc: 0.989\n",
      "epoch: 41, loss: 0.004700844641774893, train acc: 0.989, test acc: 0.9196\n",
      "loss: 0.03933599218726158, train acc: 0.9839\n",
      "loss: 0.050468966364860535, train acc: 0.9875\n",
      "loss: 0.05822176281362772, train acc: 0.9849\n",
      "loss: 0.050916419737040995, train acc: 0.9861\n",
      "loss: 0.055358692444860934, train acc: 0.9819\n",
      "loss: 0.05144843216985464, train acc: 0.9848\n",
      "loss: 0.06577557511627674, train acc: 0.9866\n",
      "loss: 0.04191151186823845, train acc: 0.9892\n",
      "epoch: 42, loss: 0.004240571521222591, train acc: 0.9892, test acc: 0.9201\n",
      "loss: 0.037478167563676834, train acc: 0.9849\n",
      "loss: 0.049090012349188326, train acc: 0.9885\n",
      "loss: 0.05702373292297125, train acc: 0.9848\n",
      "loss: 0.04788927771151066, train acc: 0.9868\n",
      "loss: 0.05317128133028746, train acc: 0.9826\n",
      "loss: 0.049414782971143725, train acc: 0.9854\n",
      "loss: 0.06305220946669579, train acc: 0.9872\n",
      "loss: 0.04005392380058766, train acc: 0.9899\n",
      "epoch: 43, loss: 0.00382392480969429, train acc: 0.9899, test acc: 0.9191\n",
      "loss: 0.03625982627272606, train acc: 0.9853\n",
      "loss: 0.04743525832891464, train acc: 0.9885\n",
      "loss: 0.055145645327866075, train acc: 0.9857\n",
      "loss: 0.04606243614107371, train acc: 0.987\n",
      "loss: 0.05069978758692741, train acc: 0.9832\n",
      "loss: 0.047381355054676534, train acc: 0.9867\n",
      "loss: 0.06070004925131798, train acc: 0.9879\n",
      "loss: 0.03861389588564634, train acc: 0.9906\n",
      "epoch: 44, loss: 0.003861915785819292, train acc: 0.9906, test acc: 0.9188\n",
      "loss: 0.035530947148799896, train acc: 0.9856\n",
      "loss: 0.04610373247414827, train acc: 0.9888\n",
      "loss: 0.05292812641710043, train acc: 0.9863\n",
      "loss: 0.04404453076422214, train acc: 0.9871\n",
      "loss: 0.048679828830063346, train acc: 0.9842\n",
      "loss: 0.04540220126509666, train acc: 0.987\n",
      "loss: 0.05792918615043163, train acc: 0.9884\n",
      "loss: 0.036855571903288364, train acc: 0.9912\n",
      "epoch: 45, loss: 0.00344645488075912, train acc: 0.9912, test acc: 0.9192\n",
      "loss: 0.03415583819150925, train acc: 0.9862\n",
      "loss: 0.04457776714116335, train acc: 0.99\n",
      "loss: 0.05108373295515776, train acc: 0.9871\n",
      "loss: 0.04245114419609308, train acc: 0.9874\n",
      "loss: 0.046785027533769605, train acc: 0.9844\n",
      "loss: 0.04409874621778727, train acc: 0.9877\n",
      "loss: 0.05591103322803974, train acc: 0.9879\n",
      "loss: 0.035288961604237556, train acc: 0.991\n",
      "epoch: 46, loss: 0.0032349838875234127, train acc: 0.991, test acc: 0.9183\n",
      "loss: 0.03334591165184975, train acc: 0.9878\n",
      "loss: 0.04317944329231978, train acc: 0.9909\n",
      "loss: 0.04914835914969444, train acc: 0.9877\n",
      "loss: 0.040410689823329446, train acc: 0.9878\n",
      "loss: 0.04481275416910648, train acc: 0.9847\n",
      "loss: 0.04220898952335119, train acc: 0.9891\n",
      "loss: 0.05322883799672127, train acc: 0.9887\n",
      "loss: 0.03393307384103537, train acc: 0.9915\n",
      "epoch: 47, loss: 0.0031515241134911776, train acc: 0.9915, test acc: 0.919\n",
      "loss: 0.03185950592160225, train acc: 0.988\n",
      "loss: 0.04143467061221599, train acc: 0.9911\n",
      "loss: 0.047356681898236275, train acc: 0.9881\n",
      "loss: 0.03894069399684667, train acc: 0.988\n",
      "loss: 0.04257872942835093, train acc: 0.9854\n",
      "loss: 0.040428893826901915, train acc: 0.9898\n",
      "loss: 0.05037054065614939, train acc: 0.9886\n",
      "loss: 0.03251838162541389, train acc: 0.9924\n",
      "epoch: 48, loss: 0.0027530938386917114, train acc: 0.9924, test acc: 0.9187\n",
      "loss: 0.031063079833984375, train acc: 0.9887\n",
      "loss: 0.04016837496310473, train acc: 0.9918\n",
      "loss: 0.04584396500140429, train acc: 0.9885\n",
      "loss: 0.037300674989819524, train acc: 0.9884\n",
      "loss: 0.040949021466076374, train acc: 0.9863\n",
      "loss: 0.03874513264745474, train acc: 0.9902\n",
      "loss: 0.048351625353097914, train acc: 0.9885\n",
      "loss: 0.031224060244858264, train acc: 0.9919\n",
      "epoch: 49, loss: 0.0027013178914785385, train acc: 0.9919, test acc: 0.9179\n",
      "loss: 0.029315626248717308, train acc: 0.99\n",
      "loss: 0.03916074987500906, train acc: 0.9921\n",
      "loss: 0.044386987760663035, train acc: 0.9883\n",
      "loss: 0.03550324328243733, train acc: 0.9887\n",
      "loss: 0.039241984486579895, train acc: 0.9863\n",
      "loss: 0.037517258711159227, train acc: 0.9906\n",
      "loss: 0.047077024914324285, train acc: 0.9886\n",
      "loss: 0.030168039724230766, train acc: 0.9922\n",
      "epoch: 50, loss: 0.002452597953379154, train acc: 0.9922, test acc: 0.919\n",
      "loss: 0.028835248202085495, train acc: 0.9899\n",
      "loss: 0.037540207616984844, train acc: 0.9924\n",
      "loss: 0.04236464463174343, train acc: 0.9889\n",
      "loss: 0.034029011707752944, train acc: 0.9893\n",
      "loss: 0.03689647652208805, train acc: 0.9871\n",
      "loss: 0.03581759911030531, train acc: 0.9911\n",
      "loss: 0.04420309159904719, train acc: 0.9888\n",
      "loss: 0.029275836236774922, train acc: 0.9931\n",
      "epoch: 51, loss: 0.00239012879319489, train acc: 0.9931, test acc: 0.9194\n",
      "loss: 0.028515730053186417, train acc: 0.9906\n",
      "loss: 0.03643598705530167, train acc: 0.9935\n",
      "loss: 0.04161571469157934, train acc: 0.9892\n",
      "loss: 0.03286234140396118, train acc: 0.9893\n",
      "loss: 0.03569168709218502, train acc: 0.9881\n",
      "loss: 0.034251532331109044, train acc: 0.9912\n",
      "loss: 0.04341907817870379, train acc: 0.9889\n",
      "loss: 0.028108908608555794, train acc: 0.993\n",
      "epoch: 52, loss: 0.0021816445514559746, train acc: 0.993, test acc: 0.9192\n",
      "loss: 0.027816446498036385, train acc: 0.9906\n",
      "loss: 0.03516610711812973, train acc: 0.9938\n",
      "loss: 0.039003705605864525, train acc: 0.99\n",
      "loss: 0.031432459596544504, train acc: 0.9897\n",
      "loss: 0.03370646983385086, train acc: 0.9881\n",
      "loss: 0.03321526758372784, train acc: 0.9922\n",
      "loss: 0.04115260951220989, train acc: 0.989\n",
      "loss: 0.02746316362172365, train acc: 0.9935\n",
      "epoch: 53, loss: 0.0022131826262921095, train acc: 0.9935, test acc: 0.9197\n",
      "loss: 0.025541480630636215, train acc: 0.9912\n",
      "loss: 0.033546707965433596, train acc: 0.9941\n",
      "loss: 0.037811645306646824, train acc: 0.9906\n",
      "loss: 0.03013125592842698, train acc: 0.9898\n",
      "loss: 0.032250435277819636, train acc: 0.9894\n",
      "loss: 0.03199210818856955, train acc: 0.9928\n",
      "loss: 0.038810992054641245, train acc: 0.9897\n",
      "loss: 0.02610308937728405, train acc: 0.994\n",
      "epoch: 54, loss: 0.002020701766014099, train acc: 0.994, test acc: 0.9194\n",
      "loss: 0.02512526884675026, train acc: 0.9923\n",
      "loss: 0.03230289742350578, train acc: 0.9939\n",
      "loss: 0.0366450360044837, train acc: 0.9907\n",
      "loss: 0.029125458374619485, train acc: 0.9901\n",
      "loss: 0.030474895052611826, train acc: 0.9894\n",
      "loss: 0.03069141525775194, train acc: 0.9933\n",
      "loss: 0.03703506197780371, train acc: 0.9896\n",
      "loss: 0.024896327033638956, train acc: 0.9935\n",
      "epoch: 55, loss: 0.0018062988528981805, train acc: 0.9935, test acc: 0.9204\n",
      "loss: 0.02426532283425331, train acc: 0.9922\n",
      "loss: 0.031201278790831566, train acc: 0.994\n",
      "loss: 0.0350460035726428, train acc: 0.9914\n",
      "loss: 0.027768331486731766, train acc: 0.99\n",
      "loss: 0.02939768396317959, train acc: 0.9907\n",
      "loss: 0.029337230511009692, train acc: 0.9936\n",
      "loss: 0.035612885281443594, train acc: 0.9899\n",
      "loss: 0.02414389420300722, train acc: 0.9948\n",
      "epoch: 56, loss: 0.0018736543133854866, train acc: 0.9948, test acc: 0.9203\n",
      "loss: 0.024081986397504807, train acc: 0.9928\n",
      "loss: 0.03026899993419647, train acc: 0.9949\n",
      "loss: 0.03339701611548662, train acc: 0.9922\n",
      "loss: 0.026696573477238417, train acc: 0.9914\n",
      "loss: 0.027880859933793543, train acc: 0.9914\n",
      "loss: 0.0280080184340477, train acc: 0.9939\n",
      "loss: 0.033737152442336084, train acc: 0.9902\n",
      "loss: 0.02303102854639292, train acc: 0.9953\n",
      "epoch: 57, loss: 0.0016666913870722055, train acc: 0.9953, test acc: 0.9205\n",
      "loss: 0.022786526009440422, train acc: 0.9932\n",
      "loss: 0.02906020302325487, train acc: 0.9945\n",
      "loss: 0.032495150528848174, train acc: 0.9921\n",
      "loss: 0.025507554691284896, train acc: 0.9912\n",
      "loss: 0.026398013997823, train acc: 0.9916\n",
      "loss: 0.026687466632574798, train acc: 0.9944\n",
      "loss: 0.0317696513608098, train acc: 0.9904\n",
      "loss: 0.02214009165763855, train acc: 0.9955\n",
      "epoch: 58, loss: 0.00165933882817626, train acc: 0.9955, test acc: 0.92\n",
      "loss: 0.02181703969836235, train acc: 0.9933\n",
      "loss: 0.02795795798301697, train acc: 0.9948\n",
      "loss: 0.030801066011190415, train acc: 0.9923\n",
      "loss: 0.02438190197572112, train acc: 0.9921\n",
      "loss: 0.025473311822861434, train acc: 0.9919\n",
      "loss: 0.026237320993095635, train acc: 0.9946\n",
      "loss: 0.03067238163203001, train acc: 0.9917\n",
      "loss: 0.02153229471296072, train acc: 0.9959\n",
      "epoch: 59, loss: 0.0014011060120537877, train acc: 0.9959, test acc: 0.9201\n",
      "loss: 0.020447472110390663, train acc: 0.9942\n",
      "loss: 0.026761349849402906, train acc: 0.9946\n",
      "loss: 0.029459212347865103, train acc: 0.9928\n",
      "loss: 0.023627464286983015, train acc: 0.9923\n",
      "loss: 0.024232042953372, train acc: 0.9931\n",
      "loss: 0.024818456172943114, train acc: 0.9951\n",
      "loss: 0.029054465517401696, train acc: 0.9912\n",
      "loss: 0.020275196898728608, train acc: 0.9959\n",
      "epoch: 60, loss: 0.0013812283286824822, train acc: 0.9959, test acc: 0.9203\n",
      "loss: 0.02035173587501049, train acc: 0.9944\n",
      "loss: 0.025737673230469226, train acc: 0.9955\n",
      "loss: 0.028067504055798054, train acc: 0.9934\n",
      "loss: 0.022561059705913065, train acc: 0.9922\n",
      "loss: 0.023053973354399203, train acc: 0.9937\n",
      "loss: 0.023733853548765182, train acc: 0.995\n",
      "loss: 0.027567501924932004, train acc: 0.9917\n",
      "loss: 0.02003717441111803, train acc: 0.9961\n",
      "epoch: 61, loss: 0.0012152954004704952, train acc: 0.9961, test acc: 0.9199\n",
      "loss: 0.01915980875492096, train acc: 0.9948\n",
      "loss: 0.02479584328830242, train acc: 0.9959\n",
      "loss: 0.02678639404475689, train acc: 0.9934\n",
      "loss: 0.021488934475928544, train acc: 0.9924\n",
      "loss: 0.02204705383628607, train acc: 0.9935\n",
      "loss: 0.022698473744094373, train acc: 0.9953\n",
      "loss: 0.025824090093374254, train acc: 0.9918\n",
      "loss: 0.01861040126532316, train acc: 0.9963\n",
      "epoch: 62, loss: 0.0011942920973524451, train acc: 0.9963, test acc: 0.9195\n",
      "loss: 0.018408996984362602, train acc: 0.9951\n",
      "loss: 0.02413120623677969, train acc: 0.9961\n",
      "loss: 0.026095027662813665, train acc: 0.9933\n",
      "loss: 0.020849303901195528, train acc: 0.9926\n",
      "loss: 0.021328003518283366, train acc: 0.9942\n",
      "loss: 0.02173194969072938, train acc: 0.9952\n",
      "loss: 0.024745291098952293, train acc: 0.9924\n",
      "loss: 0.01834788890555501, train acc: 0.9968\n",
      "epoch: 63, loss: 0.0010571142192929983, train acc: 0.9968, test acc: 0.9197\n",
      "loss: 0.017559140920639038, train acc: 0.9949\n",
      "loss: 0.023188196681439878, train acc: 0.9965\n",
      "loss: 0.024326543696224688, train acc: 0.9937\n",
      "loss: 0.020090516097843646, train acc: 0.9931\n",
      "loss: 0.020411858893930913, train acc: 0.995\n",
      "loss: 0.020556238386780022, train acc: 0.9952\n",
      "loss: 0.02349717207252979, train acc: 0.9928\n",
      "loss: 0.017523361090570688, train acc: 0.9968\n",
      "epoch: 64, loss: 0.0009819779079407454, train acc: 0.9968, test acc: 0.9192\n",
      "loss: 0.01697632111608982, train acc: 0.9952\n",
      "loss: 0.0223039073869586, train acc: 0.9965\n",
      "loss: 0.0236479545943439, train acc: 0.9938\n",
      "loss: 0.019319926761090754, train acc: 0.993\n",
      "loss: 0.019638490956276657, train acc: 0.996\n",
      "loss: 0.019949552789330482, train acc: 0.9953\n",
      "loss: 0.022444542311131955, train acc: 0.9928\n",
      "loss: 0.01669022012501955, train acc: 0.9966\n",
      "epoch: 65, loss: 0.0008789756102487445, train acc: 0.9966, test acc: 0.9184\n",
      "loss: 0.016150306910276413, train acc: 0.995\n",
      "loss: 0.021671264432370663, train acc: 0.9967\n",
      "loss: 0.02235706290230155, train acc: 0.9938\n",
      "loss: 0.018256071396172045, train acc: 0.9936\n",
      "loss: 0.018896720837801695, train acc: 0.9958\n",
      "loss: 0.0190727224573493, train acc: 0.9954\n",
      "loss: 0.021913885697722436, train acc: 0.9935\n",
      "loss: 0.01633982788771391, train acc: 0.9967\n",
      "epoch: 66, loss: 0.0008570593781769276, train acc: 0.9967, test acc: 0.9193\n",
      "loss: 0.01534787192940712, train acc: 0.9954\n",
      "loss: 0.020636539533734323, train acc: 0.9965\n",
      "loss: 0.02139045372605324, train acc: 0.994\n",
      "loss: 0.01777364043518901, train acc: 0.9937\n",
      "loss: 0.01763683957979083, train acc: 0.9963\n",
      "loss: 0.018083869200199842, train acc: 0.995\n",
      "loss: 0.02085542315617204, train acc: 0.9935\n",
      "loss: 0.015759471245110034, train acc: 0.9966\n",
      "epoch: 67, loss: 0.0007871894631534815, train acc: 0.9966, test acc: 0.9193\n",
      "loss: 0.014779466204345226, train acc: 0.995\n",
      "loss: 0.019936124421656133, train acc: 0.9966\n",
      "loss: 0.02063173186033964, train acc: 0.9941\n",
      "loss: 0.017030891589820386, train acc: 0.9933\n",
      "loss: 0.017099416814744473, train acc: 0.9963\n",
      "loss: 0.017365893349051476, train acc: 0.995\n",
      "loss: 0.020170205924659967, train acc: 0.9937\n",
      "loss: 0.015092511009424926, train acc: 0.9965\n",
      "epoch: 68, loss: 0.0006913798279128969, train acc: 0.9965, test acc: 0.9193\n",
      "loss: 0.01397739164531231, train acc: 0.9955\n",
      "loss: 0.01901910863816738, train acc: 0.9968\n",
      "loss: 0.019470229838043453, train acc: 0.9948\n",
      "loss: 0.01619062926620245, train acc: 0.9936\n",
      "loss: 0.016210576798766852, train acc: 0.9964\n",
      "loss: 0.01655529411509633, train acc: 0.9951\n",
      "loss: 0.019112312979996205, train acc: 0.9943\n",
      "loss: 0.01434390265494585, train acc: 0.9972\n",
      "epoch: 69, loss: 0.0006806475576013327, train acc: 0.9972, test acc: 0.9185\n",
      "loss: 0.014085465110838413, train acc: 0.995\n",
      "loss: 0.01872738339006901, train acc: 0.9972\n",
      "loss: 0.018815599381923676, train acc: 0.9952\n",
      "loss: 0.015584505535662175, train acc: 0.9934\n",
      "loss: 0.015520330984145403, train acc: 0.9966\n",
      "loss: 0.015450784284621478, train acc: 0.9952\n",
      "loss: 0.01823141537606716, train acc: 0.9942\n",
      "loss: 0.013891225401312112, train acc: 0.9969\n",
      "epoch: 70, loss: 0.0005900246906094253, train acc: 0.9969, test acc: 0.9189\n",
      "loss: 0.013105758465826511, train acc: 0.9952\n",
      "loss: 0.017870465759187937, train acc: 0.9968\n",
      "loss: 0.01829021368175745, train acc: 0.995\n",
      "loss: 0.015215330384671688, train acc: 0.9932\n",
      "loss: 0.01498492187820375, train acc: 0.9966\n",
      "loss: 0.014998223725706339, train acc: 0.9954\n",
      "loss: 0.01745312297716737, train acc: 0.9942\n",
      "loss: 0.013706529699265956, train acc: 0.997\n",
      "epoch: 71, loss: 0.0005067064194008708, train acc: 0.997, test acc: 0.9186\n",
      "loss: 0.012602516449987888, train acc: 0.9956\n",
      "loss: 0.01698113130405545, train acc: 0.9972\n",
      "loss: 0.01709012845531106, train acc: 0.9957\n",
      "loss: 0.01441072029992938, train acc: 0.9931\n",
      "loss: 0.014755473053082824, train acc: 0.9968\n",
      "loss: 0.014455108717083932, train acc: 0.9957\n",
      "loss: 0.016536985523998737, train acc: 0.9951\n",
      "loss: 0.01249348120763898, train acc: 0.9977\n",
      "epoch: 72, loss: 0.00045424094423651695, train acc: 0.9977, test acc: 0.919\n",
      "loss: 0.012223554775118828, train acc: 0.9955\n",
      "loss: 0.016580180358141662, train acc: 0.9972\n",
      "loss: 0.01687529366463423, train acc: 0.9959\n",
      "loss: 0.013598653906956314, train acc: 0.9933\n",
      "loss: 0.014084763918071985, train acc: 0.9969\n",
      "loss: 0.013743472937494516, train acc: 0.9958\n",
      "loss: 0.0160539167933166, train acc: 0.9948\n",
      "loss: 0.012434949446469546, train acc: 0.9974\n",
      "epoch: 73, loss: 0.0004181043477728963, train acc: 0.9974, test acc: 0.9187\n",
      "loss: 0.011144759133458138, train acc: 0.9959\n",
      "loss: 0.015624072961509227, train acc: 0.9974\n",
      "loss: 0.01563738901168108, train acc: 0.9964\n",
      "loss: 0.013355414662510157, train acc: 0.9933\n",
      "loss: 0.013878784980624915, train acc: 0.9967\n",
      "loss: 0.013160445354878902, train acc: 0.996\n",
      "loss: 0.01534286756068468, train acc: 0.9951\n",
      "loss: 0.011971999052911996, train acc: 0.9976\n",
      "epoch: 74, loss: 0.000391542911529541, train acc: 0.9976, test acc: 0.9188\n",
      "loss: 0.011089401319622993, train acc: 0.9958\n",
      "loss: 0.014964068960398435, train acc: 0.9976\n",
      "loss: 0.014894236950203777, train acc: 0.9964\n",
      "loss: 0.012395336013287305, train acc: 0.994\n",
      "loss: 0.013031449541449547, train acc: 0.9969\n",
      "loss: 0.01258762739598751, train acc: 0.996\n",
      "loss: 0.014766968972980975, train acc: 0.9959\n",
      "loss: 0.011387489316985012, train acc: 0.9974\n",
      "epoch: 75, loss: 0.0003370603662915528, train acc: 0.9974, test acc: 0.9188\n",
      "loss: 0.010352889075875282, train acc: 0.9957\n",
      "loss: 0.014690921269357204, train acc: 0.9975\n",
      "loss: 0.014779055304825307, train acc: 0.9959\n",
      "loss: 0.012010748358443379, train acc: 0.9934\n",
      "loss: 0.012404472101479769, train acc: 0.9967\n",
      "loss: 0.011993714701384306, train acc: 0.9965\n",
      "loss: 0.014096939470618963, train acc: 0.9956\n",
      "loss: 0.010937594482675195, train acc: 0.9973\n",
      "epoch: 76, loss: 0.00030268210684880614, train acc: 0.9973, test acc: 0.9187\n",
      "loss: 0.010110209695994854, train acc: 0.9957\n",
      "loss: 0.013943764008581638, train acc: 0.998\n",
      "loss: 0.013848321745172143, train acc: 0.9966\n",
      "loss: 0.011473457654938101, train acc: 0.994\n",
      "loss: 0.012138129770755767, train acc: 0.9965\n",
      "loss: 0.011396751180291176, train acc: 0.9966\n",
      "loss: 0.013392161577939987, train acc: 0.9956\n",
      "loss: 0.010401362227275968, train acc: 0.9976\n",
      "epoch: 77, loss: 0.00026595222880132496, train acc: 0.9976, test acc: 0.919\n",
      "loss: 0.009614026173949242, train acc: 0.9963\n",
      "loss: 0.013273860979825258, train acc: 0.9976\n",
      "loss: 0.013458793889731169, train acc: 0.9962\n",
      "loss: 0.01079415502026677, train acc: 0.9943\n",
      "loss: 0.011531847016885876, train acc: 0.9964\n",
      "loss: 0.010980672715231777, train acc: 0.9971\n",
      "loss: 0.012970030307769775, train acc: 0.9958\n",
      "loss: 0.010233432380482554, train acc: 0.9976\n",
      "epoch: 78, loss: 0.00024149820092134178, train acc: 0.9976, test acc: 0.9186\n",
      "loss: 0.008817773312330246, train acc: 0.9965\n",
      "loss: 0.012845605332404374, train acc: 0.9984\n",
      "loss: 0.012883407343178988, train acc: 0.996\n",
      "loss: 0.010342715540900826, train acc: 0.9944\n",
      "loss: 0.011040879040956497, train acc: 0.9967\n",
      "loss: 0.010399078764021397, train acc: 0.997\n",
      "loss: 0.012218631710857153, train acc: 0.9959\n",
      "loss: 0.009875281574204564, train acc: 0.9974\n",
      "epoch: 79, loss: 0.00021238783665467054, train acc: 0.9974, test acc: 0.9191\n",
      "#####training and testing end with K:20, P:1######\n",
      "#####training and testing start with K:40, P:0.1######\n",
      "loss: 2.3203318119049072, train acc: 0.132\n",
      "loss: 2.0134838104248045, train acc: 0.6454\n",
      "loss: 1.3388750553131104, train acc: 0.7969\n",
      "loss: 0.8867510318756103, train acc: 0.8292\n",
      "loss: 0.7135442495346069, train acc: 0.8556\n",
      "loss: 0.5858710020780563, train acc: 0.8741\n",
      "loss: 0.507236185669899, train acc: 0.8822\n",
      "loss: 0.46018767058849336, train acc: 0.8887\n",
      "epoch: 0, loss: 0.39354538917541504, train acc: 0.8887, test acc: 0.8896\n",
      "loss: 0.4774586260318756, train acc: 0.8967\n",
      "loss: 0.5032540947198868, train acc: 0.9023\n",
      "loss: 0.4480457574129105, train acc: 0.9049\n",
      "loss: 0.3808297038078308, train acc: 0.904\n",
      "loss: 0.4018219918012619, train acc: 0.9089\n",
      "loss: 0.35978013277053833, train acc: 0.911\n",
      "loss: 0.3460765242576599, train acc: 0.9122\n",
      "loss: 0.3534896939992905, train acc: 0.9118\n",
      "epoch: 1, loss: 0.2298251837491989, train acc: 0.9118, test acc: 0.9082\n",
      "loss: 0.3522345721721649, train acc: 0.9177\n",
      "loss: 0.38348798751831054, train acc: 0.9216\n",
      "loss: 0.3553051620721817, train acc: 0.9244\n",
      "loss: 0.31955252438783643, train acc: 0.9229\n",
      "loss: 0.3390628591179848, train acc: 0.9255\n",
      "loss: 0.28577241748571397, train acc: 0.9267\n",
      "loss: 0.3014771342277527, train acc: 0.9273\n",
      "loss: 0.28503668010234834, train acc: 0.9225\n",
      "epoch: 2, loss: 0.16606508195400238, train acc: 0.9225, test acc: 0.9143\n",
      "loss: 0.3419727683067322, train acc: 0.9298\n",
      "loss: 0.3353248417377472, train acc: 0.9314\n",
      "loss: 0.31072513908147814, train acc: 0.9351\n",
      "loss: 0.2683137446641922, train acc: 0.9336\n",
      "loss: 0.3110067188739777, train acc: 0.9345\n",
      "loss: 0.26386494040489195, train acc: 0.9366\n",
      "loss: 0.2631421908736229, train acc: 0.9395\n",
      "loss: 0.24757825434207917, train acc: 0.9351\n",
      "epoch: 3, loss: 0.16586780548095703, train acc: 0.9351, test acc: 0.9207\n",
      "loss: 0.26610666513442993, train acc: 0.9376\n",
      "loss: 0.2836837634444237, train acc: 0.9388\n",
      "loss: 0.28497387915849687, train acc: 0.9406\n",
      "loss: 0.24504196047782897, train acc: 0.9402\n",
      "loss: 0.2744584947824478, train acc: 0.9403\n",
      "loss: 0.22867036014795303, train acc: 0.9433\n",
      "loss: 0.23995209336280823, train acc: 0.9462\n",
      "loss: 0.21736417561769486, train acc: 0.9406\n",
      "epoch: 4, loss: 0.12108100205659866, train acc: 0.9406, test acc: 0.9225\n",
      "loss: 0.2720334827899933, train acc: 0.944\n",
      "loss: 0.2605798006057739, train acc: 0.947\n",
      "loss: 0.2390575960278511, train acc: 0.9483\n",
      "loss: 0.2216082826256752, train acc: 0.9466\n",
      "loss: 0.25295535326004026, train acc: 0.9497\n",
      "loss: 0.19109182059764862, train acc: 0.9513\n",
      "loss: 0.20301294475793838, train acc: 0.952\n",
      "loss: 0.19708091467618943, train acc: 0.9468\n",
      "epoch: 5, loss: 0.13371708989143372, train acc: 0.9468, test acc: 0.9261\n",
      "loss: 0.2607370913028717, train acc: 0.9503\n",
      "loss: 0.23488749116659163, train acc: 0.9533\n",
      "loss: 0.23042585253715514, train acc: 0.9545\n",
      "loss: 0.19941831529140472, train acc: 0.9536\n",
      "loss: 0.24030449986457825, train acc: 0.9566\n",
      "loss: 0.18207162469625474, train acc: 0.9569\n",
      "loss: 0.18583526462316513, train acc: 0.9579\n",
      "loss: 0.18712078928947448, train acc: 0.953\n",
      "epoch: 6, loss: 0.06328097730875015, train acc: 0.953, test acc: 0.928\n",
      "loss: 0.2877919673919678, train acc: 0.956\n",
      "loss: 0.22335123419761657, train acc: 0.9593\n",
      "loss: 0.217103773355484, train acc: 0.9566\n",
      "loss: 0.18437971621751786, train acc: 0.9562\n",
      "loss: 0.22576314806938172, train acc: 0.9603\n",
      "loss: 0.174713796377182, train acc: 0.9617\n",
      "loss: 0.18223107904195784, train acc: 0.9633\n",
      "loss: 0.17202849462628364, train acc: 0.9582\n",
      "epoch: 7, loss: 0.10156774520874023, train acc: 0.9582, test acc: 0.9315\n",
      "loss: 0.23907485604286194, train acc: 0.9592\n",
      "loss: 0.20276535600423812, train acc: 0.9629\n",
      "loss: 0.19612741619348525, train acc: 0.9608\n",
      "loss: 0.1869354248046875, train acc: 0.9617\n",
      "loss: 0.19709717109799385, train acc: 0.9629\n",
      "loss: 0.15939756408333777, train acc: 0.9644\n",
      "loss: 0.1670985996723175, train acc: 0.9649\n",
      "loss: 0.1504490368068218, train acc: 0.9606\n",
      "epoch: 8, loss: 0.1802610158920288, train acc: 0.9606, test acc: 0.9315\n",
      "loss: 0.19040361046791077, train acc: 0.9626\n",
      "loss: 0.18125979453325272, train acc: 0.9656\n",
      "loss: 0.17976685464382172, train acc: 0.9648\n",
      "loss: 0.16380745843052863, train acc: 0.9654\n",
      "loss: 0.17951349914073944, train acc: 0.9668\n",
      "loss: 0.14195750653743744, train acc: 0.9679\n",
      "loss: 0.15790849402546883, train acc: 0.9672\n",
      "loss: 0.1451101653277874, train acc: 0.9657\n",
      "epoch: 9, loss: 0.04169103503227234, train acc: 0.9657, test acc: 0.9338\n",
      "loss: 0.20044584572315216, train acc: 0.9673\n",
      "loss: 0.18179340958595275, train acc: 0.9689\n",
      "loss: 0.17064352110028266, train acc: 0.9691\n",
      "loss: 0.15235574617981912, train acc: 0.9671\n",
      "loss: 0.17747693359851838, train acc: 0.9688\n",
      "loss: 0.1394326828420162, train acc: 0.9708\n",
      "loss: 0.14965524524450302, train acc: 0.9702\n",
      "loss: 0.14137950986623765, train acc: 0.9677\n",
      "epoch: 10, loss: 0.01955709047615528, train acc: 0.9677, test acc: 0.935\n",
      "loss: 0.19682051241397858, train acc: 0.9704\n",
      "loss: 0.16354356557130814, train acc: 0.9716\n",
      "loss: 0.1708168789744377, train acc: 0.9697\n",
      "loss: 0.15533470883965492, train acc: 0.9707\n",
      "loss: 0.15431269109249116, train acc: 0.9714\n",
      "loss: 0.1371827557682991, train acc: 0.9741\n",
      "loss: 0.13480849415063859, train acc: 0.9746\n",
      "loss: 0.13255296200513839, train acc: 0.9705\n",
      "epoch: 11, loss: 0.04252570867538452, train acc: 0.9705, test acc: 0.9385\n",
      "loss: 0.14985793828964233, train acc: 0.972\n",
      "loss: 0.14592930674552917, train acc: 0.9748\n",
      "loss: 0.15593887120485306, train acc: 0.9746\n",
      "loss: 0.14049978777766228, train acc: 0.9736\n",
      "loss: 0.1400449626147747, train acc: 0.9747\n",
      "loss: 0.13450816869735718, train acc: 0.9747\n",
      "loss: 0.13416106030344963, train acc: 0.9764\n",
      "loss: 0.11623527184128761, train acc: 0.9731\n",
      "epoch: 12, loss: 0.01984609104692936, train acc: 0.9731, test acc: 0.9374\n",
      "loss: 0.15826864540576935, train acc: 0.9741\n",
      "loss: 0.13784421235322952, train acc: 0.9766\n",
      "loss: 0.13663952946662902, train acc: 0.977\n",
      "loss: 0.123960330337286, train acc: 0.9748\n",
      "loss: 0.14053892716765404, train acc: 0.9764\n",
      "loss: 0.11078415289521218, train acc: 0.9774\n",
      "loss: 0.12426425740122796, train acc: 0.9763\n",
      "loss: 0.11731136441230774, train acc: 0.9764\n",
      "epoch: 13, loss: 0.03905780240893364, train acc: 0.9764, test acc: 0.9376\n",
      "loss: 0.15260328352451324, train acc: 0.978\n",
      "loss: 0.12384790331125259, train acc: 0.9793\n",
      "loss: 0.13544067218899727, train acc: 0.9778\n",
      "loss: 0.11172559261322021, train acc: 0.9769\n",
      "loss: 0.14070001989603043, train acc: 0.9786\n",
      "loss: 0.09816335365176201, train acc: 0.979\n",
      "loss: 0.11697955168783665, train acc: 0.9802\n",
      "loss: 0.11142215803265572, train acc: 0.9771\n",
      "epoch: 14, loss: 0.05604575574398041, train acc: 0.9771, test acc: 0.939\n",
      "loss: 0.17241817712783813, train acc: 0.9779\n",
      "loss: 0.1206025831401348, train acc: 0.9786\n",
      "loss: 0.1263753779232502, train acc: 0.9795\n",
      "loss: 0.1246167041361332, train acc: 0.9801\n",
      "loss: 0.12654262110590936, train acc: 0.9797\n",
      "loss: 0.10278351157903672, train acc: 0.98\n",
      "loss: 0.1134034626185894, train acc: 0.9812\n",
      "loss: 0.10548616349697112, train acc: 0.9782\n",
      "epoch: 15, loss: 0.03210964798927307, train acc: 0.9782, test acc: 0.94\n",
      "loss: 0.1437297761440277, train acc: 0.9793\n",
      "loss: 0.11111306846141815, train acc: 0.9814\n",
      "loss: 0.1215537279844284, train acc: 0.9807\n",
      "loss: 0.10891472548246384, train acc: 0.9821\n",
      "loss: 0.11307720839977264, train acc: 0.9811\n",
      "loss: 0.11006839536130428, train acc: 0.9807\n",
      "loss: 0.09999398663640022, train acc: 0.9833\n",
      "loss: 0.0923217449337244, train acc: 0.9794\n",
      "epoch: 16, loss: 0.05068594962358475, train acc: 0.9794, test acc: 0.9412\n",
      "loss: 0.0888575091958046, train acc: 0.9818\n",
      "loss: 0.09541053958237171, train acc: 0.9808\n",
      "loss: 0.11548853889107705, train acc: 0.9834\n",
      "loss: 0.10420218110084534, train acc: 0.9811\n",
      "loss: 0.10752485394477844, train acc: 0.9827\n",
      "loss: 0.08700009286403657, train acc: 0.9832\n",
      "loss: 0.10402557235211134, train acc: 0.9836\n",
      "loss: 0.09436808563768864, train acc: 0.9805\n",
      "epoch: 17, loss: 0.045849792659282684, train acc: 0.9805, test acc: 0.9417\n",
      "loss: 0.14294587075710297, train acc: 0.9827\n",
      "loss: 0.10846413932740688, train acc: 0.9829\n",
      "loss: 0.10552476495504379, train acc: 0.9835\n",
      "loss: 0.10319091528654098, train acc: 0.9833\n",
      "loss: 0.11122511699795723, train acc: 0.9835\n",
      "loss: 0.09597964361310005, train acc: 0.9835\n",
      "loss: 0.09970969744026661, train acc: 0.9851\n",
      "loss: 0.09409859068691731, train acc: 0.9844\n",
      "epoch: 18, loss: 0.030270902439951897, train acc: 0.9844, test acc: 0.9403\n",
      "loss: 0.15030047297477722, train acc: 0.9832\n",
      "loss: 0.10973490551114082, train acc: 0.9836\n",
      "loss: 0.11608201935887337, train acc: 0.985\n",
      "loss: 0.09332678280770779, train acc: 0.9848\n",
      "loss: 0.09344267845153809, train acc: 0.9852\n",
      "loss: 0.08335920013487338, train acc: 0.9845\n",
      "loss: 0.10362012647092342, train acc: 0.9857\n",
      "loss: 0.0899404775351286, train acc: 0.9842\n",
      "epoch: 19, loss: 0.02698545530438423, train acc: 0.9842, test acc: 0.9412\n",
      "loss: 0.10482378304004669, train acc: 0.9845\n",
      "loss: 0.10257342755794525, train acc: 0.985\n",
      "loss: 0.10724511742591858, train acc: 0.9858\n",
      "loss: 0.10037008412182331, train acc: 0.9869\n",
      "loss: 0.11098863780498505, train acc: 0.9857\n",
      "loss: 0.0765544481575489, train acc: 0.9855\n",
      "loss: 0.09029609151184559, train acc: 0.9874\n",
      "loss: 0.07378454394638538, train acc: 0.9853\n",
      "epoch: 20, loss: 0.17272263765335083, train acc: 0.9853, test acc: 0.9426\n",
      "loss: 0.1165052279829979, train acc: 0.9876\n",
      "loss: 0.09414556995034218, train acc: 0.9848\n",
      "loss: 0.10047886036336422, train acc: 0.9865\n",
      "loss: 0.08760924302041531, train acc: 0.9859\n",
      "loss: 0.09761733859777451, train acc: 0.9879\n",
      "loss: 0.07497934028506278, train acc: 0.9862\n",
      "loss: 0.08253366090357303, train acc: 0.9874\n",
      "loss: 0.08234323635697365, train acc: 0.9859\n",
      "epoch: 21, loss: 0.03916139528155327, train acc: 0.9859, test acc: 0.943\n",
      "loss: 0.11245153099298477, train acc: 0.9888\n",
      "loss: 0.0944965485483408, train acc: 0.9886\n",
      "loss: 0.08757786080241203, train acc: 0.9886\n",
      "loss: 0.0834655735641718, train acc: 0.9887\n",
      "loss: 0.07464110367000103, train acc: 0.9898\n",
      "loss: 0.07057618163526058, train acc: 0.9874\n",
      "loss: 0.0700254000723362, train acc: 0.9896\n",
      "loss: 0.08932062350213528, train acc: 0.9873\n",
      "epoch: 22, loss: 0.006451508495956659, train acc: 0.9873, test acc: 0.9427\n",
      "loss: 0.11168541759252548, train acc: 0.9894\n",
      "loss: 0.08445788621902466, train acc: 0.9902\n",
      "loss: 0.08712962567806244, train acc: 0.9895\n",
      "loss: 0.09112788364291191, train acc: 0.9889\n",
      "loss: 0.0725889302790165, train acc: 0.9916\n",
      "loss: 0.07219329290091991, train acc: 0.9902\n",
      "loss: 0.08296487219631672, train acc: 0.9911\n",
      "loss: 0.07103252597153187, train acc: 0.986\n",
      "epoch: 23, loss: 0.016368649899959564, train acc: 0.986, test acc: 0.942\n",
      "loss: 0.15953905880451202, train acc: 0.9899\n",
      "loss: 0.07945771589875221, train acc: 0.9906\n",
      "loss: 0.076678167283535, train acc: 0.9904\n",
      "loss: 0.07361271660774946, train acc: 0.9917\n",
      "loss: 0.07989581748843193, train acc: 0.9922\n",
      "loss: 0.058601662330329415, train acc: 0.9897\n",
      "loss: 0.0725937232375145, train acc: 0.991\n",
      "loss: 0.07860458716750145, train acc: 0.9866\n",
      "epoch: 24, loss: 0.08838897943496704, train acc: 0.9866, test acc: 0.9434\n",
      "loss: 0.08249635249376297, train acc: 0.9899\n",
      "loss: 0.07605345398187638, train acc: 0.9912\n",
      "loss: 0.08983891271054745, train acc: 0.9908\n",
      "loss: 0.07405889052897692, train acc: 0.9916\n",
      "loss: 0.07292425930500031, train acc: 0.9909\n",
      "loss: 0.0573086854070425, train acc: 0.9905\n",
      "loss: 0.07108669076114893, train acc: 0.9907\n",
      "loss: 0.0653365146368742, train acc: 0.9901\n",
      "epoch: 25, loss: 0.017498338595032692, train acc: 0.9901, test acc: 0.9439\n",
      "loss: 0.06371665745973587, train acc: 0.9922\n",
      "loss: 0.06794760935008526, train acc: 0.9919\n",
      "loss: 0.07184083946049213, train acc: 0.9913\n",
      "loss: 0.07086101118475199, train acc: 0.9922\n",
      "loss: 0.06811034865677357, train acc: 0.9925\n",
      "loss: 0.051858052238821985, train acc: 0.9913\n",
      "loss: 0.06477497331798077, train acc: 0.9927\n",
      "loss: 0.05991225857287645, train acc: 0.9886\n",
      "epoch: 26, loss: 0.011333009228110313, train acc: 0.9886, test acc: 0.9439\n",
      "loss: 0.08177216351032257, train acc: 0.9901\n",
      "loss: 0.06644452456384897, train acc: 0.9923\n",
      "loss: 0.07414909563958645, train acc: 0.9925\n",
      "loss: 0.07872246094048023, train acc: 0.9905\n",
      "loss: 0.06355054043233395, train acc: 0.9927\n",
      "loss: 0.057743050158023834, train acc: 0.9912\n",
      "loss: 0.06417528837919236, train acc: 0.9922\n",
      "loss: 0.07374712862074376, train acc: 0.9908\n",
      "epoch: 27, loss: 0.042873565107584, train acc: 0.9908, test acc: 0.9433\n",
      "loss: 0.13821189105510712, train acc: 0.9915\n",
      "loss: 0.07125015407800675, train acc: 0.9915\n",
      "loss: 0.07376196272671223, train acc: 0.9925\n",
      "loss: 0.06400327757000923, train acc: 0.9938\n",
      "loss: 0.0667062021791935, train acc: 0.9941\n",
      "loss: 0.05350694302469492, train acc: 0.9928\n",
      "loss: 0.06208885461091995, train acc: 0.9934\n",
      "loss: 0.05286436174064875, train acc: 0.9899\n",
      "epoch: 28, loss: 0.009768900461494923, train acc: 0.9899, test acc: 0.945\n",
      "loss: 0.08633363246917725, train acc: 0.9932\n",
      "loss: 0.06981435138732195, train acc: 0.9937\n",
      "loss: 0.08060757145285606, train acc: 0.9948\n",
      "loss: 0.056755703687667844, train acc: 0.9943\n",
      "loss: 0.05737979561090469, train acc: 0.9949\n",
      "loss: 0.04865785576403141, train acc: 0.9932\n",
      "loss: 0.07279163300991058, train acc: 0.9936\n",
      "loss: 0.06604369301348925, train acc: 0.9901\n",
      "epoch: 29, loss: 0.038557425141334534, train acc: 0.9901, test acc: 0.9428\n",
      "loss: 0.08002225309610367, train acc: 0.992\n",
      "loss: 0.06988604515790939, train acc: 0.9928\n",
      "loss: 0.0687718492001295, train acc: 0.9945\n",
      "loss: 0.052819526940584186, train acc: 0.9949\n",
      "loss: 0.0607718825340271, train acc: 0.9955\n",
      "loss: 0.05612990688532591, train acc: 0.9942\n",
      "loss: 0.056115334294736384, train acc: 0.994\n",
      "loss: 0.059562798030674456, train acc: 0.9911\n",
      "epoch: 30, loss: 0.0564272403717041, train acc: 0.9911, test acc: 0.9445\n",
      "loss: 0.0771537721157074, train acc: 0.9941\n",
      "loss: 0.06087047085165977, train acc: 0.9928\n",
      "loss: 0.06378956958651542, train acc: 0.9942\n",
      "loss: 0.0599667364731431, train acc: 0.9947\n",
      "loss: 0.06318272538483143, train acc: 0.9952\n",
      "loss: 0.042857606522738935, train acc: 0.9944\n",
      "loss: 0.055165833421051504, train acc: 0.995\n",
      "loss: 0.05492803482338786, train acc: 0.9907\n",
      "epoch: 31, loss: 0.005590957123786211, train acc: 0.9907, test acc: 0.9421\n",
      "loss: 0.06364011764526367, train acc: 0.993\n",
      "loss: 0.05951192416250706, train acc: 0.9941\n",
      "loss: 0.05899426862597466, train acc: 0.9947\n",
      "loss: 0.06085404995828867, train acc: 0.9938\n",
      "loss: 0.060171812772750854, train acc: 0.9942\n",
      "loss: 0.039630507491528986, train acc: 0.9931\n",
      "loss: 0.05772547610104084, train acc: 0.9959\n",
      "loss: 0.05765271447598934, train acc: 0.9925\n",
      "epoch: 32, loss: 0.03272288292646408, train acc: 0.9925, test acc: 0.9442\n",
      "loss: 0.061988409608602524, train acc: 0.995\n",
      "loss: 0.06087493542581797, train acc: 0.9933\n",
      "loss: 0.07366890385746956, train acc: 0.9942\n",
      "loss: 0.06020265333354473, train acc: 0.9948\n",
      "loss: 0.05929929539561272, train acc: 0.9956\n",
      "loss: 0.045604518987238406, train acc: 0.9943\n",
      "loss: 0.05193529985845089, train acc: 0.995\n",
      "loss: 0.04676785282790661, train acc: 0.9921\n",
      "epoch: 33, loss: 0.02241566963493824, train acc: 0.9921, test acc: 0.9463\n",
      "loss: 0.07823697477579117, train acc: 0.9955\n",
      "loss: 0.06185665093362332, train acc: 0.9936\n",
      "loss: 0.05737283807247877, train acc: 0.9955\n",
      "loss: 0.05282273683696985, train acc: 0.9946\n",
      "loss: 0.05452987737953663, train acc: 0.9959\n",
      "loss: 0.04406579677015543, train acc: 0.9952\n",
      "loss: 0.05215043406933546, train acc: 0.9949\n",
      "loss: 0.04853924959897995, train acc: 0.9946\n",
      "epoch: 34, loss: 0.014799320138990879, train acc: 0.9946, test acc: 0.94\n",
      "loss: 0.07859295606613159, train acc: 0.9926\n",
      "loss: 0.05841145981103182, train acc: 0.9963\n",
      "loss: 0.056863071583211425, train acc: 0.9948\n",
      "loss: 0.0555966705083847, train acc: 0.9953\n",
      "loss: 0.06341193504631519, train acc: 0.9968\n",
      "loss: 0.0360271941870451, train acc: 0.995\n",
      "loss: 0.046317365299910304, train acc: 0.9961\n",
      "loss: 0.042084948532283305, train acc: 0.9955\n",
      "epoch: 35, loss: 0.012155737727880478, train acc: 0.9955, test acc: 0.9421\n",
      "loss: 0.04380818083882332, train acc: 0.994\n",
      "loss: 0.047621289268136024, train acc: 0.9961\n",
      "loss: 0.054807623568922284, train acc: 0.9951\n",
      "loss: 0.05027523264288902, train acc: 0.9956\n",
      "loss: 0.050742842815816404, train acc: 0.997\n",
      "loss: 0.04021938294172287, train acc: 0.9961\n",
      "loss: 0.051657567359507085, train acc: 0.9967\n",
      "loss: 0.04666737224906683, train acc: 0.9938\n",
      "epoch: 36, loss: 0.004424738232046366, train acc: 0.9938, test acc: 0.944\n",
      "loss: 0.07607164233922958, train acc: 0.9957\n",
      "loss: 0.05197025649249554, train acc: 0.9963\n",
      "loss: 0.04830197300761938, train acc: 0.9964\n",
      "loss: 0.04780234079807997, train acc: 0.996\n",
      "loss: 0.04261929783970118, train acc: 0.9971\n",
      "loss: 0.03969114050269127, train acc: 0.9959\n",
      "loss: 0.05178928133100271, train acc: 0.9957\n",
      "loss: 0.04612487806007266, train acc: 0.9957\n",
      "epoch: 37, loss: 0.015014070086181164, train acc: 0.9957, test acc: 0.9445\n",
      "loss: 0.08875159919261932, train acc: 0.996\n",
      "loss: 0.043553096055984494, train acc: 0.9969\n",
      "loss: 0.05259280763566494, train acc: 0.9958\n",
      "loss: 0.052815803326666355, train acc: 0.9947\n",
      "loss: 0.04822476599365473, train acc: 0.9969\n",
      "loss: 0.03155060727149248, train acc: 0.9958\n",
      "loss: 0.04952719956636429, train acc: 0.9972\n",
      "loss: 0.0455105060711503, train acc: 0.9959\n",
      "epoch: 38, loss: 0.007819128222763538, train acc: 0.9959, test acc: 0.942\n",
      "loss: 0.054993003606796265, train acc: 0.9946\n",
      "loss: 0.05167640615254641, train acc: 0.9971\n",
      "loss: 0.04463248997926712, train acc: 0.9967\n",
      "loss: 0.04648520238697529, train acc: 0.9977\n",
      "loss: 0.04812586102634668, train acc: 0.9976\n",
      "loss: 0.03197627700865269, train acc: 0.9972\n",
      "loss: 0.03900637160986662, train acc: 0.9977\n",
      "loss: 0.04404002539813519, train acc: 0.9967\n",
      "epoch: 39, loss: 0.0025803842581808567, train acc: 0.9967, test acc: 0.9441\n",
      "loss: 0.08143600076436996, train acc: 0.9965\n",
      "loss: 0.05503302309662104, train acc: 0.9967\n",
      "loss: 0.04619211908429861, train acc: 0.9962\n",
      "loss: 0.04657692778855562, train acc: 0.9958\n",
      "loss: 0.045325006172060964, train acc: 0.9973\n",
      "loss: 0.03971617659553885, train acc: 0.9974\n",
      "loss: 0.04638976808637381, train acc: 0.9975\n",
      "loss: 0.037690543197095396, train acc: 0.9951\n",
      "epoch: 40, loss: 0.007866992615163326, train acc: 0.9951, test acc: 0.9455\n",
      "loss: 0.09536243975162506, train acc: 0.9966\n",
      "loss: 0.04515384556725621, train acc: 0.9975\n",
      "loss: 0.03784984685480595, train acc: 0.9976\n",
      "loss: 0.04713829327374697, train acc: 0.9978\n",
      "loss: 0.04026045147329569, train acc: 0.9981\n",
      "loss: 0.03414295334368944, train acc: 0.9975\n",
      "loss: 0.028078881092369556, train acc: 0.9976\n",
      "loss: 0.04295174889266491, train acc: 0.9951\n",
      "epoch: 41, loss: 0.003986510448157787, train acc: 0.9951, test acc: 0.9424\n",
      "loss: 0.05558759719133377, train acc: 0.9964\n",
      "loss: 0.04628178281709552, train acc: 0.9982\n",
      "loss: 0.03451829496771097, train acc: 0.9976\n",
      "loss: 0.041361110750585794, train acc: 0.9983\n",
      "loss: 0.046503978036344054, train acc: 0.9987\n",
      "loss: 0.04067484512925148, train acc: 0.9977\n",
      "loss: 0.04056002125144005, train acc: 0.9978\n",
      "loss: 0.043996121175587176, train acc: 0.9954\n",
      "epoch: 42, loss: 0.007166321389377117, train acc: 0.9954, test acc: 0.9444\n",
      "loss: 0.08003941923379898, train acc: 0.9967\n",
      "loss: 0.045103625394403934, train acc: 0.9976\n",
      "loss: 0.04404031746089458, train acc: 0.9975\n",
      "loss: 0.03868912300094962, train acc: 0.9976\n",
      "loss: 0.04163478389382362, train acc: 0.9983\n",
      "loss: 0.033378257788717745, train acc: 0.998\n",
      "loss: 0.040825487952679394, train acc: 0.9988\n",
      "loss: 0.03291010055691004, train acc: 0.9973\n",
      "epoch: 43, loss: 0.006587683223187923, train acc: 0.9973, test acc: 0.9443\n",
      "loss: 0.03843585029244423, train acc: 0.997\n",
      "loss: 0.03491312488913536, train acc: 0.9979\n",
      "loss: 0.04029714427888394, train acc: 0.9968\n",
      "loss: 0.04312296584248543, train acc: 0.9976\n",
      "loss: 0.048958984576165675, train acc: 0.9981\n",
      "loss: 0.024730316922068597, train acc: 0.9976\n",
      "loss: 0.032091211061924696, train acc: 0.9977\n",
      "loss: 0.02705159056931734, train acc: 0.9979\n",
      "epoch: 44, loss: 0.0031872817780822515, train acc: 0.9979, test acc: 0.9427\n",
      "loss: 0.06489184498786926, train acc: 0.997\n",
      "loss: 0.05608818996697664, train acc: 0.9982\n",
      "loss: 0.04355382416397333, train acc: 0.998\n",
      "loss: 0.031144405994564296, train acc: 0.9979\n",
      "loss: 0.046531100012362006, train acc: 0.9983\n",
      "loss: 0.03445922313258052, train acc: 0.998\n",
      "loss: 0.03954349048435688, train acc: 0.9978\n",
      "loss: 0.03118412122130394, train acc: 0.9969\n",
      "epoch: 45, loss: 0.01146024651825428, train acc: 0.9969, test acc: 0.944\n",
      "loss: 0.05463393032550812, train acc: 0.9972\n",
      "loss: 0.03502694047056139, train acc: 0.9984\n",
      "loss: 0.03951701894402504, train acc: 0.9976\n",
      "loss: 0.0349549007602036, train acc: 0.9962\n",
      "loss: 0.03740193452686071, train acc: 0.9988\n",
      "loss: 0.02588265249505639, train acc: 0.9978\n",
      "loss: 0.05284329173155129, train acc: 0.9984\n",
      "loss: 0.04317062189802527, train acc: 0.998\n",
      "epoch: 46, loss: 0.003341032424941659, train acc: 0.998, test acc: 0.9423\n",
      "loss: 0.07345608621835709, train acc: 0.9958\n",
      "loss: 0.046474307775497437, train acc: 0.9976\n",
      "loss: 0.03585244081914425, train acc: 0.9968\n",
      "loss: 0.030964294169098138, train acc: 0.9972\n",
      "loss: 0.04548050574958325, train acc: 0.9985\n",
      "loss: 0.03535192785784602, train acc: 0.9987\n",
      "loss: 0.03245546147227287, train acc: 0.9976\n",
      "loss: 0.037122631631791594, train acc: 0.9966\n",
      "epoch: 47, loss: 0.002216148655861616, train acc: 0.9966, test acc: 0.9406\n",
      "loss: 0.04239887371659279, train acc: 0.9959\n",
      "loss: 0.04556194320321083, train acc: 0.998\n",
      "loss: 0.04176151882857084, train acc: 0.9977\n",
      "loss: 0.038535653706640006, train acc: 0.9968\n",
      "loss: 0.04081833027303219, train acc: 0.9987\n",
      "loss: 0.03329750886186957, train acc: 0.9984\n",
      "loss: 0.03839458082802594, train acc: 0.9979\n",
      "loss: 0.0371825385838747, train acc: 0.998\n",
      "epoch: 48, loss: 0.03746071830391884, train acc: 0.998, test acc: 0.9445\n",
      "loss: 0.03434186428785324, train acc: 0.9978\n",
      "loss: 0.038639398850500585, train acc: 0.9977\n",
      "loss: 0.03476472143083811, train acc: 0.998\n",
      "loss: 0.03796755485236645, train acc: 0.9977\n",
      "loss: 0.04067033622413874, train acc: 0.9986\n",
      "loss: 0.02678850251249969, train acc: 0.9983\n",
      "loss: 0.03844391005113721, train acc: 0.9981\n",
      "loss: 0.028690731432288886, train acc: 0.9982\n",
      "epoch: 49, loss: 0.0024250554852187634, train acc: 0.9982, test acc: 0.9453\n",
      "loss: 0.05801311135292053, train acc: 0.9978\n",
      "loss: 0.038046665769070384, train acc: 0.9984\n",
      "loss: 0.033091869205236435, train acc: 0.9984\n",
      "loss: 0.02847097786143422, train acc: 0.9977\n",
      "loss: 0.029911943525075913, train acc: 0.9993\n",
      "loss: 0.02622115323320031, train acc: 0.9979\n",
      "loss: 0.040100862924009564, train acc: 0.9982\n",
      "loss: 0.03190384297631681, train acc: 0.9972\n",
      "epoch: 50, loss: 0.004732424858957529, train acc: 0.9972, test acc: 0.944\n",
      "loss: 0.04522661864757538, train acc: 0.9972\n",
      "loss: 0.02489667385816574, train acc: 0.9981\n",
      "loss: 0.03554135542362928, train acc: 0.9987\n",
      "loss: 0.033968804962933065, train acc: 0.9973\n",
      "loss: 0.033436764404177664, train acc: 0.9986\n",
      "loss: 0.025639074854552746, train acc: 0.9972\n",
      "loss: 0.042791249230504036, train acc: 0.9982\n",
      "loss: 0.028081377595663072, train acc: 0.998\n",
      "epoch: 51, loss: 0.008117948658764362, train acc: 0.998, test acc: 0.943\n",
      "loss: 0.030307495966553688, train acc: 0.9978\n",
      "loss: 0.034140026476234195, train acc: 0.9988\n",
      "loss: 0.03446799498051405, train acc: 0.9984\n",
      "loss: 0.03018246265128255, train acc: 0.9989\n",
      "loss: 0.02771758530288935, train acc: 0.9988\n",
      "loss: 0.024641142319887878, train acc: 0.9987\n",
      "loss: 0.0321788830216974, train acc: 0.998\n",
      "loss: 0.03238159902393818, train acc: 0.9969\n",
      "epoch: 52, loss: 0.0018527189968153834, train acc: 0.9969, test acc: 0.9442\n",
      "loss: 0.12918640673160553, train acc: 0.9986\n",
      "loss: 0.04348385808989406, train acc: 0.9984\n",
      "loss: 0.02433423213660717, train acc: 0.9984\n",
      "loss: 0.027178533002734186, train acc: 0.9973\n",
      "loss: 0.03318043937906623, train acc: 0.9989\n",
      "loss: 0.04286653632298112, train acc: 0.9979\n",
      "loss: 0.029282499197870494, train acc: 0.9979\n",
      "loss: 0.03230257574468851, train acc: 0.9984\n",
      "epoch: 53, loss: 0.004500037990510464, train acc: 0.9984, test acc: 0.945\n",
      "loss: 0.03810817748308182, train acc: 0.9978\n",
      "loss: 0.03332080896943808, train acc: 0.999\n",
      "loss: 0.03523669820278883, train acc: 0.9985\n",
      "loss: 0.037362121231853965, train acc: 0.9969\n",
      "loss: 0.033585783839225766, train acc: 0.9986\n",
      "loss: 0.027163915755227207, train acc: 0.9987\n",
      "loss: 0.03314754674211144, train acc: 0.9987\n",
      "loss: 0.03215138833038509, train acc: 0.9991\n",
      "epoch: 54, loss: 0.00298700132407248, train acc: 0.9991, test acc: 0.9457\n",
      "loss: 0.03127269819378853, train acc: 0.9981\n",
      "loss: 0.03043847605586052, train acc: 0.9986\n",
      "loss: 0.03251110054552555, train acc: 0.9988\n",
      "loss: 0.02877825330942869, train acc: 0.9975\n",
      "loss: 0.03179630376398564, train acc: 0.9986\n",
      "loss: 0.02068005744367838, train acc: 0.9981\n",
      "loss: 0.024118021689355375, train acc: 0.9985\n",
      "loss: 0.027655037865042687, train acc: 0.998\n",
      "epoch: 55, loss: 0.005466900300234556, train acc: 0.998, test acc: 0.9463\n",
      "loss: 0.034622982144355774, train acc: 0.9982\n",
      "loss: 0.03008295763283968, train acc: 0.9977\n",
      "loss: 0.031769074872136115, train acc: 0.9993\n",
      "loss: 0.0324883590452373, train acc: 0.9983\n",
      "loss: 0.03246207684278488, train acc: 0.9992\n",
      "loss: 0.025953072309494018, train acc: 0.998\n",
      "loss: 0.023695723991841076, train acc: 0.9986\n",
      "loss: 0.018875569570809603, train acc: 0.9986\n",
      "epoch: 56, loss: 0.03597477078437805, train acc: 0.9986, test acc: 0.9474\n",
      "loss: 0.05693761259317398, train acc: 0.9985\n",
      "loss: 0.03137644724920392, train acc: 0.9972\n",
      "loss: 0.04448405597358942, train acc: 0.999\n",
      "loss: 0.03508104551583528, train acc: 0.9983\n",
      "loss: 0.03138485951349139, train acc: 0.999\n",
      "loss: 0.029218930983915925, train acc: 0.9984\n",
      "loss: 0.027112458646297456, train acc: 0.999\n",
      "loss: 0.030261185579001905, train acc: 0.998\n",
      "epoch: 57, loss: 0.0007778472499921918, train acc: 0.998, test acc: 0.9457\n",
      "loss: 0.04012976959347725, train acc: 0.9987\n",
      "loss: 0.026996408123522997, train acc: 0.9985\n",
      "loss: 0.03136761672794819, train acc: 0.9994\n",
      "loss: 0.021216140035539864, train acc: 0.998\n",
      "loss: 0.033215087559074166, train acc: 0.9996\n",
      "loss: 0.029493307787925006, train acc: 0.999\n",
      "loss: 0.024067617021501064, train acc: 0.9987\n",
      "loss: 0.02574518956243992, train acc: 0.9989\n",
      "epoch: 58, loss: 0.003317910013720393, train acc: 0.9989, test acc: 0.9454\n",
      "loss: 0.030640771612524986, train acc: 0.9986\n",
      "loss: 0.021254395553842186, train acc: 0.9987\n",
      "loss: 0.024840262811630966, train acc: 0.9993\n",
      "loss: 0.021201894711703063, train acc: 0.9988\n",
      "loss: 0.03237590780481696, train acc: 0.9994\n",
      "loss: 0.024618752766400576, train acc: 0.9986\n",
      "loss: 0.032924897596240045, train acc: 0.9993\n",
      "loss: 0.03446059850975871, train acc: 0.999\n",
      "epoch: 59, loss: 0.0004748913343064487, train acc: 0.999, test acc: 0.9466\n",
      "loss: 0.01864979788661003, train acc: 0.9996\n",
      "loss: 0.02427359437569976, train acc: 0.9985\n",
      "loss: 0.036289145983755586, train acc: 0.9987\n",
      "loss: 0.02758095059543848, train acc: 0.9992\n",
      "loss: 0.02547291535884142, train acc: 0.9996\n",
      "loss: 0.031130761466920377, train acc: 0.9994\n",
      "loss: 0.01953109707683325, train acc: 0.9989\n",
      "loss: 0.021012880373746157, train acc: 0.9993\n",
      "epoch: 60, loss: 0.016786567866802216, train acc: 0.9993, test acc: 0.9447\n",
      "loss: 0.02838294394314289, train acc: 0.9991\n",
      "loss: 0.025347730424255133, train acc: 0.9994\n",
      "loss: 0.029001004341989756, train acc: 0.9993\n",
      "loss: 0.025386686716228724, train acc: 0.9988\n",
      "loss: 0.04038998242467642, train acc: 0.9995\n",
      "loss: 0.019980294397100806, train acc: 0.9983\n",
      "loss: 0.023003251757472754, train acc: 0.9993\n",
      "loss: 0.025251981522887944, train acc: 0.9989\n",
      "epoch: 61, loss: 0.0058113508857786655, train acc: 0.9989, test acc: 0.9454\n",
      "loss: 0.029741426929831505, train acc: 0.9992\n",
      "loss: 0.027194405626505613, train acc: 0.9994\n",
      "loss: 0.030859966948628425, train acc: 0.9993\n",
      "loss: 0.026875125523656607, train acc: 0.9996\n",
      "loss: 0.03140849797055125, train acc: 0.9994\n",
      "loss: 0.018381058052182198, train acc: 0.9992\n",
      "loss: 0.027317671244964002, train acc: 0.9994\n",
      "loss: 0.027820004895329476, train acc: 0.9989\n",
      "epoch: 62, loss: 0.016568373888731003, train acc: 0.9989, test acc: 0.9454\n",
      "loss: 0.018523044884204865, train acc: 0.9987\n",
      "loss: 0.03272376004606485, train acc: 0.9987\n",
      "loss: 0.027452265750616787, train acc: 0.9993\n",
      "loss: 0.02263705125078559, train acc: 0.9975\n",
      "loss: 0.030304583441466092, train acc: 0.9997\n",
      "loss: 0.020693336706608535, train acc: 0.9996\n",
      "loss: 0.028078046254813672, train acc: 0.9982\n",
      "loss: 0.02806991687975824, train acc: 0.998\n",
      "epoch: 63, loss: 0.005988826043903828, train acc: 0.998, test acc: 0.9456\n",
      "loss: 0.016978109255433083, train acc: 0.9991\n",
      "loss: 0.02803056389093399, train acc: 0.9991\n",
      "loss: 0.02106483457610011, train acc: 0.999\n",
      "loss: 0.020714416261762382, train acc: 0.999\n",
      "loss: 0.018718074169009925, train acc: 0.9993\n",
      "loss: 0.022304447693750264, train acc: 0.9997\n",
      "loss: 0.029658897221088408, train acc: 0.9987\n",
      "loss: 0.02208480560220778, train acc: 0.9988\n",
      "epoch: 64, loss: 0.001104225986637175, train acc: 0.9988, test acc: 0.9449\n",
      "loss: 0.043184682726860046, train acc: 0.9989\n",
      "loss: 0.030715196393430233, train acc: 0.999\n",
      "loss: 0.029312861431390048, train acc: 0.9992\n",
      "loss: 0.02889105388894677, train acc: 0.9992\n",
      "loss: 0.034676648862659934, train acc: 0.9997\n",
      "loss: 0.019002723647281526, train acc: 0.9988\n",
      "loss: 0.020950390491634607, train acc: 0.9994\n",
      "loss: 0.026519120763987304, train acc: 0.9993\n",
      "epoch: 65, loss: 0.0017079036915674806, train acc: 0.9993, test acc: 0.9462\n",
      "loss: 0.018618334084749222, train acc: 0.9993\n",
      "loss: 0.01939007770270109, train acc: 0.9994\n",
      "loss: 0.02296100603416562, train acc: 0.9994\n",
      "loss: 0.02306452253833413, train acc: 0.9996\n",
      "loss: 0.030963621195405723, train acc: 0.9995\n",
      "loss: 0.022915847040712832, train acc: 0.9992\n",
      "loss: 0.030820077005773782, train acc: 0.9991\n",
      "loss: 0.023296446865424515, train acc: 0.9988\n",
      "epoch: 66, loss: 0.0011747892713174224, train acc: 0.9988, test acc: 0.9448\n",
      "loss: 0.039228685200214386, train acc: 0.9994\n",
      "loss: 0.026063123531639577, train acc: 0.9989\n",
      "loss: 0.03617470357567072, train acc: 0.9994\n",
      "loss: 0.021625358890742064, train acc: 0.999\n",
      "loss: 0.037746581342071296, train acc: 0.999\n",
      "loss: 0.022903313487768175, train acc: 0.999\n",
      "loss: 0.02172842714935541, train acc: 0.9992\n",
      "loss: 0.02612740434706211, train acc: 0.9981\n",
      "epoch: 67, loss: 0.016200052574276924, train acc: 0.9981, test acc: 0.946\n",
      "loss: 0.049473509192466736, train acc: 0.9991\n",
      "loss: 0.033517873380333185, train acc: 0.9987\n",
      "loss: 0.023538956232368947, train acc: 0.9995\n",
      "loss: 0.022734980983659624, train acc: 0.999\n",
      "loss: 0.02503415187820792, train acc: 0.9992\n",
      "loss: 0.02271862654015422, train acc: 0.9994\n",
      "loss: 0.025293542677536605, train acc: 0.9993\n",
      "loss: 0.020636629732325672, train acc: 0.9974\n",
      "epoch: 68, loss: 0.0006302142865024507, train acc: 0.9974, test acc: 0.944\n",
      "loss: 0.03293495625257492, train acc: 0.9985\n",
      "loss: 0.02716979067772627, train acc: 0.9988\n",
      "loss: 0.023883480485528706, train acc: 0.999\n",
      "loss: 0.02279638680629432, train acc: 0.9992\n",
      "loss: 0.024933952279388903, train acc: 0.9995\n",
      "loss: 0.02228120993822813, train acc: 0.9996\n",
      "loss: 0.018757683970034123, train acc: 0.9992\n",
      "loss: 0.017730810726061464, train acc: 0.9992\n",
      "epoch: 69, loss: 0.026184216141700745, train acc: 0.9992, test acc: 0.945\n",
      "loss: 0.051645684987306595, train acc: 0.9984\n",
      "loss: 0.02299423599615693, train acc: 0.9988\n",
      "loss: 0.023196741286665202, train acc: 0.9993\n",
      "loss: 0.026826750580221416, train acc: 0.999\n",
      "loss: 0.025971065275371075, train acc: 0.9992\n",
      "loss: 0.016083486983552574, train acc: 0.9997\n",
      "loss: 0.020217257738113403, train acc: 0.999\n",
      "loss: 0.027040279796347023, train acc: 0.999\n",
      "epoch: 70, loss: 0.0025055324658751488, train acc: 0.999, test acc: 0.9447\n",
      "loss: 0.013100501149892807, train acc: 0.9991\n",
      "loss: 0.03576372754760086, train acc: 0.999\n",
      "loss: 0.0248212487436831, train acc: 0.9991\n",
      "loss: 0.018822750402614476, train acc: 0.9989\n",
      "loss: 0.02115689720958471, train acc: 0.9992\n",
      "loss: 0.024859099462628365, train acc: 0.9992\n",
      "loss: 0.022267472092062236, train acc: 0.9991\n",
      "loss: 0.025332739949226378, train acc: 0.999\n",
      "epoch: 71, loss: 0.0005659781745634973, train acc: 0.999, test acc: 0.9433\n",
      "loss: 0.029323279857635498, train acc: 0.999\n",
      "loss: 0.022753588273189962, train acc: 0.9994\n",
      "loss: 0.02045614654198289, train acc: 0.9994\n",
      "loss: 0.03074768898077309, train acc: 0.9991\n",
      "loss: 0.02564750905148685, train acc: 0.9995\n",
      "loss: 0.020686974748969078, train acc: 1.0\n",
      "loss: 0.028778058965690433, train acc: 0.9992\n",
      "loss: 0.023708687722682954, train acc: 0.9987\n",
      "epoch: 72, loss: 0.0012628443073481321, train acc: 0.9987, test acc: 0.9418\n",
      "loss: 0.020571265369653702, train acc: 0.9982\n",
      "loss: 0.01881018131971359, train acc: 0.9988\n",
      "loss: 0.025552669633179904, train acc: 0.9997\n",
      "loss: 0.01821889542043209, train acc: 0.9987\n",
      "loss: 0.021864140359684824, train acc: 0.9997\n",
      "loss: 0.023621970298700036, train acc: 0.999\n",
      "loss: 0.025788905937224625, train acc: 0.9992\n",
      "loss: 0.014719619462266565, train acc: 0.9989\n",
      "epoch: 73, loss: 0.0033630495890975, train acc: 0.9989, test acc: 0.9468\n",
      "loss: 0.01688431203365326, train acc: 0.9993\n",
      "loss: 0.028441774658858776, train acc: 0.9994\n",
      "loss: 0.026817771419882775, train acc: 0.9994\n",
      "loss: 0.028297583386301995, train acc: 0.9989\n",
      "loss: 0.02960269283503294, train acc: 0.9989\n",
      "loss: 0.0300554315559566, train acc: 0.9988\n",
      "loss: 0.01865812512114644, train acc: 0.9997\n",
      "loss: 0.027640265645459294, train acc: 0.9997\n",
      "epoch: 74, loss: 0.03225518763065338, train acc: 0.9997, test acc: 0.9469\n",
      "loss: 0.032556138932704926, train acc: 0.9997\n",
      "loss: 0.02751644633244723, train acc: 0.9993\n",
      "loss: 0.02355858259834349, train acc: 0.9993\n",
      "loss: 0.021173952892422677, train acc: 0.9987\n",
      "loss: 0.032147536240518096, train acc: 0.9995\n",
      "loss: 0.024099845346063374, train acc: 0.9993\n",
      "loss: 0.026140107354149223, train acc: 0.9997\n",
      "loss: 0.013955451594665647, train acc: 0.9998\n",
      "epoch: 75, loss: 0.004083341918885708, train acc: 0.9998, test acc: 0.9431\n",
      "loss: 0.012714052572846413, train acc: 0.9991\n",
      "loss: 0.02078875098377466, train acc: 0.9994\n",
      "loss: 0.027377110812813042, train acc: 0.9997\n",
      "loss: 0.02269083266146481, train acc: 0.9989\n",
      "loss: 0.017176668252795936, train acc: 0.9996\n",
      "loss: 0.01431946991942823, train acc: 0.9997\n",
      "loss: 0.02748236572369933, train acc: 0.9998\n",
      "loss: 0.023577127419412137, train acc: 0.999\n",
      "epoch: 76, loss: 0.002222818788141012, train acc: 0.999, test acc: 0.9444\n",
      "loss: 0.01560301799327135, train acc: 0.9996\n",
      "loss: 0.015051574725657702, train acc: 0.9992\n",
      "loss: 0.018085294775664805, train acc: 0.9998\n",
      "loss: 0.023407473973929883, train acc: 0.9987\n",
      "loss: 0.02628679620102048, train acc: 0.9992\n",
      "loss: 0.021003152895718812, train acc: 0.9994\n",
      "loss: 0.028026920277625322, train acc: 0.9993\n",
      "loss: 0.027532284753397106, train acc: 0.9988\n",
      "epoch: 77, loss: 0.006924707908183336, train acc: 0.9988, test acc: 0.9437\n",
      "loss: 0.019579239189624786, train acc: 0.9991\n",
      "loss: 0.020074884733185173, train acc: 0.9989\n",
      "loss: 0.020246994355693458, train acc: 0.999\n",
      "loss: 0.01795811653137207, train acc: 0.9994\n",
      "loss: 0.02396975178271532, train acc: 0.9999\n",
      "loss: 0.023709756485186518, train acc: 0.9991\n",
      "loss: 0.020623707259073854, train acc: 0.9991\n",
      "loss: 0.022911641374230386, train acc: 0.9995\n",
      "epoch: 78, loss: 0.006550549529492855, train acc: 0.9995, test acc: 0.9452\n",
      "loss: 0.03782423958182335, train acc: 0.9997\n",
      "loss: 0.024896471854299308, train acc: 0.9994\n",
      "loss: 0.017516515124589203, train acc: 0.9992\n",
      "loss: 0.021522471308708192, train acc: 0.9984\n",
      "loss: 0.02110669743269682, train acc: 0.9991\n",
      "loss: 0.023909752536565066, train acc: 0.9994\n",
      "loss: 0.018551217019557954, train acc: 0.9989\n",
      "loss: 0.015379416244104504, train acc: 0.9996\n",
      "epoch: 79, loss: 0.0014534728834405541, train acc: 0.9996, test acc: 0.945\n",
      "#####training and testing end with K:40, P:0.1######\n",
      "#####training and testing start with K:40, P:0.5######\n",
      "loss: 2.330167531967163, train acc: 0.1596\n",
      "loss: 2.1014217853546144, train acc: 0.6055\n",
      "loss: 1.6765811324119568, train acc: 0.7294\n",
      "loss: 1.3267997622489929, train acc: 0.7911\n",
      "loss: 1.184061348438263, train acc: 0.819\n",
      "loss: 1.0531132340431213, train acc: 0.8447\n",
      "loss: 0.9683756530284882, train acc: 0.8653\n",
      "loss: 0.8902132034301757, train acc: 0.8692\n",
      "epoch: 0, loss: 0.587994396686554, train acc: 0.8692, test acc: 0.8775\n",
      "loss: 0.9451104402542114, train acc: 0.8816\n",
      "loss: 0.8286745727062226, train acc: 0.8855\n",
      "loss: 0.7889687299728394, train acc: 0.8828\n",
      "loss: 0.7168976485729217, train acc: 0.8942\n",
      "loss: 0.7494501590728759, train acc: 0.894\n",
      "loss: 0.7039471507072449, train acc: 0.8937\n",
      "loss: 0.7035386383533477, train acc: 0.9012\n",
      "loss: 0.6844221174716949, train acc: 0.8998\n",
      "epoch: 1, loss: 0.17928417026996613, train acc: 0.8998, test acc: 0.8941\n",
      "loss: 0.774760365486145, train acc: 0.9051\n",
      "loss: 0.6765897452831269, train acc: 0.9051\n",
      "loss: 0.6581094145774842, train acc: 0.9044\n",
      "loss: 0.5985936045646667, train acc: 0.9103\n",
      "loss: 0.6189320385456085, train acc: 0.9108\n",
      "loss: 0.6028764188289643, train acc: 0.9078\n",
      "loss: 0.5848552763462067, train acc: 0.9145\n",
      "loss: 0.5874720275402069, train acc: 0.9129\n",
      "epoch: 2, loss: 0.34151437878608704, train acc: 0.9129, test acc: 0.9031\n",
      "loss: 0.6885460019111633, train acc: 0.9148\n",
      "loss: 0.5936571478843689, train acc: 0.9147\n",
      "loss: 0.6290262162685394, train acc: 0.9141\n",
      "loss: 0.5675704777240753, train acc: 0.918\n",
      "loss: 0.5576473563909531, train acc: 0.9182\n",
      "loss: 0.5657791137695313, train acc: 0.9185\n",
      "loss: 0.5546531051397323, train acc: 0.9218\n",
      "loss: 0.5163070082664489, train acc: 0.9224\n",
      "epoch: 3, loss: 0.248586967587471, train acc: 0.9224, test acc: 0.9087\n",
      "loss: 0.6534740328788757, train acc: 0.9217\n",
      "loss: 0.5570558041334153, train acc: 0.9219\n",
      "loss: 0.5105715900659561, train acc: 0.9237\n",
      "loss: 0.5269142329692841, train acc: 0.9248\n",
      "loss: 0.5174592107534408, train acc: 0.9257\n",
      "loss: 0.5105536222457886, train acc: 0.9246\n",
      "loss: 0.5115356028079987, train acc: 0.9299\n",
      "loss: 0.49012826681137084, train acc: 0.9271\n",
      "epoch: 4, loss: 0.13909468054771423, train acc: 0.9271, test acc: 0.9131\n",
      "loss: 0.6930196285247803, train acc: 0.9287\n",
      "loss: 0.5351029932498932, train acc: 0.9297\n",
      "loss: 0.5036354660987854, train acc: 0.9309\n",
      "loss: 0.4960873395204544, train acc: 0.9297\n",
      "loss: 0.5309342592954636, train acc: 0.9285\n",
      "loss: 0.493139261007309, train acc: 0.9301\n",
      "loss: 0.4838907361030579, train acc: 0.9314\n",
      "loss: 0.463455793261528, train acc: 0.9328\n",
      "epoch: 5, loss: 0.24082468450069427, train acc: 0.9328, test acc: 0.9126\n",
      "loss: 0.7732400894165039, train acc: 0.9317\n",
      "loss: 0.516153535246849, train acc: 0.9351\n",
      "loss: 0.46857928931713105, train acc: 0.9344\n",
      "loss: 0.4842816650867462, train acc: 0.9363\n",
      "loss: 0.5053251594305038, train acc: 0.936\n",
      "loss: 0.49714791774749756, train acc: 0.9348\n",
      "loss: 0.501732861995697, train acc: 0.9349\n",
      "loss: 0.4526775062084198, train acc: 0.9387\n",
      "epoch: 6, loss: 0.2645881474018097, train acc: 0.9387, test acc: 0.9177\n",
      "loss: 0.578758955001831, train acc: 0.9374\n",
      "loss: 0.47400401532649994, train acc: 0.9357\n",
      "loss: 0.4593054175376892, train acc: 0.9376\n",
      "loss: 0.4601625263690948, train acc: 0.9406\n",
      "loss: 0.4691000670194626, train acc: 0.9389\n",
      "loss: 0.4316015154123306, train acc: 0.937\n",
      "loss: 0.4868828535079956, train acc: 0.939\n",
      "loss: 0.4368989259004593, train acc: 0.9384\n",
      "epoch: 7, loss: 0.0779457837343216, train acc: 0.9384, test acc: 0.9173\n",
      "loss: 0.49907803535461426, train acc: 0.9406\n",
      "loss: 0.4439486742019653, train acc: 0.9397\n",
      "loss: 0.4742536723613739, train acc: 0.9416\n",
      "loss: 0.4523782879114151, train acc: 0.9429\n",
      "loss: 0.43669180274009706, train acc: 0.9428\n",
      "loss: 0.44025455713272094, train acc: 0.9409\n",
      "loss: 0.4324836164712906, train acc: 0.9437\n",
      "loss: 0.41318237483501435, train acc: 0.944\n",
      "epoch: 8, loss: 0.22799712419509888, train acc: 0.944, test acc: 0.9201\n",
      "loss: 0.5527575016021729, train acc: 0.9444\n",
      "loss: 0.45192110240459443, train acc: 0.9417\n",
      "loss: 0.4463834404945374, train acc: 0.9439\n",
      "loss: 0.42209140956401825, train acc: 0.9436\n",
      "loss: 0.4544064313173294, train acc: 0.9447\n",
      "loss: 0.4260828286409378, train acc: 0.9429\n",
      "loss: 0.41831757724285124, train acc: 0.9455\n",
      "loss: 0.3836123555898666, train acc: 0.9458\n",
      "epoch: 9, loss: 0.17148718237876892, train acc: 0.9458, test acc: 0.9215\n",
      "loss: 0.6445828676223755, train acc: 0.9464\n",
      "loss: 0.4472276046872139, train acc: 0.9474\n",
      "loss: 0.4585753560066223, train acc: 0.9465\n",
      "loss: 0.40898577868938446, train acc: 0.9468\n",
      "loss: 0.44834584295749663, train acc: 0.9475\n",
      "loss: 0.4454408288002014, train acc: 0.9452\n",
      "loss: 0.424182254076004, train acc: 0.9474\n",
      "loss: 0.39180493652820586, train acc: 0.947\n",
      "epoch: 10, loss: 0.15924698114395142, train acc: 0.947, test acc: 0.9239\n",
      "loss: 0.5285600423812866, train acc: 0.95\n",
      "loss: 0.4495295286178589, train acc: 0.9474\n",
      "loss: 0.40015639960765836, train acc: 0.9477\n",
      "loss: 0.4110151767730713, train acc: 0.9467\n",
      "loss: 0.4290298521518707, train acc: 0.95\n",
      "loss: 0.4115911811590195, train acc: 0.9473\n",
      "loss: 0.41363064050674436, train acc: 0.9469\n",
      "loss: 0.3730958104133606, train acc: 0.9469\n",
      "epoch: 11, loss: 0.20954956114292145, train acc: 0.9469, test acc: 0.9248\n",
      "loss: 0.46494999527931213, train acc: 0.95\n",
      "loss: 0.41709505021572113, train acc: 0.9469\n",
      "loss: 0.4346728563308716, train acc: 0.9493\n",
      "loss: 0.42240793704986573, train acc: 0.9502\n",
      "loss: 0.4193587452173233, train acc: 0.9497\n",
      "loss: 0.42387600541114806, train acc: 0.9492\n",
      "loss: 0.3926436275243759, train acc: 0.9495\n",
      "loss: 0.3922847658395767, train acc: 0.9485\n",
      "epoch: 12, loss: 0.07647838443517685, train acc: 0.9485, test acc: 0.9229\n",
      "loss: 0.5063450932502747, train acc: 0.9516\n",
      "loss: 0.3949048548936844, train acc: 0.9499\n",
      "loss: 0.3892974376678467, train acc: 0.9515\n",
      "loss: 0.40230887830257417, train acc: 0.9527\n",
      "loss: 0.401138111948967, train acc: 0.951\n",
      "loss: 0.4024019181728363, train acc: 0.9508\n",
      "loss: 0.39477559030056, train acc: 0.9527\n",
      "loss: 0.3696354776620865, train acc: 0.9496\n",
      "epoch: 13, loss: 0.16254448890686035, train acc: 0.9496, test acc: 0.9248\n",
      "loss: 0.538981020450592, train acc: 0.9535\n",
      "loss: 0.3886060565710068, train acc: 0.9524\n",
      "loss: 0.38197425603866575, train acc: 0.9523\n",
      "loss: 0.3909804493188858, train acc: 0.9535\n",
      "loss: 0.36349124312400816, train acc: 0.9541\n",
      "loss: 0.40093170404434203, train acc: 0.9539\n",
      "loss: 0.3588592618703842, train acc: 0.9527\n",
      "loss: 0.3740264385938644, train acc: 0.9533\n",
      "epoch: 14, loss: 0.10883969813585281, train acc: 0.9533, test acc: 0.9261\n",
      "loss: 0.4481618106365204, train acc: 0.9561\n",
      "loss: 0.384636852145195, train acc: 0.9523\n",
      "loss: 0.3946045398712158, train acc: 0.9534\n",
      "loss: 0.3900217294692993, train acc: 0.9557\n",
      "loss: 0.39837884306907656, train acc: 0.9551\n",
      "loss: 0.3710225075483322, train acc: 0.9554\n",
      "loss: 0.36379751563072205, train acc: 0.9547\n",
      "loss: 0.3686335474252701, train acc: 0.9543\n",
      "epoch: 15, loss: 0.13957281410694122, train acc: 0.9543, test acc: 0.9253\n",
      "loss: 0.47827503085136414, train acc: 0.9554\n",
      "loss: 0.4106150597333908, train acc: 0.9554\n",
      "loss: 0.3686587154865265, train acc: 0.9569\n",
      "loss: 0.35096450746059416, train acc: 0.9557\n",
      "loss: 0.37590130269527433, train acc: 0.9559\n",
      "loss: 0.39635535180568693, train acc: 0.9557\n",
      "loss: 0.39727664589881895, train acc: 0.9547\n",
      "loss: 0.3443091452121735, train acc: 0.9549\n",
      "epoch: 16, loss: 0.33191174268722534, train acc: 0.9549, test acc: 0.9268\n",
      "loss: 0.42978352308273315, train acc: 0.9557\n",
      "loss: 0.3950445532798767, train acc: 0.9546\n",
      "loss: 0.37808134853839875, train acc: 0.9576\n",
      "loss: 0.36676386296749114, train acc: 0.9564\n",
      "loss: 0.37456050515174866, train acc: 0.9579\n",
      "loss: 0.3967788279056549, train acc: 0.9585\n",
      "loss: 0.3602334499359131, train acc: 0.9569\n",
      "loss: 0.31537472903728486, train acc: 0.9555\n",
      "epoch: 17, loss: 0.12315095216035843, train acc: 0.9555, test acc: 0.9286\n",
      "loss: 0.47813934087753296, train acc: 0.958\n",
      "loss: 0.37872203439474106, train acc: 0.9581\n",
      "loss: 0.3588629215955734, train acc: 0.9582\n",
      "loss: 0.3648363173007965, train acc: 0.9589\n",
      "loss: 0.3435859173536301, train acc: 0.9579\n",
      "loss: 0.3855461984872818, train acc: 0.9586\n",
      "loss: 0.353738009929657, train acc: 0.9574\n",
      "loss: 0.362623530626297, train acc: 0.9576\n",
      "epoch: 18, loss: 0.18309347331523895, train acc: 0.9576, test acc: 0.9257\n",
      "loss: 0.5276991128921509, train acc: 0.9589\n",
      "loss: 0.3904083341360092, train acc: 0.9597\n",
      "loss: 0.37338305711746217, train acc: 0.9596\n",
      "loss: 0.35223460495471953, train acc: 0.9571\n",
      "loss: 0.3912465512752533, train acc: 0.9595\n",
      "loss: 0.38136591017246246, train acc: 0.9577\n",
      "loss: 0.36571372151374815, train acc: 0.9582\n",
      "loss: 0.30946825444698334, train acc: 0.959\n",
      "epoch: 19, loss: 0.0662546381354332, train acc: 0.959, test acc: 0.9289\n",
      "loss: 0.42272132635116577, train acc: 0.9612\n",
      "loss: 0.37396847903728486, train acc: 0.958\n",
      "loss: 0.33807710111141204, train acc: 0.9604\n",
      "loss: 0.3642939984798431, train acc: 0.9603\n",
      "loss: 0.3597469314932823, train acc: 0.9609\n",
      "loss: 0.37432182133197783, train acc: 0.9599\n",
      "loss: 0.3500270158052444, train acc: 0.9594\n",
      "loss: 0.34243394136428834, train acc: 0.9603\n",
      "epoch: 20, loss: 0.12368462979793549, train acc: 0.9603, test acc: 0.9279\n",
      "loss: 0.34449607133865356, train acc: 0.9615\n",
      "loss: 0.3325838282704353, train acc: 0.9611\n",
      "loss: 0.3493905246257782, train acc: 0.9621\n",
      "loss: 0.36047421395778656, train acc: 0.9625\n",
      "loss: 0.39607343822717667, train acc: 0.9622\n",
      "loss: 0.34303438663482666, train acc: 0.9614\n",
      "loss: 0.33659453094005587, train acc: 0.9611\n",
      "loss: 0.3703929603099823, train acc: 0.962\n",
      "epoch: 21, loss: 0.07892141491174698, train acc: 0.962, test acc: 0.9292\n",
      "loss: 0.45399922132492065, train acc: 0.9629\n",
      "loss: 0.3522174745798111, train acc: 0.9636\n",
      "loss: 0.34080316722393034, train acc: 0.963\n",
      "loss: 0.33874871730804446, train acc: 0.9632\n",
      "loss: 0.3220110505819321, train acc: 0.9615\n",
      "loss: 0.3108566850423813, train acc: 0.9624\n",
      "loss: 0.33608856201171877, train acc: 0.9627\n",
      "loss: 0.315301188826561, train acc: 0.9622\n",
      "epoch: 22, loss: 0.05946846678853035, train acc: 0.9622, test acc: 0.93\n",
      "loss: 0.43512162566185, train acc: 0.9645\n",
      "loss: 0.3834798812866211, train acc: 0.9628\n",
      "loss: 0.3466242849826813, train acc: 0.9625\n",
      "loss: 0.3429287180304527, train acc: 0.9628\n",
      "loss: 0.35183214843273164, train acc: 0.9644\n",
      "loss: 0.3508323237299919, train acc: 0.9626\n",
      "loss: 0.31733097732067106, train acc: 0.9613\n",
      "loss: 0.3125086784362793, train acc: 0.9615\n",
      "epoch: 23, loss: 0.19928252696990967, train acc: 0.9615, test acc: 0.9295\n",
      "loss: 0.44200390577316284, train acc: 0.9645\n",
      "loss: 0.33955408334732057, train acc: 0.9614\n",
      "loss: 0.3601206123828888, train acc: 0.963\n",
      "loss: 0.32464814782142637, train acc: 0.9641\n",
      "loss: 0.3262331739068031, train acc: 0.9649\n",
      "loss: 0.34220321774482726, train acc: 0.9653\n",
      "loss: 0.33266051709651945, train acc: 0.9647\n",
      "loss: 0.33009282052516936, train acc: 0.9648\n",
      "epoch: 24, loss: 0.1756686270236969, train acc: 0.9648, test acc: 0.9298\n",
      "loss: 0.5057289600372314, train acc: 0.9665\n",
      "loss: 0.3669422596693039, train acc: 0.9648\n",
      "loss: 0.3117002293467522, train acc: 0.9652\n",
      "loss: 0.33739353120327, train acc: 0.965\n",
      "loss: 0.33982615172863007, train acc: 0.9649\n",
      "loss: 0.3602434813976288, train acc: 0.9651\n",
      "loss: 0.33785609900951385, train acc: 0.9635\n",
      "loss: 0.31913629323244097, train acc: 0.9644\n",
      "epoch: 25, loss: 0.07760314643383026, train acc: 0.9644, test acc: 0.9299\n",
      "loss: 0.4337286651134491, train acc: 0.9668\n",
      "loss: 0.33007489442825316, train acc: 0.9658\n",
      "loss: 0.32277346551418307, train acc: 0.9664\n",
      "loss: 0.3568981051445007, train acc: 0.9675\n",
      "loss: 0.3379339501261711, train acc: 0.9672\n",
      "loss: 0.3179834097623825, train acc: 0.9654\n",
      "loss: 0.3381974518299103, train acc: 0.9649\n",
      "loss: 0.32014261186122894, train acc: 0.9642\n",
      "epoch: 26, loss: 0.13873730599880219, train acc: 0.9642, test acc: 0.9306\n",
      "loss: 0.44454795122146606, train acc: 0.9683\n",
      "loss: 0.3543289229273796, train acc: 0.965\n",
      "loss: 0.3504460781812668, train acc: 0.9654\n",
      "loss: 0.3410859614610672, train acc: 0.9665\n",
      "loss: 0.3577337503433228, train acc: 0.9675\n",
      "loss: 0.3472614288330078, train acc: 0.9662\n",
      "loss: 0.31939317882061, train acc: 0.9652\n",
      "loss: 0.3110555365681648, train acc: 0.9667\n",
      "epoch: 27, loss: 0.12136583030223846, train acc: 0.9667, test acc: 0.9308\n",
      "loss: 0.406160444021225, train acc: 0.9683\n",
      "loss: 0.31784883290529253, train acc: 0.9676\n",
      "loss: 0.3050784707069397, train acc: 0.9689\n",
      "loss: 0.32026001065969467, train acc: 0.9672\n",
      "loss: 0.33999147862195966, train acc: 0.9673\n",
      "loss: 0.31408016085624696, train acc: 0.9677\n",
      "loss: 0.31753508299589156, train acc: 0.9667\n",
      "loss: 0.31465439349412916, train acc: 0.9683\n",
      "epoch: 28, loss: 0.06417644768953323, train acc: 0.9683, test acc: 0.9302\n",
      "loss: 0.4981267750263214, train acc: 0.9699\n",
      "loss: 0.34381684511899946, train acc: 0.9662\n",
      "loss: 0.34504412710666654, train acc: 0.9687\n",
      "loss: 0.3112161546945572, train acc: 0.9683\n",
      "loss: 0.3334595516324043, train acc: 0.968\n",
      "loss: 0.3664923205971718, train acc: 0.9681\n",
      "loss: 0.31711275577545167, train acc: 0.9676\n",
      "loss: 0.32729735523462294, train acc: 0.9679\n",
      "epoch: 29, loss: 0.1157693862915039, train acc: 0.9679, test acc: 0.9303\n",
      "loss: 0.4243263900279999, train acc: 0.969\n",
      "loss: 0.3079530283808708, train acc: 0.9651\n",
      "loss: 0.327816778421402, train acc: 0.9682\n",
      "loss: 0.320052307844162, train acc: 0.9685\n",
      "loss: 0.3430885702371597, train acc: 0.9701\n",
      "loss: 0.3241742208600044, train acc: 0.9683\n",
      "loss: 0.31782970726490023, train acc: 0.966\n",
      "loss: 0.3034437119960785, train acc: 0.9683\n",
      "epoch: 30, loss: 0.07347523421049118, train acc: 0.9683, test acc: 0.9313\n",
      "loss: 0.4483548104763031, train acc: 0.9684\n",
      "loss: 0.3338415905833244, train acc: 0.9692\n",
      "loss: 0.32732759565114977, train acc: 0.9686\n",
      "loss: 0.3101930722594261, train acc: 0.9689\n",
      "loss: 0.32561410069465635, train acc: 0.9695\n",
      "loss: 0.33009378910064696, train acc: 0.97\n",
      "loss: 0.30056858956813814, train acc: 0.9681\n",
      "loss: 0.309117029607296, train acc: 0.9686\n",
      "epoch: 31, loss: 0.2304374724626541, train acc: 0.9686, test acc: 0.9294\n",
      "loss: 0.4455474317073822, train acc: 0.9692\n",
      "loss: 0.34932448267936705, train acc: 0.9705\n",
      "loss: 0.33817148953676224, train acc: 0.9695\n",
      "loss: 0.3341419041156769, train acc: 0.97\n",
      "loss: 0.30579385459423064, train acc: 0.97\n",
      "loss: 0.30693536549806594, train acc: 0.9689\n",
      "loss: 0.32180448770523074, train acc: 0.9685\n",
      "loss: 0.3380474954843521, train acc: 0.9692\n",
      "epoch: 32, loss: 0.13686317205429077, train acc: 0.9692, test acc: 0.9319\n",
      "loss: 0.39022576808929443, train acc: 0.9717\n",
      "loss: 0.3241120740771294, train acc: 0.9679\n",
      "loss: 0.3278555780649185, train acc: 0.9707\n",
      "loss: 0.3239278197288513, train acc: 0.9701\n",
      "loss: 0.311298044025898, train acc: 0.9692\n",
      "loss: 0.309328693151474, train acc: 0.9674\n",
      "loss: 0.3234821081161499, train acc: 0.9681\n",
      "loss: 0.3005417972803116, train acc: 0.9695\n",
      "epoch: 33, loss: 0.11580756306648254, train acc: 0.9695, test acc: 0.9327\n",
      "loss: 0.3444472849369049, train acc: 0.9708\n",
      "loss: 0.29968937784433364, train acc: 0.9704\n",
      "loss: 0.3167789071798325, train acc: 0.9707\n",
      "loss: 0.31966208964586257, train acc: 0.9706\n",
      "loss: 0.33049542307853697, train acc: 0.9707\n",
      "loss: 0.3205286219716072, train acc: 0.971\n",
      "loss: 0.2982913374900818, train acc: 0.9702\n",
      "loss: 0.2747307986021042, train acc: 0.9695\n",
      "epoch: 34, loss: 0.0395202562212944, train acc: 0.9695, test acc: 0.9299\n",
      "loss: 0.4359693229198456, train acc: 0.9709\n",
      "loss: 0.30745898336172106, train acc: 0.9693\n",
      "loss: 0.34172966331243515, train acc: 0.9726\n",
      "loss: 0.3068174868822098, train acc: 0.973\n",
      "loss: 0.3093677818775177, train acc: 0.9712\n",
      "loss: 0.3248269885778427, train acc: 0.9705\n",
      "loss: 0.31011307537555693, train acc: 0.973\n",
      "loss: 0.28867197334766387, train acc: 0.9728\n",
      "epoch: 35, loss: 0.2638859450817108, train acc: 0.9728, test acc: 0.9335\n",
      "loss: 0.3568527400493622, train acc: 0.9722\n",
      "loss: 0.3182211250066757, train acc: 0.9715\n",
      "loss: 0.34165955781936647, train acc: 0.972\n",
      "loss: 0.328414548933506, train acc: 0.9723\n",
      "loss: 0.3110513135790825, train acc: 0.9717\n",
      "loss: 0.30534456074237826, train acc: 0.9723\n",
      "loss: 0.3260891646146774, train acc: 0.9718\n",
      "loss: 0.3191572606563568, train acc: 0.9714\n",
      "epoch: 36, loss: 0.07496227324008942, train acc: 0.9714, test acc: 0.9344\n",
      "loss: 0.46208447217941284, train acc: 0.9728\n",
      "loss: 0.3073219656944275, train acc: 0.9708\n",
      "loss: 0.30391310155391693, train acc: 0.9712\n",
      "loss: 0.3066969275474548, train acc: 0.9731\n",
      "loss: 0.3151039585471153, train acc: 0.9715\n",
      "loss: 0.31465470641851423, train acc: 0.9716\n",
      "loss: 0.29335368871688844, train acc: 0.9712\n",
      "loss: 0.2985985726118088, train acc: 0.9722\n",
      "epoch: 37, loss: 0.09202063083648682, train acc: 0.9722, test acc: 0.9326\n",
      "loss: 0.44130632281303406, train acc: 0.9729\n",
      "loss: 0.3003922149538994, train acc: 0.9745\n",
      "loss: 0.2695850759744644, train acc: 0.974\n",
      "loss: 0.3011944815516472, train acc: 0.9739\n",
      "loss: 0.2956889718770981, train acc: 0.9738\n",
      "loss: 0.29492727518081663, train acc: 0.9721\n",
      "loss: 0.30250123590230943, train acc: 0.9724\n",
      "loss: 0.2999920055270195, train acc: 0.9735\n",
      "epoch: 38, loss: 0.08261757344007492, train acc: 0.9735, test acc: 0.9329\n",
      "loss: 0.34997910261154175, train acc: 0.9742\n",
      "loss: 0.3078341245651245, train acc: 0.9728\n",
      "loss: 0.29510798752307893, train acc: 0.9731\n",
      "loss: 0.3243062913417816, train acc: 0.9722\n",
      "loss: 0.3192885920405388, train acc: 0.9728\n",
      "loss: 0.2861748367547989, train acc: 0.9722\n",
      "loss: 0.2838434413075447, train acc: 0.9742\n",
      "loss: 0.2937789589166641, train acc: 0.975\n",
      "epoch: 39, loss: 0.04228021949529648, train acc: 0.975, test acc: 0.9342\n",
      "loss: 0.39231210947036743, train acc: 0.9735\n",
      "loss: 0.31656359881162643, train acc: 0.9733\n",
      "loss: 0.30116512775421145, train acc: 0.9732\n",
      "loss: 0.28783640414476397, train acc: 0.9747\n",
      "loss: 0.3113608181476593, train acc: 0.9734\n",
      "loss: 0.3103808224201202, train acc: 0.9755\n",
      "loss: 0.3099621206521988, train acc: 0.9741\n",
      "loss: 0.2751620501279831, train acc: 0.9724\n",
      "epoch: 40, loss: 0.07454653829336166, train acc: 0.9724, test acc: 0.9328\n",
      "loss: 0.5191372036933899, train acc: 0.9741\n",
      "loss: 0.3202509805560112, train acc: 0.973\n",
      "loss: 0.33541308492422106, train acc: 0.9743\n",
      "loss: 0.2988175541162491, train acc: 0.9749\n",
      "loss: 0.2816716209053993, train acc: 0.9737\n",
      "loss: 0.2968281716108322, train acc: 0.9721\n",
      "loss: 0.2777513787150383, train acc: 0.9738\n",
      "loss: 0.2941953897476196, train acc: 0.9752\n",
      "epoch: 41, loss: 0.010359937325119972, train acc: 0.9752, test acc: 0.9343\n",
      "loss: 0.33234113454818726, train acc: 0.9744\n",
      "loss: 0.29350789040327074, train acc: 0.9729\n",
      "loss: 0.29625339061021805, train acc: 0.9756\n",
      "loss: 0.3029956504702568, train acc: 0.9747\n",
      "loss: 0.3254781275987625, train acc: 0.9744\n",
      "loss: 0.29680095613002777, train acc: 0.974\n",
      "loss: 0.2948491260409355, train acc: 0.973\n",
      "loss: 0.2936643987894058, train acc: 0.9756\n",
      "epoch: 42, loss: 0.20498211681842804, train acc: 0.9756, test acc: 0.9321\n",
      "loss: 0.3204936385154724, train acc: 0.976\n",
      "loss: 0.31780767142772676, train acc: 0.9744\n",
      "loss: 0.3283085972070694, train acc: 0.9759\n",
      "loss: 0.27737783193588256, train acc: 0.9764\n",
      "loss: 0.2980296239256859, train acc: 0.9761\n",
      "loss: 0.2739368662238121, train acc: 0.9746\n",
      "loss: 0.25554727017879486, train acc: 0.9736\n",
      "loss: 0.2899845436215401, train acc: 0.9733\n",
      "epoch: 43, loss: 0.053019072860479355, train acc: 0.9733, test acc: 0.9316\n",
      "loss: 0.31357452273368835, train acc: 0.9756\n",
      "loss: 0.28942524194717406, train acc: 0.9744\n",
      "loss: 0.3026677057147026, train acc: 0.9752\n",
      "loss: 0.28847826421260836, train acc: 0.9754\n",
      "loss: 0.31202451437711715, train acc: 0.9748\n",
      "loss: 0.28077512979507446, train acc: 0.9759\n",
      "loss: 0.25811176300048827, train acc: 0.975\n",
      "loss: 0.2842428207397461, train acc: 0.976\n",
      "epoch: 44, loss: 0.019829899072647095, train acc: 0.976, test acc: 0.9306\n",
      "loss: 0.39167022705078125, train acc: 0.9759\n",
      "loss: 0.2714891374111176, train acc: 0.9758\n",
      "loss: 0.2995449542999268, train acc: 0.9771\n",
      "loss: 0.29088355898857116, train acc: 0.977\n",
      "loss: 0.3094448059797287, train acc: 0.977\n",
      "loss: 0.2975939750671387, train acc: 0.9769\n",
      "loss: 0.2885959804058075, train acc: 0.977\n",
      "loss: 0.2847387880086899, train acc: 0.9775\n",
      "epoch: 45, loss: 0.09041349589824677, train acc: 0.9775, test acc: 0.9316\n",
      "loss: 0.44933784008026123, train acc: 0.9774\n",
      "loss: 0.2842829987406731, train acc: 0.9764\n",
      "loss: 0.3024399414658546, train acc: 0.9767\n",
      "loss: 0.28256874680519106, train acc: 0.977\n",
      "loss: 0.2613927885890007, train acc: 0.9771\n",
      "loss: 0.28035486340522764, train acc: 0.9762\n",
      "loss: 0.28265976160764694, train acc: 0.9772\n",
      "loss: 0.2692479595541954, train acc: 0.9776\n",
      "epoch: 46, loss: 0.20297764241695404, train acc: 0.9776, test acc: 0.9301\n",
      "loss: 0.33463194966316223, train acc: 0.9758\n",
      "loss: 0.29909871369600294, train acc: 0.9737\n",
      "loss: 0.29552838355302813, train acc: 0.977\n",
      "loss: 0.27929033935070036, train acc: 0.9768\n",
      "loss: 0.2929244726896286, train acc: 0.9773\n",
      "loss: 0.30276247709989546, train acc: 0.9769\n",
      "loss: 0.26823455691337583, train acc: 0.9779\n",
      "loss: 0.2732776805758476, train acc: 0.9766\n",
      "epoch: 47, loss: 0.003933966625481844, train acc: 0.9766, test acc: 0.9329\n",
      "loss: 0.3194934129714966, train acc: 0.9775\n",
      "loss: 0.280833537876606, train acc: 0.9788\n",
      "loss: 0.2870324596762657, train acc: 0.9771\n",
      "loss: 0.29478398710489273, train acc: 0.9762\n",
      "loss: 0.2876672580838203, train acc: 0.9771\n",
      "loss: 0.2812305808067322, train acc: 0.9779\n",
      "loss: 0.272718945145607, train acc: 0.9763\n",
      "loss: 0.28373615592718127, train acc: 0.975\n",
      "epoch: 48, loss: 0.05331609770655632, train acc: 0.975, test acc: 0.9326\n",
      "loss: 0.33429908752441406, train acc: 0.9767\n",
      "loss: 0.28470588475465775, train acc: 0.9754\n",
      "loss: 0.2615741312503815, train acc: 0.9776\n",
      "loss: 0.2737625911831856, train acc: 0.9776\n",
      "loss: 0.26117104291915894, train acc: 0.9762\n",
      "loss: 0.2611896857619286, train acc: 0.9783\n",
      "loss: 0.3085916370153427, train acc: 0.9783\n",
      "loss: 0.2724181294441223, train acc: 0.9783\n",
      "epoch: 49, loss: 0.16948449611663818, train acc: 0.9783, test acc: 0.9318\n",
      "loss: 0.37027668952941895, train acc: 0.9794\n",
      "loss: 0.31239780634641645, train acc: 0.9778\n",
      "loss: 0.28305579870939257, train acc: 0.9754\n",
      "loss: 0.301322466135025, train acc: 0.9792\n",
      "loss: 0.24576202929019927, train acc: 0.978\n",
      "loss: 0.2917971536517143, train acc: 0.9781\n",
      "loss: 0.2632569715380669, train acc: 0.979\n",
      "loss: 0.25473686307668686, train acc: 0.9764\n",
      "epoch: 50, loss: 0.06466604769229889, train acc: 0.9764, test acc: 0.9326\n",
      "loss: 0.3872055113315582, train acc: 0.9775\n",
      "loss: 0.28377472162246703, train acc: 0.9772\n",
      "loss: 0.2919553115963936, train acc: 0.9779\n",
      "loss: 0.2726800933480263, train acc: 0.9788\n",
      "loss: 0.2932168677449226, train acc: 0.9775\n",
      "loss: 0.2684054672718048, train acc: 0.9772\n",
      "loss: 0.3038109719753265, train acc: 0.98\n",
      "loss: 0.2619108632206917, train acc: 0.9789\n",
      "epoch: 51, loss: 0.0213147159665823, train acc: 0.9789, test acc: 0.9322\n",
      "loss: 0.270759642124176, train acc: 0.9779\n",
      "loss: 0.2810381680727005, train acc: 0.9777\n",
      "loss: 0.2835747078061104, train acc: 0.9789\n",
      "loss: 0.26753153949975966, train acc: 0.9783\n",
      "loss: 0.2802557349205017, train acc: 0.9792\n",
      "loss: 0.2866056188941002, train acc: 0.9795\n",
      "loss: 0.2774791747331619, train acc: 0.979\n",
      "loss: 0.2727261558175087, train acc: 0.9775\n",
      "epoch: 52, loss: 0.11564578860998154, train acc: 0.9775, test acc: 0.933\n",
      "loss: 0.3464968204498291, train acc: 0.9777\n",
      "loss: 0.29176218509674073, train acc: 0.9774\n",
      "loss: 0.28964646756649015, train acc: 0.9787\n",
      "loss: 0.2715384364128113, train acc: 0.9793\n",
      "loss: 0.29275224506855013, train acc: 0.979\n",
      "loss: 0.291974613070488, train acc: 0.9787\n",
      "loss: 0.26833702623844147, train acc: 0.9801\n",
      "loss: 0.2550286650657654, train acc: 0.9803\n",
      "epoch: 53, loss: 0.06351524591445923, train acc: 0.9803, test acc: 0.931\n",
      "loss: 0.38269829750061035, train acc: 0.9785\n",
      "loss: 0.31238677352666855, train acc: 0.9803\n",
      "loss: 0.28695576041936877, train acc: 0.9795\n",
      "loss: 0.26546541452407835, train acc: 0.9799\n",
      "loss: 0.28605387210845945, train acc: 0.9788\n",
      "loss: 0.25876824259757997, train acc: 0.9796\n",
      "loss: 0.2853193238377571, train acc: 0.9797\n",
      "loss: 0.27786420434713366, train acc: 0.9786\n",
      "epoch: 54, loss: 0.16147468984127045, train acc: 0.9786, test acc: 0.9283\n",
      "loss: 0.28135302662849426, train acc: 0.9795\n",
      "loss: 0.25317868292331697, train acc: 0.978\n",
      "loss: 0.2824618950486183, train acc: 0.9803\n",
      "loss: 0.2918438196182251, train acc: 0.9807\n",
      "loss: 0.2667967826128006, train acc: 0.9807\n",
      "loss: 0.26263594031333926, train acc: 0.9795\n",
      "loss: 0.2512217670679092, train acc: 0.9785\n",
      "loss: 0.2625453144311905, train acc: 0.9773\n",
      "epoch: 55, loss: 0.061679959297180176, train acc: 0.9773, test acc: 0.9305\n",
      "loss: 0.34821730852127075, train acc: 0.9784\n",
      "loss: 0.2857215851545334, train acc: 0.9771\n",
      "loss: 0.28498626947402955, train acc: 0.9799\n",
      "loss: 0.2764452427625656, train acc: 0.9799\n",
      "loss: 0.28249042183160783, train acc: 0.9803\n",
      "loss: 0.2639507681131363, train acc: 0.9786\n",
      "loss: 0.2773663491010666, train acc: 0.9794\n",
      "loss: 0.2623834162950516, train acc: 0.9782\n",
      "epoch: 56, loss: 0.05031081289052963, train acc: 0.9782, test acc: 0.9304\n",
      "loss: 0.4018171727657318, train acc: 0.98\n",
      "loss: 0.3090317264199257, train acc: 0.9792\n",
      "loss: 0.2678519323468208, train acc: 0.98\n",
      "loss: 0.28300866186618806, train acc: 0.9788\n",
      "loss: 0.27085238248109816, train acc: 0.9807\n",
      "loss: 0.2708464652299881, train acc: 0.9794\n",
      "loss: 0.26876506507396697, train acc: 0.9783\n",
      "loss: 0.2656091943383217, train acc: 0.9778\n",
      "epoch: 57, loss: 0.036647628992795944, train acc: 0.9778, test acc: 0.9324\n",
      "loss: 0.27646586298942566, train acc: 0.9803\n",
      "loss: 0.28568297177553176, train acc: 0.9803\n",
      "loss: 0.28199882209300997, train acc: 0.9797\n",
      "loss: 0.2816919147968292, train acc: 0.9803\n",
      "loss: 0.2580685168504715, train acc: 0.9798\n",
      "loss: 0.29206754118204115, train acc: 0.9791\n",
      "loss: 0.2698463395237923, train acc: 0.9812\n",
      "loss: 0.25540932863950727, train acc: 0.9817\n",
      "epoch: 58, loss: 0.12498711049556732, train acc: 0.9817, test acc: 0.9326\n",
      "loss: 0.374250203371048, train acc: 0.9808\n",
      "loss: 0.28652441054582595, train acc: 0.9806\n",
      "loss: 0.2765298068523407, train acc: 0.9812\n",
      "loss: 0.27098713517189027, train acc: 0.9822\n",
      "loss: 0.27975269705057143, train acc: 0.9808\n",
      "loss: 0.2595866620540619, train acc: 0.9813\n",
      "loss: 0.24096646755933762, train acc: 0.9798\n",
      "loss: 0.2463271960616112, train acc: 0.9792\n",
      "epoch: 59, loss: 0.19852589070796967, train acc: 0.9792, test acc: 0.9284\n",
      "loss: 0.29366156458854675, train acc: 0.9796\n",
      "loss: 0.23463965505361556, train acc: 0.9792\n",
      "loss: 0.2710557237267494, train acc: 0.9802\n",
      "loss: 0.2537236765027046, train acc: 0.9808\n",
      "loss: 0.2891134351491928, train acc: 0.98\n",
      "loss: 0.3017234742641449, train acc: 0.9815\n",
      "loss: 0.28928522318601607, train acc: 0.9816\n",
      "loss: 0.2617248460650444, train acc: 0.9815\n",
      "epoch: 60, loss: 0.18589378893375397, train acc: 0.9815, test acc: 0.9294\n",
      "loss: 0.23561276495456696, train acc: 0.981\n",
      "loss: 0.27499558627605436, train acc: 0.9816\n",
      "loss: 0.2778731226921082, train acc: 0.9826\n",
      "loss: 0.25525905936956406, train acc: 0.9809\n",
      "loss: 0.2789832055568695, train acc: 0.9803\n",
      "loss: 0.2542694464325905, train acc: 0.981\n",
      "loss: 0.2385627344250679, train acc: 0.9816\n",
      "loss: 0.2581723749637604, train acc: 0.9801\n",
      "epoch: 61, loss: 0.039615895599126816, train acc: 0.9801, test acc: 0.9319\n",
      "loss: 0.3639655113220215, train acc: 0.9813\n",
      "loss: 0.2794096142053604, train acc: 0.981\n",
      "loss: 0.2696498021483421, train acc: 0.9819\n",
      "loss: 0.28431340008974076, train acc: 0.9828\n",
      "loss: 0.2563580974936485, train acc: 0.9825\n",
      "loss: 0.27651876360177996, train acc: 0.9828\n",
      "loss: 0.268575856089592, train acc: 0.981\n",
      "loss: 0.26688418835401534, train acc: 0.9807\n",
      "epoch: 62, loss: 0.02342376857995987, train acc: 0.9807, test acc: 0.9329\n",
      "loss: 0.37062281370162964, train acc: 0.982\n",
      "loss: 0.2816713646054268, train acc: 0.982\n",
      "loss: 0.2760835275053978, train acc: 0.9825\n",
      "loss: 0.25576890259981155, train acc: 0.983\n",
      "loss: 0.25726964622735976, train acc: 0.9822\n",
      "loss: 0.2932013586163521, train acc: 0.9815\n",
      "loss: 0.2802030727267265, train acc: 0.9809\n",
      "loss: 0.24898678809404373, train acc: 0.9802\n",
      "epoch: 63, loss: 0.051992643624544144, train acc: 0.9802, test acc: 0.9324\n",
      "loss: 0.36827588081359863, train acc: 0.9827\n",
      "loss: 0.2699455991387367, train acc: 0.9818\n",
      "loss: 0.2634157419204712, train acc: 0.9805\n",
      "loss: 0.2588146135210991, train acc: 0.9816\n",
      "loss: 0.28240593522787094, train acc: 0.9814\n",
      "loss: 0.27829294055700304, train acc: 0.9836\n",
      "loss: 0.26268022656440737, train acc: 0.9821\n",
      "loss: 0.26763847917318345, train acc: 0.9827\n",
      "epoch: 64, loss: 0.05253909155726433, train acc: 0.9827, test acc: 0.9305\n",
      "loss: 0.35607120394706726, train acc: 0.9829\n",
      "loss: 0.2985427528619766, train acc: 0.9829\n",
      "loss: 0.24803369492292404, train acc: 0.9825\n",
      "loss: 0.2568847581744194, train acc: 0.9819\n",
      "loss: 0.2767173543572426, train acc: 0.9807\n",
      "loss: 0.24495983868837357, train acc: 0.9808\n",
      "loss: 0.2543561741709709, train acc: 0.9818\n",
      "loss: 0.23337938487529755, train acc: 0.9823\n",
      "epoch: 65, loss: 0.11328957229852676, train acc: 0.9823, test acc: 0.931\n",
      "loss: 0.38169679045677185, train acc: 0.9811\n",
      "loss: 0.2732858657836914, train acc: 0.9806\n",
      "loss: 0.25930958688259126, train acc: 0.9802\n",
      "loss: 0.2790289118885994, train acc: 0.9835\n",
      "loss: 0.2757378965616226, train acc: 0.9824\n",
      "loss: 0.25505383759737016, train acc: 0.9831\n",
      "loss: 0.26663740128278735, train acc: 0.9818\n",
      "loss: 0.2543113812804222, train acc: 0.9831\n",
      "epoch: 66, loss: 0.09903246909379959, train acc: 0.9831, test acc: 0.9286\n",
      "loss: 0.3983600437641144, train acc: 0.9815\n",
      "loss: 0.2895214930176735, train acc: 0.9801\n",
      "loss: 0.2682077795267105, train acc: 0.9834\n",
      "loss: 0.24269332736730576, train acc: 0.9817\n",
      "loss: 0.26226173937320707, train acc: 0.9823\n",
      "loss: 0.258358807861805, train acc: 0.9816\n",
      "loss: 0.25538318902254104, train acc: 0.9823\n",
      "loss: 0.266936157643795, train acc: 0.9819\n",
      "epoch: 67, loss: 0.04404774308204651, train acc: 0.9819, test acc: 0.9303\n",
      "loss: 0.34951674938201904, train acc: 0.9826\n",
      "loss: 0.2525419771671295, train acc: 0.9832\n",
      "loss: 0.2453322172164917, train acc: 0.9824\n",
      "loss: 0.24063462466001512, train acc: 0.9825\n",
      "loss: 0.2613788515329361, train acc: 0.9823\n",
      "loss: 0.24459222406148912, train acc: 0.9816\n",
      "loss: 0.2519292488694191, train acc: 0.9827\n",
      "loss: 0.22409280836582185, train acc: 0.981\n",
      "epoch: 68, loss: 0.014331953600049019, train acc: 0.981, test acc: 0.9291\n",
      "loss: 0.37859079241752625, train acc: 0.9838\n",
      "loss: 0.26152998208999634, train acc: 0.9828\n",
      "loss: 0.24373072236776352, train acc: 0.9826\n",
      "loss: 0.2682501241564751, train acc: 0.982\n",
      "loss: 0.2595684602856636, train acc: 0.9832\n",
      "loss: 0.2502997800707817, train acc: 0.9821\n",
      "loss: 0.2496014192700386, train acc: 0.9813\n",
      "loss: 0.2470698297023773, train acc: 0.9829\n",
      "epoch: 69, loss: 0.22013631463050842, train acc: 0.9829, test acc: 0.93\n",
      "loss: 0.3878147006034851, train acc: 0.9835\n",
      "loss: 0.2697235569357872, train acc: 0.983\n",
      "loss: 0.2604143887758255, train acc: 0.9847\n",
      "loss: 0.24528766870498658, train acc: 0.9839\n",
      "loss: 0.26340296417474746, train acc: 0.9836\n",
      "loss: 0.2476511836051941, train acc: 0.9831\n",
      "loss: 0.2681923478841782, train acc: 0.985\n",
      "loss: 0.2231538787484169, train acc: 0.9836\n",
      "epoch: 70, loss: 0.13897766172885895, train acc: 0.9836, test acc: 0.9316\n",
      "loss: 0.2953624725341797, train acc: 0.984\n",
      "loss: 0.26092238128185274, train acc: 0.9832\n",
      "loss: 0.2660411670804024, train acc: 0.9848\n",
      "loss: 0.23223144859075545, train acc: 0.9838\n",
      "loss: 0.27686539441347124, train acc: 0.9829\n",
      "loss: 0.23130528926849364, train acc: 0.9835\n",
      "loss: 0.22171170264482498, train acc: 0.9833\n",
      "loss: 0.24294303953647614, train acc: 0.9837\n",
      "epoch: 71, loss: 0.07759131491184235, train acc: 0.9837, test acc: 0.9293\n",
      "loss: 0.33106565475463867, train acc: 0.9829\n",
      "loss: 0.2578322470188141, train acc: 0.9836\n",
      "loss: 0.264074943959713, train acc: 0.9842\n",
      "loss: 0.23389918208122254, train acc: 0.9841\n",
      "loss: 0.2642905831336975, train acc: 0.9846\n",
      "loss: 0.24856963455677034, train acc: 0.9834\n",
      "loss: 0.26262199878692627, train acc: 0.9815\n",
      "loss: 0.2624077573418617, train acc: 0.983\n",
      "epoch: 72, loss: 0.06396126747131348, train acc: 0.983, test acc: 0.9301\n",
      "loss: 0.34061315655708313, train acc: 0.9839\n",
      "loss: 0.28688650876283645, train acc: 0.9821\n",
      "loss: 0.2420749232172966, train acc: 0.9837\n",
      "loss: 0.2553201764822006, train acc: 0.9837\n",
      "loss: 0.26832676529884336, train acc: 0.9855\n",
      "loss: 0.2549149006605148, train acc: 0.9842\n",
      "loss: 0.2593886524438858, train acc: 0.9841\n",
      "loss: 0.22447083741426468, train acc: 0.9837\n",
      "epoch: 73, loss: 0.09490921348333359, train acc: 0.9837, test acc: 0.9295\n",
      "loss: 0.28285202383995056, train acc: 0.9838\n",
      "loss: 0.255350886285305, train acc: 0.9832\n",
      "loss: 0.23872730135917664, train acc: 0.9838\n",
      "loss: 0.26980515867471694, train acc: 0.9831\n",
      "loss: 0.23985137343406676, train acc: 0.9846\n",
      "loss: 0.2584150180220604, train acc: 0.9849\n",
      "loss: 0.23385778814554214, train acc: 0.983\n",
      "loss: 0.2594625040888786, train acc: 0.9836\n",
      "epoch: 74, loss: 0.02438267134130001, train acc: 0.9836, test acc: 0.9304\n",
      "loss: 0.32690268754959106, train acc: 0.9845\n",
      "loss: 0.2701095432043076, train acc: 0.9844\n",
      "loss: 0.23353563100099564, train acc: 0.9837\n",
      "loss: 0.25799412429332735, train acc: 0.983\n",
      "loss: 0.260625983774662, train acc: 0.9834\n",
      "loss: 0.26494534462690356, train acc: 0.9837\n",
      "loss: 0.246797114610672, train acc: 0.984\n",
      "loss: 0.2462273955345154, train acc: 0.9836\n",
      "epoch: 75, loss: 0.06930269300937653, train acc: 0.9836, test acc: 0.9312\n",
      "loss: 0.257425993680954, train acc: 0.9852\n",
      "loss: 0.24083810895681382, train acc: 0.9841\n",
      "loss: 0.2367977425456047, train acc: 0.9844\n",
      "loss: 0.25954384803771974, train acc: 0.9849\n",
      "loss: 0.2584658846259117, train acc: 0.9833\n",
      "loss: 0.25429556965827943, train acc: 0.9847\n",
      "loss: 0.25633794516325, train acc: 0.9844\n",
      "loss: 0.26216578483581543, train acc: 0.9855\n",
      "epoch: 76, loss: 0.11241085082292557, train acc: 0.9855, test acc: 0.9293\n",
      "loss: 0.26964104175567627, train acc: 0.9852\n",
      "loss: 0.23362383246421814, train acc: 0.9852\n",
      "loss: 0.2618688136339188, train acc: 0.9845\n",
      "loss: 0.24979640990495683, train acc: 0.9833\n",
      "loss: 0.2534826219081879, train acc: 0.9851\n",
      "loss: 0.23224446326494216, train acc: 0.9844\n",
      "loss: 0.23052606880664825, train acc: 0.9835\n",
      "loss: 0.24433287978172302, train acc: 0.9847\n",
      "epoch: 77, loss: 0.04548127204179764, train acc: 0.9847, test acc: 0.9306\n",
      "loss: 0.31223350763320923, train acc: 0.9837\n",
      "loss: 0.24902103394269942, train acc: 0.9847\n",
      "loss: 0.25467223972082137, train acc: 0.9844\n",
      "loss: 0.25406823456287386, train acc: 0.9854\n",
      "loss: 0.2611693575978279, train acc: 0.9856\n",
      "loss: 0.2385791152715683, train acc: 0.9855\n",
      "loss: 0.20875832960009574, train acc: 0.985\n",
      "loss: 0.24668932408094407, train acc: 0.9856\n",
      "epoch: 78, loss: 0.125621497631073, train acc: 0.9856, test acc: 0.932\n",
      "loss: 0.3624490797519684, train acc: 0.9855\n",
      "loss: 0.25711391866207123, train acc: 0.9854\n",
      "loss: 0.2307753622531891, train acc: 0.9841\n",
      "loss: 0.2435110867023468, train acc: 0.9855\n",
      "loss: 0.23548500686883928, train acc: 0.9858\n",
      "loss: 0.25155994594097136, train acc: 0.9851\n",
      "loss: 0.22173899561166763, train acc: 0.9853\n",
      "loss: 0.244138003885746, train acc: 0.9836\n",
      "epoch: 79, loss: 0.30527105927467346, train acc: 0.9836, test acc: 0.9296\n",
      "#####training and testing end with K:40, P:0.5######\n",
      "#####training and testing start with K:40, P:1######\n",
      "loss: 2.3472588062286377, train acc: 0.1555\n",
      "loss: 1.988618803024292, train acc: 0.6692\n",
      "loss: 1.2636162519454956, train acc: 0.7901\n",
      "loss: 0.8278221845626831, train acc: 0.8387\n",
      "loss: 0.5945242583751679, train acc: 0.8616\n",
      "loss: 0.5400015026330948, train acc: 0.8733\n",
      "loss: 0.4693467736244202, train acc: 0.8817\n",
      "loss: 0.46402300894260406, train acc: 0.8875\n",
      "epoch: 0, loss: 0.432271271944046, train acc: 0.8875, test acc: 0.8852\n",
      "loss: 0.28643542528152466, train acc: 0.8947\n",
      "loss: 0.38593844175338743, train acc: 0.8986\n",
      "loss: 0.38186593651771544, train acc: 0.9032\n",
      "loss: 0.3536078929901123, train acc: 0.9027\n",
      "loss: 0.3101888582110405, train acc: 0.9126\n",
      "loss: 0.3475507214665413, train acc: 0.9128\n",
      "loss: 0.3208839654922485, train acc: 0.9131\n",
      "loss: 0.34850798845291137, train acc: 0.9132\n",
      "epoch: 1, loss: 0.1653565615415573, train acc: 0.9132, test acc: 0.9058\n",
      "loss: 0.2066240906715393, train acc: 0.9147\n",
      "loss: 0.30118932873010634, train acc: 0.9204\n",
      "loss: 0.3112362429499626, train acc: 0.9213\n",
      "loss: 0.2865238457918167, train acc: 0.9231\n",
      "loss: 0.2514400646090508, train acc: 0.9261\n",
      "loss: 0.28749980479478837, train acc: 0.9289\n",
      "loss: 0.269093444943428, train acc: 0.927\n",
      "loss: 0.2986769050359726, train acc: 0.9287\n",
      "epoch: 2, loss: 0.062255386263132095, train acc: 0.9287, test acc: 0.9124\n",
      "loss: 0.16057266294956207, train acc: 0.9267\n",
      "loss: 0.26114485412836075, train acc: 0.9332\n",
      "loss: 0.27366765290498735, train acc: 0.9346\n",
      "loss: 0.24687391072511672, train acc: 0.933\n",
      "loss: 0.21603798270225524, train acc: 0.9351\n",
      "loss: 0.24821620732545852, train acc: 0.9387\n",
      "loss: 0.23533383011817932, train acc: 0.9352\n",
      "loss: 0.26495536863803865, train acc: 0.9368\n",
      "epoch: 3, loss: 0.04492585361003876, train acc: 0.9368, test acc: 0.9166\n",
      "loss: 0.12832356989383698, train acc: 0.9344\n",
      "loss: 0.23233236819505693, train acc: 0.9411\n",
      "loss: 0.2448581114411354, train acc: 0.9432\n",
      "loss: 0.2176016941666603, train acc: 0.941\n",
      "loss: 0.190970079600811, train acc: 0.9407\n",
      "loss: 0.2185291662812233, train acc: 0.9462\n",
      "loss: 0.20906615182757377, train acc: 0.9432\n",
      "loss: 0.23728024661540986, train acc: 0.9442\n",
      "epoch: 4, loss: 0.032168950885534286, train acc: 0.9442, test acc: 0.9203\n",
      "loss: 0.10645923763513565, train acc: 0.9421\n",
      "loss: 0.20908275619149208, train acc: 0.9464\n",
      "loss: 0.21960700750350953, train acc: 0.9482\n",
      "loss: 0.1934204325079918, train acc: 0.9472\n",
      "loss: 0.1705585777759552, train acc: 0.9476\n",
      "loss: 0.19555551633238794, train acc: 0.9502\n",
      "loss: 0.18829437494277954, train acc: 0.9495\n",
      "loss: 0.2139668345451355, train acc: 0.9487\n",
      "epoch: 5, loss: 0.02534506842494011, train acc: 0.9487, test acc: 0.924\n",
      "loss: 0.08975387364625931, train acc: 0.9486\n",
      "loss: 0.19003184512257576, train acc: 0.9501\n",
      "loss: 0.19737999886274338, train acc: 0.9527\n",
      "loss: 0.17328694760799407, train acc: 0.9526\n",
      "loss: 0.15359446182847022, train acc: 0.9523\n",
      "loss: 0.17581798881292343, train acc: 0.9545\n",
      "loss: 0.17165287137031554, train acc: 0.9535\n",
      "loss: 0.19592801481485367, train acc: 0.9536\n",
      "epoch: 6, loss: 0.01964694820344448, train acc: 0.9536, test acc: 0.9256\n",
      "loss: 0.07761200517416, train acc: 0.9533\n",
      "loss: 0.17248863205313683, train acc: 0.9552\n",
      "loss: 0.17737567275762559, train acc: 0.9593\n",
      "loss: 0.1564286805689335, train acc: 0.9566\n",
      "loss: 0.13974098563194276, train acc: 0.9562\n",
      "loss: 0.1594623640179634, train acc: 0.9581\n",
      "loss: 0.15782081335783005, train acc: 0.9575\n",
      "loss: 0.1788901373744011, train acc: 0.9581\n",
      "epoch: 7, loss: 0.0163253303617239, train acc: 0.9581, test acc: 0.928\n",
      "loss: 0.07045290619134903, train acc: 0.9583\n",
      "loss: 0.15761188864707948, train acc: 0.9596\n",
      "loss: 0.16049401834607124, train acc: 0.962\n",
      "loss: 0.14174634516239165, train acc: 0.9611\n",
      "loss: 0.1278722144663334, train acc: 0.9594\n",
      "loss: 0.14505754932761192, train acc: 0.9621\n",
      "loss: 0.14467719718813896, train acc: 0.9612\n",
      "loss: 0.16344893649220466, train acc: 0.9618\n",
      "epoch: 8, loss: 0.013714101165533066, train acc: 0.9618, test acc: 0.9311\n",
      "loss: 0.06278607994318008, train acc: 0.9626\n",
      "loss: 0.14447679743170738, train acc: 0.9633\n",
      "loss: 0.1441224843263626, train acc: 0.965\n",
      "loss: 0.1282376192510128, train acc: 0.965\n",
      "loss: 0.11666847541928291, train acc: 0.9633\n",
      "loss: 0.131234572827816, train acc: 0.9646\n",
      "loss: 0.1335383653640747, train acc: 0.9641\n",
      "loss: 0.14888601899147033, train acc: 0.9659\n",
      "epoch: 9, loss: 0.012134622782468796, train acc: 0.9659, test acc: 0.9327\n",
      "loss: 0.056845009326934814, train acc: 0.9669\n",
      "loss: 0.1332798846065998, train acc: 0.9673\n",
      "loss: 0.13041991740465164, train acc: 0.9682\n",
      "loss: 0.11655086651444435, train acc: 0.9689\n",
      "loss: 0.1071015641093254, train acc: 0.9668\n",
      "loss: 0.1191735841333866, train acc: 0.968\n",
      "loss: 0.12321643456816674, train acc: 0.968\n",
      "loss: 0.13691269978880882, train acc: 0.9703\n",
      "epoch: 10, loss: 0.010501790791749954, train acc: 0.9703, test acc: 0.9344\n",
      "loss: 0.05037182942032814, train acc: 0.9708\n",
      "loss: 0.12257931344211101, train acc: 0.9709\n",
      "loss: 0.11752366423606872, train acc: 0.9711\n",
      "loss: 0.10591829344630241, train acc: 0.9738\n",
      "loss: 0.09903653785586357, train acc: 0.9702\n",
      "loss: 0.10854837968945504, train acc: 0.9716\n",
      "loss: 0.11379709988832473, train acc: 0.9708\n",
      "loss: 0.12608857676386834, train acc: 0.9723\n",
      "epoch: 11, loss: 0.009805520996451378, train acc: 0.9723, test acc: 0.9358\n",
      "loss: 0.047368355095386505, train acc: 0.9742\n",
      "loss: 0.1135001964867115, train acc: 0.9746\n",
      "loss: 0.10694018229842187, train acc: 0.9743\n",
      "loss: 0.096724783629179, train acc: 0.9763\n",
      "loss: 0.09111155904829502, train acc: 0.9733\n",
      "loss: 0.09914129003882408, train acc: 0.9731\n",
      "loss: 0.10539996549487114, train acc: 0.9731\n",
      "loss: 0.11669199168682098, train acc: 0.9759\n",
      "epoch: 12, loss: 0.00884887296706438, train acc: 0.9759, test acc: 0.9369\n",
      "loss: 0.04278635233640671, train acc: 0.9783\n",
      "loss: 0.10470891371369362, train acc: 0.9783\n",
      "loss: 0.09696716219186782, train acc: 0.9769\n",
      "loss: 0.08826157338917255, train acc: 0.9787\n",
      "loss: 0.08431609719991684, train acc: 0.9759\n",
      "loss: 0.09097733311355113, train acc: 0.9755\n",
      "loss: 0.09766861349344254, train acc: 0.9751\n",
      "loss: 0.10756879672408104, train acc: 0.9788\n",
      "epoch: 13, loss: 0.007887506857514381, train acc: 0.9788, test acc: 0.9371\n",
      "loss: 0.039860885590314865, train acc: 0.9807\n",
      "loss: 0.09690530486404896, train acc: 0.9807\n",
      "loss: 0.08831126354634762, train acc: 0.9796\n",
      "loss: 0.08029372617602348, train acc: 0.9819\n",
      "loss: 0.0781844813376665, train acc: 0.9783\n",
      "loss: 0.08368011452257633, train acc: 0.9776\n",
      "loss: 0.09030385129153728, train acc: 0.9773\n",
      "loss: 0.09969606027007102, train acc: 0.9814\n",
      "epoch: 14, loss: 0.006912576034665108, train acc: 0.9814, test acc: 0.9377\n",
      "loss: 0.03699716925621033, train acc: 0.9828\n",
      "loss: 0.08914441652595997, train acc: 0.9825\n",
      "loss: 0.08109465762972831, train acc: 0.9816\n",
      "loss: 0.07387460805475712, train acc: 0.9836\n",
      "loss: 0.07206904217600822, train acc: 0.9814\n",
      "loss: 0.07663209736347198, train acc: 0.9792\n",
      "loss: 0.08309499807655811, train acc: 0.9796\n",
      "loss: 0.09242809154093265, train acc: 0.9835\n",
      "epoch: 15, loss: 0.006116114091128111, train acc: 0.9835, test acc: 0.9391\n",
      "loss: 0.03574464097619057, train acc: 0.9839\n",
      "loss: 0.08239311538636684, train acc: 0.9843\n",
      "loss: 0.07391434721648693, train acc: 0.9836\n",
      "loss: 0.06776314526796341, train acc: 0.9848\n",
      "loss: 0.06688779294490814, train acc: 0.9823\n",
      "loss: 0.07022993005812168, train acc: 0.9805\n",
      "loss: 0.07687166072428227, train acc: 0.9807\n",
      "loss: 0.08523804172873498, train acc: 0.9852\n",
      "epoch: 16, loss: 0.005457837600260973, train acc: 0.9852, test acc: 0.9397\n",
      "loss: 0.0332796648144722, train acc: 0.9855\n",
      "loss: 0.07619448751211166, train acc: 0.986\n",
      "loss: 0.0683867234736681, train acc: 0.9858\n",
      "loss: 0.062026244774460795, train acc: 0.9861\n",
      "loss: 0.062240174040198326, train acc: 0.9845\n",
      "loss: 0.06489083953201771, train acc: 0.9819\n",
      "loss: 0.07102947533130646, train acc: 0.9829\n",
      "loss: 0.07854748740792275, train acc: 0.9859\n",
      "epoch: 17, loss: 0.005138855427503586, train acc: 0.9859, test acc: 0.9394\n",
      "loss: 0.03162148594856262, train acc: 0.987\n",
      "loss: 0.07037961184978485, train acc: 0.9869\n",
      "loss: 0.06244709268212319, train acc: 0.9869\n",
      "loss: 0.05659330077469349, train acc: 0.9871\n",
      "loss: 0.057692541554570195, train acc: 0.9855\n",
      "loss: 0.059719007648527625, train acc: 0.9828\n",
      "loss: 0.06572792269289493, train acc: 0.9831\n",
      "loss: 0.07246347479522228, train acc: 0.9874\n",
      "epoch: 18, loss: 0.004413459915667772, train acc: 0.9874, test acc: 0.9392\n",
      "loss: 0.02883036807179451, train acc: 0.9879\n",
      "loss: 0.06465481221675873, train acc: 0.9882\n",
      "loss: 0.05785958953201771, train acc: 0.9884\n",
      "loss: 0.052004289254546164, train acc: 0.9886\n",
      "loss: 0.0538631634786725, train acc: 0.9868\n",
      "loss: 0.05492560416460037, train acc: 0.9838\n",
      "loss: 0.06081815883517265, train acc: 0.9839\n",
      "loss: 0.06685090772807598, train acc: 0.989\n",
      "epoch: 19, loss: 0.004147940780967474, train acc: 0.989, test acc: 0.9401\n",
      "loss: 0.02714812383055687, train acc: 0.9896\n",
      "loss: 0.06020241118967533, train acc: 0.9898\n",
      "loss: 0.05360461063683033, train acc: 0.9895\n",
      "loss: 0.048012954741716386, train acc: 0.9892\n",
      "loss: 0.05038122870028019, train acc: 0.9891\n",
      "loss: 0.05102368127554655, train acc: 0.9846\n",
      "loss: 0.05568786412477493, train acc: 0.9851\n",
      "loss: 0.06133715771138668, train acc: 0.9905\n",
      "epoch: 20, loss: 0.0036626416258513927, train acc: 0.9905, test acc: 0.9402\n",
      "loss: 0.024846844375133514, train acc: 0.9906\n",
      "loss: 0.05544925779104233, train acc: 0.9903\n",
      "loss: 0.049668848514556885, train acc: 0.9907\n",
      "loss: 0.04362691193819046, train acc: 0.9904\n",
      "loss: 0.04687685109674931, train acc: 0.99\n",
      "loss: 0.046813466027379035, train acc: 0.986\n",
      "loss: 0.05119790267199278, train acc: 0.9862\n",
      "loss: 0.05601957403123379, train acc: 0.9915\n",
      "epoch: 21, loss: 0.003439867403358221, train acc: 0.9915, test acc: 0.9407\n",
      "loss: 0.022463636472821236, train acc: 0.9915\n",
      "loss: 0.05124244336038828, train acc: 0.9916\n",
      "loss: 0.04572532698512077, train acc: 0.9913\n",
      "loss: 0.040093261003494265, train acc: 0.9906\n",
      "loss: 0.043662487901747225, train acc: 0.9911\n",
      "loss: 0.043127998150885104, train acc: 0.9866\n",
      "loss: 0.04707633573561907, train acc: 0.9878\n",
      "loss: 0.0517848189920187, train acc: 0.9922\n",
      "epoch: 22, loss: 0.003094769548624754, train acc: 0.9922, test acc: 0.9406\n",
      "loss: 0.021318702027201653, train acc: 0.9928\n",
      "loss: 0.04776888620108366, train acc: 0.9923\n",
      "loss: 0.042552410066127776, train acc: 0.9923\n",
      "loss: 0.0367646224796772, train acc: 0.9915\n",
      "loss: 0.04131446219980717, train acc: 0.9916\n",
      "loss: 0.040171320736408236, train acc: 0.9879\n",
      "loss: 0.04309893436729908, train acc: 0.9891\n",
      "loss: 0.04710699543356896, train acc: 0.9931\n",
      "epoch: 23, loss: 0.0028584394603967667, train acc: 0.9931, test acc: 0.9405\n",
      "loss: 0.01973486877977848, train acc: 0.9933\n",
      "loss: 0.04387487471103668, train acc: 0.9932\n",
      "loss: 0.03879960384219885, train acc: 0.9924\n",
      "loss: 0.033618508279323576, train acc: 0.9926\n",
      "loss: 0.03852987214922905, train acc: 0.9922\n",
      "loss: 0.03694955576211214, train acc: 0.9896\n",
      "loss: 0.0394451592117548, train acc: 0.9903\n",
      "loss: 0.0435017429292202, train acc: 0.9936\n",
      "epoch: 24, loss: 0.0026688529178500175, train acc: 0.9936, test acc: 0.9399\n",
      "loss: 0.01882792077958584, train acc: 0.9939\n",
      "loss: 0.04068250358104706, train acc: 0.9941\n",
      "loss: 0.03609388545155525, train acc: 0.9928\n",
      "loss: 0.031167836859822273, train acc: 0.9931\n",
      "loss: 0.03591043222695589, train acc: 0.9928\n",
      "loss: 0.03424775321036577, train acc: 0.9901\n",
      "loss: 0.036266865208745, train acc: 0.9912\n",
      "loss: 0.03996171057224274, train acc: 0.9946\n",
      "epoch: 25, loss: 0.0024785331916064024, train acc: 0.9946, test acc: 0.9402\n",
      "loss: 0.01717241108417511, train acc: 0.9941\n",
      "loss: 0.03690861184149981, train acc: 0.9947\n",
      "loss: 0.03288709204643965, train acc: 0.993\n",
      "loss: 0.028638599999248983, train acc: 0.9939\n",
      "loss: 0.03387696463614702, train acc: 0.9941\n",
      "loss: 0.03161540077999234, train acc: 0.9906\n",
      "loss: 0.03354618828743696, train acc: 0.9926\n",
      "loss: 0.03664732333272695, train acc: 0.9949\n",
      "epoch: 26, loss: 0.002389193046838045, train acc: 0.9949, test acc: 0.9395\n",
      "loss: 0.01642609015107155, train acc: 0.9951\n",
      "loss: 0.034454811923205854, train acc: 0.9951\n",
      "loss: 0.030425108224153518, train acc: 0.9937\n",
      "loss: 0.02670361436903477, train acc: 0.9947\n",
      "loss: 0.031966018583625556, train acc: 0.9945\n",
      "loss: 0.02937403190881014, train acc: 0.9916\n",
      "loss: 0.03090667873620987, train acc: 0.9932\n",
      "loss: 0.03404672760516405, train acc: 0.9953\n",
      "epoch: 27, loss: 0.00223831320181489, train acc: 0.9953, test acc: 0.9395\n",
      "loss: 0.015747448429465294, train acc: 0.9951\n",
      "loss: 0.03169458191841841, train acc: 0.9953\n",
      "loss: 0.02812376581132412, train acc: 0.994\n",
      "loss: 0.02438370231539011, train acc: 0.995\n",
      "loss: 0.029958456568419934, train acc: 0.9951\n",
      "loss: 0.026684968173503874, train acc: 0.992\n",
      "loss: 0.02926613772287965, train acc: 0.9934\n",
      "loss: 0.03121473006904125, train acc: 0.9956\n",
      "epoch: 28, loss: 0.002198878675699234, train acc: 0.9956, test acc: 0.9394\n",
      "loss: 0.014734684489667416, train acc: 0.9947\n",
      "loss: 0.02974060969427228, train acc: 0.9956\n",
      "loss: 0.025987812504172324, train acc: 0.9943\n",
      "loss: 0.022936861589550973, train acc: 0.9957\n",
      "loss: 0.02777621680870652, train acc: 0.9953\n",
      "loss: 0.024835454206913708, train acc: 0.9928\n",
      "loss: 0.026855708472430705, train acc: 0.9951\n",
      "loss: 0.028690348006784917, train acc: 0.9962\n",
      "epoch: 29, loss: 0.0022124052047729492, train acc: 0.9962, test acc: 0.939\n",
      "loss: 0.01460809726268053, train acc: 0.9946\n",
      "loss: 0.027837117295712234, train acc: 0.9959\n",
      "loss: 0.02410211432725191, train acc: 0.9945\n",
      "loss: 0.021229006908833982, train acc: 0.9962\n",
      "loss: 0.025770456716418266, train acc: 0.9958\n",
      "loss: 0.02271075388416648, train acc: 0.9929\n",
      "loss: 0.024543958064168692, train acc: 0.9956\n",
      "loss: 0.026748608238995074, train acc: 0.9964\n",
      "epoch: 30, loss: 0.0021022227592766285, train acc: 0.9964, test acc: 0.9396\n",
      "loss: 0.013206529431045055, train acc: 0.995\n",
      "loss: 0.025864138174802064, train acc: 0.9954\n",
      "loss: 0.0224028792232275, train acc: 0.9951\n",
      "loss: 0.019993004202842713, train acc: 0.997\n",
      "loss: 0.02407161481678486, train acc: 0.9963\n",
      "loss: 0.02087541949003935, train acc: 0.9933\n",
      "loss: 0.023687808122485877, train acc: 0.996\n",
      "loss: 0.02489218842238188, train acc: 0.9959\n",
      "epoch: 31, loss: 0.002110943431034684, train acc: 0.9959, test acc: 0.9383\n",
      "loss: 0.01328977756202221, train acc: 0.9945\n",
      "loss: 0.02454738561064005, train acc: 0.9961\n",
      "loss: 0.021064491476863623, train acc: 0.9955\n",
      "loss: 0.018540998455137014, train acc: 0.9976\n",
      "loss: 0.02234358713030815, train acc: 0.9967\n",
      "loss: 0.01926784375682473, train acc: 0.9936\n",
      "loss: 0.022101891040802003, train acc: 0.9969\n",
      "loss: 0.023145094513893127, train acc: 0.9956\n",
      "epoch: 32, loss: 0.0020759766921401024, train acc: 0.9956, test acc: 0.9378\n",
      "loss: 0.013097167946398258, train acc: 0.9938\n",
      "loss: 0.02336379252374172, train acc: 0.9961\n",
      "loss: 0.019720104802399874, train acc: 0.9958\n",
      "loss: 0.017239425517618655, train acc: 0.9976\n",
      "loss: 0.020219519175589083, train acc: 0.9971\n",
      "loss: 0.017709511052817106, train acc: 0.9948\n",
      "loss: 0.020295293442904948, train acc: 0.9976\n",
      "loss: 0.021181125566363333, train acc: 0.9954\n",
      "epoch: 33, loss: 0.002081790007650852, train acc: 0.9954, test acc: 0.9379\n",
      "loss: 0.01249889750033617, train acc: 0.9934\n",
      "loss: 0.02192228976637125, train acc: 0.9961\n",
      "loss: 0.01850840700790286, train acc: 0.9962\n",
      "loss: 0.01628247033804655, train acc: 0.9977\n",
      "loss: 0.01868362445384264, train acc: 0.9975\n",
      "loss: 0.01637166612781584, train acc: 0.9951\n",
      "loss: 0.019197136256843807, train acc: 0.9979\n",
      "loss: 0.02001681728288531, train acc: 0.9956\n",
      "epoch: 34, loss: 0.0018656827742233872, train acc: 0.9956, test acc: 0.9374\n",
      "loss: 0.011794381774961948, train acc: 0.9932\n",
      "loss: 0.0208771837875247, train acc: 0.996\n",
      "loss: 0.01757830921560526, train acc: 0.9967\n",
      "loss: 0.015110474545508623, train acc: 0.9978\n",
      "loss: 0.017023360077291726, train acc: 0.9976\n",
      "loss: 0.015160356275737285, train acc: 0.9953\n",
      "loss: 0.0180606534704566, train acc: 0.9984\n",
      "loss: 0.018210154306143523, train acc: 0.995\n",
      "epoch: 35, loss: 0.0016323640011250973, train acc: 0.995, test acc: 0.9383\n",
      "loss: 0.010453511960804462, train acc: 0.9933\n",
      "loss: 0.019607275351881982, train acc: 0.9958\n",
      "loss: 0.01634165607392788, train acc: 0.9973\n",
      "loss: 0.014056150801479816, train acc: 0.9979\n",
      "loss: 0.015296836104243993, train acc: 0.9978\n",
      "loss: 0.014010004233568907, train acc: 0.996\n",
      "loss: 0.016722531523555517, train acc: 0.9986\n",
      "loss: 0.016899518296122552, train acc: 0.9948\n",
      "epoch: 36, loss: 0.0015144055942073464, train acc: 0.9948, test acc: 0.9387\n",
      "loss: 0.010050676763057709, train acc: 0.9937\n",
      "loss: 0.018635845649987458, train acc: 0.9958\n",
      "loss: 0.015456891153007746, train acc: 0.9975\n",
      "loss: 0.013315157033503056, train acc: 0.9981\n",
      "loss: 0.013976514805108309, train acc: 0.9977\n",
      "loss: 0.01292936229147017, train acc: 0.9963\n",
      "loss: 0.015643434692174198, train acc: 0.9986\n",
      "loss: 0.015645225532352924, train acc: 0.9944\n",
      "epoch: 37, loss: 0.001358494278974831, train acc: 0.9944, test acc: 0.9398\n",
      "loss: 0.009411037899553776, train acc: 0.994\n",
      "loss: 0.01747890142723918, train acc: 0.9958\n",
      "loss: 0.014370411727577448, train acc: 0.9974\n",
      "loss: 0.012472103629261256, train acc: 0.9984\n",
      "loss: 0.012477005645632745, train acc: 0.9982\n",
      "loss: 0.011991716641932725, train acc: 0.9968\n",
      "loss: 0.014193417876958847, train acc: 0.9987\n",
      "loss: 0.01429177075624466, train acc: 0.9944\n",
      "epoch: 38, loss: 0.0011855646735057235, train acc: 0.9944, test acc: 0.9398\n",
      "loss: 0.008643334731459618, train acc: 0.9941\n",
      "loss: 0.016450149193406104, train acc: 0.9957\n",
      "loss: 0.013281536288559437, train acc: 0.9974\n",
      "loss: 0.011646854225546122, train acc: 0.9985\n",
      "loss: 0.011315153073519468, train acc: 0.9984\n",
      "loss: 0.011000181036069989, train acc: 0.9971\n",
      "loss: 0.01314671253785491, train acc: 0.999\n",
      "loss: 0.013294771406799554, train acc: 0.9946\n",
      "epoch: 39, loss: 0.0010224643629044294, train acc: 0.9946, test acc: 0.9408\n",
      "loss: 0.0075196814723312855, train acc: 0.9952\n",
      "loss: 0.015245625888928771, train acc: 0.9952\n",
      "loss: 0.012621070817112922, train acc: 0.9974\n",
      "loss: 0.010848094802349805, train acc: 0.9984\n",
      "loss: 0.01032848977483809, train acc: 0.9987\n",
      "loss: 0.010144988680258394, train acc: 0.9977\n",
      "loss: 0.011496387235820294, train acc: 0.9992\n",
      "loss: 0.012135805422440171, train acc: 0.9945\n",
      "epoch: 40, loss: 0.000900739396456629, train acc: 0.9945, test acc: 0.9402\n",
      "loss: 0.006291928235441446, train acc: 0.9958\n",
      "loss: 0.014282208820804953, train acc: 0.9942\n",
      "loss: 0.011793880350887775, train acc: 0.9973\n",
      "loss: 0.010174411814659834, train acc: 0.9986\n",
      "loss: 0.009783503133803606, train acc: 0.9986\n",
      "loss: 0.009710783092305064, train acc: 0.9979\n",
      "loss: 0.010523250605911017, train acc: 0.9993\n",
      "loss: 0.011058839038014412, train acc: 0.9951\n",
      "epoch: 41, loss: 0.0007288609631359577, train acc: 0.9951, test acc: 0.9417\n",
      "loss: 0.006261778529733419, train acc: 0.997\n",
      "loss: 0.013159032491967083, train acc: 0.9936\n",
      "loss: 0.01094729765318334, train acc: 0.9971\n",
      "loss: 0.009625635342672467, train acc: 0.9989\n",
      "loss: 0.008933880226686597, train acc: 0.9988\n",
      "loss: 0.00896153375506401, train acc: 0.9984\n",
      "loss: 0.009279215056449176, train acc: 0.9997\n",
      "loss: 0.010137150343507529, train acc: 0.9954\n",
      "epoch: 42, loss: 0.00070357252843678, train acc: 0.9954, test acc: 0.9422\n",
      "loss: 0.005334050860255957, train acc: 0.9977\n",
      "loss: 0.011916741402819753, train acc: 0.9939\n",
      "loss: 0.010046575544402003, train acc: 0.9974\n",
      "loss: 0.009121529711410403, train acc: 0.999\n",
      "loss: 0.008045102516189218, train acc: 0.9989\n",
      "loss: 0.008379620406776667, train acc: 0.9987\n",
      "loss: 0.008260695403441787, train acc: 0.9997\n",
      "loss: 0.009330310439690948, train acc: 0.9961\n",
      "epoch: 43, loss: 0.0005837100907228887, train acc: 0.9961, test acc: 0.9431\n",
      "loss: 0.0048188152723014355, train acc: 0.9985\n",
      "loss: 0.010869294498115778, train acc: 0.9945\n",
      "loss: 0.009621621901169419, train acc: 0.9975\n",
      "loss: 0.0084316267631948, train acc: 0.999\n",
      "loss: 0.0074636095203459265, train acc: 0.999\n",
      "loss: 0.007987054344266654, train acc: 0.9986\n",
      "loss: 0.007568661449477076, train acc: 0.9995\n",
      "loss: 0.008503119461238384, train acc: 0.9966\n",
      "epoch: 44, loss: 0.00048108125338330865, train acc: 0.9966, test acc: 0.9429\n",
      "loss: 0.0044303457252681255, train acc: 0.9992\n",
      "loss: 0.010054292855784297, train acc: 0.9953\n",
      "loss: 0.00889859078451991, train acc: 0.997\n",
      "loss: 0.008048890857025981, train acc: 0.9991\n",
      "loss: 0.006989604793488979, train acc: 0.999\n",
      "loss: 0.007609170768409968, train acc: 0.9989\n",
      "loss: 0.007016159035265446, train acc: 0.9995\n",
      "loss: 0.007787235314026475, train acc: 0.9969\n",
      "epoch: 45, loss: 0.0004286949697416276, train acc: 0.9969, test acc: 0.942\n",
      "loss: 0.004060331266373396, train acc: 0.9993\n",
      "loss: 0.009168259939178825, train acc: 0.9966\n",
      "loss: 0.008152750506997108, train acc: 0.9963\n",
      "loss: 0.007406403264030814, train acc: 0.9993\n",
      "loss: 0.00650056372396648, train acc: 0.9991\n",
      "loss: 0.007020980515517294, train acc: 0.9994\n",
      "loss: 0.006461554579436779, train acc: 0.9995\n",
      "loss: 0.007393012708052993, train acc: 0.9966\n",
      "epoch: 46, loss: 0.00039456362719647586, train acc: 0.9966, test acc: 0.9424\n",
      "loss: 0.0037020030431449413, train acc: 0.9994\n",
      "loss: 0.008257058449089527, train acc: 0.997\n",
      "loss: 0.007388781430199743, train acc: 0.9964\n",
      "loss: 0.006877320725470782, train acc: 0.9994\n",
      "loss: 0.006338452524505556, train acc: 0.9993\n",
      "loss: 0.006687190639786423, train acc: 0.9993\n",
      "loss: 0.006027461565099657, train acc: 0.9994\n",
      "loss: 0.007046367786824703, train acc: 0.9974\n",
      "epoch: 47, loss: 0.00036417291266843677, train acc: 0.9974, test acc: 0.942\n",
      "loss: 0.003715815953910351, train acc: 0.9997\n",
      "loss: 0.0077741282992064955, train acc: 0.9975\n",
      "loss: 0.006746512837707996, train acc: 0.9961\n",
      "loss: 0.006506365770474076, train acc: 0.9994\n",
      "loss: 0.005900690681301057, train acc: 0.9995\n",
      "loss: 0.006184823019430042, train acc: 0.9995\n",
      "loss: 0.00575663261115551, train acc: 0.9993\n",
      "loss: 0.006835993099957705, train acc: 0.9974\n",
      "epoch: 48, loss: 0.000332574883941561, train acc: 0.9974, test acc: 0.9418\n",
      "loss: 0.0033798578660935163, train acc: 0.9997\n",
      "loss: 0.007148138829506934, train acc: 0.9978\n",
      "loss: 0.006225732946768403, train acc: 0.9962\n",
      "loss: 0.006013119267299772, train acc: 0.9994\n",
      "loss: 0.00562211568467319, train acc: 0.9993\n",
      "loss: 0.005764243053272366, train acc: 0.9997\n",
      "loss: 0.005347134545445442, train acc: 0.9993\n",
      "loss: 0.0065551276318728926, train acc: 0.998\n",
      "epoch: 49, loss: 0.0003038572904188186, train acc: 0.998, test acc: 0.9414\n",
      "loss: 0.003545106155797839, train acc: 0.9998\n",
      "loss: 0.006752607622183859, train acc: 0.9977\n",
      "loss: 0.005756913544610142, train acc: 0.9966\n",
      "loss: 0.005873374594375491, train acc: 0.9995\n",
      "loss: 0.005256271571852267, train acc: 0.9991\n",
      "loss: 0.005522853136062622, train acc: 0.9997\n",
      "loss: 0.005165978171862662, train acc: 0.9994\n",
      "loss: 0.006160617666319012, train acc: 0.9986\n",
      "epoch: 50, loss: 0.0002956387761514634, train acc: 0.9986, test acc: 0.9406\n",
      "loss: 0.003304175566881895, train acc: 0.9998\n",
      "loss: 0.006024289852939546, train acc: 0.9975\n",
      "loss: 0.005532566388137638, train acc: 0.9972\n",
      "loss: 0.005538418190553784, train acc: 0.9996\n",
      "loss: 0.004950669244863093, train acc: 0.9992\n",
      "loss: 0.005106844287365675, train acc: 0.9997\n",
      "loss: 0.004628254647832364, train acc: 0.9997\n",
      "loss: 0.005730301770381629, train acc: 0.9993\n",
      "epoch: 51, loss: 0.0002974223461933434, train acc: 0.9993, test acc: 0.9385\n",
      "loss: 0.003689834848046303, train acc: 0.9994\n",
      "loss: 0.006437320844270289, train acc: 0.9972\n",
      "loss: 0.005518886470235884, train acc: 0.9973\n",
      "loss: 0.005433628568425775, train acc: 0.9996\n",
      "loss: 0.004420429677702486, train acc: 0.9994\n",
      "loss: 0.004806530079804361, train acc: 0.9997\n",
      "loss: 0.004287300026044249, train acc: 0.9998\n",
      "loss: 0.005092903226613999, train acc: 0.9992\n",
      "epoch: 52, loss: 0.0002484892902430147, train acc: 0.9992, test acc: 0.9385\n",
      "loss: 0.0031705531291663647, train acc: 0.9991\n",
      "loss: 0.0054164380766451355, train acc: 0.9973\n",
      "loss: 0.0049446348799392584, train acc: 0.9984\n",
      "loss: 0.0050656278384849426, train acc: 0.9994\n",
      "loss: 0.004131744359619915, train acc: 0.9991\n",
      "loss: 0.004780076770111918, train acc: 0.9996\n",
      "loss: 0.004087483754847199, train acc: 0.9999\n",
      "loss: 0.004636958194896579, train acc: 0.9991\n",
      "epoch: 53, loss: 0.0002490607148502022, train acc: 0.9991, test acc: 0.9384\n",
      "loss: 0.003333301516249776, train acc: 0.9983\n",
      "loss: 0.00504352324642241, train acc: 0.9978\n",
      "loss: 0.004537344840355217, train acc: 0.9992\n",
      "loss: 0.004315391648560762, train acc: 0.9993\n",
      "loss: 0.0039014223497360946, train acc: 0.9989\n",
      "loss: 0.004551354283466935, train acc: 0.9994\n",
      "loss: 0.003806959267240018, train acc: 0.9999\n",
      "loss: 0.004280156921595335, train acc: 0.9992\n",
      "epoch: 54, loss: 0.00021823607676196843, train acc: 0.9992, test acc: 0.9394\n",
      "loss: 0.0029856727924197912, train acc: 0.9986\n",
      "loss: 0.004647960746660828, train acc: 0.9979\n",
      "loss: 0.004171023797243834, train acc: 0.9995\n",
      "loss: 0.0038146215956658125, train acc: 0.9994\n",
      "loss: 0.0036551970755681396, train acc: 0.9992\n",
      "loss: 0.004100037994794548, train acc: 0.9991\n",
      "loss: 0.003471041924785823, train acc: 1.0\n",
      "loss: 0.003895845520310104, train acc: 0.9994\n",
      "epoch: 55, loss: 0.00018249785352963954, train acc: 0.9994, test acc: 0.9397\n",
      "loss: 0.002398007083684206, train acc: 0.9992\n",
      "loss: 0.004095984878949821, train acc: 0.9981\n",
      "loss: 0.003770460025407374, train acc: 0.9993\n",
      "loss: 0.003291159216314554, train acc: 0.9994\n",
      "loss: 0.0034211197635158896, train acc: 0.9995\n",
      "loss: 0.0036010045325383542, train acc: 0.9991\n",
      "loss: 0.003374939667992294, train acc: 1.0\n",
      "loss: 0.0036006965907290577, train acc: 0.9995\n",
      "epoch: 56, loss: 0.000143720957566984, train acc: 0.9995, test acc: 0.9406\n",
      "loss: 0.0021018078550696373, train acc: 0.9997\n",
      "loss: 0.0036744030192494394, train acc: 0.9985\n",
      "loss: 0.003391669178381562, train acc: 0.9993\n",
      "loss: 0.003094931202940643, train acc: 0.9996\n",
      "loss: 0.003083653177600354, train acc: 0.9996\n",
      "loss: 0.003295443905517459, train acc: 0.9991\n",
      "loss: 0.003296231722924858, train acc: 1.0\n",
      "loss: 0.0033409504452720285, train acc: 0.9994\n",
      "epoch: 57, loss: 0.00012021511065540835, train acc: 0.9994, test acc: 0.9407\n",
      "loss: 0.0018419212428852916, train acc: 0.9999\n",
      "loss: 0.003338334464933723, train acc: 0.9987\n",
      "loss: 0.0031089023454114796, train acc: 0.9993\n",
      "loss: 0.0029309296747669577, train acc: 0.9997\n",
      "loss: 0.002858675504103303, train acc: 0.9996\n",
      "loss: 0.0031581831979565324, train acc: 0.9987\n",
      "loss: 0.003251900488976389, train acc: 1.0\n",
      "loss: 0.0030883116414770484, train acc: 0.9993\n",
      "epoch: 58, loss: 0.00010579606896499172, train acc: 0.9993, test acc: 0.9414\n",
      "loss: 0.0018526905914768577, train acc: 0.9999\n",
      "loss: 0.0030927117215469478, train acc: 0.999\n",
      "loss: 0.002895899291615933, train acc: 0.9992\n",
      "loss: 0.0027147546294145285, train acc: 0.9998\n",
      "loss: 0.002632566716056317, train acc: 0.9995\n",
      "loss: 0.002965073974337429, train acc: 0.9988\n",
      "loss: 0.0031237590010277927, train acc: 1.0\n",
      "loss: 0.0029619632521644236, train acc: 0.9992\n",
      "epoch: 59, loss: 0.00010001555347116664, train acc: 0.9992, test acc: 0.9417\n",
      "loss: 0.0017729185055941343, train acc: 1.0\n",
      "loss: 0.002836037508677691, train acc: 0.9988\n",
      "loss: 0.002762124082073569, train acc: 0.9993\n",
      "loss: 0.00254848652984947, train acc: 0.9999\n",
      "loss: 0.0024756060680374502, train acc: 0.9994\n",
      "loss: 0.00280280978186056, train acc: 0.9991\n",
      "loss: 0.0028907774249091744, train acc: 1.0\n",
      "loss: 0.0027171954745426772, train acc: 0.9991\n",
      "epoch: 60, loss: 9.006464824778959e-05, train acc: 0.9991, test acc: 0.9424\n",
      "loss: 0.0015818389365449548, train acc: 1.0\n",
      "loss: 0.002666092140134424, train acc: 0.9989\n",
      "loss: 0.00259776342427358, train acc: 0.9993\n",
      "loss: 0.0023980352678336205, train acc: 1.0\n",
      "loss: 0.0022900417912751435, train acc: 0.9993\n",
      "loss: 0.0026233885902911423, train acc: 0.9991\n",
      "loss: 0.002795299340505153, train acc: 1.0\n",
      "loss: 0.002622130361851305, train acc: 0.9989\n",
      "epoch: 61, loss: 7.703708979533985e-05, train acc: 0.9989, test acc: 0.9421\n",
      "loss: 0.001692097052000463, train acc: 1.0\n",
      "loss: 0.0025704295258037746, train acc: 0.9989\n",
      "loss: 0.0025235372595489025, train acc: 0.9993\n",
      "loss: 0.002228838368318975, train acc: 1.0\n",
      "loss: 0.002148519898764789, train acc: 0.9994\n",
      "loss: 0.0024258049787022175, train acc: 0.9998\n",
      "loss: 0.0024982828996144234, train acc: 1.0\n",
      "loss: 0.0024075757944956424, train acc: 0.9992\n",
      "epoch: 62, loss: 7.11152606527321e-05, train acc: 0.9992, test acc: 0.9424\n",
      "loss: 0.0015486415941268206, train acc: 1.0\n",
      "loss: 0.0024166488787159325, train acc: 0.9988\n",
      "loss: 0.002368935535196215, train acc: 0.9992\n",
      "loss: 0.0021242006332613528, train acc: 1.0\n",
      "loss: 0.0020271746325306594, train acc: 0.9997\n",
      "loss: 0.0022696699656080455, train acc: 0.9996\n",
      "loss: 0.0023334285244345663, train acc: 1.0\n",
      "loss: 0.0023021102766506373, train acc: 0.9994\n",
      "epoch: 63, loss: 6.436630064854398e-05, train acc: 0.9994, test acc: 0.9418\n",
      "loss: 0.0014956668019294739, train acc: 1.0\n",
      "loss: 0.0023371218238025905, train acc: 0.9988\n",
      "loss: 0.0022827650769613682, train acc: 0.9993\n",
      "loss: 0.0020021873875521123, train acc: 1.0\n",
      "loss: 0.0019039950566366315, train acc: 0.9996\n",
      "loss: 0.002138636034214869, train acc: 0.9999\n",
      "loss: 0.00215188650181517, train acc: 1.0\n",
      "loss: 0.0021936217905022203, train acc: 0.9996\n",
      "epoch: 64, loss: 5.766957110608928e-05, train acc: 0.9996, test acc: 0.9413\n",
      "loss: 0.0015376327792182565, train acc: 1.0\n",
      "loss: 0.002278089011088014, train acc: 0.999\n",
      "loss: 0.0022211606497876345, train acc: 0.9993\n",
      "loss: 0.001924141333438456, train acc: 1.0\n",
      "loss: 0.001819582376629114, train acc: 0.9997\n",
      "loss: 0.0019895966397598386, train acc: 0.9999\n",
      "loss: 0.001985395955853164, train acc: 1.0\n",
      "loss: 0.002096865838393569, train acc: 0.9998\n",
      "epoch: 65, loss: 5.3512958402279764e-05, train acc: 0.9998, test acc: 0.941\n",
      "loss: 0.0016275905072689056, train acc: 1.0\n",
      "loss: 0.002218210813589394, train acc: 0.9992\n",
      "loss: 0.0021072241361252964, train acc: 0.9992\n",
      "loss: 0.0018308700178749858, train acc: 1.0\n",
      "loss: 0.0017411521752364934, train acc: 0.9997\n",
      "loss: 0.0018911364022642374, train acc: 0.9999\n",
      "loss: 0.0018588053295388817, train acc: 1.0\n",
      "loss: 0.0019970987923443317, train acc: 0.9998\n",
      "epoch: 66, loss: 4.9169968406204134e-05, train acc: 0.9998, test acc: 0.9404\n",
      "loss: 0.001421312801539898, train acc: 1.0\n",
      "loss: 0.0020860307617112995, train acc: 0.9992\n",
      "loss: 0.002024523389991373, train acc: 0.9992\n",
      "loss: 0.0017626962857320906, train acc: 1.0\n",
      "loss: 0.001678427739534527, train acc: 0.9996\n",
      "loss: 0.0017930719011928886, train acc: 1.0\n",
      "loss: 0.0016952208592556418, train acc: 1.0\n",
      "loss: 0.001920280116610229, train acc: 0.9998\n",
      "epoch: 67, loss: 4.6853085223119706e-05, train acc: 0.9998, test acc: 0.9403\n",
      "loss: 0.0014561626594513655, train acc: 0.9998\n",
      "loss: 0.002034128177911043, train acc: 0.9995\n",
      "loss: 0.001871529733762145, train acc: 0.999\n",
      "loss: 0.0017192907864227891, train acc: 1.0\n",
      "loss: 0.0015974509296938777, train acc: 0.9996\n",
      "loss: 0.001725670660380274, train acc: 1.0\n",
      "loss: 0.0016617959714494645, train acc: 1.0\n",
      "loss: 0.0017948665539734066, train acc: 0.9998\n",
      "epoch: 68, loss: 4.222696225042455e-05, train acc: 0.9998, test acc: 0.9406\n",
      "loss: 0.0013197558000683784, train acc: 0.9998\n",
      "loss: 0.001944274187553674, train acc: 0.9995\n",
      "loss: 0.001808826974593103, train acc: 0.9988\n",
      "loss: 0.001661067525856197, train acc: 1.0\n",
      "loss: 0.001530741120222956, train acc: 0.9996\n",
      "loss: 0.0016642211528960615, train acc: 1.0\n",
      "loss: 0.0015824182890355587, train acc: 1.0\n",
      "loss: 0.001695775194093585, train acc: 0.9998\n",
      "epoch: 69, loss: 4.211520354147069e-05, train acc: 0.9998, test acc: 0.9403\n",
      "loss: 0.0012434961972758174, train acc: 0.9998\n",
      "loss: 0.0017620876373257489, train acc: 0.9996\n",
      "loss: 0.0016214539762586355, train acc: 0.9988\n",
      "loss: 0.0016323600080795585, train acc: 1.0\n",
      "loss: 0.0014577532187104226, train acc: 0.9996\n",
      "loss: 0.00158352623693645, train acc: 1.0\n",
      "loss: 0.001488649263046682, train acc: 1.0\n",
      "loss: 0.001649604830890894, train acc: 0.9998\n",
      "epoch: 70, loss: 4.151153552811593e-05, train acc: 0.9998, test acc: 0.9403\n",
      "loss: 0.001154008787125349, train acc: 0.9998\n",
      "loss: 0.0017047820962034165, train acc: 0.9998\n",
      "loss: 0.0015189358266070486, train acc: 0.9987\n",
      "loss: 0.0015801705420017242, train acc: 1.0\n",
      "loss: 0.0013446952565573157, train acc: 0.9996\n",
      "loss: 0.0015576190373394639, train acc: 0.9999\n",
      "loss: 0.0014372829988133161, train acc: 1.0\n",
      "loss: 0.0015518411993980408, train acc: 0.9998\n",
      "epoch: 71, loss: 3.6349185393191874e-05, train acc: 0.9998, test acc: 0.94\n",
      "loss: 0.001065544900484383, train acc: 0.9998\n",
      "loss: 0.0016054100473411382, train acc: 0.9999\n",
      "loss: 0.0014285370125435293, train acc: 0.9987\n",
      "loss: 0.0015078325988724828, train acc: 1.0\n",
      "loss: 0.0012834983703214676, train acc: 0.9996\n",
      "loss: 0.0014686407637782394, train acc: 0.9999\n",
      "loss: 0.0013703427743166686, train acc: 1.0\n",
      "loss: 0.0014955857186578214, train acc: 0.9998\n",
      "epoch: 72, loss: 3.961177571909502e-05, train acc: 0.9998, test acc: 0.9404\n",
      "loss: 0.0009289191220887005, train acc: 0.9998\n",
      "loss: 0.0014806071121711284, train acc: 0.9999\n",
      "loss: 0.0013373265042901038, train acc: 0.9987\n",
      "loss: 0.001476796332281083, train acc: 1.0\n",
      "loss: 0.0011965655954554676, train acc: 0.9996\n",
      "loss: 0.0014155711513012647, train acc: 0.9998\n",
      "loss: 0.00127648773486726, train acc: 1.0\n",
      "loss: 0.0014436343568377196, train acc: 0.9998\n",
      "epoch: 73, loss: 3.6974659451516345e-05, train acc: 0.9998, test acc: 0.9401\n",
      "loss: 0.0008762935176491737, train acc: 0.9998\n",
      "loss: 0.0014329220168292523, train acc: 0.9999\n",
      "loss: 0.0012705453787930309, train acc: 0.9988\n",
      "loss: 0.0013960636279080063, train acc: 1.0\n",
      "loss: 0.001127440354321152, train acc: 0.9997\n",
      "loss: 0.0013691862870473415, train acc: 0.9998\n",
      "loss: 0.0012039099412504584, train acc: 1.0\n",
      "loss: 0.0013719910173676908, train acc: 0.9998\n",
      "epoch: 74, loss: 3.821114296442829e-05, train acc: 0.9998, test acc: 0.94\n",
      "loss: 0.0007710614008828998, train acc: 0.9998\n",
      "loss: 0.0013842373271472752, train acc: 1.0\n",
      "loss: 0.0011902235099114477, train acc: 0.9989\n",
      "loss: 0.001342214609030634, train acc: 1.0\n",
      "loss: 0.0010652345838025213, train acc: 0.9995\n",
      "loss: 0.0012892442173324525, train acc: 0.9998\n",
      "loss: 0.001137782569276169, train acc: 1.0\n",
      "loss: 0.0013302997627761214, train acc: 0.9999\n",
      "epoch: 75, loss: 3.658710920717567e-05, train acc: 0.9999, test acc: 0.94\n",
      "loss: 0.0007266917382366955, train acc: 0.9998\n",
      "loss: 0.0013171910773962735, train acc: 1.0\n",
      "loss: 0.0011461350426543505, train acc: 0.9989\n",
      "loss: 0.001273628935450688, train acc: 1.0\n",
      "loss: 0.0009973008127417415, train acc: 0.9995\n",
      "loss: 0.0012360841385088861, train acc: 0.9998\n",
      "loss: 0.0010905872797593475, train acc: 1.0\n",
      "loss: 0.0012440021382644773, train acc: 0.9999\n",
      "epoch: 76, loss: 3.9261158235603943e-05, train acc: 0.9999, test acc: 0.9402\n",
      "loss: 0.0006599274347536266, train acc: 0.9998\n",
      "loss: 0.0012494643684476613, train acc: 1.0\n",
      "loss: 0.0010750211076810956, train acc: 0.9987\n",
      "loss: 0.001224166841711849, train acc: 1.0\n",
      "loss: 0.0009566145192366094, train acc: 0.9996\n",
      "loss: 0.0011594887997489422, train acc: 0.9998\n",
      "loss: 0.0010078469931613654, train acc: 1.0\n",
      "loss: 0.001208089420106262, train acc: 0.9999\n",
      "epoch: 77, loss: 3.909727820428088e-05, train acc: 0.9999, test acc: 0.9402\n",
      "loss: 0.000607406604103744, train acc: 0.9999\n",
      "loss: 0.0011899757024366408, train acc: 1.0\n",
      "loss: 0.001021059526829049, train acc: 0.9987\n",
      "loss: 0.0011741616704966872, train acc: 1.0\n",
      "loss: 0.0009029294189531357, train acc: 0.9995\n",
      "loss: 0.0011114580411231145, train acc: 0.9998\n",
      "loss: 0.0009571574395522475, train acc: 1.0\n",
      "loss: 0.0011353585869073868, train acc: 1.0\n",
      "epoch: 78, loss: 4.16148359363433e-05, train acc: 1.0, test acc: 0.9406\n",
      "loss: 0.0005508284666575491, train acc: 0.9999\n",
      "loss: 0.0011290173162706197, train acc: 1.0\n",
      "loss: 0.000965725036803633, train acc: 0.9989\n",
      "loss: 0.0011206001741811632, train acc: 1.0\n",
      "loss: 0.0008582131180446595, train acc: 0.9996\n",
      "loss: 0.0010716968885390089, train acc: 0.9998\n",
      "loss: 0.0009112280735280365, train acc: 1.0\n",
      "loss: 0.001084579835878685, train acc: 1.0\n",
      "epoch: 79, loss: 4.3454674596432596e-05, train acc: 1.0, test acc: 0.9403\n",
      "#####training and testing end with K:40, P:1######\n"
     ]
    }
   ],
   "source": [
    "param_dropout_grid(128, [1, 5, 10, 20, 40], [0.1, 0.5, 1], noise=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-06T05:29:47.073174958Z",
     "start_time": "2023-06-06T05:26:59.752946657Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## No dropout regularization and effect of $k$"
   ],
   "metadata": {
    "collapsed": false,
    "id": "ySiObjsCyrPy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training accuracy for each $k$ and $p$"
   ],
   "metadata": {
    "collapsed": false,
    "id": "qX0Nk0N-yrPy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test accuracy for each $k$ and $p$"
   ],
   "metadata": {
    "collapsed": false,
    "id": "uoFjSvZ8yrPy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 4 - Comments"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Z_DAEOKByrPz"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
