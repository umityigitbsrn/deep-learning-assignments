{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:20:41.217912Z",
     "end_time": "2023-04-27T14:20:41.875980Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: torch.Size([60000, 784]), label shape: torch.Size([60000, 10])\n",
      "test data shape: torch.Size([10000, 784]), label shape: torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "######TASK 1######\n",
    "# code for downloading and formatting the data\n",
    "transforms_fnc = transforms.Compose([\n",
    "    # transforms.Resize((784, 1)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "target_transform_fnc = transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), 1))\n",
    "\n",
    "train_data = MNIST('./data', train=True, download=True, transform=transforms_fnc, target_transform=target_transform_fnc)\n",
    "test_data = MNIST('./data', train=False, download=True, transform=transforms_fnc)\n",
    "\n",
    "train_bs = len(train_data)\n",
    "test_bs = len(test_data)\n",
    "\n",
    "train_loader = iter(DataLoader(train_data, batch_size=train_bs, shuffle=False))\n",
    "test_loader = iter(DataLoader(test_data, batch_size=test_bs, shuffle=False))\n",
    "\n",
    "train_data_X, train_data_y = next(train_loader)\n",
    "train_data_X = train_data_X.reshape(train_data_X.size(0), -1)\n",
    "\n",
    "test_data_X, test_data_y = next(test_loader)\n",
    "test_data_X = test_data_X.reshape(test_data_X.size(0), -1)\n",
    "\n",
    "print('train data shape: {}, label shape: {}'.format(train_data_X.size(), train_data_y.size()))\n",
    "print('test data shape: {}, label shape: {}'.format(test_data_X.size(), test_data_y.size()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:20:43.284914Z",
     "end_time": "2023-04-27T14:20:45.494877Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "######TASK 2######\n",
    "# code for minibatch SGD implementation\n",
    "def _gradient(data, label, weight):\n",
    "    return torch.matmul(torch.t(data), torch.matmul(data, weight) - label) / data.size(0)\n",
    "\n",
    "def _loss(data, label, weight):\n",
    "    inner = label - torch.matmul(data, weight)\n",
    "    norm = torch.linalg.norm(inner)\n",
    "    return 0.5 * (norm ** 2) / data.size(0)\n",
    "\n",
    "def _acc(data, label, weight):\n",
    "    preds = torch.matmul(data, weight)\n",
    "    return torch.sum((label == torch.argmax(preds, dim=1)).int()) / data.size(0)\n",
    "\n",
    "def sgd_train(train_data_X_arg, train_data_y_arg, test_data_X_arg, test_data_y_arg, num_of_iterations, batch_size, learning_rate):\n",
    "    # init weight\n",
    "    weight = torch.empty(784, 10)\n",
    "    torch.nn.init.zeros_(weight)\n",
    "\n",
    "    # uni_dist_weight = torch.ones(train_data_X_arg.size(0))\n",
    "    running_loss = []\n",
    "    running_acc = []\n",
    "    for iter_idx in range(num_of_iterations):\n",
    "\n",
    "        # sampled_idx = torch.multinomial(uni_dist_weight, batch_size, replacement=True)\n",
    "        sampled_idx = np.random.randint(0, train_data_X_arg.size(0), batch_size)\n",
    "        sampled_batch_X, sampled_batch_y = train_data_X_arg[sampled_idx], train_data_y_arg[sampled_idx]\n",
    "\n",
    "        # loss and gradient\n",
    "        loss = _loss(sampled_batch_X, sampled_batch_y, weight)\n",
    "        gradient = _gradient(sampled_batch_X, sampled_batch_y, weight)\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "        # acc\n",
    "        acc = _acc(test_data_X_arg, test_data_y_arg, weight)\n",
    "        running_acc.append(acc.item())\n",
    "\n",
    "        # update\n",
    "        weight = weight - learning_rate * gradient\n",
    "        if iter_idx == 0 or (iter_idx + 1) % 100 == 0:\n",
    "            print('iter: {}, loss: {}, acc: {}'.format(iter_idx + 1, loss.item(), acc))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:20:48.806146Z",
     "end_time": "2023-04-27T14:20:48.807217Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1, loss: 0.5, acc: 0.09799999743700027\n",
      "iter: 100, loss: 0.36535966396331787, acc: 0.7544999718666077\n",
      "iter: 200, loss: 0.33776628971099854, acc: 0.788100004196167\n",
      "iter: 300, loss: 0.30938228964805603, acc: 0.7833999991416931\n",
      "iter: 400, loss: 0.2985003590583801, acc: 0.795799970626831\n",
      "iter: 500, loss: 0.27080923318862915, acc: 0.8014000058174133\n",
      "iter: 600, loss: 0.27861061692237854, acc: 0.8065000176429749\n",
      "iter: 700, loss: 0.2492048144340515, acc: 0.8115000128746033\n",
      "iter: 800, loss: 0.250323623418808, acc: 0.8148999810218811\n",
      "iter: 900, loss: 0.24730165302753448, acc: 0.8162000179290771\n",
      "iter: 1000, loss: 0.2528766989707947, acc: 0.8203999996185303\n",
      "iter: 1100, loss: 0.24798446893692017, acc: 0.8248000144958496\n",
      "iter: 1200, loss: 0.2303975522518158, acc: 0.8259999752044678\n",
      "iter: 1300, loss: 0.2494015246629715, acc: 0.8278999924659729\n",
      "iter: 1400, loss: 0.2510981261730194, acc: 0.8299000263214111\n",
      "iter: 1500, loss: 0.23252353072166443, acc: 0.8303999900817871\n",
      "iter: 1600, loss: 0.2652430534362793, acc: 0.8313999772071838\n",
      "iter: 1700, loss: 0.2070697396993637, acc: 0.833899974822998\n",
      "iter: 1800, loss: 0.21656422317028046, acc: 0.8356999754905701\n",
      "iter: 1900, loss: 0.22542446851730347, acc: 0.8370000123977661\n",
      "iter: 2000, loss: 0.24797621369361877, acc: 0.8374000191688538\n",
      "iter: 2100, loss: 0.2267230600118637, acc: 0.8373000025749207\n",
      "iter: 2200, loss: 0.21329204738140106, acc: 0.8371000289916992\n",
      "iter: 2300, loss: 0.23262308537960052, acc: 0.8400999903678894\n",
      "iter: 2400, loss: 0.22007347643375397, acc: 0.840399980545044\n",
      "iter: 2500, loss: 0.23777514696121216, acc: 0.8411999940872192\n",
      "iter: 2600, loss: 0.22859041392803192, acc: 0.8424000144004822\n",
      "iter: 2700, loss: 0.22280919551849365, acc: 0.8428999781608582\n",
      "iter: 2800, loss: 0.20517578721046448, acc: 0.8440999984741211\n",
      "iter: 2900, loss: 0.22767871618270874, acc: 0.8432999849319458\n",
      "iter: 3000, loss: 0.19301672279834747, acc: 0.843500018119812\n",
      "iter: 3100, loss: 0.2055578976869583, acc: 0.8446999788284302\n",
      "iter: 3200, loss: 0.23893697559833527, acc: 0.843999981880188\n",
      "iter: 3300, loss: 0.19406680762767792, acc: 0.8449000120162964\n",
      "iter: 3400, loss: 0.2317846119403839, acc: 0.845300018787384\n",
      "iter: 3500, loss: 0.2236480712890625, acc: 0.8464999794960022\n",
      "iter: 3600, loss: 0.206080824136734, acc: 0.8453999757766724\n",
      "iter: 3700, loss: 0.2272159904241562, acc: 0.8472999930381775\n",
      "iter: 3800, loss: 0.20819520950317383, acc: 0.847000002861023\n",
      "iter: 3900, loss: 0.22269774973392487, acc: 0.847000002861023\n",
      "iter: 4000, loss: 0.21558509767055511, acc: 0.847100019454956\n",
      "iter: 4100, loss: 0.23618589341640472, acc: 0.8485000133514404\n",
      "iter: 4200, loss: 0.22630342841148376, acc: 0.8496000170707703\n",
      "iter: 4300, loss: 0.21414071321487427, acc: 0.8490999937057495\n",
      "iter: 4400, loss: 0.2127208113670349, acc: 0.8492000102996826\n",
      "iter: 4500, loss: 0.222897469997406, acc: 0.8511000275611877\n",
      "iter: 4600, loss: 0.21192461252212524, acc: 0.8507999777793884\n",
      "iter: 4700, loss: 0.2576177716255188, acc: 0.8489000201225281\n",
      "iter: 4800, loss: 0.20724180340766907, acc: 0.8501999974250793\n",
      "iter: 4900, loss: 0.1995457410812378, acc: 0.8513000011444092\n",
      "iter: 5000, loss: 0.22885528206825256, acc: 0.848800003528595\n",
      "iter: 5100, loss: 0.21440833806991577, acc: 0.8500999808311462\n",
      "iter: 5200, loss: 0.2366989552974701, acc: 0.8510000109672546\n",
      "iter: 5300, loss: 0.23609767854213715, acc: 0.8500999808311462\n",
      "iter: 5400, loss: 0.21713772416114807, acc: 0.8518999814987183\n",
      "iter: 5500, loss: 0.24879218637943268, acc: 0.8533999919891357\n",
      "iter: 5600, loss: 0.19092030823230743, acc: 0.8529999852180481\n",
      "iter: 5700, loss: 0.21419757604599, acc: 0.8522999882698059\n",
      "iter: 5800, loss: 0.2064054161310196, acc: 0.8539999723434448\n",
      "iter: 5900, loss: 0.22002992033958435, acc: 0.8514000177383423\n",
      "iter: 6000, loss: 0.22427915036678314, acc: 0.8535000085830688\n"
     ]
    }
   ],
   "source": [
    "sgd_train(train_data_X, train_data_y, test_data_X, test_data_y, int(train_data_X.size(0) / 10), 100, 0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:21:53.814974Z",
     "end_time": "2023-04-27T14:22:00.427133Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
